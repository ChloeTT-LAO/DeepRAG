import json
from collections import defaultdict
from copy import deepcopy
from typing import List, Dict, Set, Tuple, Optional, Any
import re
from DeepRAG_Multimodal.deep_retrieve.ming.agent_gpt4 import AzureGPT4Chat, create_response_format
from datetime import datetime
import sys
import torch

sys.path.append(
    "/Users/chloe/Documents/Academic/AI/Project/åŸºäºColpaliçš„å¤šæ¨¡æ€æ£€ç´¢æ ‡å‡†æ¡†æ¶/multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")
from DeepRAG_Multimodal.deep_retrieve.retriever_multimodal_bge import DocumentRetriever, RetrieverConfig, \
    MultimodalMatcher
import asyncio
import concurrent.futures
from textwrap import dedent
from langchain_core.documents import Document
from FlagEmbedding import FlagReranker, FlagModel
from DeepRAG_Multimodal.deep_retrieve.deepsearch import DeepSearch_Alpha
import numpy as np
import pytesseract
from pdf2image import convert_from_path
import os
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import re, string, joblib
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.info("DeepSearch_Betaæ¨¡å—åˆå§‹åŒ–")
print(f"å½“å‰æ¨¡å—çš„æ—¥å¿—å™¨åç§°: {logger.name}")


class DeepSearch_Beta(DeepSearch_Alpha):
    def __init__(self, max_iterations: int = 2, reranker: FlagReranker = None, params: dict = None):
        super().__init__(max_iterations, reranker, params)

    def result_processor(self, results):
        matched_docs = []
        for doc, score in results:
            matched_docs.append({
                'text': doc.page_content,
                'score': 1 - score,
                'image_score': doc.metadata.get('image_score', 0),
                'text_score': doc.metadata.get('text_score', 0)
            })
        return matched_docs

    def llm_rerank(self, query, retrieval_list, reranker, topk=None):
        pairs = [[query, doc['text']] for doc in retrieval_list]
        rerank_scores = reranker.compute_score(pairs, normalize=True)
        output_list = []
        for score, doc in sorted(zip(rerank_scores, retrieval_list), key=lambda x: x[0], reverse=True):
            output_list.append({
                'text': doc['text'],
                'page': doc['metadata']['page_index'],
                'score': score,
                # 'image_score': doc['image_score'],
                # 'text_score': doc['text_score']
            })
        if topk is not None:
            output_list = output_list[:topk]
        return output_list

    def rerank_index_processor(self, results):
        """é»˜è®¤çš„ç›¸ä¼¼æ€§æœç´¢ç»“æœå¤„ç†å™¨"""
        matched_docs = []
        for doc, score in results:
            # åˆ›å»ºæ–°å­—å…¸è€Œä¸æ˜¯ä¿®æ”¹Documentå¯¹è±¡
            matched_doc = {
                'text': doc.page_content,
                'score': 1 - score,
                # å¤åˆ¶å…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰éœ€è¦ï¼‰
                **doc.metadata
            }
            matched_docs.append(matched_doc)
        return matched_docs

    def search_retrieval(self, data: dict, multi_intent: False, retriever: MultimodalMatcher):
        original_query = deepcopy(data['query'])
        data_ori = deepcopy(data)
        embedding_topk = self.params['embedding_topk']
        rerank_topk = self.params['rerank_topk']

        all_search_results = {}
        final_search_results = []
        seen_texts = set()

        # åˆæ­¥æ¢ç´¢æ£€ç´¢
        if multi_intent:
            initial_retrieval_list = retriever.retrieve(original_query, data['documents'])
            initial_retrieval_list = initial_retrieval_list[:embedding_topk]
            for r in initial_retrieval_list:
                if r['text'] not in seen_texts:
                    seen_texts.add(r['text'])
                    all_search_results[original_query] = [r['text']]
                    final_search_results.append(r)

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨LLMæ‹†åˆ†æŸ¥è¯¢æ„å›¾
        if multi_intent:
            intent_queries = self._split_query_intent(original_query,
                                                      json.dumps(all_search_results, ensure_ascii=False, indent=2))
            # intent_queries = self._split_query_intent(original_query)
            logger.info(f"ğŸ” æ„å›¾æ‹†åˆ†ç»“æœ: {intent_queries}")
        else:
            intent_queries = [original_query]

        # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªæ„å›¾è¿›è¡Œç¬¬ä¸€è½®æ£€ç´¢
        for intent_idx, intent_query in enumerate(intent_queries):
            logger.info(f"ğŸ” æ£€ç´¢æ„å›¾ {intent_idx + 1}/{len(intent_queries)}: {intent_query}")

            retrieval_list = retriever.retrieve(intent_query, data['documents'])
            retrieval_list = retrieval_list[:embedding_topk // len(intent_queries)]
            for r in retrieval_list:
                if r['text'] not in seen_texts:
                    seen_texts.add(r['text'])
                    all_search_results[intent_query] = [r['text']]
                    final_search_results.append(r)

        # ç¬¬ä¸‰æ­¥ï¼šåŸºäºç¬¬ä¸€è½®æ£€ç´¢ç»“æœè¿›è¡Œæ„å›¾ç»†åŒ–
        if multi_intent:
            # refined_intent_queries = self._refine_query_intent(original_query, intent_queries,
            #                                                    json.dumps(all_search_results, ensure_ascii=False, indent=2))
            # logger.info(f"æ„å›¾ç»†åŒ–ç»“æœ: {refined_intent_queries}")

            refined_intent_queries = self._refine_query_intent_with_knowledge_graph(
                original_query,
                intent_queries,
                json.dumps(all_search_results, ensure_ascii=False, indent=2)
            )
        else:
            refined_intent_queries = [original_query]

        # ç¬¬å››æ­¥ï¼šå¯¹ç»†åŒ–åçš„æ„å›¾è¿›è¡Œç¬¬äºŒè½®æ£€ç´¢
        if set(refined_intent_queries) != set(intent_queries):
            for intent_idx, intent_query in enumerate(refined_intent_queries):
                logger.info(f"ğŸ” æ£€ç´¢ç»†åŒ–æ„å›¾ {intent_idx + 1}/{len(refined_intent_queries)}: {intent_query}")

                retrieval_list = retriever.retrieve(intent_query, data_ori['documents'])

                # åˆå¹¶ç»“æœå¹¶å»é‡
                for result in retrieval_list:
                    if result['text'] not in seen_texts:
                        seen_texts.add(result['text'])
                        final_search_results.append(result)

        # ç¬¬äº”æ­¥ï¼šå¯¹æ‰€æœ‰ç»“æœè¿›è¡Œæœ€ç»ˆæ’åº
        final_search_results = self.llm_rerank(original_query, final_search_results, self.reranker, rerank_topk)

        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(final_search_results)} æ¡")
        logger.info([doc['score'] for doc in final_search_results])

        # æå–æœ€ç»ˆç»“æœçš„é¡µç 
        final_results_with_pages = [
            {
                "text": doc['text'],
                "score": doc['score'],
                "page": doc['page']  # è·å–é¡µç 
            }
            for doc in final_search_results
        ]

        return final_results_with_pages

    # 6.28ä¿®æ”¹
    def _split_query_intent(self, query: str, context=None) -> List[str]:
        """å°†æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªä¸åŒç»´åº¦çš„æ„å›¾æŸ¥è¯¢"""
        # SYSTEM_MESSAGE = dedent("""
        # You are a professional expert in analyzing query intentions. Your task is to analyze the user's query and break it down into multiple sub-queries of different dimensions.
        #
        # Please follow the following rules:
        # 1. If the query contains multiple different information requirements or concerns, split it into multiple sub-queries.
        # 2. Ensure that each sub-query focuses on a different dimension or aspect to maintain diversity.
        # 3. Do not merely change the form of the question; instead, focus on different information dimensions.
        # 4. If the original query is already very clear and only focuses on a single dimension, there is no need to split it.
        # 5. Sub-queries should be more specific and clear, which helps to retrieve more accurate information.
        # 6. The split sub-queries must be relevant to the context of the document.
        #
        # Please return in JSON format, including the following fields:
        # {
        #     "intent_queries": ["subquery1", "subquery2", ...]
        # }
        # """)
        # messages = [
        #     {"role": "system", "content": SYSTEM_MESSAGE},
        #     {"role": "user", "content": f"""Please analyze the following query and break it down into multiple sub-queries:
        #
        #     Original query:
        #     {query}
        #
        #     """}
        # ]
        '''æ”¹è¿›åçš„prompt'''
        SYSTEM_MESSAGE = dedent("""
                You are a professional expert in analyzing query intentions. Your task is to analyze the user's query based on the retrieved context information of the document and break it down into multiple sub-queries of different dimensions.

                Please follow the following rules:
                1. If the query contains multiple different information requirements or concerns, split it into multiple sub-queries.
                2. Ensure that each sub-query focuses on a different dimension or aspect to maintain diversity.
                3. Do not merely change the form of the question; instead, focus on different information dimensions.
                4. If the original query is already very clear and only focuses on a single dimension, there is no need to split it.
                5. Sub-queries should be more specific and clear, which helps to retrieve more accurate information.
                6. The split sub-queries must be relevant to the context of the document.

                Please return in JSON format, including the following fields:
                {
                    "intent_queries": ["subquery1", "subquery2", ...]
                }
                """)

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE},
            {"role": "user", "content": f"""
                    Please analyze the following query and break it down into multiple sub-queries based on different dimensions based on retrieved context.ï¼š

                    Original Query: {query}

                    Retrieved Context:
                    {context}
                    """}
        ]
        # SYSTEM_MESSAGE = dedent("""
        #                 You are a professional expert in analyzing query intentions. Your task is to analyze the user's query based on the retrieved context information of the document and break it down into multiple sub-queries of different dimensions.
        #                 Your task has *two stages*:
        #                 **Stage 1 Â· Clean the query**
        #                 â€¢ Remove any words that do NOT help locate information inside the document:
        #                   â€“ answer-format instructions (e.g. "write in float format", "return as integer",
        #                     "round to two decimals", "answer Yes/No");
        #                   â€“ general politeness / meta phrases ("please", "thanks", "æ ¹æ®æ–‡æ¡£â€¦");
        #                   â€“ output-scene hints ("for a presentation", "for my homework");
        #                   â€“ citations of page numbers UNLESS the page itself is the target of the question.
        #                 â€¢ Preserve domain keywords, entities, units, and page numbers **when** they are
        #                   essential for retrieval.
        #
        #                 **Stage 2 Â· Split the query**
        #                 Please follow the following rules:
        #                 1. If the query contains multiple different information requirements or concerns, split it into multiple sub-queries.
        #                 2. Ensure that each sub-query focuses on a different dimension or aspect to maintain diversity.
        #                 3. Do not merely change the form of the question; instead, focus on different information dimensions.
        #                 4. If the original query is already very clear and only focuses on a single dimension, there is no need to split it.
        #                 5. Sub-queries should be more specific and clear, which helps to retrieve more accurate information.
        #                 6. The split sub-queries must be relevant to the context of the document.
        #
        #                 Please return in JSON format, including the following fields:
        #                 {
        #                     "intent_queries": ["subquery1", "subquery2", ...]
        #                 }
        #                 """)
        #
        # messages = [
        #     {"role": "system", "content": SYSTEM_MESSAGE},
        #     {"role": "user", "content": f"""
        #                     Please analyze the following query. First clean it, then break it down into multiple sub-queries based on different dimensions based on retrieved context.ï¼š
        #
        #                     Original Query: {query}
        #
        #                     Retrieved Context:
        #                     {context}
        #                     """}
        # ]

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages
        )

        try:
            result = parse_llm_response(response)
            intent_queries = result.get("intent_queries", [query])
            print("intent_queries:", intent_queries)
            return intent_queries if intent_queries else [query]
        except Exception as e:
            logger.error(f"æ„å›¾æ‹†åˆ†å‡ºé”™: {e}")
            return [query]

    def _refine_query_intent_with_knowledge_graph(
            self,
            original_query: str,
            intent_queries: List[str],
            context: str
    ) -> List[str]:
        """åŸºäºæ£€ç´¢ç»“æœå’ŒçŸ¥è¯†å›¾è°±ç»†åŒ–æŸ¥è¯¢æ„å›¾"""

        SYSTEM_MESSAGE = dedent("""
            You are a professional query intent optimization expert with knowledge graph construction capabilities. Your task is to first build a knowledge graph from the retrieved content and decomposed queries, then use this graph to refine and enhance the user's search intent.

        **CRITICAL CONSTRAINT: All refined queries MUST stay strictly within the scope and semantic boundaries of the original query. Do NOT introduce new concepts, domains, or topics not present in the original query.**
    
        Please follow these steps:
    
        **Step 1: Knowledge Graph Construction**
        1. Extract key entities from the retrieved context that are directly related to the original query:
           - Named entities (persons, organizations, locations, dates, etc.)
           - Domain-specific concepts and terminologies that appear in both the original query and context
           - Important events, processes, or phenomena that are semantically connected to the original query
           - Technical terms that help answer the original query
        
        2. Identify relationships between entities, but ONLY those that are relevant to the original query:
           - Hierarchical relationships (is-a, part-of, belongs-to)
           - Functional relationships (causes, affects, enables)
           - Temporal relationships (before, after, during)
           - Spatial relationships (located-in, connected-to)
           - Semantic relationships (related-to, similar-to, opposite-to)
    
        **Step 2: Intent Refinement with Strict Scope Control**
        Based on the constructed knowledge graph, refine the queries following these STRICT rules:
        
        **MUST DO:**
        1. Keep all refined queries semantically aligned with the original query's core intent
        2. Only explore aspects, facets, or dimensions of the SAME topic from the original query
        3. Use the knowledge graph to find more specific ways to ask about the SAME information
        4. Maintain the same domain, context, and information type as the original query
        5. Generate queries that are complementary parts of answering the original question
        
        **MUST NOT DO:**
        1. Introduce completely new topics or domains not in the original query
        2. Shift focus to tangentially related but different questions
        3. Expand beyond the scope of what the original query is asking
        4. Generate queries about general background information unless specifically asked in the original query
        5. Create queries that could be answered independently without contributing to the original question
    
        **Refinement Guidelines:**
        - Create sub-queries that target different aspects of the SAME answer
        - Use entity relationships to create more precise versions of the SAME question
        - Explore different angles or perspectives on the SAME topic
        - Limit the number of refined sub-queries to a maximum of **three**
    
        **Validation Check:**
        Before finalizing, ask yourself: "Would answering this refined query directly contribute to answering the original query?" If not, discard it.
    
        
        Return your output in JSON format with the following structure:
        {
            "knowledge_graph": {
                "entities": [
                    {"name": "entity_name", "type": "entity_type", "description": "brief_description"},
                    ...
                ],
                "relationships": [
                    {"source": "entity1", "target": "entity2", "relation": "relationship_type", "description": "relationship_description"},
                    ...
                ]
            },
            "refined_intent_queries": ["Refined sub-query 1", "Refined sub-query 2", "Refined sub-query 3"]
        }
        """)

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE},
            {"role": "user", "content": f"""
            Original query:
            {original_query}
    
            Decomposed intent queries:
            {json.dumps(intent_queries, ensure_ascii=False, indent=2)}
    
            Retrieved context:
            {context}
    
            Based on the information above, please:
            1. First construct a knowledge graph from the retrieved context and decomposed queries
            2. Then refine and optimize the search intent using the knowledge graph insights
            """}
        ]

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages
        )

        try:
            result = parse_llm_response_with_kg(response)

            # è®°å½•çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
            knowledge_graph = result.get("knowledge_graph", {})
            visualize_knowledge_graph(knowledge_graph)
            logger.info(f"æ„å»ºçš„çŸ¥è¯†å›¾è°±åŒ…å« {len(knowledge_graph.get('entities', []))} ä¸ªå®ä½“å’Œ {len(knowledge_graph.get('relationships', []))} ä¸ªå…³ç³»")

            # æå–ç»†åŒ–åçš„æŸ¥è¯¢
            refined_queries_with_reasoning = result.get("refined_intent_queries", [])
            refined_queries = [item.get("query", "") for item in refined_queries_with_reasoning if item.get("query")]

            # å¦‚æœæ²¡æœ‰æˆåŠŸæå–åˆ°æŸ¥è¯¢ï¼Œå›é€€åˆ°åŸå§‹æ„å›¾
            if not refined_queries:
                refined_queries = intent_queries

            logger.info(f"åŸºäºçŸ¥è¯†å›¾è°±ç»†åŒ–çš„æŸ¥è¯¢: {refined_queries}")
            return refined_queries

        except Exception as e:
            logger.error(f"æ„å›¾ç»†åŒ–å‡ºé”™: {e}")
            return intent_queries

    def _refine_query_intent(self, original_query: str, intent_queries: List[str], context: str) -> List[str]:
        """åŸºäºæ£€ç´¢ç»“æœç»†åŒ–æŸ¥è¯¢æ„å›¾"""

        SYSTEM_MESSAGE = dedent("""
        You are a professional query intent optimization expert. Your task is to refine and enhance the user's search intent based on the retrieved content.

        Please follow these guidelines:
        1. Analyze the retrieved content to identify information gaps and areas that require further exploration.
        2. Based on the original query and the decomposed intent queries, generate more precise and targeted sub-queries.
        3. Ensure that the new sub-queries address the information needs that were not fully satisfied by the original query.
        4. Sub-queries should be more specific, incorporating domain-specific terminology and clearly defined information requirements.
        5. Avoid generating overly similar sub-queries; ensure diversity and coverage of different aspects.
        6. Limit the number of refined sub-queries to a maximum of **three**.

        Return your output in JSON format with the following structure:
        {
            "refined_intent_queries": ["Refined sub-query 1", "Refined sub-query 2", ...]
        }
        """)

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE},
            {"role": "user", "content": f"""
            Original query:
            {original_query}

            Decomposed intent queries:
            {json.dumps(intent_queries, ensure_ascii=False)}

            Retrieved context:
            {context}

            Based on the information above, please refine and optimize the search intent:
            """}
        ]

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages
        )

        try:
            result = parse_llm_response(response)
            refined_queries = result.get("refined_intent_queries", intent_queries)
            print("Refined intent queries:", refined_queries)
            return refined_queries if refined_queries else intent_queries
        except Exception as e:
            logger.error(f"æ„å›¾ç»†åŒ–å‡ºé”™: {e}")
            return intent_queries

    def _prepare_context(self, search_results: List[Dict[str, str]]) -> str:
        """Prepare context from search results."""
        return '\n'.join([f"{index + 1}. {result['text']}" for index, result in enumerate(search_results)])


def parse_llm_response(response_text: str) -> dict:
    """
    ä»LLMå“åº”ä¸­æå–JSONæ•°æ®ï¼Œå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼
    """
    import re
    import json

    # 1. æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ¼å¼
    cleaned_text = re.sub(r'```(?:json|python)?', '', response_text)
    cleaned_text = re.sub(r'`', '', cleaned_text).strip()

    # 2. å°è¯•ç›´æ¥è§£æJSON
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        # 3. å°è¯•æŸ¥æ‰¾JSONå†…å®¹
        json_pattern = r'\{[\s\S]*\}'
        match = re.search(json_pattern, cleaned_text)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

    # 4. å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨æå–å…³é”®å­—æ®µ
    output_dict = {}

    # æå–refined_queriesæ•°ç»„
    queries_pattern = r'"refined_queries"\s*:\s*\[(.*?)\]'
    queries_match = re.search(queries_pattern, cleaned_text, re.DOTALL)
    if queries_match:
        # æ›´å¤æ‚çš„è§£æ
        query_items = re.findall(r'\{[^}]*\}', queries_match.group(1))
        refined_queries = []
        for item in query_items:
            query_match = re.search(r'"query"\s*:\s*"([^"]+)"', item)
            sources_match = re.search(r'"sources"\s*:\s*\[(.*?)\]', item)
            if query_match:
                query = query_match.group(1)
                sources = []
                if sources_match:
                    sources = re.findall(r'"([^"]+)"', sources_match.group(1))
                refined_queries.append({"query": query, "sources": sources})
        output_dict["refined_queries"] = refined_queries

    # æå–intent_queriesæ•°ç»„ï¼ˆå¦‚æœæœ‰ï¼‰
    intent_pattern = r'"intent_queries"\s*:\s*\[(.*?)\]'
    intent_match = re.search(intent_pattern, cleaned_text, re.DOTALL)
    if intent_match:
        intent_items = re.findall(r'"([^"]+)"', intent_match.group(1))
        output_dict["intent_queries"] = intent_items

    return output_dict


def parse_llm_response_with_kg(response_text: str) -> dict:
    """
    ä»LLMå“åº”ä¸­æå–åŒ…å«çŸ¥è¯†å›¾è°±çš„JSONæ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    import re
    import json

    # 1. æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ¼å¼
    cleaned_text = re.sub(r'```(?:json|python)?', '', response_text)
    cleaned_text = re.sub(r'`', '', cleaned_text).strip()

    # 2. å°è¯•ç›´æ¥è§£æJSON
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        # 3. å°è¯•æŸ¥æ‰¾JSONå†…å®¹
        json_pattern = r'\{[\s\S]*\}'
        match = re.search(json_pattern, cleaned_text)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

    # 4. å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨æå–å…³é”®å­—æ®µ
    output_dict = {}

    # æå–knowledge_graph
    kg_pattern = r'"knowledge_graph"\s*:\s*\{([\s\S]*?)\}(?=\s*,\s*"refined_intent_queries"|\s*\})'
    kg_match = re.search(kg_pattern, cleaned_text)
    if kg_match:
        try:
            kg_json = "{" + kg_match.group(1) + "}"
            output_dict["knowledge_graph"] = json.loads(kg_json)
        except:
            # æ‰‹åŠ¨æå–å®ä½“å’Œå…³ç³»
            entities_pattern = r'"entities"\s*:\s*\[([\s\S]*?)\]'
            relationships_pattern = r'"relationships"\s*:\s*\[([\s\S]*?)\]'

            entities_match = re.search(entities_pattern, kg_match.group(1))
            relationships_match = re.search(relationships_pattern, kg_match.group(1))

            kg_dict = {}
            if entities_match:
                entities = []
                entity_matches = re.findall(r'\{[^}]*"name"\s*:\s*"([^"]+)"[^}]*\}', entities_match.group(1))
                for entity_name in entity_matches:
                    entities.append(
                        {"name": entity_name, "type": "unknown", "description": "", "relevance_to_original": ""})
                kg_dict["entities"] = entities

            if relationships_match:
                relationships = []
                relationship_matches = re.findall(
                    r'\{[^}]*"source"\s*:\s*"([^"]+)"[^}]*"target"\s*:\s*"([^"]+)"[^}]*"relation"\s*:\s*"([^"]+)"[^}]*\}',
                    relationships_match.group(1))
                for src, tgt, rel in relationship_matches:
                    relationships.append(
                        {"source": src, "target": tgt, "relation": rel, "description": "", "relevance_to_original": ""})
                kg_dict["relationships"] = relationships

            output_dict["knowledge_graph"] = kg_dict

    # æå–refined_intent_queriesï¼ˆç®€åŒ–ç‰ˆ - ç›´æ¥æå–å­—ç¬¦ä¸²æ•°ç»„ï¼‰
    refined_pattern = r'"refined_intent_queries"\s*:\s*\[([\s\S]*?)\]'
    refined_match = re.search(refined_pattern, cleaned_text)
    if refined_match:
        try:
            # ç›´æ¥æå–å­—ç¬¦ä¸²æ•°ç»„
            queries = re.findall(r'"([^"]+)"', refined_match.group(1))
            output_dict["refined_intent_queries"] = queries
        except:
            output_dict["refined_intent_queries"] = []

    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºç»“æ„
    if not output_dict:
        output_dict = {
            "knowledge_graph": {"entities": [], "relationships": []},
            "refined_intent_queries": []
        }

    return output_dict

def calculate_accuracy(json_file_path, retrieved_pages):
    with open(json_file_path, 'r') as f:
        logs = [json.loads(line.strip()) for line in f]

    total = 0
    correct = 0

    for log in logs:
        evidence_pages = set(log.get('evidence_pages', []))
        if evidence_pages:
            total += 1
            if evidence_pages.intersection(retrieved_pages):
                correct += 1

    accuracy = (correct / total * 100) if total > 0 else 0.0
    logger.info("\n===== Retrieval Accuracy =====")
    logger.info(f"Accuracy: {accuracy:.2f}% ({correct}/{total})")


def visualize_knowledge_graph(knowledge_graph: dict):
    """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
    entities = knowledge_graph.get('entities', [])
    relationships = knowledge_graph.get('relationships', [])

    print("\n" + "=" * 60)
    print(" çŸ¥è¯†å›¾è°±å¯è§†åŒ–")
    print("=" * 60)

    # ç»Ÿè®¡ä¿¡æ¯
    entity_types = {}
    for entity in entities:
        entity_type = entity.get('type', 'Unknown')
        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1

    print(f"\n ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»å®ä½“æ•°: {len(entities)}")
    print(f"   æ€»å…³ç³»æ•°: {len(relationships)}")
    print(f"   å®ä½“ç±»å‹åˆ†å¸ƒ:")
    for entity_type, count in entity_types.items():
        print(f"     - {entity_type}: {count}")

    # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºå®ä½“
    print(f"\n æŒ‰ç±»å‹åˆ†ç»„çš„å®ä½“:")
    for entity_type in entity_types.keys():
        print(f"\n    {entity_type}:")
        type_entities = [e for e in entities if e.get('type') == entity_type]
        for entity in type_entities:
            name = entity.get('name', 'Unknown')
            desc = entity.get('description', '')
            if desc:
                print(f"     â€¢ {name} - {desc}")
            else:
                print(f"     â€¢ {name}")

    # å…³ç³»ç½‘ç»œ
    print(f"\n å…³ç³»ç½‘ç»œ:")
    relation_types = {}
    for rel in relationships:
        rel_type = rel.get('relation', 'Unknown')
        relation_types[rel_type] = relation_types.get(rel_type, 0) + 1

    print(f"   å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    for rel_type, count in relation_types.items():
        print(f"     - {rel_type}: {count}")

    print(f"\n   è¯¦ç»†å…³ç³»:")
    for rel in relationships:
        source = rel.get('source', 'Unknown')
        target = rel.get('target', 'Unknown')
        relation = rel.get('relation', 'Unknown')
        desc = rel.get('description', '')

        print(f"      {source} --[{relation}]--> {target}")
        if desc:
            print(f"         {desc}")

    print("=" * 60)


if __name__ == "__main__":
    # Initialize DeepSearch_Beta instance with parameters
    retriever = DeepSearch_Beta(params={
        "embedding_topk": 15,
        "rerank_topk": 10
    },
        reranker=FlagReranker(model_name_or_path="BAAI/bge-reranker-large")
    )

    # Initialize MultimodalMatcher with external configuration
    retriever_config = RetrieverConfig(
        model_name="vidore/colqwen2.5-v0.2",
        processor_name="vidore/colqwen2.5-v0.1",
        bge_model_name="BAAI/bge-large-en-v1.5",
        device="cuda",
        use_fp16=True,
        batch_size=32,
        threshold=0.4,
        mode="mixed"
    )
    matcher = MultimodalMatcher(config=retriever_config)

    # Load test data
    base_dir = "/Users/chloe/Documents/Academic/AI/Project/åŸºäºColpaliçš„å¤šæ¨¡æ€æ£€ç´¢æ ‡å‡†æ¡†æ¶/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc"
    test_data_path = "/Users/chloe/Documents/Academic/AI/Project/åŸºäºColpaliçš„å¤šæ¨¡æ€æ£€ç´¢æ ‡å‡†æ¡†æ¶/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc/selected_LongDocURL_public_with_subtask_category.jsonl"
    with open(test_data_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            doc_data = json.loads(line)
            documents = []
            query = doc_data.get("question", "Provide a query here for testing.")  # Extract query from each record

            if "pdf_path" in doc_data:
                # Handle PDF documents by converting them into pages
                pdf_pages = matcher._pdf_to_pages(os.path.join(base_dir, doc_data["pdf_path"]))
                for page_index, page_content in enumerate(pdf_pages):
                    documents.append({
                        "text": page_content.get("text", ""),
                        "image": page_content.get("image", None),
                        "metadata": {
                            **doc_data.get("metadata", {}),
                            "page_index": page_index + 1  # Ensure page_index is added
                        }
                    })
            else:
                # Handle regular documents
                documents.append(Document(page_content=doc_data['content'], metadata=doc_data.get('metadata', {})))

            data = {
                "query": query,  # Use the extracted query
                "documents": documents
            }

            # Perform search retrieval
            results = retriever.search_retrieval(data, retriever=matcher)

            # Save results to a file for each doc_data
            results_output_path = f"retrieval_results_{i}.json"
            with open(results_output_path, 'w', encoding='utf-8') as f_out:
                json.dump(results, f_out, ensure_ascii=False, indent=4)
            logger.info(f"Results saved to {results_output_path}")

            # Extract retrieved pages from results
            retrieved_pages = set(result['metadata'].get('page_index') for result in results if 'metadata' in result)

            # Calculate and print accuracy
            calculate_accuracy(test_data_path, retrieved_pages)