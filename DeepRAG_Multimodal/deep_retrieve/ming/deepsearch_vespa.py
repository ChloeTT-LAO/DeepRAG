import os
import socket
import subprocess
import sys
import time
import traceback

sys.path.append(
    "/Users/chloe/Documents/Academic/AI/Project/åŸºäºColpaliçš„å¤šæ¨¡æ€æ£€ç´¢æ ‡å‡†æ¡†æ¶/multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")
import requests
import torch
import json
import numpy as np
from PIL import Image
from vespa.deployment import VespaCloud, VespaDocker
from DeepRAG_Multimodal.deep_retrieve.ming.agent_gpt4 import AzureGPT4Chat, create_response_format
from vespa.package import ApplicationPackage, Function, FirstPhaseRanking, SecondPhaseRanking
from vespa.application import Vespa
from vespa.package import Schema, Document, Field, FieldSet, RankProfile, HNSW
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import io
import base64
from copy import deepcopy
from textwrap import dedent
import logging
from transformers import AutoTokenizer, AutoModel

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.info("DeepSearch_Vespaæ¨¡å—åˆå§‹åŒ–")
print(f"å½“å‰æ¨¡å—çš„æ—¥å¿—å™¨åç§°: {logger.name}")

class DeepSearch_Vespa:
    """ä½¿ç”¨Vespaä½œä¸ºåç«¯çš„å¤šæ¨¡æ€PDFæ£€ç´¢ç³»ç»Ÿ"""

    def __init__(
            self,
            text_model,
            image_model,
            processor,
            max_iterations: int = 2,
            reranker=None,
            params: dict = None,
            vespa_endpoint: str = None,
            tenant_name: str = None,
            local_deployment: bool = False,
            docker_image: str = "vespaengine/vespa",
            local_port: int = 8080,
            work_dir: str = "./vespa_deepsearch",
            api_key: str = None,
            application_name: str = "pdf-multimodal-search"
    ):
        """
        åˆå§‹åŒ–DeepSearch_Vespa

        Args:
            text_model: æ–‡æœ¬åµŒå…¥æ¨¡å‹
            image_model: å›¾åƒåµŒå…¥æ¨¡å‹
            processor: å›¾åƒå¤„ç†å™¨
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            reranker: é‡æ’åºå™¨
            params: å‚æ•°å­—å…¸
            vespa_endpoint: Vespaäº‘ç«¯ç‚¹ï¼ˆå¦‚æœå·²æœ‰åº”ç”¨ï¼‰
            tenant_name: Vespaäº‘ç§Ÿæˆ·å
            api_key: Vespaäº‘APIå¯†é’¥
            application_name: Vespaåº”ç”¨åç§°
        """
        self.text_model = text_model
        self.image_model = image_model
        self.processor = processor
        self.max_iterations = max_iterations
        self.reranker = reranker
        self.params = params or {
            "embedding_topk": 15,
            "rerank_topk": 10,
            "text_weight": 0.6,
            "image_weight": 0.4
        }

        self.app = None
        self.app_package = None
        self.tenant_name = tenant_name
        self.api_key = api_key
        self.application_name = application_name
        self.local_deployment = local_deployment
        self.docker_image = docker_image
        self.local_port = local_port
        self.work_dir = work_dir
        self.text_tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-large-en-v1.5", use_fast=True)
        self.text_embedding_weight = 0.6
        self.embedding_topk = 15

        # è¿æ¥åˆ°ç°æœ‰Vespaç«¯ç‚¹æˆ–åˆ›å»ºæ–°çš„åº”ç”¨
        if vespa_endpoint:
            logger.info(f"è¿æ¥åˆ°ç°æœ‰Vespaç«¯ç‚¹: {vespa_endpoint}")
            self.app = Vespa(url=vespa_endpoint)
        elif local_deployment:
            logger.info("å‡†å¤‡åœ¨æœ¬åœ°éƒ¨ç½²Vespaåº”ç”¨")
            self._create_and_deploy_app_locally()
        elif tenant_name and api_key:
            logger.info(f"ä½¿ç”¨ç§Ÿæˆ· {tenant_name} éƒ¨ç½²Vespaäº‘åº”ç”¨")
            self._create_and_deploy_app()
        else:
            logger.warning("æœªæä¾›Vespaç«¯ç‚¹æˆ–ç§Ÿæˆ·ä¿¡æ¯ï¼Œå°†ä½¿ç”¨å†…å­˜æ£€ç´¢")

        logger.info("DeepSearch_Vespaåˆå§‹åŒ–å®Œæˆ")

    def _check_if_port_in_use(self, port):
        """æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨"""
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(('localhost', port)) == 0

    def _check_docker_running(self):
        """æ£€æŸ¥Dockeræ˜¯å¦è¿è¡Œä¸­"""
        try:
            subprocess.run(["docker", "info"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("Dockeræœªè¿è¡Œæˆ–æœªå®‰è£…ï¼Œè¯·ç¡®ä¿DockeræœåŠ¡å·²å¯åŠ¨")
            return False

    def _create_and_deploy_app_locally(self):
        """åˆ›å»ºå¹¶åœ¨æœ¬åœ°éƒ¨ç½²Vespaåº”ç”¨"""
        try:
            # æ£€æŸ¥Dockeræ˜¯å¦è¿è¡Œ
            if not self._check_docker_running():
                return

            # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
            if self._check_if_port_in_use(self.local_port):
                logger.error(f"ç«¯å£ {self.local_port} å·²è¢«å ç”¨ï¼Œè¯·é€‰æ‹©å…¶ä»–ç«¯å£æˆ–å…³é—­å ç”¨ç«¯å£çš„ç¨‹åº")
                return

            # åˆ›å»ºå·¥ä½œç›®å½•
            os.makedirs(self.work_dir, exist_ok=True)
            app_dir = os.path.join(self.work_dir, "application")
            os.makedirs(app_dir, exist_ok=True)

            # åˆ›å»ºå®¹å™¨åç§°
            container_name = f"vespa-{self.application_name}"

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå®¹å™¨
            try:
                result = subprocess.run(
                    ["docker", "ps", "-a", "--format", "{{.Names}}"],
                    capture_output=True,
                    text=True,
                    check=True
                )
                if container_name in result.stdout.split():
                    # å…ˆåœæ­¢å®¹å™¨
                    subprocess.run(["docker", "stop", container_name], check=False)
                    # å†ç§»é™¤å®¹å™¨
                    subprocess.run(["docker", "rm", container_name], check=False)
                    logger.info(f"å·²ç§»é™¤æ—§çš„å®¹å™¨: {container_name}")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥å®¹å™¨æ—¶å‡ºé”™: {str(e)}")

            # åˆ›å»ºservices.xmlæ–‡ä»¶
            services_path = os.path.join(app_dir, "services.xml")
            with open(services_path, "w") as f:
                f.write(f'''<?xml version="1.0" encoding="utf-8" ?>
                <services version="1.0">
                  <container id="default" version="1.0">
                    <search />
                    <document-api />
                  </container>
                  <content id="pdf_pages" version="1.0">
                    <redundancy>1</redundancy>
                    <documents>
                      <document type="pdf_page" mode="index" />
                    </documents>
                    <nodes>
                      <node distribution-key="0" hostalias="node1" />
                    </nodes>
                  </content>
                </services>
                ''')

            # åˆ›å»ºpdf_page.sdæ¨¡å¼å®šä¹‰æ–‡ä»¶
            schema_path = os.path.join(app_dir, "schemas", "pdf_page.sd")
            os.makedirs(os.path.join(app_dir, "schemas"), exist_ok=True)
            with open(schema_path, "w") as f:
                f.write('''schema pdf_page {
              document pdf_page {
                field id type string {
                  indexing: summary | index
                  match: word
                }
                field pdf_path type string {
                  indexing: summary | index
                }
                field page_index type int {
                  indexing: summary | attribute
                }
                field text type string {
                  indexing: index
                  match: text
                  index: enable-bm25
                }
                # ä½¿ç”¨äºŒè¿›åˆ¶å‘é‡è¡¨ç¤ºæé«˜æ•ˆç‡
                field embedding type tensor(patch{}, v[16]) {
                  indexing: attribute | index
                  attribute {
                    distance-metric: hamming
                  }
                  index {
                    hnsw {
                      max-links-per-node: 32
                      neighbors-to-explore-at-insert: 400
                    }
                  }
                }
                field text_embedding type tensor<float>(x[384]) {
                  indexing: attribute | index
                  attribute {
                    distance-metric: angular
                  }
                  index {
                    hnsw {
                      max-links-per-node: 16
                      neighbors-to-explore-at-insert: 200
                    }
                  }
                }
              }

              fieldset default {
                fields: text
              }

              rank-profile default {
                inputs {
                  query(qt) tensor<float>(querytoken{}, v[128])
                  query(text_weight) double
                }

                function max_sim() {
                    expression: sum(reduce(sum(query(qt) * unpack_bits(attribute(embedding)), v), max, patch), querytoken)
                }

                function bm25_score() {
                    expression: bm25(text)
                }

                function combinedScore() {
                  expression: query(text_weight) * bm25_score + (1 - query(text_weight)) * max_sim
                }

                first-phase {
                    expression: bm25_score
                }

                second-phase {
                    expression: combinedScore
                    rerank-count: 100
                }
              }
            }
            ''')

            # åˆ›å»ºhosts.xmlæ–‡ä»¶
            hosts_path = os.path.join(app_dir, "hosts.xml")
            with open(hosts_path, "w") as f:
                f.write(f'''<?xml version="1.0" encoding="utf-8" ?>
                <hosts>
                  <host name="localhost">
                    <alias>node1</alias>
                  </host>
                </hosts>
                ''')

            # ä½¿ç”¨Dockerå¯åŠ¨Vespaå®¹å™¨
            mount_path = os.path.abspath(self.work_dir)
            docker_cmd = [
                "docker", "run",
                "-d",
                "--name", container_name,
                "-p", f"{self.local_port}:8080",
                "-p", "19071:19071",
                "-v", f"{mount_path}:/app",
                self.docker_image
            ]

            logger.info(f"å¯åŠ¨Dockerå®¹å™¨: {' '.join(docker_cmd)}")
            subprocess.run(docker_cmd, check=True)

            # ç­‰å¾…å®¹å™¨å¯åŠ¨å¹¶æœåŠ¡å‡†å¤‡å¥½
            vespa_url = f"http://localhost:{self.local_port}"
            # self._wait_for_vespa_ready(vespa_url)

            # ä½¿ç”¨Vespaçš„deployå‘½ä»¤æ¥éƒ¨ç½²åº”ç”¨
            deploy_cmd = [
                "docker", "exec", container_name,
                "bash", "-c",
                f"cd /app && /opt/vespa/bin/vespa-deploy prepare /app/application && /opt/vespa/bin/vespa-deploy activate"
            ]

            logger.info(f"éƒ¨ç½²åº”ç”¨: {' '.join(deploy_cmd)}")
            subprocess.run(deploy_cmd, check=True)

            # ç­‰å¾…åº”ç”¨éƒ¨ç½²å®Œæˆ
            time.sleep(10)

            # è¿æ¥åˆ°è¿è¡Œä¸­çš„Vespaå®ä¾‹
            self.app = Vespa(url=vespa_url)
            logger.info(f"Vespa Dockeråº”ç”¨éƒ¨ç½²æˆåŠŸï¼Œç«¯ç‚¹: {vespa_url}")

        except Exception as e:
            logger.error(f"æœ¬åœ°éƒ¨ç½²Vespaåº”ç”¨å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            self.app = None

    def _wait_for_vespa_ready(self, url, max_attempts=30, interval=2):
        """ç­‰å¾…VespaæœåŠ¡å°±ç»ª"""
        logger.info("ç­‰å¾…VespaæœåŠ¡å°±ç»ª...")

        for attempt in range(max_attempts):
            try:
                response = requests.get(f"{url}/ApplicationStatus")
                if response.status_code == 200:
                    status = response.json()
                    if status.get("status", {}).get("code") == "up":
                        logger.info("VespaæœåŠ¡å·²å°±ç»ª")
                        return True
            except Exception:
                pass

            logger.info(f"VespaæœåŠ¡å°šæœªå°±ç»ªï¼Œç­‰å¾…ä¸­... ({attempt + 1}/{max_attempts})")
            time.sleep(interval)

        logger.warning(f"ç­‰å¾…VespaæœåŠ¡å°±ç»ªè¶…æ—¶ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨")
        return False

    def _compute_text_embedding(self, text: str):
        """è®¡ç®—å½’ä¸€åŒ–æ–‡æœ¬åµŒå…¥"""
        if not text or not text.strip():
            return np.zeros(384)

        try:
            with torch.no_grad():
                # å°è¯•ä½¿ç”¨ä¸åŒæ¨¡å‹æ¥å£
                if hasattr(self.text_model, 'encode'):
                    # BGE/FlagModelæ–¹å¼
                    embedding = self.text_model.encode([text])
                    if isinstance(embedding, torch.Tensor):
                        vector = embedding[0].cpu().numpy()
                    else:
                        vector = embedding[0]
                else:
                    # å…¶ä»–æ–¹å¼
                    inputs = self.processor.process_queries([text]).to(self.text_model.device)
                    outputs = self.text_model(**inputs)
                    vector = outputs[0].cpu().numpy() if isinstance(outputs, torch.Tensor) else outputs[0]

                # å½’ä¸€åŒ–å‘é‡
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm

                return vector[:384]
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡æœ¬åµŒå…¥æ—¶å‡ºé”™: {str(e)}")
            return np.zeros(384)

    def index_all_pdfs_from_directory(self, directory_path, ocr_method='paddleocr'):
        """
        Index all PDFs from a directory into Vespa

        Args:
            directory_path: Path to the directory containing PDF files
            ocr_method: OCR method to use ('paddleocr' or 'pytesseract')

        Returns:
            bool: True if indexing was successful
        """
        import os
        from pdf2image import convert_from_path
        import json
        from tqdm import tqdm

        logger.info(f"Indexing all PDFs from {directory_path}...")

        # Get all PDF files in the directory
        pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]
        logger.info(f"Found {len(pdf_files)} PDF files")
        successfully_index_pdf = []
        pdf_list = [
            '4046173.pdf', '4176503.pdf', '4057524.pdf', '4064501.pdf', '4057121.pdf', '4174854.pdf',
            '4148165.pdf', '4129570.pdf', '4010333.pdf', '4147727.pdf', '4066338.pdf', '4031704.pdf',
            '4050613.pdf', '4072260.pdf', '4091919.pdf', '4094684.pdf', '4063393.pdf', '4132494.pdf',
            '4185438.pdf', '4129670.pdf', '4138347.pdf', '4190947.pdf', '4100212.pdf', '4173940.pdf',
            '4069930.pdf', '4174181.pdf', '4027862.pdf', '4012567.pdf', '4145761.pdf', '4078345.pdf',
            '4061601.pdf', '4170122.pdf', '4077673.pdf', '4107960.pdf', '4005877.pdf', '4196005.pdf',
            '4126467.pdf', '4088173.pdf', '4106951.pdf', '4086173.pdf', '4072232.pdf', '4111230.pdf',
            '4057714.pdf'
        ]
        # Process each PDF file
        for pdf_idx, pdf_file in enumerate(tqdm(pdf_files, desc="Indexing PDFs")):
            if pdf_file not in pdf_list:
                continue
            pdf_path = os.path.join(directory_path, pdf_file)

            try:
                # Convert PDF to images
                pages = convert_from_path(pdf_path)
                logger.info(f"PDF {pdf_file} has {len(pages)} pages")

                # Get OCR text
                ocr_file = os.path.join(directory_path, f"{ocr_method}_save", f"{pdf_file.replace('.pdf', '.json')}")

                if os.path.exists(ocr_file):
                    with open(ocr_file, 'r', encoding='utf-8') as f:
                        loaded_data = json.load(f)
                    logger.info(f"Successfully read preprocessed text file: {ocr_file}")
                else:
                    logger.warning(f"OCR file not found: {ocr_file}, skipping PDF")
                    continue

                # Validate page count
                if len(loaded_data) != len(pages):
                    logger.warning(f"OCR data pages ({len(loaded_data)}) do not match PDF pages ({len(pages)})")
                    page_count = min(len(loaded_data), len(pages))
                else:
                    page_count = len(pages)

                # Create documents for each page
                documents = []
                page_keys = list(loaded_data.keys())

                for idx in range(page_count):
                    if idx >= len(pages):
                        break

                    # Check for valid page dimensions
                    page = pages[idx]
                    width, height = page.size
                    if width <= 0 or height <= 0:
                        logger.warning(f"Skipping invalid page {idx + 1}: dimensions {width}x{height}")
                        continue

                    # Get OCR text
                    page_text = loaded_data[page_keys[idx]] if idx < len(page_keys) else ""

                    # Create document structure
                    documents.append({
                        "text": page_text,
                        "image": page,
                        "metadata": {
                            "page_index": idx + 1,
                            "pdf_path": pdf_file
                        }
                    })

                # Index the documents
                if documents:
                    success = self.index_documents(documents)
                    if not success:
                        logger.warning(f"Failed to index documents for {pdf_file}")
                    else:
                        successfully_index_pdf.append(pdf_file)
                else:
                    logger.warning(f"No valid documents created for {pdf_file}")

            except Exception as e:
                logger.error(f"Error processing PDF {pdf_file}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())

        logger.info("PDF indexing complete")
        print("PDF indexing complete: ", successfully_index_pdf)
        return True

    def index_documents(self, documents):
        """å°†æ–‡æ¡£ç´¢å¼•åˆ°Vespa"""
        if self.app is None:
            logger.warning("Vespaåº”ç”¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç´¢å¼•æ–‡æ¡£")
            return False

        logger.info(f"å¼€å§‹ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")

        # åˆ›å»ºvespa_feedåˆ—è¡¨å­˜å‚¨æ‰€æœ‰è¦ç´¢å¼•çš„æ–‡æ¡£
        images = [doc["image"] for doc in documents]
        img_embeddings = []
        batch_size = 2

        for i in range(0, len(images), batch_size):
            batch_images = images[i:i + batch_size]
            with torch.no_grad():
                # å¤„ç†å›¾åƒæ‰¹æ¬¡
                batch_inputs = self.processor.process_images(batch_images).to('cpu')
                batch_embeddings = self.image_model(**batch_inputs)
                img_embeddings.extend(list(torch.unbind(batch_embeddings.to("cpu"))))

        vespa_feed = []

        for i, (doc, embedding) in enumerate(zip(documents, img_embeddings)):
            # åˆ›å»ºæ–‡æ¡£ID
            pdf_name = doc["metadata"]["pdf_path"].split('.')[0]
            doc_id = f"{pdf_name}_page{doc['metadata']['page_index']}"
            text = doc.get("text", "")

            # å¤„ç†åµŒå…¥å‘é‡
            embedding_dict = {}
            for idx, patch_embedding in enumerate(embedding):
                # å°†å‘é‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶è¡¨ç¤º
                binary_vector = (
                    np.packbits(np.where(patch_embedding.numpy() > 0, 1, 0))
                    .astype(np.int8)
                    .tobytes()
                    .hex()
                )
                embedding_dict[idx] = binary_vector

            # åˆ›å»ºVespaæ–‡æ¡£
            vespa_doc = {
                "id": doc_id,
                "fields": {
                    "id": doc_id,
                    "pdf_path": doc['metadata']['pdf_path'],
                    "page_index": doc['metadata']['page_index'],
                    "text": doc['text'],
                    "embedding": embedding_dict,
                    # "text_embedding": self._compute_text_embedding(text).tolist()
                }
            }

            vespa_feed.append(vespa_doc)

        # å®šä¹‰å›è°ƒå‡½æ•°å¤„ç†ç´¢å¼•ç»“æœ
        def callback(response, id):
            if not response.is_successful():
                logger.warning(f"Failed to feed document {id}: {response.get_status_code()}")
                logger.warning(response.json)

        # ä½¿ç”¨feed_iterableæ‰¹é‡ç´¢å¼•
        try:
            # ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°
            BATCH_SIZE = 2  # å‡å°æ‰¹å¤„ç†å¤§å°
            logger.info(f"æ‰¹é‡ç´¢å¼• {len(vespa_feed)} ä¸ªæ–‡æ¡£åˆ°Vespa...")
            for i in range(0, len(vespa_feed), BATCH_SIZE):
                batch = vespa_feed[i:i + BATCH_SIZE]
                try:
                    logger.info(
                        f"ç´¢å¼•æ‰¹æ¬¡ {i // BATCH_SIZE + 1}/{(len(vespa_feed) - 1) // BATCH_SIZE + 1}ï¼Œå…± {len(batch)} ä¸ªæ–‡æ¡£...")
                    self.app.feed_iterable(
                        batch,
                        schema="unified",
                        callback=callback,
                        timeout=180  # å¢åŠ è¶…æ—¶æ—¶é—´
                    )
                    # æ¯æ‰¹åçŸ­æš‚æš‚åœ
                    time.sleep(1)
                except Exception as e:
                    logger.error(f"æ‰¹é‡ç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™ (æ‰¹æ¬¡ {i // BATCH_SIZE + 1}): {str(e)}")
                    logger.error(traceback.format_exc())

            logger.info("æ–‡æ¡£ç´¢å¼•å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"æ‰¹é‡ç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False

    def _split_query_intent(self, query: str) -> List[str]:
        """å°†æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªä¸åŒç»´åº¦çš„æ„å›¾æŸ¥è¯¢"""
        SYSTEM_MESSAGE = dedent("""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œå¹¶å°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªä¸åŒç»´åº¦çš„å­æŸ¥è¯¢ã€‚

        è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
        1. å¦‚æœæŸ¥è¯¢åŒ…å«å¤šä¸ªä¸åŒçš„ä¿¡æ¯éœ€æ±‚æˆ–å…³æ³¨ç‚¹ï¼Œè¯·å°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªå­æŸ¥è¯¢
        2. ç¡®ä¿æ¯ä¸ªå­æŸ¥è¯¢å…³æ³¨ä¸åŒçš„ç»´åº¦æˆ–æ–¹é¢ï¼Œä¿è¯å¤šæ ·æ€§
        3. ä¸è¦ä»…ä»…æ”¹å˜é—®é¢˜çš„è¡¨è¿°å½¢å¼ï¼Œè€Œåº”è¯¥å…³æ³¨ä¸åŒçš„ä¿¡æ¯ç»´åº¦
        4. å¦‚æœåŸå§‹æŸ¥è¯¢å·²ç»éå¸¸æ˜ç¡®ä¸”åªå…³æ³¨å•ä¸€ç»´åº¦ï¼Œåˆ™ä¸éœ€è¦æ‹†åˆ†
        5. å­æŸ¥è¯¢åº”è¯¥æ›´åŠ å…·ä½“å’Œæ˜ç¡®ï¼Œæœ‰åŠ©äºæ£€ç´¢åˆ°æ›´ç²¾å‡†çš„ä¿¡æ¯

        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        {
            "intent_queries": ["å­æŸ¥è¯¢1", "å­æŸ¥è¯¢2", ...]
        }
        """)

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢å¹¶æ‹†åˆ†ä¸ºå¤šä¸ªä¸åŒç»´åº¦çš„å­æŸ¥è¯¢ï¼š\n\n{query}"}
        ]

        response_format = create_response_format({
            "intent_queries": {
                "type": "array",
                "description": "æ‹†åˆ†åçš„å­æŸ¥è¯¢åˆ—è¡¨",
                "items": {"type": "string"}
            }
        })

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages,
            # response_format=response_format
        )

        try:
            result = parse_llm_response(response)
            intent_queries = result.get("intent_queries", [query])
            print("intent_queries:", intent_queries)
            return intent_queries if intent_queries else [query]
        except Exception as e:
            logger.error(f"æ„å›¾æ‹†åˆ†å‡ºé”™: {e}")
            return [query]

    def _refine_query_intent(self, original_query: str, intent_queries: List[str], context: str) -> List[str]:
        """åŸºäºæ£€ç´¢ç»“æœç»†åŒ–æŸ¥è¯¢æ„å›¾"""
        # SYSTEM_MESSAGE = dedent("""
        # ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºå·²æœ‰çš„æ£€ç´¢ç»“æœï¼Œè¿›ä¸€æ­¥ç»†åŒ–å’Œä¼˜åŒ–æŸ¥è¯¢æ„å›¾ã€‚

        # è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
        # 1. åˆ†æå·²æœ‰æ£€ç´¢ç»“æœï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£å’Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘
        # 2. åŸºäºåŸå§‹æŸ¥è¯¢å’Œå·²æ‹†åˆ†çš„æ„å›¾ï¼Œç”Ÿæˆæ›´åŠ ç²¾ç¡®çš„å­æŸ¥è¯¢
        # 3. ç¡®ä¿æ–°çš„å­æŸ¥è¯¢èƒ½å¤Ÿè¦†ç›–åŸå§‹æŸ¥è¯¢æœªè¢«æ»¡è¶³çš„ä¿¡æ¯éœ€æ±‚
        # 4. å­æŸ¥è¯¢åº”è¯¥æ›´åŠ å…·ä½“ï¼ŒåŒ…å«ä¸“ä¸šæœ¯è¯­å’Œæ˜ç¡®çš„ä¿¡æ¯éœ€æ±‚
        # 5. é¿å…ç”Ÿæˆè¿‡äºç›¸ä¼¼çš„å­æŸ¥è¯¢ï¼Œä¿è¯å¤šæ ·æ€§

        # è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        # {
        #     "refined_intent_queries": ["ç»†åŒ–å­æŸ¥è¯¢1", "ç»†åŒ–å­æŸ¥è¯¢2", ...]
        # }
        # """)

        SYSTEM_MESSAGE = dedent("""
        You are a professional query intent optimization expert. Your task is to refine and enhance the user's search intent based on the retrieved content.

        Please follow these guidelines:
        1. Analyze the retrieved content to identify information gaps and areas that require further exploration.
        2. Based on the original query and the decomposed intent queries, generate more precise and targeted sub-queries.
        3. Ensure that the new sub-queries address the information needs that were not fully satisfied by the original query.
        4. Sub-queries should be more specific, incorporating domain-specific terminology and clearly defined information requirements.
        5. Avoid generating overly similar sub-queries; ensure diversity and coverage of different aspects.
        6. Limit the number of refined sub-queries to a maximum of **three**.

        Return your output in JSON format with the following structure:
        {
            "refined_intent_queries": ["Refined sub-query 1", "Refined sub-query 2", ...]
        }
        """)

        # messages = [
        #     {"role": "system", "content": SYSTEM_MESSAGE},
        #     {"role": "user", "content": f"""
        #     åŸå§‹æŸ¥è¯¢ï¼š
        #     {original_query}

        #     å·²æ‹†åˆ†çš„æ„å›¾æŸ¥è¯¢ï¼š
        #     {json.dumps(intent_queries, ensure_ascii=False)}

        #     å·²æ£€ç´¢åˆ°çš„å†…å®¹ï¼š
        #     {context}

        #     è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç»†åŒ–å’Œä¼˜åŒ–æŸ¥è¯¢æ„å›¾ï¼š
        #     """}
        # ]

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE},
            {"role": "user", "content": f"""
            Original query:
            {original_query}

            Decomposed intent queries:
            {json.dumps(intent_queries, ensure_ascii=False)}

            Retrieved context:
            {context}

            Based on the information above, please refine and optimize the search intent:
            """}
        ]

        response_format = create_response_format({
            "refined_intent_queries": {
                "type": "array",
                "description": "ç»†åŒ–åçš„å­æŸ¥è¯¢åˆ—è¡¨",
                "items": {"type": "string"}
            }
        })

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages,
            # response_format=response_format
        )

        try:
            result = parse_llm_response(response)
            refined_queries = result.get("refined_intent_queries", intent_queries)
            print("Refined intent queries:", refined_queries)
            return refined_queries if refined_queries else intent_queries
        except Exception as e:
            logger.error(f"æ„å›¾ç»†åŒ–å‡ºé”™: {e}")
            return intent_queries

    def _prepare_context(self, search_results: List[Dict[str, str]]) -> str:
        """Prepare context from search results."""
        return '\n'.join([f"{index + 1}. {result['text']}" for index, result in enumerate(search_results)])


    def search_pdf_retrieval(self, data: dict, pdf_path: str):
        """æ‰§è¡Œæœç´¢æ£€ç´¢"""
        if self.app is None:
            raise ValueError("Vespaåº”ç”¨æœªåˆå§‹åŒ–")
        original_query = deepcopy(data['query'])
        data_ori = deepcopy(data)

        # 2. æ‹†åˆ†æŸ¥è¯¢æ„å›¾
        intent_queries = self._split_query_intent(original_query)
        logger.info(f"ğŸ” æ„å›¾æ‹†åˆ†ç»“æœ: {intent_queries}")

        # 3. å¯¹æ¯ä¸ªæ„å›¾è¿›è¡Œæ£€ç´¢
        all_results = []
        seen_texts = set()

        pdf_filename = os.path.basename(pdf_path)
        print("pdf_filename: ", pdf_filename)

        for intent_idx, intent_query in enumerate(intent_queries):
            logger.info(f"æ£€ç´¢æ„å›¾ {intent_idx + 1}/{len(intent_queries)}: {intent_query}")

            # ä½¿ç”¨Vespaæ£€ç´¢
            # results = self.retrieve(intent_query, data['documents'])
            results = self._search_with_vespa(intent_query, data['documents'][0]['metadata']['pdf_path'])
            results = results[:self.embedding_topk // len(intent_queries)]

            # åˆå¹¶ç»“æœï¼Œå»é‡
            for result in results:
                if result['text'] not in seen_texts:
                    seen_texts.add(result['text'])
                    all_results.append(result)

        # # ç¬¬ä¸‰æ­¥ï¼šåŸºäºç¬¬ä¸€è½®æ£€ç´¢ç»“æœè¿›è¡Œæ„å›¾ç»†åŒ–
        # refined_intent_queries = self._refine_query_intent(original_query, intent_queries,
        #                                                    json.dumps(all_results, ensure_ascii=False,
        #                                                               indent=2))
        # logger.info(f"ğŸ” æ„å›¾ç»†åŒ–ç»“æœ: {refined_intent_queries}")
        #
        # for refine_idx, refine_query in enumerate(refined_intent_queries):
        #     logger.info(f"æ£€ç´¢æ„å›¾ {refine_idx + 1}/{len(refined_intent_queries)}: {refine_query}")
        #
        #     # ä½¿ç”¨Vespaæ£€ç´¢
        #     # results = self.retrieve(refine_query, data_ori['documents'])
        #     results = self._search_with_vespa(refine_query, len(data['documents']),
        #                                       data['documents'][0]['metadata']['pdf_path'])
        #
        #     # åˆå¹¶ç»“æœï¼Œå»é‡
        #     for result in results:
        #         if result['text'] not in seen_texts:
        #             seen_texts.add(result['text'])
        #             all_results.append(result)

        # # 4. ä½¿ç”¨rerankeré‡æ’åº
        # if self.reranker and all_results:
        #     final_search_results = self._rerank_results(original_query, all_results)
        #     logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(final_search_results)} æ¡")
        #     logger.info([doc['score'] for doc in final_search_results])
        #     return final_search_results

        # æŒ‰åˆ†æ•°æ’åº
        logger.info([doc['score'] for doc in sorted(all_results, key=lambda x: x['score'], reverse=True)[:self.params['rerank_topk']]])
        return sorted(all_results, key=lambda x: x['score'], reverse=True)[:self.params['rerank_topk']]

    def retrieve(self, query: str, documents: List[dict], save_path: Optional[str] = None) -> List[dict]:
        results = self._search_with_vespa(query, documents[0]['metadata']['pdf_path'])
        for doc in documents:
            try:
                # è·å–æ–‡æœ¬å’Œå›¾åƒ
                matching_scores = [x['score'] for x in results if x["page"] == doc['metadata']['page_index']]
                if len(matching_scores) == 0:
                    continue
                text = doc.get("text", "")
                text_score = self._compute_text_score(query, text)

                image_score = matching_scores[0]
                combined_score = self._combine_scores(text_score, image_score)

                doc["score"] = combined_score
                doc["metadata"] = doc.get("metadata", {})  # Ensure metadata exists
            except Exception as e:
                # å¤„ç†ä»»ä½•å…¶ä»–é”™è¯¯
                print(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()
                continue
        for doc in documents:
            if "metadata" in doc and "page_index" not in doc["metadata"]:
                doc["metadata"]["page_index"] = None  # Default to None if page_index is missing


        return sorted(documents, key=lambda x: x["score"], reverse=True)[:self.params['rerank_topk']]

    def _search_with_vespa(self, query, pdf_filter=None):
        """ä½¿ç”¨Vespaæ‰§è¡Œæœç´¢"""
        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            logger.info("åˆ©ç”¨vespaåŠ é€Ÿcolpaliä¸­...")
            query_inputs = self.processor.process_queries([query]).to('cpu')
            with torch.no_grad():
                query_embeddings = self.image_model(**query_inputs)
            query_embeddings = query_embeddings.to("cpu")
            query_embedding = torch.unbind(query_embeddings)[0]

            # å¤„ç†æµ®ç‚¹æŸ¥è¯¢åµŒå…¥
            float_query_embedding = {str(k): v.tolist() for k, v in enumerate(query_embedding)}

            # å¤„ç†äºŒè¿›åˆ¶æŸ¥è¯¢åµŒå…¥
            binary_query_embeddings = {}
            for k, v in float_query_embedding.items():
                binary_query_embeddings[k] = (
                    np.packbits(np.where(np.array(v) > 0, 1, 0))
                    .astype(np.int8)
                    .tobytes()
                    .hex()
                )

            # æ„å»ºæŸ¥è¯¢å¼ é‡
            query_tensors = {
                "input.query(qtb)": binary_query_embeddings,
                "input.query(qt)": float_query_embedding,
            }
            for i in range(len(binary_query_embeddings)):
                query_tensors[f"input.query(rq{i})"] = binary_query_embeddings[str(i)]

            # æ„å»ºæœ€è¿‘é‚»æŸ¥è¯¢éƒ¨åˆ†
            target_hits_per_query_tensor = 20  # å¯è°ƒæ•´çš„è¶…å‚æ•°
            nn = []
            for i in range(len(binary_query_embeddings)):
                nn.append(
                    f"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))"
                )
            # ä½¿ç”¨ORè¿ç®—ç¬¦ç»„åˆæœ€è¿‘é‚»è¿ç®—ç¬¦
            nn_expr = " OR ".join(nn)

            # æ„å»ºYQLæŸ¥è¯¢
            yql_query = f"select * from unified where ({nn_expr})"
            if pdf_filter:
                yql_query += f" and pdf_path contains '{pdf_filter}'"

            response = self.app.query(
                yql=yql_query,
                ranking="retrieval-and-rerank",
                timeout=120,
                hits=15,
                body={**query_tensors, "presentation.timing": True},
            )

            # vespa_qt_format = {}
            # if hasattr(query_embedding, 'shape') and len(query_embedding.shape) > 1:
            #     # å¤špatch/tokenæ ¼å¼ - ä¾‹å¦‚ (n_patches, vector_dim)
            #     for i, patch_embedding in enumerate(query_embedding):
            #         # å°†å‘é‡è½¬æ¢ä¸ºåˆ—è¡¨
            #         vector_values = patch_embedding.tolist()
            #         # æ·»åŠ åˆ°vespaæ ¼å¼
            #         vespa_qt_format[i] = vector_values
            #
            # else:
            #     # å•ä¸€å‘é‡æ ¼å¼
            #     vector_values = query_embedding.tolist()
            #     vespa_qt_format[0] = vector_values
            #
            # request_body = {
            #     "input.query(qt)": vespa_qt_format,
            #     "input.query(text_weight)": 0.6,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ–‡æœ¬æƒé‡
            #     "presentation.timing": True
            # }
            #
            # response = self.app.query(
            #     yql=f"select * from pdf_page where userInput(@userQuery) and pdf_path contains '{pdf_filter}'",
            #     ranking="retrieval-and-rerank",
            #     userQuery=query,
            #     timeout=120,
            #     hits=hits,  # è¿”å›å‰5ä¸ªç»“æœ
            #     body=request_body,
            # )

            # è°ƒè¯•ä¿¡æ¯
            logger.info(f"æŸ¥è¯¢å“åº”çŠ¶æ€: {response.status_code if hasattr(response, 'status_code') else 'æ— çŠ¶æ€ç '}")

            def get_text_by_docapi(doc_id, vespa_url="http://localhost:8080"):
                url = f"{vespa_url}/document/v1/unified/unified/docid/{doc_id}"
                response = requests.get(url)

                if response.status_code == 200:
                    data = response.json()
                    fields = data.get("fields", {})
                    text = fields.get("text", "")
                    return text
                else:
                    print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                    return None

            # å¤„ç†ç»“æœ
            if response.is_successful():
                logger.info(f"æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(response.hits)} æ¡ç»“æœ")
                results = []
                for hit in response.hits:
                    fields = hit['fields']
                    doc_id = fields['id']
                    text = get_text_by_docapi(doc_id=doc_id)
                    results.append({
                        "score": hit['relevance'],
                        "page": fields['page_index'],
                        "text": text,
                        "metadata": {
                            "page_index": fields['page_index'],
                            "pdf_path": fields['pdf_path']
                        }
                    })
                print("results: ", results)
                return sorted(results, key=lambda x: x["score"], reverse=True)[:15]
            else:
                error_msg = response.json if hasattr(response, 'json') else "æœªçŸ¥é”™è¯¯"
                logger.warning(f"VespaæŸ¥è¯¢å¤±è´¥: {error_msg}")
                return []

        except Exception as e:
            logger.error(f"VespaæŸ¥è¯¢å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())  # æ‰“å°å®Œæ•´å †æ ˆä¿¡æ¯
            return []

    def _combine_scores(self, text_score: float, image_score: float) -> float:
        image_score = image_score / 100
        # total_weight = self.embedding_weight + (1 - self.embedding_weight)
        text_weight = self.params['text_weight']
        image_weight = (1 - text_weight)
        return text_weight * text_score + image_weight * image_score

    def _compute_text_score(self, query: str, text: str) -> float:
        bge_model_name = "BAAI/bge-large-en-v1.5"
        text_tokenizer = AutoTokenizer.from_pretrained(bge_model_name, use_fast=True)
        text_model = AutoModel.from_pretrained(bge_model_name).to("cpu")
        if not text.strip():
            return 0.0
        query_tokens = text_tokenizer(query, padding=True, truncation=True, return_tensors="pt",
                                           max_length=512).to("cpu")
        text_tokens = text_tokenizer(text, padding=True, truncation=True, return_tensors="pt", max_length=512).to("cpu")
        with torch.no_grad():
            query_embedding = text_model(**query_tokens)
            text_embedding = text_model(**text_tokens)
        query_embedding = query_embedding.last_hidden_state[:, 0].cpu().numpy()
        text_embedding = text_embedding.last_hidden_state[:, 0].cpu().numpy()
        # å½’ä¸€åŒ–
        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
        text_embedding = text_embedding / np.linalg.norm(text_embedding, axis=1, keepdims=True)
        return float(query_embedding @ text_embedding.T)

    def _rerank_results(self, query, results):
        """ä½¿ç”¨rerankeré‡æ–°æ’åºç»“æœ"""
        try:
            pairs = [[query, result["text"]] for result in results]
            rerank_scores = self.reranker.compute_score(pairs, normalize=True)

            # æ›´æ–°åˆ†æ•°
            for i, score in enumerate(rerank_scores):
                results[i]["score"] = float(score)

            # æ’åº
            return sorted(results, key=lambda x: x["score"], reverse=True)[:self.params['rerank_topk']]
        except Exception as e:
            logger.error(f"é‡æ’åºå‡ºé”™: {str(e)}")
            return results

def parse_llm_response(response_text: str) -> dict:
    """
    ä»LLMå“åº”ä¸­æå–JSONæ•°æ®ï¼Œå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼

    Args:
        response_text: æ¨¡å‹è¿”å›çš„åŸå§‹æ–‡æœ¬

    Returns:
        dict: è§£æåçš„JSONå¯¹è±¡
    """
    import re
    import json

    # 1. æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ¼å¼
    cleaned_text = re.sub(r'```(?:json|python)?', '', response_text)
    cleaned_text = re.sub(r'`', '', cleaned_text).strip()

    # 2. å°è¯•ç›´æ¥è§£æJSON
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        # 3. å°è¯•æŸ¥æ‰¾JSONå†…å®¹
        json_pattern = r'\{[\s\S]*\}'
        match = re.search(json_pattern, cleaned_text)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

    # 4. å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨æå–å…³é”®å­—æ®µ
    output_dict = {}

    # æå–refined_intent_queriesæ•°ç»„
    queries_pattern = r'"refined_intent_queries"\s*:\s*\[(.*?)\]'
    queries_match = re.search(queries_pattern, cleaned_text, re.DOTALL)
    if queries_match:
        query_items = re.findall(r'"([^"]+)"', queries_match.group(1))
        output_dict["refined_intent_queries"] = query_items

    # æå–intent_queriesæ•°ç»„ï¼ˆå¦‚æœæœ‰ï¼‰
    intent_pattern = r'"intent_queries"\s*:\s*\[(.*?)\]'
    intent_match = re.search(intent_pattern, cleaned_text, re.DOTALL)
    if intent_match:
        intent_items = re.findall(r'"([^"]+)"', intent_match.group(1))
        output_dict["intent_queries"] = intent_items

    return output_dict

