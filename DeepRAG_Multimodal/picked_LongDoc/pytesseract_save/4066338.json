{
    "Page_1": "Clemson University\n\nTigerPrints\n\nAll Dissertations Dissertations\n\n12-2014\n\nGender and Ethnicity Classification Using Partial\nFace in Biometric Applications\n\nJamie Lyle\nClemson University, jlyle@g.clemson.edu\n\nFollow this and additional works at: https://tigerprints.clemson.edu/all_ dissertations\n\nG Part of the Computer Sciences Commons\n\nRecommended Citation\n\nLyle, Jamie, \"Gender and Ethnicity Classification Using Partial Face in Biometric Applications\" (2014). All Dissertations. 1412.\nhttps://tigerprints.clemson.edu/all_dissertations/1412\n\n‘This Dissertation is brought to you for free and open access by the Dissertations at TigerPrints. It has been accepted for inclusion in All Dissertations by\n\nan authorized administrator of TigerPrints. For more information, please contact kokeefe@clemson.edu.",
    "Page_2": "GENDER AND ETHNICITY CLASSIFICATION USING PARTIAL FACE\nIN BIOMETRIC APPLICATIONS\n\n \n\nA Dissertation\nPresented to\nthe Graduate School of\n\nClemson University\n\n \n\nIn Partial Fulfillment\nof the Requirements for the Degree\nDoctor of Philosophy\n\nComputer Science\n\n \n\nby\nJamie Lyle\nDecember 2014\n\n \n\nAccepted by:\nDr. Damon L. Woodard, Committee Chair\nDr. Shaundra Daily\nDr. Juan Gilbert\n\nDr. Jason Hallstrom",
    "Page_3": "Abstract\n\nAs the number of biometric applications increases, the use of non-ideal information such as\nimages which are not strictly controlled, images taken covertly, or images where the main interest\nis partially occluded, also increases. Face images are a specific example of this. In these non-ideal\ninstances, other information, such as gender and ethnicity, can be determined to narrow the search\nspace and/or improve the recognition results. Some research exists for gender classification using\npartial-face images, but there is little research involving ethnic classifications on such images. Few\ndatasets have had the ethnic diversity needed and sufficient subjects for each ethnicity to perform\nthis evaluation. Research is also lacking on how gender and ethnicity classifications on partial\nface are impacted by age. If the extracted gender and ethnicity information is to be integrated\ninto a larger system, some measure of the reliability of the extracted information is needed. This\nstudy will provide an analysis of gender and ethnicity classification on large datasets captured\nby non-researchers under day-to-day operations using texture, color, and shape features extracted\nfrom partial-face regions. This analysis will allow for a greater understanding of the limitations\nof various facial regions for gender and ethnicity classifications. These limitations will guide the\nintegration of automatically extracted partial-face gender and ethnicity information with a biometric\nface application in order to improve recognition under non-ideal circumstances.\n\nOverall, the results from this work showed that reliable gender and ethnic classification can\nbe achieved from partial face images. Different regions of the face hold varying amount of gender\nand ethnicity information. For machine classification, the upper face regions hold more ethnicity\ninformation while the lower face regions hold more gender information. All regions were impacted\nby age, but the eyes were impacted the most in texture and color. The shape of the nose changed\n\nmore with respect to age than any of the other regions.\n\nil",
    "Page_4": "Contents\n\n \n\n \n\n     \n\n \n\nTitle Page 2... ee ee ee ee ee i\nAbstract 20. ee ee ee ee ii\nList of Tables 2... ee ee ee ee eee es v\nList of Figures. 2... ee ee ee ee ee es vi\n1 Introduction .. 0... ee ee eee 1\n1.1 Biometrics Overview 3\n1.2. Previous Work .. 1... 2. 2 5\n2 Research Design........ 15\n2.1 Data... .... 2.2000. 15\n2.2 Preprocessing ........ 20\n2.3 Feature Extraction Methods 23\n2.4 Feature Reduction Methods 24\n2.5 Classification Methods... 1... ee 25\n2.6 General Experiment Setup... 2... ee 28\n3B Color 2... ee ee ee ee ee ee 32\n3.1 Feature Extraction Methods... ......0....0.0 00.00.0000 0 0000000. 32\n3.2 Color Spaces 2... ee 33\n3.3 Experiment Setup 2.2... ee 36\n3.4 Analysis... ee 37\n3.5 Gender... ........... 42\n3.6 Ethnicity ........... 50\n3.7 Conclusions... ee 57\n4 Shape... ... ee ee 59\n4.1 Feature Extraction Methods . 59\n4.2 Experiment Setup ...... 60\n4.3 Analysis. ........0.. 61\n44 Gender... 2... ee 61\n4.5 Ethnicity 2... 0. ee 65\n4.6 Conclusions... 6. ee 70\n5 Texture 2... ee ee 72\n5.1 Feature Extraction Methods... ........ 0000000000000 0000000. 72\n5.2 Experiment Setup 2.2... 2. ee 74\n5.38 Analysis... ee 75\n\niil",
    "Page_5": "5.4 Gender... 2. ee 76\n5.5 Ethnicity 2... ee 83\n5.6 Conclusions .. 1... ee 88\n\n6 Application... .. ee ee ee ee 91\n6.1 Experiment Setup 2.2... 2. ee 91\n6.2 Analysis... 0 93\n6.3 Results... . 0.0.0.0... 102\n6.4 Conclusions... ee 107\n\n7 Conclusions and Future Work .. 2... 2. ee ee ee eee 110\n7.1 Reliability... ee\n\n7.2 Age 2.\n7.3 Application\n7.4 Future Work\n\n \n\nGlossary ©. 6 ee ee ee 115\n\nReferences 2... 6 ee ee ee ee ee 116\n\niv",
    "Page_6": "List of Tables\n\n1.2\n1.3\n\n \n\n6.10\n6.11\n6.12\n6.13\n6.14\n6.15\n\n  \n \n\n \n\nGender and ethnicity results for partial face experiments in literature......... 7\nShort description and details from databases in literature and experiments... ... 8\nDetails of partial face experiments found in literature. ..............00.0. 9\nDemographic breakdown of FRGC and MORPH experiment sets ........... 17\nFacial region details by dataset... 2... 2. 2 ee 21\nMORPH image counts by age and demographic. ................000. AL\nGender performance using color and full face 2... 2.0... ee ee 48\nEthnic performance using color and full face, FRGC ..............000. 54\nEthnic performance using color and full face, MORPH ................. 54\nPoints used per region for shape features... 0... ee 60\nGender performance using shape and full face... 2... 2 ee ee 64\nEthnic performance using shape and full face, FRGC ...............00.. 68\nEthnic performance using shape and full face, MORPH ................ 68\nGender performance using LBP texture and full face... 2.2... 02.2 ee 82\nEthnic performance using LBP texture and full face, FRGC .............. 87\nEthnic performance using LBP texture and full face, MORPH............. 87\nDemographic breakdown of Pinellas experiment sets ..............000- 92\nRegion/Feature/Classifier combinations chosen... 6... ee 93\nConfusion matrices from clas ng the gallery set on color combinations ...... 94\nConfusion matrices from classifying the gallery set on texture combinations ..... 95\nConfusion matrix from cle ng the gallery set on shape combination ....... 95\nPerformance details on facial recognition fusion experiments, baseline. ........ 101\n\nAverage comparisons per probe, baseline... 2... 0... ee\nWeighted sum fusion with color... ..........\nWeighted sum fusion with shape and other information\nWeighted sum fusion with texture 2... 0. 0. ee\n\n \n\nWeighted sum fusion with mixed color and texture features .............. 104\nWeighted sum fusion best mix... 2 0. ee 105\nScore fusion with color... 6... ee 106\nScore fusion with texture 2.2... 2 ee 107\nScore fusion with mixed gender, ethnicity, color and texture... ........... 107",
    "Page_7": "List of Figures\n\nLl\n\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n2.10\n2.11\n2.12\n\n3.1\n3.2\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n3.10\n3.11\n3.12\n3.13\n3.14\n3.15\n\n41\n4.2\n4.3\n44\n4.5\n4.6\n4.7\n48\n\n5.1\n5.2\n\nTypical biometric system .. 2... 2. ee\n\nExample images from the FRGC database. .. 2... 0... 2 ee\nDistribution of FRGC according to age, gender, and ethnicity... ..........\nExample images from the MORPH database. .. 2... .....0..0..0.0004\nDistribution of MORPH according to age, gender, and ethnicity. ...........\nExample images from the Pinellas database... 2... 0... ee ee\nDistribution of Pinellas according to age, gender, and ethnicity ............\nExample feature points detected by VeriLook SDK ................00.4.\nExample facial regions for experiments .........\nPoint subset used to extract facial regions .......\nPreprocessing method ................02\n\nRGB and HSI histogram visualization ... 2.0... 00.0000... 0 000000045\nYIQ and YCbCr histogram visualization. .......\nLUV and LCH histogram visualization... 2... eee\nAverage global feature vector for MORPH color features... ..........000.\nMean difference between FRGC global color region vectors by demographic .... .\nNearest neighbor gender classification on FRGC color features... ..........\nNearest neighbor gender classification on MORPH color features ...........\nANN and SVM gender classification on color features... 2.2.0.0 0 eee\nEasy and hard subjects in color gender classification, MORPH ............\nGender performance on color by age\nNearest neighbor ethnic classification on FRGC color features... ..........\nNearest neighbor ethnic classification on MORPH color features... .........\nANN and SVM ethnic classification on color features... 2.0.0.0. 0 ee\nHard subjects in color ethnicity classification, MORPH.................\nEthnic performance on color by age... 6... ee\n\n \n\n \n\n \n \n \n\nExample of shape feature calculation... 2... 2.20... 0 ee\nGender nearest neighbor results on shape... 2...\nGender ANN and SVM results on shape... 2.\nHard subjects in shape gender classification, MORPH\nGender and ethnic performance on shape by age\n\nEthnic nearest neighbor results on shape... 2... 2 2 ee\nEasy and hard subjects in shape ethnic classification, MORPH ............\nEthnic ANN and SVM results on shape... 2...\n\nAverage global feature vector for MORPH HOG features... ..........0...\nAverage global feature vector for MORPH LBP features... ...........0..\n\nvi\n\n \n\n \n\n \n\n33\n34\n36\n38\n39\n44\n\nana\nAaAawe\n\nan\n\n77",
    "Page_8": "5.3\n5.4\n5.5\n5.6\n5.7\n5.8\n5.9\n5.10\n5.11\n\n6.1\n6.2\n6.3\n6.4\n6.5\n6.6\n6.7\n6.8\n6.9\n\nMean difference between MORPH region-wide global texture vectors by demographic\nNearest neighbor gender classification on texture features ............0..\nANN and SVM gender classification on texture features... 2... .0..00000.\nEasy and hard subjects in texture gender classification, MORPH ...........\nGender performance on texture by age. 2... 2. ee\nNearest neighbor ethnic classification on texture feature\nANN and SVM ethnic classification on texture features\nEasy and hard subjects in texture ethnic classification, MORPH ...........\nEthnic performance on texture by age .. 1... ee ee\n\n \n\n \n\nAge distribution of Pinellas experiment sets\nPinellas images with negative ages ..........2.\nPinellas images with ages below 16... ........\nPinellas images with ages over 100 2... 1 oe ee\n\n \n\nGender and ethnic performance on chosen combinations by age ............\nScore distribution for straight whole face experiment ...............000.\nBaseline performance on identification and verification\nBest weighted sum fusion experiments .........\nBest score fusion experiments .. 2... ee\n\n \n\n \n\nvii\n\n78",
    "Page_9": "Chapter 1\n\nIntroduction\n\nThe world today is becoming more and more identity-driven. As the number of applications\nwhich require identification increases, the chance of identity-theft also rises. Modern biometric\napplications were developed as a solution to this problem. Identification can now take place by who\nyou are, physically and behaviorally, as opposed to what you have or what you know. Biometric\napplications, which uniquely identify the individual, are useful, but not perfect under all conditions.\nSometimes the data available to the application is not ideal and identification fails.\n\nAn example of this non-ideal information is grainy surveillance footage of a gas station\nrobbery. The police probably will not be able to identity the suspect on the video, but they can\ndetermine certain information. This information could be the approximate height, gender, or eth-\nnicity of the suspect. These descriptions will not identify the individual to the police, but they will\nnarrow the possible suspects to the people who match whatever information is extracted. This type\nof information, which describes the individual but does not uniquely identity them, is known as soft\nbiometric information.\n\nSoft biometric information covers a wide range of details about a person. It can include\nheight, weight, hair color, eye color, gender, age, and even clothing color. Some soft biometric traits\nare more permanent than others, gender versus clothing color for example. Some traits change\nnaturally over time, such as height, weight, and age. Even though some traits are not entirely\nstable, certain applications might only need short-term information where the soft biometrics would\nbe unlikely to change, or they could use the more permanent information such as gender and ethnicity.\n\nWhich traits are the most useful depend on the application and the type of data it collects.",
    "Page_10": "Soft biometric information can be used in various aspects of culture today including advertis-\ning, computer interaction customization, surveillance and tracking, as well as biometric applications.\nAdvertisers today target ads to specific populations. Web advertisements can appear based on pre-\nvious browsing behaviors. Targeted advertising even shows up in movies; for example, shoppers\nare identified based on iris scans and their purchase history made available in order to personalize\nthe store’s interaction with them [50]. Most likely, soft biometric information would allow targeted\nads based on demographic groups as opposed to a specific identity, but the idea remains the same.\nHuman Computer Interaction (HCI) is a growing field of study which can benefit from the use of\nsoft biometrics. Suppose the computer could determine an age and take appropriate steps based on\nthat age, such as enlarge the font for an older individual or enable parental controls for a younger\nindividual. Law enforcement uses soft biometric information to describe subjects as mentioned pre-\nviously and algorithms exist that can track an individual across camera views using soft biometric\ninformation [19].The main focus of this paper will be an analysis of gender and ethnicity classifica-\ntions on various regions of the face, culminating in using the results of the analysis in an effort to\nimprove the performance of a biometric application using face information.\n\nPark and Jain [56] group the uses of soft biometric information in biometric and computer\napplications into four main categories. The first category is using soft biometric information to\nsupplement existing biometric systems to improve identification accuracy. The second category is\nenabling faster image retrieval which could be used in a biometric application or an image database.\nThe next category mentioned is enabling matching or retrieval of facial images that are occluded\nor captured at an angle. The last category is using soft biometric information to provide more\ndescriptive evidence about the similarity or dissimilarity between facial images, which could be useful\nin judicial settings. These categories were created with face images in mind, but can generalize well\nto other biometric modalities. This work seeks to further the understanding of the use of partial\nface in the first two categories to enable future use in all categories.\n\nThe face modality is a well-researched modality in the biometrics community. Most human\nrecognition takes place based on the face and it is a relatively easy modality to acquire. The face\nis a good source of gender and ethnicity information, which is the soft biometric information of\ninterest in this work. Partial face has not been researched as in-depth as full face for both gender\nand ethnicity classification, but deserves more attention. As surveillance increases and awareness\n\nof surveillance increases, the difficulty of acquiring a good quality full face image increases as well.",
    "Page_11": "The probability that some part of the face will be occluded or blocked from the camera is high.\nTherefore, an analysis of what information, namely gender and ethnicity, can reliably be found in\ndifferent regions of the face is needed. Since surveillance is not limited to any certain age group, an\ninvestigation of how age impacts the partial-face gender and ethnicity classification is needed as well.\nBefore the details of the analysis are discussed, a short overview of biometric systems is included for\n\nbackground information.\n\n1.1 Biometrics Overview\n\nBiometrics, within the security and computing fields, is defined as the science of identifying\nan individual based on physiological or behavioral characteristics [6]. Physiological characteristics\nbelonging to an individual that have been used for biometric purposes include fingerprints, face,\niris, and hands. Behavioral characteristics include voice, signature, and the way a person walks\n(gait). The specific characteristic used is known in the literature as a biometric modality or simply\na modality. Biometric applications can use a single modality, such as in face recognition systems, or\ncan use multiple modalities, such as a system that combines face and gait information to identify an\nindividual. The type of modality used will depend on the problem the application is trying to solve.\n\nThe two main types of problems for biometric applications are verification and identification.\nApplications that deal with both problem types have a similar structure, which can be seen in\nFigure 1.1. First, the system needs data, which is captured by a sensor. The sensor could be a camera\nor an audio recorder. The data is preprocessed to get it in the right form for feature extraction.\nIn facial recognition this step includes face detection, alignment, and contrast enhancement. Once\nthe data is in the right format, the system extracts features from the data. These could be texture,\nshape, or color features from an image. The features are then used to generate a template. If the\nsystem is in the learning or enrollment phase, the templates are stored in a database along with\nan identity. If the system is in testing phase, the template, also known as the probe, is compared\nagainst other stored templates, known collectively as the gallery. A match score is given for each\ncomparison. The system uses these scores to make a decision. The decision is different for verification\nand identification systems.\n\nAccess control is an example of a verification problem. Only certain individuals are permit-\n\nted access and their identities must be verified before they can gain access. The person who wishes",
    "Page_12": "Preprocessing\n\nTemplate Generator\n\nSensor\n\n   \n\n \n\n  \n\nEnrollment\n\n   \n   \n   \n\nStored Templates\n\nSystem Theshold\n\n  \n\n    \n\nFigure 1.1: Typical biometric system\n\naccess claims an identity. The biometric system processes the data and compares the probe to the\nstored template that corresponds to the claimed identity. The system has a threshold for match\nscores in its decision module. If the score is above the threshold, the system decides that the person\nand the identity are the same and grants access. If the score is below the threshold, the system\ndecides that the person and the claimed identity are not a match and denies access.\n\nIn an identification problem, the unknown person makes no identity claim. Since no claim\nis made, the probe is compared against multiple, possibly all, gallery templates stored during the\nenrollment phase. The match scores are sorted. The corresponding identities are ranked from the\nmost likely match to the least likely. The most likely identity can be returned or a list of most likely\nidentities can be returned for a human expert to make the final decision.\n\nSome cases arise where identity is not the main goal or is not plausible with the given\ndata. In those cases classification by some other criteria, such as age or gender, is desired. This\nsoft biometric classification may be the end result of the system or it could be incorporated into a\nbiometric application in some way. Classifications on the probe could be used as another feature\nto increase confidence in a possible identity match. They could also be used as a filter to only\n\ncompare the probe with identities in the gallery that have matching soft biometric information. A",
    "Page_13": "soft biometric classification system starts the same as either a verification or identification system.\nA sensor collects the data which is then preprocessed. Features are extracted from the preprocessed\ndata. In place of enrollment, a classification system undergoes a training phase. The training can\neither be supervised, where class labels are provided for the training data, or unsupervised, where\nlabels are not provided. The trained system will take the features extracted from the data and\noutput a class label, such as male or female.\n\nWith a general understanding of biometrics and classification systems, the proposed analysis\nwill be placed in context with previous research involving partial-face classification of gender and\n\nethnicity.\n\n1.2. Previous Work\n\nMuch of biometric research focuses on using the face for recognition purposes. The face is\nvery visible in most cultures and is a large part of how humans recognize one another. In many\ninstances, not even the entire face is needed for a human to recognize an individual. When looking\nat face recognition by machines, the quality of the face image is an important factor. Blurred\nimages [39], occluded faces, and low or uncontrolled lighting [7] can all negatively impact the results\nof facial recognition algorithms.\n\nIdentity is not the only information to be found in the human face. Previous studies have\nshown that the face holds both gender [24, 29, 72] and ethnicity [28, 29, 30, 45] information in\naddition to identity. In the event that a face recognition system is not confident of its result,\ngender and ethnicity information can be used to increase the confidence of the system. In an image\nof a face though, there can be noise which would inhibit an accurate determination of gender or\nethnicity. Studies have been performed to show the usefulness of various subregions of the face for\ngender [5, 9, 35, 47] and ethnicity [40, 47] determination; however, most of the studies are on a\nsmall scale and do not carry out experiments on all regions of the face. The goal of the research\ndetailed herein is to gain an understanding of the limitations of partial-face regions for gender and\nethnicity classifications. This understanding will including learning how reliable each part of the\nface is for gender and ethnicity classification and how each region is impacted by age. Knowing these\nlimitations, a method using machine-based gender and ethnicity classifications made on partial face\n\nis proposed to improve performance in a facial biometric application.",
    "Page_14": "1.2.1 Reliability\n\nWhat parts of the face hold reliable gender and ethnicity information for machine applications?\nThe first question to be considered is which parts of the face hold reliable gender and\nethnicity information for machine applications. It is possible that a subset of the face holds more\ngender or ethnicity information than another subset. Some regions of the face may be gender\nor ethnic neutral. Another consideration is what type of features encode the gender or ethnicity\ninformation. Texture, color, and shape features can be found in facial images. Knowing which\nregions of the face hold reliable gender and ethnicity information and what features encode the\ninformation best will provide a foundation for improving gender and ethnicity classifications based\n\non partial face.\n\n1.2.1.1 Face Regions\n\nPrevious studies have focused mainly on gender when working with partial face [5, 8, 35,\n36, 44, 55, 69]. Details for partial face studies mentioned can be found in Table 1.1. Some details\nof the databases used can be found in Table 1.2. Most of these studies use human-defined regions\nto subdivide the face, such as the eyes, nose, mouth, and chin. Regions containing the eyes ranked\namong the top two regions for several studies [5, 8, 44]. These results were derived from three different\ndatasets, so the good performance of the eye region is not a peculiarity of a specific dataset. It is\nmore likely that the eye region holds more gender information than other regions tested. Other best\nperforming regions were the nose in FERET experiments and the mouth in XM2VTS experiments [5].\nThe nose performed the worst in the XM2VTS experiments. This could be due to the different\nethnic compositions of the datasets, or the authors suggested that the nose was more susceptible to\nillumination than the mouth. The differences between the best performing regions in the various\ndatasets suggest that more research is needed in this area. It could be that the sample/experiment\nsize was not large enough. Most of the studies perform experiments with less than 1,000 subjects,\nas seen in Table 1.3. Lapedriza et al. [37] have one of the larger studies based on image number, but\nthe number of subjects is unknown and their research focuses more on features outside the face such\nas the ears, forehead, and hair, instead of the eyes, nose, mouth, and chin. Aside from the nose [5],\nmouth [5, 36] and eyes [5, 8, 44, 47] mentioned above, another best performing region of the face is\n\nthe jaw or chin [35, 36].",
    "Page_15": "Study Feature Classifier Rate Modality Trait Dataset\nOzbudak [55] | DWT+PCA FLD 93% Face” G FERET, SUMS\n(52%) (w.o. nose)\n9 G, ‘ay\nKawano [36] FDF LDA “oo Gem G Softopia Japan\n<a sy ry, 90.6% Face \\ , \\\nAndreu [5] Pixels+PCA SVM (84.0%) (Mouth) G XM2VTS\nwo _—v 95.2% Face \\\nAndreu [5] Pixels+PCA SVM (86.4%) (Nose) G FERET\nso — 86.5% Face 1 FERET, AR,\nBuchala [8] Pixels+PCA | SVM(RBF) (92.3%) (Composite) G BioID\n5G, ‘ay\nLu [44] Pixels SVM(RBF) 029%) (Upper) G CAS-PEAL\nG ‘ay\nLu [43] 2D-PCA | SVM(RBF) on) (Fusion) G CAS-PEAL\n. a ary) 93.5% Face \\ .\nHu [31] Curvature SVM (94.3%) (Fusion) G own, UND (3D)\nLapedriza (38) | Tee SVM 94.2% Extemal |g FRGC\nragments Face\n1 ; . 9 . 7 FRGC,\nManesh [47] Gabor SVM(RBF) 94.0% Face Fusion G CAS-PEAL\n. ote Face . . 96.8% Face \\ \\\nLapedriza [37] Fragments Joint Boost (96.7%) (External) G Controlled FRGC\n: . Face . . 91.7% Face \\ Uncontrolled\nLapedriza [37] Fragments JointBoost (90.6%) (External) G FRGC\nThomas [68] Texture DT 75% Tris G Custom\nLyle [46] fete , ANN/SVM 97.3% Periocular G FRGC\nLyle [46] Texture ANN/SVM 90% Periocular G MBGC\nMerkow [48] Texture, LDA+SVM 85% Periocular G web\npixels\n1 4 Why 6 . FRGC,\nManesh [47] Gabor SVM(RBF) 97.4% Face Fusion | E(2) CAS-PEAL\n. Gabor . . CASIA, UPOL,\nQiu [59] features AdaBoost 86% Iris E(2) UBIRIS\nQiu [60] Iris-Textons SVM 91% Iris E(2)_ | CASIA-BioSecure\n. Eyelash 2 90 Sy CMU-PIER,\nLi [40] direction 1-NN 93.2% Periocular E(2) UBIRISv1\nLyle [46] fete > | ANN/SVM 94% Periocular | E(2) FRGC\nLyle [46] Texture ANN/SVM 89 % Periocular E(2) MBGC\n\n \n\n \n\n \n\n \n\n \n\n \n\nTable 1.1: Details and results for partial face experiments. Traits are gender (G)\n\nwith (x) classes. Results from best performing subregion were included i\n\n \n\nf applical\n\n \n\nand ethnicity (E)\nle.",
    "Page_16": "Database Images Subjects Short Description\n\nBioID 1,521 23 | Grayscale face images captured under real-world con-\nditions\n\nUPOL 384 64 | Color iris images\n\nCMU-PIER 321 107 | Iris images collected for ethnicity classification based\non eyelash direction\n\nMBGC 149 (videos) 114 | Multi-modal face and iris database captured under\nnear infrared illumination, video and stills\n\nAR 4,000 126 | Color face image captured under varying illumina-\ntion and facial expressions. Occlusions were created\nwith sunglasses and scarves.\n\nUBIRIS (v1) 1,877 241 | Noisy iris images captured under visible light condi-\ntions (color)\n\nXM2VTS 1,180 295 | Multi-modal face database including high-quality\ncolor face images\n\nSoftopia Japan 1,200 300 | Color face images, equal male and female subjects\n\nSUMS 400 400 | Grayscale face images from equal male and female\nsubjects from Stanford University Medical students\n\nFRGC 39,329 568 | High-quality color face images from students and fac-\nulty at University of Notre Dame\n\nCASIA (v3) 22,051 700 | Grayscale iris images captured under near infrared\nillumination\n\nCAS-PEAL 30,871 1,040 | Grayscale Chinese face database captured under\nvarying pose, expression, accessory, and lighting con-\nditions\n\nFERET 14,126 1,199 | Color face images captured under semi-controlled\nconditions.\n\nMORPH 55,134 13,314 | Color face images collected from public records\ntaken under real-world operating conditions (semi-\ncontrolled)\n\nPinellas 1,447,607 403,619 | Color face images collected from Pinellas County\nSheriff’s Office\n\n \n\n \n\n \n\n \n\nTable 1.2: Short description and details from databases found in previous work in partial-face gender\nand ethnicity classification and those to be used.\n\nOther studies broke the face into regions differently, both larger and smaller than the pre-\nviously mentioned human-defined regions. The area around the eye still performs well in these\nstudies [44, 47]. In a study by Manesh et al. [47], the eyebrows and the area between the eyes\nperformed best for gender classification. The upper region of the face, which includes the eyes, also\nperformed well in a study by Lu et al. [44]. The drawback of using the larger regions is that it is\nunclear which smaller portion of the face actually encodes the gender information.\n\nEthnicity research on partial face are not as widespread as gender research, but some research\n\nhas been done in the past. The eyes are still a good choice for ethnicity classification according to",
    "Page_17": "Study Trait Type Subjects Images Datasets\nOzbudak [55] Gender Texture - 480 2\nKawano [36] Gender Shape 300 1,200\nAndreu [5] Gender Pixels 203 1,378 1(XM2VTS)\nAndreu [5] Gender Pixels 834 2,147 1(FERET)\nBuchala [8] Gender Pixels - 400 3\nLu [43, 44] Gender Pixels - 800\nHu [31] Gender Shape 321 945 2\nLapedriza [38] Gender Texture - 2,640\nManesh [47] Gender, Ethnicity Texture 1,691 1,691 2\nLapedriza [37] Gender Texture - 3,440/1,886\nThomas [68] Gender Texture - 57,137\nQiu [59] Ethnic Texture - 39,982 3\nQiu [60] Ethnic Texture 60 2,400\nLi [40] Ethnic Texture 214 642 2\nLyle [46] Gender, Ethnicity | Texture, Color 404 4,232\nLyle [46] Gender, Ethnicity Texture 60 350\nMerkow [48] Gender Pixels, Texture - 936\n\nTable 1.3: Details of partial face experiments found in literature.\n\n \n\n \n\nManesh et al. [47], but their results suggest that the cheeks are also useful for determining ethnicity.\nManesh et al. [47] also have the largest number of subjects in their gender and ethnicity experiments\nwith 1,691 subjects from the FERET and CAS-PEAL datasets. However, both datasets were needed\nto gain the desired ethnic diversity. The majority of each ethnic class was found in only one of the\ndatabases. The authors took steps to try to minimize the influence of using two different datasets,\nbut their system could have learned the image properties of the database instead of the ethnic class.\n\nStudies on just the iris and periocular regions have been performed in both gender [46, 48, 68]\nand ethnicity [40, 46, 59, 60]. This is the region of the face where most ethnic research has been\nperformed. The eye region is able to obtain at least 85% on most binary gender and ethnicity\nclassifications, indicating that it holds a large amount of gender and ethnicity.\n\nTaking all this previous work under consideration, experiments will be performed on the\neyes, nose, mouth, and chin regions of the face in reliability experiments. Most of the facial regions,\nat one point or another have all been the best performing region on some dataset. It is hoped\nthat experiments within this work will have some measure of agreement and be able to relate back\nto previous works. Regions including the eyes are expected to perform well for both gender and\nethnicity classification across all datasets as they are among the best regions for the majority of\n\nprevious studies.",
    "Page_18": "1.2.1.2 Features\n\nVarious types of features have been used in the past for gender and ethnicity classification.\nThe main types of features found in images can be categorized into pixels, texture, shape, color, and\nkey-point. Pixel intensity values are one of the most basic features used for these purposes [44, 48].\nThis type of feature is commonly used in combination with a projection into a different space, such\nas Principal Component Analysis (PCA) [5] or Independent Component Analysis (ICA) [32], for\nfeature reduction.\n\nLocal texture features are another type of feature that can be extracted from facial images.\nImage texture can be characterized as a set of repeating patterns. Within the face, texture can\nrange from very fine skin texture to larger texture caused by hair and wrinkles. Texture feature\nextraction techniques used in partial-face research include Gabor features [47, 59], iris texture [60,\n68], Histograms of Oriented Gradient (HOG) [46], Discrete Cosine Transform (DCT) [46], and the\ntexture of the face fragments used by Lapedriza et al. [37| is derived using Gaussian derivative\nfilters. The Local Binary Patterns (LBP) feature extraction method is another widely used texture\nrepresentation that has been used for gender and ethnicity classification in both full face [41, 66, 75]\nand partial [46, 48].\n\nAside from local texture information, shape information can also be found in the face.\n3D shape information has been utilized for gender classification [31]. Active Appearance Models\n(AAMs), which combine shape and texture information, can be used to represent a face. AAMs\nmodel the statistical shape of an object, as well as texture, and can be used to find a shape, such as\nthe face, in an image as well as for matching or classification [15]. AAMs have been used successfully\nfor gender classification [64]. Another approach which uses the distance and angles between facial\nlandmarks is called face metrology. This method has been used to successfully predict gender in\nfacial images [10].\n\nAnother type of feature representation is Scale Invariant Feature Transform (SIFT). This\ntechnique is different from the others in that it only looks at specific key-points on the image instead\nof calculating statistics over the entire image or finding a sequence of points (shape). SIFT has been\nused to successfully perform gender classification using full or occluded facial images [69].\n\nThe last category is color. Local Color Histograms (LCH) are a simple color representation\n\nthat have been used for both gender and ethnic classification [46]. Color is often an important cue\n\n10",
    "Page_19": "for humans in determining ethnicity. One drawback to using color is that it can be easily changed by\nvarious factors such as temperature, cosmetics, or illumination. Because of this, it might be useful\nto combine color features with features from another category, such as texture or shape.\n\nSince texture is a large part of the information to be found in partial face regions, several\ndifferent texture representations will be investigated to see if any one is better suited to a particular\nregion than another. The LBP feature extraction technique was chosen due mainly to its success in\ngender classification using full face. HOG is another interesting texture representation which was\nfirst developed for pedestrian detection [18]. Its discriminating capability has been shown to be\nsuccessful in facial recognition [20, 65], as well as periocular gender and ethnicity classification [46].\nLocal Phase Quantization (LPQ) is the last local texture representation chosen for investigation.\nLPQ is a recently proposed texture descriptor [54] which is said to be robust to image blurring.\nIt has been used successfully for facial recognition [2, 11, 12]. It is expected that its descriptive\ncapabilities will also work well for gender and ethnicity classification on partial-face images. LPQ\nwas also chosen because the robustness to blur would mean that image quality might not be as\nimportant in order to achieve good performance. Local Color Histograms (LCH) will be used to\nrepresent color information based on previous performance [46]. Shape features will be investigated\nusing a version of face metrology [10] adapted to partial face. A more detailed description of these\n\nmethods can be found in the following chapters.\n\n1.2.1.3 Classification\n\nEarly work in gender classification of faces relied on neural networks [1, 16, 22, 24, 67] to\nclassify the data. The classifiers most widely used for gender and ethnicity determination in more\nrecent works are still various forms of of Artificial Neural Networks (ANN) as well as Support Vector\nMachines (SVM) which can be seen in Table 1.1. In order to give a comparison, both SVM and\nANN classifiers will be investigated as classifiers for this work. A simpler classifier, nearest neighbor,\nwill also be used as a baseline with which to compare ANN and SVM classification. A more detailed\n\ndescription of these classification methods can be found in Section 2.5.\n\n1.2.2 Impact of Age\n\nHow is machine classification of gender and ethnicity in partial face impacted by age?\n\nThe second question to be considered is how age impacts gender and ethnicity classification\n\n11",
    "Page_20": "for each part of the face. The human face changes as an individual ages. Younger faces are fuller,\nlacking the fine lines and wrinkles the face acquires as it ages [14]. The skin starts to sag as the face\nages, losing it elasticity, and fine lines become more apparent. The eyebrows can even fall to the\nlevel of the brow. The eyes seem to sink into the face and “crow’s feet” appear at the eye corners.\nThe cheeks hollow as the fat deposits sink or are redistributed in the face. These changes are natural\nin the aging process and can occur at different times for different individuals. How do these changes\naffect gender and ethnicity classification using partial face? Is there a specific region of the face\nthat works best for a certain age range or one that works well across various ages? Being able to\ncharacterize how performance is impacted by age will allow for better design choices in a biometric\n\napplication when the demographics of the population are known in advance.\n\n1.2.2.1 Facial Regions\n\nResearch in gender classification on full face shows that gender classification is impacted by\nage. The results of multiple studies [26, 62] indicate that gender classification is easier on adult faces\nthan younger or more senior faces. The results in [26] were 10% higher on adult faces than seniors\nor children, with subjects ranging from 0 to 93 years of age. Successful recognition is possible on\nyounger faces though. One machine classification method [71] achieved better gender recognition\nin toddler faces than humans performing the same task. Not very many researchers have looked at\nhow age affects gender classification using partial face images. Kawano et al. [36] only looked at\nhow age affected their best performing partial-face region, which was the jaw. They found that the\nbest gender performance on the jaw was on subjects in their 30’s. The error rate for subjects that\nwere either older or younger increased. This is very similar to full face results; however, it would be\ninteresting to see if the trend holds for other regions of the face, not just the jaw.\n\nNot as much research has been done on ethnicity classification and the impact that age has\non performance. Ethnicity results in [27] on full face were not as affected by age as the gender results\nmentioned above, but the age range of the subjects is different from the gender experiments. The\nsubjects in the dataset used for the ethnicity experiments were limited mostly to adult faces with\nages ranging from 16 to 67 years of age, while the gender study included children as well as adults\nup to 93 years old. No partial face research was found which looked at ethnicity classification in the\npresence of age.\n\nA specific study of how gender and ethnicity classifications on the various facial regions are\n\n12",
    "Page_21": "affected by age would be beneficial. Knowing how each individual part is affected by age would\ncontribute to the confidence level for gender and ethnicity classifications. If there is a specific part of\nthe face in which gender and ethnicity information is least impacted by age, that would be the best\nregion when age is a factor. Ethnicity classification on partial face is expected to be less impacted\n\nby age than gender classifications based on the literature for classification of full face.\n\n1.2.2.2 Features\n\nThe feature extraction methods used when looking at gender and ethnicity over an age\nrange include many of the same ones mentioned in the previous section. Pixel intensity values,\nHOG features, and LBP features are among the features compared by Guo e¢ al. [26]. A different\nfeature, named Biologically Inspired Features (BIF), is used by Guo et al. [26, 27] for both gender\nand ethnicity classification in the presence of aging. BIF are based, at the lowest level, on the results\nof Gabor filtering on an image. The other feature extraction most used in the presence of aging is\nActive Appearance Models (AAM) used by Wang et al. [71] to classify the gender of toddlers. Shape\nfeatures are important because the change in features from infants to adults are mainly shape-based.\nTexture is the next big change as the adult face ages. This is when the skin loses its elasticity and fat\nlayer underneath, allowing the wrinkles to appear. The main features used for gender and ethnicity\nclassification over age fall into the categories of pixels, shape, and texture.\n\nExperiments on how age impacts performance will use the same features mentioned in\nSection 1.2.1.2. These features belong to the color, texture, and shape categories. Since facial\ntexture changes as an adult ages, it is expected that results for texture features will be impacted\nby age. Color also changes some with age, but will probably not be as impacted by the age range\n\npresent in the datasets used for experiments.\n\n1.2.3. Application\n\nTo what extent can automatically determined gender and ethnicity information improve performance\nin a biometric experiment using the face or partial face modality?\n\nThe final question to consider is how to use gender and ethnicity classifications on partial\nfaces to improve the performance of a biometric application. Various studies have investigated\nfusing soft biometric information with a biometric experiment in an effort to improve performance\n\n[3, 33, 34, 45, 76]. Jain et al. [34] were the first to introduce the terminology “soft biometrics”\n\n13",
    "Page_22": "and used gender, ethnicity, and height information to improve the performance of a fingerprint\nexperiment. They achieved an improvement of approximately 5% when including the soft biometric\ninformation. In a later experiment involving facial features [33], the extracted gender and ethnicity\ninformation failed to improve facial performance, while the simulated height information improved\nperformance by approximately 5% again. Jain et al. [33] concluded that soft biometric information\nimproves performance only if the information is complementary, or independent, of the primary\nmodality used in the experiment.\n\nAnother way to utilize the soft biometric information is to use it to retrieve a candidate list\nfrom the stored templates, similar to indexing a database. Park et al. [56] use facial marks, such\nas freckles, moles, scars, and birthmarks, along with gender and ethnicity information to filter the\nstored templates in their experiments. Using these soft biometric traits, they were able to increase\nperformance by 1.5%. An improvement is seen, even though the soft biometric information was\nnot independent of the facial features used in the biometric recognition experiment. However, the\ngender and ethnicity information used here was not automatically extracted in the course of the\nexperiment.\n\nIf the soft biometric information does not need to be independent of the primary modality, it\nis possible that gender and ethnicity information can improve the performance of a face experiment.\nThe best methods from the previous sections will be combined with face features in an effort to\nimprove performance. The machine-based soft biometric classifications will be used as a filter for\nthe stored templates to see what improvement can be achieved. These combinations can be useful\nfor both recognition and verification applications.\n\nEach of these research questions will be addressed in the subsequent chapters, along with\nthe design of the experiments for each question (Chapter 2). The analysis on reliability and impact\nof age will be performed for the three main types of features found in face images: color (Chapter 3),\nshape (Chapter 4), and texture (Chapter 5). The application of the research to use the acquired\ngender and ethnicity information to improve an everyday face biometric system (Chapter 6) will\nutilize conclusions from the analysis. Overall conclusions from each chapter will provide the basis\n\nfor future work (Chapter 7.)\n\n14",
    "Page_23": "Chapter 2\n\nResearch Design\n\n2.1 Data\n\nThree databases will be used for experiments in this work: the Facial Recognition Grand\nChallenge database, the Craniofacial Longitudinal Morphological Face database, and a face database\ncollected by the Pinellas County Sheriff’s office. The collection and composition of each database\n\nare discussed in the following sections as well as the motivation behind including these databases.\n\n2.1.1 Facial Recognition Grand Challenge\n\nThe Facial Recognition Grand Challenge database (FRGC) [57] was collected at the Uni-\nversity of Notre Dame. The database is composed of full frontal still images. Images were collected\nfrom 568 subjects under varying lighting conditions and with different facial expressions, see example\nimages in Figure 2.1. The images were collected on campus over the 2002-2003 and 2003-2004 aca-\ndemic school years. As a result, the majority of subjects in the database are between the ages of 18\nand 27 years old. The approximate ages of the subjects at the start of collection in 2002 can be seen\nin Figure 2.2a. With a median age of 19, this database will not be used in age experiments; however,\nthe distribution of the subjects by gender, Figure 2.2c, and ethnicity, Figure 2.2b, show enough\ndiversity for FRGC to be used in reliability experiments for gender and ethnicity classification.\n\nThe FRGC database was chosen for reliability experiments for several reasons. First, the\n\nFRGC database is widely used in facial recognition research [49, 58] and gender classification [37,",
    "Page_24": "Figure 2.1: Example images from the FRGC database.\n\n38, 47|. This database was included to provide a basis of comparison with some of the previous full-\nface and partial-face studies performed in gender and ethnicity classification. Second, the images are\nhigh-resolution. The high-resolution allows for skin texture to be captured in detail. Experiments on\nthe high resolution images will provide a baseline for experiments performed on the other databases\nwhich are captured at a lower resolution.\n\nNot all the data in FRGC is usable, and the data that is usable must first be processed. The\nsubjects corresponding to the Unknown ethnicity label cannot be used in ethnicity experiments, so\nthose images are discarded for both gender and ethnicity experiments. Only one subject exists in the\ndatabase in the Middle Eastern ethnicity label. Since distinct subjects cannot be used for training\nand testing with only one subject, this class will be discarded. Images taken under uncontrolled\nlighting conditions were discarded, as well as images where the subject was wearing glasses. Four\nimages were used for each subject. Some subjects did not have four which met these requirements\nand were excluded. The total number of subjects for FRGC experiments is 535. With four images per\nsubject, the number of face images in FRGC experiments is 2,140. The average interocular distance\nfor these images is 270.32 pixels. Images were not sorted on expression, so a subject can have images\nwith either a neutral or smiling expression. A breakdown of the subjects in the experiment set can\n\nbe seen in Table 2.1. Other considerations and preprocessing steps will be discussed later.\n\n2.1.2 Craniofacial Longitudinal MORPHological Face\n\nThe Craniofacial Longitudinal Morphological Face database (MORPH) [63] is a collection\n\nof facial images taken from public records. The images were taken under real-world conditions and\n\n16",
    "Page_25": "Middle\nEastern\n0%\n\n  \n\n \n\nCd)\n\nFigure 2.2: Distribution of the FRGC dataset according to a) age, b) ethnicity, and c) gender.\n\n \n\n \n\n \n\nFRGC MORPH\nEthnicity Gender Ethnicity Gender\n\nWhite 379 (4) ] Male 309 (4) White 2,606 (2.7) | Male 11,182 (2.7)\nAsian 119 (4) | Female 226 (4) Asian 50 (2.6) | Female — 2,132 (2.7)\nHispanic 15 (4) Hispanic 529 (2.8)\n\nIndian 12 (4) Black 10,056 (2.7)\n\nBlack 10 (4) Native American 13 (3.1)\n\nTotal 535 (4) 535 (4) 13,314 (2.7) 13,314 (2.7)\n\n \n\n \n\n \n\nTable 2.1: Breakdown of subjects in the FRGC and MORPH experiment sets by gender and ethnicity.\n[he average number of images per subject in each class is given in parentheses.\n\n \n\nnot by researchers under controlled conditions. Example images from this database can be seen\nin Figure 2.3. The MORPH database was formed for the purpose of studying age progression. It\ncontains multiple images of an individual spanning both large and small age gaps. The MORPH\ndatabase has two subsets, Album 1 and Album 2. The MORPH public release [61], used for this\nwork, contains all of Album 1 and a subset of Album 2. This version contains over 55,000 images\nof more than 13,000 subjects. Subjects in the dataset range from 16 to 77 years old with a median\nage of 33. Gender and ethnicity information are included along with the age information for this\ndataset, as seen in Figure 2.4, providing ground truth for later experiments. The resolution of images\nin MORPH is either 200 x 240 or 400 x 480.\n\nThe MORPH database, chosen for reliability and age experiments, was included for several\n\n17",
    "Page_26": "Figure 2.3: Example images from the MORPH database.\n\nreasons. MORPH Album 2 has been used in previous work [27] for ethnicity estimation on faces.\nMORPH includes five ethnicity labels that provide the diversity needed for ethnicity classification.\nAlso, since the data was captured under real-world conditions and not in a research environment [51],\nresults from MORPH should provide a more accurate measure of performance of the proposed\nmethods in real-world applications. The demographic distribution provides the opportunity to see if\nperformance is impacted by any specific demographic, which is the final reason MORPH was chosen\nfor this work.\n\nAs with the FRGC database, some of the data in MORPH cannot be used. Images of\nsubjects belonging to the Unknown class will be discarded. The Unknown class is not useful for\nsupervised learning techniques. The image list for MORPH was created by selecting a maximum of\nfour images per subject. Some subjects do not have four images in the database. These images were\nexamined to determine if the point fit was good and the subject was not wearing glasses. A subject\nwas excluded from the experiment only if all of his or her candidate images did not meet the criteria\nmentioned above. The experiments on the MORPH data set included 35,601 face images of 13,314\ndistinct subjects. The breakdown of subjects in the chosen experiment set can be seen in Table 2.1\nalong with the average number of images per subject. The average interocular distance for images\n\nused in MORPH experiments was 93.51 pixels.\n\n2.1.3 Pinellas\n\nThe Pinellas database is a collection of booking (mug shot) images from the Pinellas County\nSheriff’s Office! in Florida. Example images can be seen in Figure 2.5. With approximately 1.4\n\nmillion facial images from over 400,000 subjects, this database is one of the largest facial databases\n1The mug shot data used in these experiments were acquired in the public domain through Florida’s “Sunshine”\n\nlaws. Subjects shown in this manuscript may or may not have been convicted of a criminal charge, and thus should\nbe presumed innocent of any wrongdoing.\n\n18",
    "Page_27": "Cd)\n\nFigure 2.4: Distribution of the MORPH dataset according to a) age, b) ethnicity, and c) gender.\n\n \n\nFigure 2.5: Example images from the Pinellas database.\n\nto date. The distribution of images according to age, gender, and ethnicity can be seen in Figure 2.6.\nImages in the database average 480 x 600 pixels.\n\nThis dataset was chosen for its size as well as the ethnic and age diversity present in the\nimages. Pinellas is much larger than MORPH and provides plenty of images for the application\nexperiments. It was also included for reasons similar to MORPH. The data was captured under real-\nworld conditions, at the sheriff’s office, and so can provide a more accurate measure of performance\nof the proposed methods in the real world.\n\nAs with the MORPH and FRGC databases, images of subjects belonging to the Other class\nwill be excluded from experiments. Further details on the selection of images from this dataset can\n\nbe found in Chapter 6.\n\n19",
    "Page_28": "Oriental /\nAsian\n1%\n\nOther\n0%\n\nAmerican\nIndian\n0%\n\n \n\n(a) (b)\n\n \n\nCd)\n\nFigure 2.6: Distribution of the Pinellas dataset according to a) age, b) ethnicity, and c) gender.\n\n2.2 Preprocessing\n\nThe first steps of preprocessing are to annotate the facial images and correct for in-plane\nrotation. All images were processed with a script using the VeriLook 5.4 Standard SDK. Included\nin the SDK is the FaceExtractor component which extracts 68 facial feature points. These points\ninclude locations for the eyes, nose, mouth, and chin, and can be seen in Figure 2.7. The fit of the\npoints was another consideration for including images in the experiment sets. If all 68 points were\nnot detected in an image, it was automatically discarded. Using the eye centers from the detected\nfeature points, the faces are rotated so the eyes are level in the image plane. The rest of the points\nare updated as well to be used later for separating the face into its subregions and extracting the\nface. More information on VeriLook can be found at the end of this chapter.\n\nFigure 2.8 shows example facial regions of the chin, mouth, nose, nose tip, and eyes. A subset\nof the 68 points detected by VeriLook, shown in Figure 2.9, were used to extract facial regions. The\neye regions are centered on the eyes (points 4 and 5) with a width equal to the distance between\nthe centers and a height equal to the vertical distance from the eye centers to the center of the nose\n(12). The nose tip region starts midway between the eye centers (4, 5) and the nose center (12) and\nextends to midway between the nose center and the mouth center (11). The left and right boundaries\ncorrespond to the z-coordinates of the eye centers. The nose region starts at the y-coordinate of\n\npoint 6 and extends to midway between the nose center (12) and the mouth center (11). The\n\n20",
    "Page_29": "FRGC\n\nMORPH\n\nPinellas\n\n \n\n \n\nAvg size Exp size Avg size Exp size Avg size Exp size\n\nFace | 482x486 480x480 | 195x196 200x200 | 246x248 250250\nEyes | 220x120 220x120] 91x41 90x40 | 114x68 = 120x70\n\nNose | 146x172 150x170 | 69x68 70x70 80x94 80x 100\nN Tip | 220x119 220x120 | 91x47 90x50 | 114x64 12070\nMouth | 261x144 260x150 | 102x59 100x60 | 124x70  130x70\nChin | 261x90  260x90 | 102x37  100x40 | 124x45  130x50\n\n \n\n \n\n \n\n \n\nTable 2.2: The average size of the regions from raw images and\n\n \n\nthe resolution the regions were\n\nresized to for experiments. The images used in calculations were those that had a face and all 68\npoints detected by VeriLook script: 39,250 of 39,328 for FRGC, 53,964 of 55,608 for MORPH, and\n1,436,799 of 1,447,607 for Pinellas.\n\n \n\n(a)\n\n \n\nFigure 2.7: Example feature points detected by VeriLook SDK a) good fit on FRGC image, b) bad\n\nfit on MORPH image\n\n21",
    "Page_30": "Figure 2.8: Example facial regions used for experiments, extracted from an FRGC image.\n\n \n\nleft and right boundaries are 5 pixels outside the z-coordinates of points 7 and 8 on the outline\nof the nose. The mouth region starts halfway between the mouth (1) and nose (12) centers and\nextends to halfway between the mouth center (11) and the chin (2). The left and right boundaries\nare midway between the z-coordinates of the mouth corners (9, 10) and the closest points on the\nface contour (1, 3). The chin region keeps the same left and right boundaries of the mouth region.\nThe chin region starts midway between the mouth center (11) and chin (2) and extends 5 pixels\nbelow the chin tip (2). Table 2.2 show the average resolution of each of the facial regions for each\ndataset. Based on these calculations, an experiment size was chosen for each region and dataset. All\nface regions were resized to the experiment resolution before preprocessing continued and feature\nextraction began. The extracted face region for baseline comparison experiments extends over the\nentire are were points were detected, from the eyebrows to the chin. Prior to further processing an\nelliptical mask of neutral color is placed around the face to minimize the effect of the background\non performance.\n\nThe next preprocessing step is to enhance the contrast of the image. Two methods are\nused. For texture-based features that will be extracted from a grayscale image, a simple histogram\nequalization is performed. The contrast enhancement procedure is slightly more complex for color-\nbased features. In order to preserve the relative color information between color channels, the image\nis first converted to the L*ab color space. Illumination is stored in the L channel, while the other\nchannels hold the color information. Histogram equalization is performed on the L channel and the\nimage is converted back into RGB space. An example of the preprocessing steps for the face and\none of the eye regions can be seen in Figure 2.10.\n\nA local, patch-based approach is used for feature extraction with both the texture- and color-\n\n22",
    "Page_31": "Figure 2.9: Point subset from VeriLook annotation used to extract facial regions.\n\nbased features. Each facial region image is divided into smaller images, called patches. Features\nare extracted from each patch and the feature vectors are concatenated to form the feature vector\nfor the entire image. The MORPH and Pinellas experiments use a patch size of 10x10 pixels and\nthe FRGC experiments use a patch size of 2020 pixels. Since the FRGC images are of greater\nresolution, the patch sizes are larger so that the patch size relative to the image size is similar to\nMORPH and Pinellas. This also allows for a comparable number of features to be extracted from\n\neach of the datasets.\n\n2.3 Feature Extraction Methods\n\nThree main types of information can be found in 2D facial images: texture, color, and shape.\nFace images provide texture in both the skin and hair portions of the image. Color information can\nbe gathered to provide hair, eye, and skin color. Moles and birthmarks can also be indicated by\ncolor. The outside contour of the face and position of the eyes, nose, mouth, and chin within the\nface give shape information that can be used for classification. The following chapters detail feature\n\nextraction methods and experiments performed in each of the three main categories of features.\n\n23",
    "Page_32": "Figure 2.10: Preprocessing of a FRGC image following facial annotation. The original image is to\nthe far left. The next image is corrected for in-plane rotation. The LEYE and FACE regions are\nextracted. The smaller images to the right represent the preprocessing steps for color (top) and\ntexture (bottom) features for each region. The first images are contrast enhancement and resizing.\nThe final images represent the patches used for feature extraction.\n\n2.4 Feature Reduction Methods\n\nFeature reduction techniques seek to eliminate noise and non-essential information from\nthe feature vectors being used for classification. In this sense, feature extraction methods can be\nconsidered feature reduction techniques when the output of the extraction method is smaller than the\noriginal number of pixels in the image. Non-essential information could still be useful information,\njust not for the current problem. For example, information about a mole might be useful to help\nidentify an individual, but not for gender classification. For this work, there is more interest in\nretaining features that encode gender or ethnicity information or even a combination of both, rather\nthan identity.\n\nPrincipal Component Analysis (PCA) is widely used for data compression and feature re-\nduction. It is very useful for reconstructing data and is the basis for the well-known facial recognition\nalgorithm “EigenFaces” [70]. PCA, known originally as the Karhunen-Loéve transform, finds the\ndirections of maximum variance for a training set, regardless of class, using the eigenvectors of the\ncovariance matrix.\n\nFor feature reduction using PCA, some of these eigenvectors are discarded; those corre-\nsponding to the smallest eigenvalues. The larger the eigenvalue, the more the variance represented\n\nby that eigenvector is present in the training set. In practice, researchers normally keep the eigen-\n\n24",
    "Page_33": "vectors that account for 95% of the variance. This can be determined by how many eigenvalues,\nfrom largest to smallest, are needed to have 95% of the sum of all the eigenvalues. The retained\neigenvectors are used to build a projection matrix. Features are projected into the PCA space by\nmultiplying by the projection matrix. The Eigen library [25] was used to implement PCA for this\nwork.\n\nAnother type of feature reduction is Linear Discriminant Analysis (LDA). This method is\ndesigned for classification problems with multiple examples of each class. This is another transform,\nsimilar to PCA, which projects the features into a lower-dimensional space; however, LDA takes\ninto account the different classes that are present in the data. In this transform, the variance\nbetween classes is maximized, while the scatter within each class is minimized. By incorporating\nclass information into the training, LDA does better at discriminating between classes than PCA\nin most classification cases. The number of features retained is equal to the number of classes used\nin the analysis. In preliminary experiments on FRGC, using LDA for feature reduction resulted\nin a large increase in training time, a large decrease in features used, but only small increase in\nperformance for some features. Performance on some actually decreased. For this reason, feature\n\nreduction was limited to PCA.\n\n2.5 Classification Methods\n\n2.5.1 k-Nearest Neighbor\n\nThe k-nearest neighbor (k-NN) is one of the simplest classification algorithms available to\nresearchers. k-NN is a simple classifier which relies on the training data. No training is necessary\nbefore classification starts. A test sample is classified according to the class of its k closest neighbors.\nEach neighbor votes and whichever class has the most votes is the class assigned to the sample. In\npractice, k should be odd to avoid ties, although this is not enforced within the algorithm. The\nFLANN library within OpenCV was used for experiments with k-NN classification.\n\nThe closeness of the neighbors can be determined by any distance metric. The choice of\ndistance metric often depends on the type of data and the area of the problem. Common distance\nmetrics used for any real-numbered data are the Manhattan, Euclidean, and Maximum distance\n\nmetrics. These are also known as the L;, L2, and L.. norms respectively. These three distance",
    "Page_34": "metrics are derived from the Minkowski or p-norm measure, which can be defined as\n\nn\n\ni\n\nLp(w,y) =. |e — vil)”,\ni=l\n\nwhere x and y are vectors with length n. Euclidean distance is the case where p = 2 and corresponds\nto the intuitive idea that the distance between two points (in two dimensions) is a straight line.\nManhattan distance, also known as city-block or taxicab, is the case where p = 1. The more\ncolorful names stem from visualizing the distance measure on city streets. Manhattan distance is\nthe distance a cab would have to travel on the street around square city blocks to get between two\npoints, assuming all streets are two-way. Maximum distance is the case as p approaches infinity.\nThis simplifies to simply the maximum of the distances between each dimension. This is also known\nas Chebyshev distance.\n\nMany of the features presented in this work will be in the form of histograms. Widely used\ndistance measures for histogram data are the Chi-Square, Histogram Intersection, and Hellinger\ndistances. The Chi-Square (x7) distance is a weighted form of Euclidean distance with differences\nbetween larger elements being less important than differences between smaller elements. This dis-\ntance can be defined as\n\nn\n\nLQ (ei = yi)?\n2 st 4 i,\n(ay) = 5 > aay\n\ni=1\nThe Histogram Intersection distance measure starts out as a similarity metric, measuring the overlap\n\nbetween the two histograms. It is transformed into a distance metric by normalizing and subtracting\nfrom 1, as given by the follow equation:\n\nno\n\nYe min(wi, yi)\n\nHI (x,y) =1- =.\nlyl)\n\n \n\nmin(|a\n\nThe Hellinger distance, very similar to the Bhattacharyya distance is used in probability and statis-\ntics to provide a measure of similarity between two probability distributions. Hellinger distance\n\nbetween two discrete distributions is calculated as follows:\n\n \n\n26",
    "Page_35": "2.5.2 Support Vector Machine\n\nSupport Vector Machines (SVM) are a popular classification method in gender and ethnicity\nclassification [5, 9, 31, 38, 44, 47, 60]. As a supervised machine-learning algorithm, the SVM requires\nlabeled training samples. The underlying idea of SVM training is to find the hyperplane that best\nseparates the training samples. In kernel-based SVMs, the samples are transformed by the kernel\ninto a higher dimensional feature space. This allows more room for the best separation to be found.\nThe algorithm searches for the best hyperplane that separates the data with the largest margins\non either side of the hyperplane to the training data. The training samples used to define the\nbest hyperplane are known as support vectors. These vectors are stored with the equation of the\nhyperplane for testing.\n\nSVMs are inherently binary classifiers, but can be modified to work with multiple classes.\nThe LIBSVM library [13], which was used for SVM classification, implements the one-against-one\n\nk(k—1)\n2\n\n \n\napproach. If there are k classes, the multi-class SVM consists of (4), or , binary classifiers.\nThe final class-decision of the multi-class SVM is the class that ‘won’ the most binary decisions.\nExperiments performed with SVM classification used linear kernels. Radial Basis Function\n(RBF) kernels were investigated since literature shows that RBF SVM have equal or greater perfor-\nmance than linear SVMs. Unfortunately, these SVMs are very sensitive to chosen parameters. Using\na tool provided in LIBSVM and features from one image per subject, for a given subset of subjects,\na grid search was performed for cost (C) and gamma (7) parameters. The cost parameter is the\npenalty given for misclassifying a sample and ¥ is a kernel specific parameter. Features from all\nimages were not used to increase the speed of the parameter search and to avoid over-fitting. After\nchoosing parameters with this technique, test results were mostly of only one class, which indicates\nthe chosen parameters were not in the correct parameter space to achieve optimal performance. For\n\nthis reason, RBF results are omitted.\n\n2.5.3 Artificial Neural Network\n\nArtificial Neural Networks (ANN) are a supervised machine-learning algorithm developed\nto imitate the way scientists believe the human brain works. Individual neurons are highly inter-\nconnected and are the basic blocks which compose the brain. In an ANN, a neuron is modeled by\n\na weight vector of its incoming edges and an activation function. Networks can have any number of\n\n27",
    "Page_36": "layers of neurons, but many have just three, an input layer, a hidden layer, and the output layer.\nANNs use labeled training data. Training is an iterative process with labels being computed, an\nerror function evaluated, and the weights for each neuron being updated. The final ANN models\nthe relationship between inputs and outputs and can capture patterns present in the data.\nClassification for multiple classes can be achieved by using an output layer of c neurons\nwhere c is the number of classes. Each class corresponds to a position in the output vector. The\nposition that has a positive value indicates membership in the corresponding class, while all other\npositions are zero or negative. This is the method used in the classification experiments within this\nwork. ANN classification was implemented using the machine-learning section of OpenCV 2.3.1\nas well as the Fast Artificial Neural Network Library (FANN) [23, 52]. All ANNs used in this\nwork are 3-layer with weights from training calculated by the resilient backpropagation (RPROP)\nalgorithm. The RPROP algorithm is the default training method for OpenCV and is usually faster\nthan classic backpropagation. Hidden layer sizes of 50, 100, and 200 were chosen for experiments to\n\nshow performance with small and larger hidden layers.\n\n2.6 General Experiment Setup\n\n2.6.1 Performance Measures\n\nIn many classification experiments, the overall accuracy is used to report the performance.\nOverall accuracy is defined as the percentage of instances classified correctly regardless of the class\nor label for each instance. Another performance measure used in classification and machine learning\nprovides a finer level of detail. A confusion matrix, or matching matrix, looks at both the predicted\nclass and given class of an instance and places it into the correct position of the table. The rows\nof the table represent the given class while the columns represent the predicted class. This allows\nan observer to see the similarity between classes by looking at the misclassification rates of a given\nclass to each of the other classes. Within the confusion matrix, the correct class-wise classification\ncan be found along the diagonal. This is useful in instances when the test data is not balanced by\nclass. An average class-wise accuracy would give a better idea of the performance of the classifier\nacross the classes instead of the overall average which can be biased by an unbalanced set.\n\nIn many cases, a box plot of the class-wise accuracies will be shown. A box plot is a way\n\nto graphically represent groups of numbers. The box itself represents the first and third quartiles of\n\n28",
    "Page_37": "— method!\n100 >\n\n—method 1 5 : : — method2\n\n—method 2\n\nx\n\n \n\nFRR(%)\nMiss probabil\n\n  \n\n0 20\n\n40 60 80 100 0.10.2 05 1 2 5 10 20\nFAR(%) False Alarm probability (in %)\n\n40\n\nFigure 2.11: Example ROC and DET curves. (L to R): ROC, DET.\n\nthe data while the line inside represents the median of the data. The whiskers of the box are the\nlargest and smallest values within 1.5 x IQR (interquartile range) which is the difference between\n\nthe first and third quartiles. Points that do not fall within +IQR of the box are marked as outliers\n\n \n\nand plotted as points on the graph.\n\nIn biometric verification/authentication problems, a curve known as the Receiver Operating\nCharacteristic (ROC) is used as a measure of performance. In this problem, a threshold is used on\nthe match score by the system to determine whether two samples are from the same individual or\ndifferent individuals. The ROC curve is created by varying the threshold from its lowest to highest\npossible values. At each threshold, a certain number of individuals are accepted or ruled as the\nsame person, when they should not be. These are known as False Matches or False Accepts. At\nthe same time, genuine matches are ruled to be different people and rejected. These are known as\nFalse Rejects or False Non-matches. The axes on the ROC curve are the False Accept Rate (FAR)\nand the False Reject Rate (FRR). An example two ROC curves can be seen in Figure 2.11. The\ncurve which is closest to the origin is considered the best. In this instance ‘method 1’ outperforms\n‘method 2’. Another measure to evaluate the performance is the Equal Error Rate (EER). This is\nthe point on the graph where the FAR is equal to the FRR, in other words, the point where the\ncurve intersects the diagonal in the graph. It is generally accepted that better systems will have a\nlower EER.\n\nAnother curve, the Detection Error Trade-off curve (DET), can be used to display the FRR\n\n29",
    "Page_38": "—method 1\n—method 2\n\nCumulative Performance (%)\n\n \n\n \n\n150 200 250 300 350 400\nRank\n\nFigure 2.12: Example CMC curve\n\nversus the FAR in a different format from the ROC. Instead of scaling the axes linearly, a logarithmic\ntransformation is used on both the z- and y-axes. This results in a more linear trade-off curve than\nthose seen in the ROC curves. An example can be seen in Figure 2.11. The EER can still be found\nby where the curve intersects a line going diagonally from the origin to the upper right corner. The\ncurve which is closest to the origin is generally considered the best in these graphs.\n\nIn biometric recognition/identification problems, a curve known as the Cumulative Match\nCharacteristic (CMC) is used as a measure of performance. The CMC curve is created by sorting\nthe match scores for each given probe. The axes on the curve are the rank in the sorted list (1... N,\nwhere N is typically the number of entries in the gallery) and the performance of the system at the\ngiven rank. In other words, the performance at Rank-K is the percentage of probes where the true\nmatch is found in the first A entries of the sorted match scores. The higher the Rank-1 performance\nand the faster the curve approaches 1 or 100%, the better the system when comparing CMC curves.\nAn example of two CMC curves can been seen Figure 2.12. In this figure, ‘method 1’ has a higher\nRank-1 performance than ‘method 2’. Both methods reach 100% very late, but ‘method 1’ would\nstill be considered the better of the two, since its curve is higher than the curve for ‘method 2’\n\nthroughout the graph.\n\n2.6.2 Cross-Validation Evaluation\n\nReliability and age experiments for color, shape, and texture are evaluated using a stratified\n\nfive-fold cross-validation (CV) approach. In a five-fold CV experiment, the subjects are divided into\n\n30",
    "Page_39": "five parts, keeping the proportions of classes in each part approximately equivalent to the proportions\nof the whole set. In each fold, a smaller experiment takes place. Images, or features from images,\ncorresponding to the subjects from four parts are used for training while the other set is used for\ntesting. For each fold, the part used for testing changes. In this way, each subject appears in the\ntest set only once in the whole CV experiment. Cross-validation is done based on subjects instead of\nimages so that images of the same subject will not appear in both the training set and testing set for\nany particular fold when multiple images are used per subject. The performance of a CV experiment\nis normally the average overall classification performance of the folds. Confusion matrices will also\n\nbe shown to view the distribution of classifications for each class.\n\n2.6.3 Biometric Application Experiments\n\nApplication experiments will have a different set-up than cross-validation. Specific images\nare partitioned to be used solely for training the classifiers. A list of gallery images was created with\ntwo images per subject. A list of probe images was created with one image per subject using the\nsame subjects as the gallery list. Subjects in the probe and gallery sets are distinct from subjects in\nthe training sets. Classification results on the probe will be used to filter which gallery entries will\nbe used in recognition and verification experiments. ROC and CMC graphs will be used to measure\nthe performance of these experiments as well as the EER and Rank-1 performance. Classification\nresults will be reported with confusion matrices and average class-wise accuracies.\n\nThe match scores for the base face experiment will be calculated using the VeriLook 5.4\nStandard SDK previously mentioned during preprocessing. This is a commercial software develop-\nment kit which supplies biometric functionality to its users. Functionality includes face detection\nand annotation, template generation, gender prediction, and template matching. All images that\nhave templates generated for matching must pass a quality check, including lighting conditions\nand a minimum size requirement. More information can be found at the Neurotechnology website,\nhttp: //www.neurotechnology.com/vl_sdk.html.\n\nThis concludes the description of data and methods to be used. The next three chapters\nwill include results and discussion from reliability and age experiments for both gender and ethnicity\n\nclassification using color, shape, and texture information. Color experiments will be discussed first.\n\n3l",
    "Page_40": "Chapter 3\n\nColor\n\nImages in digital color formats are normally saved in three channels. Therefore, each location\nin the image, known as a pixel, has three values associated with it. The values for the pixel depend\non the color space the image is stored in. The most prevalent color space, or more accurately color\nmodel, is the RGB space. In this color space the values for each pixel represent the amount of red,\ngreen, and blue present in that particular location in the image. Some color spaces were developed\nto be independent of the device the image was captured with while others were developed as more\ndevice specific. With the exception of the RGB color space, the color spaces mentioned here divide\n\nluminance- or lighting-type from the chroma or color information.\n\n3.1 Feature Extraction Methods\n\nColor has been a useful feature in the area of content-based image retrieval. One simple\ncolor representation is Local Color Histograms (LCH), used in previous work for gender and ethnicity\nclassifications on pericular images [46]. As mentioned before, each color pixel in an image has an\nintensity value for each channel, red, green, and blue for example. For this work, in the RGB\ncolor space, the red and green intensity values are quantized into four levels. Using both values to\ncount occurrences in a two-dimensional histogram, a feature vector is produced of length 4x4 or\n16 elements per patch. These parameters were chosen based on findings in preliminary work [74].\n\nFeatures from other color spaces will use the two channels that contain color information.\n\n32",
    "Page_41": "Figure 3.1: RGB and HSI histogram visualization. Left: RGB, RG Histogram. B at 0%. Right:\nHSI, HS Histogram. I at 50%.\n\n3.2 Color Spaces\n\nA brief description for each of the color spaces that are used in this work follows, as well as\na visualization of the colors found in each histogram bin. Since the histograms involve two channels,\n\nwhile images are composed of three, the value in the third channel is held constant.\n\n3.2.1 RGB\n\nThe RGB color model is an additive color model. The vales for red, green, and blue are\nadded together to form all the other colors available in this space. This model is based on the light\nspectrum where light of different wavelengths combine to produce color. As mentioned before, RGB\nis not a specific color space, but rather a color model. In a typical RGB image, values are quantized\nto 256 different levels. Figure 3.1 shows the various colors that can be found in each bin of a 4 x 4\n\nred-green histogram with no blue added. Other colors are possible with different levels of blue.\n\n3.2.2 Device Dependent\n3.2.2.1 HSI\n\nThe HSI color space is a cylindrical representation of points found in the RGB color model.\nThe purpose of this arrangement is to more closely imitate how artists choose colors with a color\n\nwheel or a palette. The information for each pixel is divided into hue (H), saturation (S), and\n\n33",
    "Page_42": "Figure 3.2: YIQ and YCbCr histogram visualization. Left: YIQ, IQ Histogram. Y at 25%. Right:\nYCbCr, CbCr Histogram. Y at 25%.\n\nintensity (I) values. The intensity value gives the height on the cylinder, saturation the distance\nfrom the center, and hue gives the angle from the center of the cylinder. Other variations of this\ncolor space are the HSV and HSL spaces. The value (V) and lightness/brightness (L) channels are\nslightly different than the I channel of HSI, but mostly all represent the grayscale image with no\ncolor information. Hue information is roughly the same for all of the spaces, but the definition of\nsaturation between the three color spaces is very different. Figure 3.1 shows the various colors that\n\ncan be found in each bin of a 4 x 4 histogram with 50% illumination.\n\n3.2.2.2 YIQ\n\nThe YIQ color space was originally developed for use with televisions. The main usefulness\nwas that images could be sent to both color and non-color television sets because the luminance\ninformation was roughly captured in the Y channel. The I stands for in-phase and Q stands for\nquadrature, referring to components used in quadrature amplitude modulation. The I channel\nencodes color in the orange-blue range while the Q channel encodes color in the purple-green range.\nFigure 3.2 shows the various colors that can be found in each bin of a 4 x 4 histogram with 25%\n\nillumination.\n\n34",
    "Page_43": "3.2.2.3. YCbCr\n\nThe YCbCr color space is widely used for digital video storage and is related to the YIQ\ncolor space. The luminance information is found in the Y channel, just as it is in the YIQ space.\nThe Cb and Cr channels encode the color information. The color information is stored as difference\ncomponents. The Cb channel is the difference between the blue component of a pixel and a blue\nreference value, while the Cr channel is the same with the red component of a pixel. Figure 3.2\n\nshows the various colors that can be found in each bin of a 4 x 4 histogram with 25% illumination.\n\n3.2.3. Device Independent\n\nThe color spaces in this section were developed by the International Commission on Illumi-\nnation (CIE). For this reason, these color spaces are often prefixed by CIE. To transform an image\nfrom an RGB space, the image must first be converted to the CIE XYZ color space. The XYZ\nspace was one of the first mathematically defined color spaces, specified in 1931 by the CIE, and is\nthe basis for the following spaces. Once there, conversions may be made to all the other CIE color\n\nspaces.\n\n3.2.3.1 LAB\n\nThe intention of the CIELAB, or L*a*b*, color space was to produce a color space where\na change in the color value should produce the same amount of change in the perceived color. It\nwas designed to approximate human vision and was adopted by the CIE in 1976. The CIELAB\nspace was used during the contrast enhancement of the color images. The luminance information\nis stored in the L* channel while the color information was stored in the other two channels. The\na* channel holds information in the red to green range, while the b* channel holds information in\nthe blue to yellow range. The reason this color space was not used for color histograms was that no\n\nhard boundaries could be found for the color channels a* and b*.\n\n3.2.3.2 LUV\n\nThe CIELUV color space also attempts to gain perceptual uniformity as mentioned for the\nCIELAB color space. It was adopted by the CIE in 1976. This space is widely used in computer\n\ngraphics and other applications which use colored lights, as it is an additive space. It is related to",
    "Page_44": "Figure 3.3: LUV and LCH histogram visualization. Left: LUV, UV Histogram. L at 25%. Right:\nLCH, CH Histogram. L at 50%.\n\nCIELAB, in that each of the color spaces preserve the same L channel, but the chroma information\nis represented differently. Figure 3.3 shows the various colors that can be found in each bin of a\n\n4x 4 histogram with 25% illumination.\n\n3.2.3.3 LCH\n\nThe CIELCH color space is related to the CIELAB color space. It is a cylindrical version\nof CIELAB where C is the chroma channel and H is the hue channel. As with the CIELAB and\nCIELUV color spaces, the LCH color space attempts to gain perceptual uniformity for changes in\ncolor values. Figure 3.3 shows the various colors that can be found in each bin of a 4 x 4 histogram\n\nwith 50% illumination.\n\n3.3. Experiment Setup\n\nThe experiments for this section will be performed on subsets of the FRGC and MORPH\ndatabases, using the color images. Table 2.1 shows the breakdown of the subjects according to gender\nand ethnicity for the subsets used. Features are extracted using the RGB, HSI, LCH, LUV, YCbCr,\nand YIQ color spaces. Classification of gender and ethnicity for all six color spaces will be performed\nover all regions using the k-NN classifiers (Zi, L2, and L..). SVM and ANN classification will be\n\nperformed on three of the color spaces, RGB, HSI, and LCH. The total number of color experiments\n\n36",
    "Page_45": "is 180 for each dataset and demographic. Five runs of a stratified cross-validation experiment are\n\nperformed for each feature, region, and classifier combination.\n\n3.4 Analysis\n\nGlobal feature vectors were created to look at the color histograms within each region and\ncolor space. The two-dimensional histograms were flattened by taking rows from top to bottom.\nFigure 3.4 shows the average global features from the MORPH dataset. Many of the global feature\nvectors look similar over multiple regions. For all but the RGB features, the value in the majority\nof the bins is close to zero, indicating that colors within the region were limited. The range of skin\ntones is so small compared to the entire range of the color space, that a 4 x 4 histogram over the\nentire space may not be sufficient to capture differences due to ethnic group or gender. Since the\nglobal vectors for each region are similar, the colors found in each region are similar to each other\nas well. This indicates that the different regions of an individual’s face are of a similar color. This\nis logical provided make-up, tattoos, and birthmarks do not dominate any specific region of the\nface. Differences between all combinations of regions will be calculated to further investigate the\nsimilarity of color over the entire face.\n\nThe features from the YCbCr color space are mostly concentrated in a single bin for the\nFRGC database, so this color space may not be very useful for classification purposes with the\ngranularity used. It is possible that a 2D histogram with a finer granularity would capture more\ndifferences in the classes, but for now, it is not likely that the YCbCr features will work well for\ngender and ethnicity classification. In the MORPH dataset another very small peak is apparent\nin these features, which may increase the usefulness of the YCbCr space in differentiating between\nclasses.\n\nDifferences between the global feature vector for each region were computed for each facial\nimage to gauge the similarity of color over the face. The distance measures used were Ly, L2, Loo,\nHistogram Intersection, Hellinger, and y?. Means were calculated according to each gender and\nethnicity class. Similar trends were seen with each of the distance measures. Figure 3.5 shows the\ndifferences between regions in the RGB and HSI spaces using the Histogram Intersection distance\nmeasure for the FRGC dataset. The LCH, LUV, and YIQ color spaces had results similar to the\nRGB graphs shown.\n\n37",
    "Page_46": "02\n\nRelative frequency\nRelative frequency\n\n0.05\n\n \n\n6 8 10 12 14\nBin number\n\n°\n\nRelative frequency\n8\nRelative frequency\n\n°\n\n  \n\n10 12 4 0 2 4 10 12 14\n\n6 8 6 8\nBin number Bin number\n\nes 2s ee\n\nRelative frequency\ng\n\nRelative frequency\n\na)\n\n  \n\n10 12 14 0 2 4 10 12 14\n\n6 8 6 8\nBin number Bin number\n\nFigure 3.4: Average global feature vector for MORPH data. These are the normalized color his-\ntograms for each region. The z-axis corresponds to the bin in the flattened histogram. The y-axis\ncorresponds to the relative frequency of values found in each bin. Top row (L to R): RGB, HSI.\nMiddle row (L to R): YIQ, YCbCr. Bottom row (L to R): LUV, LCH.\n\n38",
    "Page_47": "Difference\n\n= Male\n\nDifference\n\n0.14 A cemale\n012\n0.\n0.08\n0.06\n0.04\n0.02\ndepp Op Wee “Cpe Che 0g ie ie Oh in Yop pp NOg N05 Me\nep Me te re te Se rey ree ey hy IP.  5¢ Se OU,\nRee Vig Nog Mon, Sipe Ving Non Mog. Say Nog Moy, Kin hg Cay\nCr ip Ose! Cunt Myo Vs! out Ose Cunt “Moustty “yy\n0.06 ® Male\n1 Female\n0.05\n0.04\n0.03\n0.02\n0.01\n(Oe Ore A Oe fe Gore Re fe fe i a in Ove Noe up,\n\n* é ‘\nen Nose Mou sity Win Moscow ty Ose Mou tin Mou ny *y,\n\n  \n  \n  \n  \n \n \n \n  \n\nWhite\nAsian\nHispanic\nBlack\n= Indian\n\nDifference\n\n0\noe Abe, fe on fe fe, ne, ihe, oe Nop Vip Nip, bes fOsetOun,\nug Min Nose Mog My Mose Ou ty Moy\n\nee ip Vos, Win, Mey\n‘e m Urge,\n\n= White\nAsian\n1 Hispanic\nBlack\n= Indian\n\n0.2\n\n0.15\n\nDifference\n\n0.05\n\nMp Mos los Moy,\n\n0-\ney,\n‘e, 6.8, my\nDp Voge! Outi Pp Mosehousiny Ose! oun Mo, ny,\n\nAer pe fe fe Fea ere fer fg Yin rn,\nhe\n\nFigure 3.5: Mean difference between FRGC global color region vectors by demographic. Differences\n\nshown are for the RGB (top) and HSI (bottom)\ndistance.\n\n39\n\ncolor spaces using the Histogram Intersection",
    "Page_48": "Overall, the differences between the regions were small, further indicating that color is\nfairly stable over the human face. The two pairs of regions most similar to each other in all the\ncolor features were LEYE/REYE, and NOSE/NTIP. The eye regions both hold an eye and for the\nmajority of the population, both eyes are the same color. Congenital heterochromia iridis, two\ndifferent colors in the eyes from birth, only occurs in approximately 6 out of 1,000 births [17], and\nin some cases is hardly noticeable. In this work, the nose and nose tip regions overlap. For these\nreasons, it makes sense that these two pairs of regions are the most similar out of all combinations.\n\nThe two pairs of regions least similar to each other in the RGB, LCH, LUV, and YIQ color\nspaces were the NOSE/CHIN and NTIP/CHIN pairs. In the HSI color space, the chin region was\nfurthest away from all the other regions. The chin region is likely to have different colors for several\nreasons. In males, this region likely has facial hair which provides a larger difference than when\ncomparing regions in females. For all classes, the chin is the only facial region that is on the border\nof the face, so some neck or clothing color could also be included, depending on landmark detection\naccuracy. The nose regions are not likely to have facial hair, unless the subject has a mustache,\nwhich would partially be included in the nose regions. The nose is also prone to illumination which\nmay wash out the color information in the original RGB image.\n\nIn looking at the means of each gender class, the difference between the chin and any other\nregion is noticeably larger for males than females. None of the other regions have a noticeable trend.\nBesides the chin region, color varies similarly between both males and females. This indicates that\ncolor is slightly more stable around the face for females than males.\n\nIn many of the HSI region comparisons and several in each of the other color spaces, the\nFRGC Black and Indian classes have much higher region differences than the other classes. This\ncould indicate that color is not as consistent across the face in these demographics, but it could\nalso indicate that the colors found in these regions lie close to the histogram bin borders. Small\nchanges in color could produce very different histograms in that case. Another reason for the large\ndifferences could be that with just 10-12 subjects in the FRGC set, the mean is representing more\nof an individual case than the class as a whole. Color around the face is much more stable in HSI\nfor White, Asian, and Hispanic classes, whereas in the Black and Indian classes it resembles the\nvariance found in the other color spaces. In the MORPH dataset, the Black class spikes in the CHIN\nregion comparisons which is likely the influence of the large majority of males in the class.\n\nIn the FRGC YCbCr space, the mean differences were much, much smaller than the other\n\n40",
    "Page_49": "Age Male Female || Black White Hispanic Asian Native Total\nAmerican\n\n16-20 5,882 817 5,199 1,028 428 36 8 6,699\n21-25 5,408 879 4,681 1,161 384 59 2 6,287\n26-30 3,865 752 3,466 859 276 13 3 4,617\n31-35 3,894 878 3,484 1,091 184 9 4 4,772\n36-40 3,919 936 3,564 1,169 112 6 4 4,855\n41-45 3,439 843 3,267 957 49 0 9 4,282\n46-50 2,034 382 1,816 563 26 5 6 2,416\n51-55 1,019 180 907 277 11 2 2 1,199\n56-60 292 49 241 98 0 0 2 341\n61-77 119 14 89 44 0 0 0 133\nTotal || 29,871 5,730 || 26,714 7,247 1,470 130 40 || 35,601\n\nTable 3.1: The number of images present per demographic label and age range for the MORPH\n\ndataset.\n\ncolor spaces. The minimum distance between regions recorded was zero. This is the effect of what\n\nwas seen in the global feature vectors, all colors concentrated in just one bin for a large portion of\nthe subjects. In this color space, the mouth region was the most separated, likely due to the lips\nproviding a color not found in the same histogram bin as the rest of the face. For MORPH, this\ncolor space was more similar to the other color spaces.\n\nMORPH features will also be analyzed by age. The subjects’ ages at the time of the image\ncapture were recorded and provided with this dataset. The ages of the subjects ranged from 16 to\n77 years old. The ages were grouped in age ranges of 5 years with the last group, 61-77, covering\n17 years. This choice was made based on the low number of images present for that particular\nage group. Table 3.1 gives the number of images per class over the ages present in the experiment\nset. As the age of the subject goes above 60 years of age, the number of images present in the age\nrange drops dramatically. The over 60 subjects are also not represented in three out of five ethnic\nclasses. Since experiment sets were chosen without regard to age, it is most likely that performance\non images of older subjects will be lower than that of younger subjects. A large majority of the\ntraining images will be on younger subjects. This may distort the training space if the features\nthemselves are not age-invariant.\n\nDiscounting the 61+ group, the RGB features vary less than 5% across the age groups. The\nrest of the color spaces vary slightly more than 5% in pairs that include the chin or the mouth.\nThis indicates that there is a difference of color in the chin and mouth regions with respect to age.\n\nAs MORPH is a predominately male dataset, this could be facial hair. The eye regions do not\n\n41",
    "Page_50": "necessarily include the eyebrow, which would possibly increase the similarity. The RGB color spaces\nfeatures seem to be slightly more stable with respect to age than the other color spaces.\n\nThe region differences in the younger age groups tend to be higher in the region comparisons\nwith the most variance. The exceptions to this trend are the region comparisons involving the\nmouth. In looking at the mean differences per class, older males tend to have more variation in\nthe comparisons with the mouth than younger males, while younger females have more variation\nin these comparisons than the older ones. The male variation is likely due to facial hair, while the\nfemale variation could be a result of lipstick or make-up. More work would be required to verify a\nrelationship between age and facial hair or age and lipstick, so this remains a theory. One result of\naging that could account for this is that lips thin over time [4], providing more skin tone and less\nlip color to help stabilize color in older females. As previously mentioned, most of the images are of\n\nyounger subjects and a larger sample space allows for more variance within the classes.\n\n3.5 Gender\n\n3.5.1 Reliability\n\nNearest neighbor classification was performed using the L,, D2, and L,, distance measures\nto explore which features had classes that grouped together. PCA was performed on the features\nfor feature reduction, retaining 95% of the variance. Figure 3.6 shows the 1-NN results using gender\non the FRGC dataset. The values plotted in the box plot are the class-specific accuracies from five\nruns of each cross-validation experiment.\n\nFor most of the features in the different color spaces, the plots are fairly high and small,\nindicating that the performance for both Male and Female was good. The whiskers are all very short\nas well. The class-specific accuracies do not tend to vary much between cross-validation experiments.\nThis results in two sets of bunched numbers. With just the two classes, these two bunches determine\nthe height of the box plot, and the numbers that fall in the first and fourth quartiles will not fall far\noutside the box, resulting in short whiskers. This indicates that, most of the time, the features for\nthe Male and Female classes have a close neighbor of the same class, and the results are not highly\ninfluenced by the random choice of training samples. The MOUTH and EYE regions perform the\nbest out of the regions for several color spaces.\n\nThe class accuracies for the CHIN region and the YCbCr color space were more spread out\n\n42",
    "Page_51": "than the other experiments. For the YCbCr features, this is the result of all the features looking very\nsimilar and being concentrated mainly in one bin of the histogram. The nearest neighbor algorithm\ndoes not deal with ties. Depending on the implementation, the “closest” neighbor could be either\nthe first neighbor the algorithm saw at that distance or the last. In this instance, samples in the\nMale class outnumber samples in the Female class, so the chances of being the first or last neighbor\nare better for the Male class. This results in most samples being classified as Male, giving the Male\nclass a very high performance rate and the Female class a very low performance rate, which results\nin a large box plot. The CHIN region has a higher classification rate for Males than the other\nregions, but a lower rate for Females.\n\nIn a large majority of the experiments, the Male class had a higher classification rate than\nthe Female class. With more samples in the training space, the likelihood of the closest neighbor\nin an overlapping area is greater for the Male class than the Female class. The exceptions to this\ntrend are found in the YCbCr experiments. Using the Lz distance measure, the Female class has a\nhigher accuracy in every region. The LZ; and L,, distance measures in this color space both follow\nthe larger trend of higher Male classification rates.\n\nFigure 3.7 shows the results of gender nearest neighbor classification on the MORPH dataset\nusing the Li, L2, and L., distance measures. Performance on the MORPH dataset declines from\nthe high-quality dataset results. The bias towards Male classification is more evident with larger\ndiscrepancies between the class-wise accuracies. In every instance, the class-wise accuracy is higher\nfor the Male class than the Female class. This may not be due to the drop in image quality though.\nThe ratio of female to male samples went from approximately 3:4 in FRGC to around 1:5 in the\nMORPH dataset, which could account for part of the difference, if not all. The RGB features have\nthe smallest discrepancy between the class-wise accuracies, but it is still a large discrepancy.\n\nFigure 3.8 shows the results of gender classification using the more sophisticated machine-\nlearning classifiers ANN and SVM for both datasets. The RGB, LCH, and HSI color spaces were\nchosen for these experiments based on performance in 1-NN experiments and analysis. The perfor-\nmance gap between classes increased a little in the FRGC LCH and HSI results, but the performance\nincreased overall. The RGB features increased performance and decreased the performance gap be-\ntween genders. These results indicate that a more sophisticated classifier would be useful in further\ngender classification. For MORPH, performance in the RGB space improved for the most part by\n\nusing a more sophisticated classifier. The most improvement was seen in the MOUTH region. The\n\n43",
    "Page_52": "100\n\n40\n\nClass-wise performance (%)\n\n20\n\n100\n\nClass-wise performance (%6)\ns zg 2\nSs 6 8\n\nx\n8\n\n°\n\n100\n\n40\n\nClass-wise performance (%)\n\n20\n\n0\n\nLEYE\n\nREYE\n\n \n\nNOSE\n\nNTIP\n\nMOUTH\n\n \n\nCHIN\n\nooo\nBS\n\nol,\nal\n| | B Leo\n\nLEYE\n\nLEYE\n\nREYE\n\nREYE\n\nNOSE\n\nNOSE\n\nNTIP\n\nNTIP\n\nMOUTH\n\nMOUTH\n\nCHIN\n\n80g Bie toy 8H fey OOS\n\nCHIN\n\nooo\nBS\n\n8\n\nClass-wise performance (%6) Class-wise performance (%)\n\nClass-wise performance (%)\n\n100\n\n60\n\n40\n\n20\n\n100\n\n80\n\n3\n8\n\n40\n\n20\n\n100\n\n80\n\n60\n\n40\n\n20\n\n0\n\nLEYE REYE\n\nLEYE REYE\n\nLEYE REYE\n\ngoo\nso\n\nNOSE\n\nNOSE\n\nNOSE\n\nNTIP\n\nNTIP\n\nNTIP\n\nMOUTH\n\nCHIN\n\nMOUTH\n\nCHIN\n\ngoo\nso\n\n8\n\nMOUTH\n\nCHIN\n\nFigure 3.6: 1-NN gender classification on FRGC color features over all regions. Values included in\nthe box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L\nto R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV.\n\n44",
    "Page_53": "100 100\n\nol, ol,\nol ol\nE 80 2 E 80 8 Leo\ng g\nzg zg\n5 5\nE 60 E 60\né é\n3 3\na a\nB 40 8 40\n& &\n8 8\n3 20 o 20\noO TEVE REYE NOSE TIP. = MOUTH —CHIN oO TEVE REYE NOSE TIP. = MOUTH —CHIN\n100 oc 100 oc\nal al\nE 80 B Loo z 80 D Leo\n8 8\ne e\n5 5\nE 60 E 60\n£ £\n3 3\na a\n¥ 40 ¥ 40\ni i\n8 8\nDo 20 Do 20\nOT LEYE REYE NOSE —-NTIP._ = MOUTH ~—CHIN OT LEYE REYE NOSE —-NTIP._ = MOUTH ~—CHIN\n100 co 100 co\nol ol\nE 80 2 E 80 8 Leo\ng g\nzg zg\n5 5\n— 60 — 60\né é\n3 3\na a\ng 40 g 40\n& &\n8 8\n3 20 o 20\noO TEVE REYE NOSE TIP. = MOUTH —CHIN oO TEVE REYE NOSE TIP. = MOUTH —CHIN\n\nFigure 3.7: 1-NN gender classification on MORPH color features over all regions. Values included\nin the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row\n(L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV.\n\n45",
    "Page_54": "100 100\nB ANNs B ANNs\nB ANN B ANN\n= pp #888 9908 Bags egee ied @ ANN 2) B @ ANN\nz Gaal H SVMyincar = B SVMrincar\n8 8\n5 5\nE 60 E 60\n2 2\n2 40 2 40\n= =\n3 3\nO 20 O 20\ns LEYE REYE NOSE NTIP MOUTH CHIN s LEYE REYE NOSE NTIP MOUTH CHIN\n100 100\nB ANNs B ANNs\n: B ANN ANNovo\na a ANN, a a ANN,\n7 A E A ag 100 7 100\nz « OO08 ggge aged beed Boe BSVMinr — & B SV Myinear\n8 8\n5 5\nE 60 E 60\n2 2\n2 40 2 40\n= =\n3 3\nO 20 O 20\ns LEYE REYE NOSE NTIP MOUTH CHIN s LEYE REYE NOSE NTIP MOUTH CHIN\n100 100\nB ANNs B ANNs\nB ANN B ANN\n= @ ANN => ANNioo\n80 80\n8 8\n5 5\nE 60 E 60\n2 2\n2 40 2 40\n= =\n3 3\nO 20 O 20\ns LEYE REYE NOSE NTIP MOUTH CHIN s LEYE REYE NOSE NTIP MOUTH CHIN\n\nFigure 3.8: ANN and SVM gender classification on color features over all regions. Values included in\n\nthe box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom:\nRGB, HSI, LCH. L to R: FRGC, MORPH.\n\n46",
    "Page_55": "Figure 3.9: Easy and hard subjects in color gender classification on MORPH images. The subject\non the far left was never misclassified. The other subjects were misclassified in over 95% of the color\nexperiments. The Male in this section was mislabeled as Female.\n\nNTIP is the only region that did not improve overall. The class performance of one class increased\nwhile the other decreased. That is actually the case for most of the LCH and HSI experiments as\nwell. The ANN and SVM classifiers netted no overall improvement of performance in these spaces,\nwith the exception of the MOUTH region in the HSI color space. The linear SVM results in the all\nthe MORPH LCH and MORPH HSI upper face experiments where the box ranges from 0 to 100,\nshow that these genders are not linearly separable. The poor performance also indicate that many\nof the features in these experiments are overlapping. MORPH results indicate that SVM and ANN\nclassifiers would be most useful with RGB features or in HSI MOUTH experiments.\n\nOver 80% of the MORPH images were misclassified in less than 25% of color experiments.\nOne image was never misclassified in these experiments. No subjects were misclassified in all color\ngender experiments, but eight were misclassified in over 95% of the experiments. Images corre-\nsponding to these subjects can be seen in Figure 3.9. The classifiers were able to catch one error\nin the metadata. The Male image in the right grouping of the figure, misclassified over 95% of the\ntime, is mistakenly labeled as Female. Taking that into account, that subject actually had a correct\nclassification most of the time. These images suggest that subjects in the Black Female group are\nthe hardest to predict gender on using color information.\n\nTable 3.2 shows the results of baseline experiments on full face for the FRGC and MORPH\ndatasets. Included are the results of gender classification using the VeriLook SDK. In the FRGC\n\ndataset, the eye regions, using the ANN classifiers, fall approximately 10% below the VeriLook\n\n47",
    "Page_56": "FRGC MORPH\nClassifier Male Female Average | Male Female Average\nAN Nio0 99.95 0.13 50.04 | 96.07 61.68 78.88\nSV Miinear | 92.39 89.50 90.94 | 96.84 58.82 77.83\nLz 79.48 69.22 74.35 | 87.45 25.62 56.54\nVeriLook 92.72 98.45 95.59 | 99.14 67.75 83.45\n\n \n\nTable 3.2: Gender performance using color and full face. Percentage values represent the class-\nspecific accuracies and the average class-specific accuracy for each experiment. VeriLook refrained\nfrom predicting gender on 2.75% and 0.89% of male and female images respectively in the FRGC\ndataset. In the MORPH dataset it predicted no gender on 0.5% and 11.26% of male and female\nimages.\n\ngender classification accuracy. Following the same feature extraction but with face images, the gap\nis lessened to 5% with the best classifier, but the regions still perform less favorably than the whole\nface. In the MORPH dataset, the MOUTH region comes within 1-2% of the VeriLook performance.\nThis indicates that, with images of a lower resolution, the color of the mouth region holds comparable\n\ngender information to the entire face.\n\n3.5.2 Age\n\nFigure 3.10 shows classification performance by age groups for the RGB space using 1-\nNN with Euclidean distance for classification. Other classification measures and color spaces give\nsimilar performance with exceptions noted below. The graphs show less variance on the top of the\nbox suggesting that color in one of the classes is less impacted by age than the other. This would be\nthe Male class in this instance. Again, the Male class does have the highest representation, which\ngiven any overlap, would provide that class an advantage.\n\nThere does not seem to be an age that is easiest to classify by gender overall. Younger\nfemales are easier to classify in the eye regions than older females. This is true in face experiments\nas well. In the eye regions again, there is no definite trend in the male class. For the nose regions, an\nerratic downward trend is noticed in the class-wise accuracies as age increases, suggesting that the\ncolor of the nose is not age invariant and behaves similarly for both males and females. The mouth\nregion has a definite peak for both male and female performance between 26 and 45 yrs old. This\nsuggests that gender classification based on color is more difficult for younger and older subjects\nthan middle-aged subjects in the mouth region. For the chin region, there is a small upward trend\n\nfor male classification, but the trend is down for the female class. This indicates that 21-35 year old\n\n48",
    "Page_57": "100\n16-20\n21-25\n26-30\n31-35\n36-40\n41-45\n46-50\n51-55\n56-60\n61-77\n\n40\n\nroy\nSo\n\na\n3\nBeso SB88808088\n\nClass-wise performance (%)\n\n20\n\n \n\n: LEYE REYE NOSE NTIP MOUTH CHIN\n\nFigure 3.10: Gender performance by age group for features in the RGB space using 1-NN classifica-\ntion (Lz distance).\n\nmales are easier to classify by the chin region than others.\n\nIt seems as though all regions are impacted by age either in one class or the other. The\nregions that are least impacted by age are the nose, nose tip, and mouth regions. The most impacted\nregions, those with the most variance, are the eye regions, not for the Male class, but definitely for\nthe Female class. This same trend is seen in full face experiments.\n\nOver all k-NN classification experiments, all the color spaces perform similarly. Some dif-\nferences occur in the RGB, LCH, and HSI color spaces using the ANN and SVM classifiers. With\nthe ANN classifier, the variance for females is more pronounced in the HSI space. In both ANN\nand SVM classification, more of an upward trend is noticed for females as age increases in the NTIP\nand MOUTH regions in the HSI and RGB color spaces. As females age, less color variance is noted\nin these regions resulting in better performance. The LCH and HSI color space features perform\nsimilar to 1-NN, except with the SVM classifier. In the eye and nose regions, the classifier does not\n\nget enough separation in the training set and ends up predicting all Male class.\n\n49",
    "Page_58": "3.6 Ethnicity\n\n3.6.1 Reliability\n\nFigure 3.11 shows the results of ethnic nearest neighbor classification on the FRGC dataset\nusing the L;, Lz, and L., distance measures. The boxes in the ethnic results are much larger than\nthe ones seen in the gender results. This indicates a wider range of class-wise accuracies. One or two\nclasses may have good performance, as seen by the upper portion of the plot, but the other classes\nare not well recognized, extending the plot close to the zero line. The anomaly in these graphs is\nonce again the performance of the YCbCr features, with outliers that fall well above the box plots.\nWith 25 class-wise accuracies to plot, when one class performs very well and the others do not, the\nthird quartile falls within the second-best group of accuracies. With a small IQR from the poor\nperformance of the other classes, the best performances will be outliers on the graph.\n\nFor most of the experiments, the class with the best performance is the White class, followed\nby the Asian class. Since the White and Asian classes have the largest number of samples present in\nthe experiment set, it makes sense that they would have the best class-wise accuracies. With the L\ndistance measure on YCbCr features, the best performance is on the Indian class, followed by the\nWhite class for the nose regions and the chin. As previously discussed, not much variance is found\nwith the YCbCr features, and since the trend is only present in one of the three distance measures\nit very likely deals with the peculiarity of the data and not differences within the ethnic groups.\n\nIn most of the experiments, the class-wise accuracies of the Hispanic, Black, and Indian\nclasses are less than 20%. In the LUV and YIQ and the eye and nose regions, the class-wise\naccuracy for the Indian class reaches up to 46% performance. The best accuracies for the Black\nclass are found in the mouth region using the LCH features, but only reach 30%. The Hispanic class\n\nhas a consistently low class-specific accuracy, rarely exceeding 15%, but it is less likely to have a 0%\n\n \n\naccuracy than the Black class. This indicates that there is a small subset of the Hispanic class which\nis more distinguishable than the rest, but also more generalized than just one person. The rest of\nthe class, however, overlaps with another class. These results indicate that different regions might\nbe better for ethnic classifications for different ethnic groups. With these three classes, the number\nof subjects was 15 or less, so any conclusions drawn from this information may not generalize to the\nlarger problem.\n\nFigure 3.12 shows the results of ethnic nearest neighbor classification on the MORPH dataset",
    "Page_59": "100 to\n\n \n\n \n\nol, ol,\nal al\n= 80 8 Leo S 80 B Le\ng g\n5 5\n—E 60 E 60\n2 2\n3 3\na a\n¥ 40 ¥ 40\n= =\n8 8\n3 20 o 20\nOo LEYE REYE NOSE NTIP MOUTH CHIN oO TEVE REYE NOSE NTIP MOUTH CHIN\n100 Ll a 100 cc\not Co. : al al\nEw ' oe Btw gs 80 O Dew\n2 2\n5 5\nE 60 E 60\n£ . £\n3 3\na a\n& 40 B40\ni i\n8 8\ni 4 ii f “\n7 LEYE REYE NOSE NTIP MOUTH CHIN 7 LEYE REYE NOSE NTIP MOUTH CHIN\n100 al, 10 oly\nal al\n= 80 8 Leo S 80 B Le\ng g\n5 5\n—E 60 E 60\n2 2\n3 3\na a\n¥ 40 ¥ 40\n= =\n8 8\n3 20 o 20\nOo LEYE REYE NOSE -NTIP MOUTH CHIN Oo LEYE REYE NOSE -NTIP MOUTH CHIN\n\nFigure 3.11: 1-NN ethnic classification on FRGC color features over all regions. Values included in\nthe box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L\nto R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV.\n\n51",
    "Page_60": "100 aL. 100\n\n2\nol OL,\nE 80 a E 80 8 Leo\ng g\nzg zg\n5 5\n— 60 — 60\né é\n3 3\na a\nB 40 8 40\n& &\n8 8\n3 20 o 20\nOo LEYE REYE NOSE NTP. MOUTH CHIN Oo LEYE REYE NOSE NTIP- MOUTH CHIN\n100 100\n2\nal al,\nE 80 B Loo z 80 D Leo\n8 8\ne e\n5 5\nE 60 E 60\n£ £\n3 3\na a\n¥ 40 ¥ 40\ni i\n8 8\nDo 20 Do 20\non LEYE REYE NOSE NTIP- MOUTH CHIN oT LEYE REYE NOSE NTIP- MOUTH CHIN\n100 100\n2\nol OL,\nE 80 a E 80 B Lo»\ng g\nzg zg\n5 5\n— 60 — 60\né é\n3 3\na a\ng 40 g 40\n& &\n8 8\no 20 o 20\nOo LEYE REYE NOSE NTP. MOUTH CHIN oO LEYE REYE NOSE NTIP- MOUTH CHIN\n\nFigure 3.12: 1-NN ethnic classification on MORPH color features over all regions. Values included\nin the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row\n(L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV.\n\n52",
    "Page_61": "using the Li, L2, and L.. distance measures. With this dataset, the best and worst performing\nclasses change. The class with the best class-specific accuracies is the Black class, followed by the\nWhite and Hispanic classes. This is partially due to the number of samples for each of these classes.\nThe remaining two classes have less than 100 subjects present in the experiment set.\n\nThe Black class has the highest class-specific accuracies of the MORPH ethnic experiments\nwith accuracies falling between 85% to 95% on average. The best results are found using the RGB\nor HSI features for all of the regions. The best class-wise accuracies for this class are in the MOUTH\nregion using the RGB and HSI features. Performance in this region ranges up to 97%. The next\nbest region performance is around 95% with either a nose or eye region depending on the color space\nand the distance measure.\n\nThe class with the next best performance is the White class. The RGB and HSI color spaces\nproduce the best results for this class, reaching 79% in the MOUTH region. This class has a larger\nvariance than the other ethnic classes with the majority of class-wise accuracies falling between 50%\nand 70%. After the MOUTH region, the next best performing regions are the nose regions. The\nbest performance on these regions is between 70% and 75% with the HSI and LCH features.\n\nThe highest class-specific accuracies for the Hispanic class can be found using the RGB,\nHSI, and LCH color space features for most regions. The best performance for this class in 1-\nNN experiments was approximately 30% using RGB and HSI features in the MOUTH region. On\naverage, the class accuracy for Hispanics was between 10% and 25% in these experiments.\n\nThe best performance for the Asian class can be found using the RGB, LUV, and HSI\nspaces. This class has a fairly erratic performance indicating there is a lot of overlaps with another\nclass and is highly dependent on the training data. Class-wise accuracies for the Asian class range\nbetween 0% and 10% for the most part. The features that most consistently achieved a non-zero\nclass accuracy were found in the RGB space. The best region was different for each distance measure.\n\nWith only 13 subjects, classification on the Native American class is very poor, most often\nwith no correct classifications. In less than 30 out of 108 gender experiments, color features managed\nto achieve at least one correct classification for this class over each of the five CV runs. These\nexperiments were concentrated in the eye and nose regions with a few using the MOUTH region.\nSeveral color spaces were represented within these experiments as well.\n\nSince the best ethnic performance is found for most of the classes in the RGB and HSI color\n\nspaces, these color spaces will be investigated using ANN and SVM classifiers. The hope is that with",
    "Page_62": "Classifier | White Asian Hispanic Black Indian Average\n\n \n\nAN Nio0 99.95 0.13 0.00 0.00 0.00 20.01\nSV Miinear | 95.73 82.76 5.33 11.67 26.50 44.40\nLy 84.30 56.01 3.67 9.17 10.75 32.78\n\nTable 3.3: Ethnic performance using color and full face, FRGC. Percentage values represent the\nclass-specific accuracies and the average class-specific accuracy for each experiment.\n\n \n\nClassifier White Asian Hispanic Black anaute Average\nmerican\n\nAN Nio0 87.82 0.00 36.17 97.96 0.00 44.39\n\nSV Myinear | 84.98 11.59 39.99 97.89 0.00 46.89\n\nLy 66.63 4.76 26.77 97.28 4.09 39.50\n\nTable 3.4: Ethnic performance using color and full face, MORPH. Percentage values represent the\nclass-specific accuracies and the average class-specific accuracy for each experiment.\n\na more sophisticated classifier, class-wise accuracy will increase. The global feature vectors for these\nspaces show values in approximately half of the bins. This is different from the YIQ, YCbCr, and\nLUV color spaces with values in only a third of the bins. It is similar, however, to the global features\nin the LCH color space. On the assumption that the less sparse the feature vector, the better the\nclassification, the LCH color space will also be examined with the ANN and SVM classifiers.\n\nResults of ethnicity classification using the RGB, HSI, and LCH color space features with the\nANN and SVM classifiers are shown in Figure 3.13. Most of the experiments increased performance\non the two most represented classes in the dataset, but did little to improve performance of the other\nclasses. In most of the MORPH experiments, the class-wise accuracy for these classes decreased to\n0%. This indicates that these classes do not have enough representation and that the samples that\nbelong to them overlap with the larger classes.\n\nImages from 30 subjects were never misclassified according to ethnicity. All of the subjects\nwere males from the Black class. This is logical since this class has the largest representation and the\nlargest classes have performed the best in the classifiers. Images from 20 subjects were misclassified\nby every run of every experiment. These subjects can be seen in Figure 3.14. With only 13 subjects\npresent in the Native American class and 10 of them being misclassified in every experiment, there\nis a problem with this class. Either it is indistinguishable from another class based on color, or it is\nlacking the training samples needed to created an accurate representation of the class. The problem\nstill exists for the Asian class, but not as badly with only 10 out of 50 subjects misclassified in all\n\nexperiments.",
    "Page_63": "m ANNs a00 m ANNs\n\n   \n\n   \n\n \n\nB ANN. B ANN.\n= @ ANNioo > @ ANNioo\n80\né B SV Myincor é B SV Myincor\n8 8\n5 5\nE E 60\ng g\nB B 40\n= =\n& &\nOo oO 2\nLEYE REYE NOSE NTIP MOUTI s LEYE REYE NOSE NTIP MOUTH CHIN\n100\n@ ANNs @ ANNs\nAN N29 AN N29\nam ANN\\o0 s ANN\\o0\n80\né B SV Myincor é B SV Myincor\n8 8\n5 5\nE E 60\ng g\nB B 40\n= =\n& &\nOo oO 2\nLEYE REYE NOSE NTIP MOUTI s LEYE REYE NOSE NTIP MOUTH CHIN\n100\nANNs ANNs\nAN N29 AN N29\n= @ ANNioo > @ ANNioo\n80\nss B SV Myincor é B SV Myincor\n8 8\n5 5\nE E 60\ng g\nB B 40\n= =\n& &\nOo oO 2\ns LEYE REYE NOSE NTIP MOUTH CHIN s LEYE REYE NOSE NTIP MOUTH CHIN\n\n \n\nFigure 3.13: ANN and SVM ethnic classification on color features over all regions. Values included\n\nin the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to\nbottom: RGB, HSI, LCH. L to R: FRGC, MORPH.\n\n \n\nFigure 3.14: Hard subjects in color ethnicity classification on MORPH images. The subjects to the\nleft are from the Asian class. The subject to the right are from the Native American class.\n\n55",
    "Page_64": "Tables 3.3 and 3.4 show the results of ethnic classification on the full face using RGB color\nspace features. In the FRGC dataset all regions but the CHIN region performed similarly on the\nbest performance for each region with all the classifiers. The best CHIN performance was 10% below\nthe face results. In the MORPH experiments, the MOUTH region compares favorably to the full\nface results with the eye regions slightly below. This indicates that the MOUTH and eye regions\n\nhold a comparable amount of color ethnicity information as the whole face.\n\n3.6.2 Age\n\nFigure 3.15 shows ethnic performance broken down over the age groups for MORPH exper-\niments. The 41-45, 56-60 and 61+ age groups do not have subjects from each ethnic class in them,\nas seen in Table 3.1. The 41-45 age group has no Asian subjects in it. The 56-60 age group is\nmissing Asian subjects as well as Hispanic subjects while the 61+ group only has subjects from the\nBlack and White classes in it. If a class was not present in the age group, no value was plotted for\nthe class-wise accuracy. This shows an improved performance for these age groups since the missing\nclasses normally have poorer performance.\n\nPerformance on the best performing class does not vary much with age. There is a trend\non most of these experiments for the next best performing classes. Excluding categories that do not\nhave all age groups present, ethnic classification is more accurate on subjects under 30 years of age\nthan older subjects in the second best performing class. The exceptions to this trend lie in the RGB\nfeatures. Ethnic performance across the age groups varies less in this color space and does not show\na definite trend. For all the others, the performance drops off as the subjects age until the spike\nin the 41-45 group. From there it depends on the region. In the CHIN region, ethnic performance\non the 46-55 age groups increases again, but rarely more than the best performance on the younger\nsubjects.\n\nIt seems as though all regions are impacted by age either in one class or another. The most\nheavily affected region for ethnic classification is the CHIN. Performance on all classes in this region\nshow variance over age. In the other regions the best performing class is not as affected by age as it\nis in the CHIN region; however, the other classes do show some variance. The amount of variance\nper region is dependent on the color space. Features from the RGB space show less variance with\nage than the remaining color spaces. The most stable region with respect to age in that space is the\n\nMOUTH.",
    "Page_65": "100\n\nB 16-20\n@ 21-25\nBD 26-30\n80 B 31-35\n= B 36-40\n& @ 41-45\n& eo ©\" 46-50\n5 51-55\n5 B 56-60\no @ 61-77\n£ 40\n8\n8\no\n20\n\n \n\n: LEYE REYE NOSE NTIP MOUTH CHIN\n\nFigure 3.15: Ethnic performance by age group for features in the RGB space using 1-NN classification\n(Lz distance).\n\n3.7 Conclusions\n\nOverall color is fairly stable around the face. The main color information comes from skin\nand is similar across the face. The region that is most likely to have different color information than\nthe others is the CHIN region, very likely due to the presence of facial hair in some males.\n\nUpon further analysis, coarse 2D histograms covering the entire space may generalize color\ninformation too much. The range of possible skin tones is very small compared to the entire color\nspace represented by the color histogram. Future work will most likely use a finer histogram over\na smaller portion of the color space. Colors detected outside this area could indicate make-up, face\npaint, glasses, or really anything that could be on the face and be a different color. This could help\ngive a measure of confidence. If all detected colors fall within the colors of interest, the classification\ncould proceed. If a certain percentage fell outside the colors of interest, a flag could be raised or\na low-confidence value returned. Features from different color spaces could also be combined to\nimprove color representation.\n\nIn high quality images, color information can be used to predict gender correctly over 80%\nof the time using the eyes, nose tip, and mouth regions. It is not as reliable for the lower-quality\nimages, having between 75-85% average class-wise accuracy. In this dataset, the mouth provides\nthe most reliable results, comparing favorably to face. The color of the eye regions performs fairly\n\nwell which agrees with previous color experiments [46]. Based upon this, the conclusion is that color",
    "Page_66": "information is fairly reliable for gender classification. Color information is not as reliable in ethnic\nclassification as it is in gender classification for the experiments included within, but the mouth and\neye regions compare favorably to to the face. This suggests that better representation is needed,\nboth in training samples and feature extraction, since color can be major indicator for human-based\nethnic classification. It also suggests a need for better ethnic labels.\n\nEthnicity classification is, by nature, a hard problem. Ethnicity is based on how an individ-\nual perceives either himself or others. This perception may not coincide with how another person\nwould perceive the same individual. Most databases rely on self-reported ethnicity which gives no\nmeasure of consistency between ethnicities on the various subjects. Two people with the same mixed\nheritage might identify themselves with different ethnic groups. Add that to the problems of non-\nbinary classification and less representation per class, and one can see how ethnicity results tend to\nbe worse than those for gender classification.\n\nAll regions and colors are impacted in some way by age. The most affected regions for\ngender are the mouth and chin regions for both genders and the eye regions for females. The most\nimpacted region in ethnic classification is CHIN. The color space that was least affected by age was\nthe RGB color space. The least impacted region in gender is nose. The least impacted regions in\nethnicity are the mouth and nose tip regions. This suggests that regions covering the nose are the\nmost stable regions with respect to age in both gender and ethnic classifications.\n\nBased upon the results and analysis from this chapter, features from the RGB color space\nand the MOUTH and NTIP regions will be used for both gender and ethnicity classification in\nChapter 6 fusion experiments. The next type of features to be investigated is shape. The following\n\nchapter will cover results and discussion on partial-face shape experiments.",
    "Page_67": "Chapter 4\n\nShape\n\nShape can mean several different things. It can mean the actual shape of an object, the\noutline and contours, or the distribution of smaller pieces within the object, such as the eyes, nose,\nand mouth. The relationship between the pieces gives an idea of the shape. Each of these can\nbe represented differently. Common representations of shape include Active Shape Models (ASM),\n\nedges and lines, and the relationship between specified points.\n\n4.1 Feature Extraction Methods\n\nFace metrology has been used in previous works by Cao et al. to determine gender using\npoints from the entire face [10]. Pairwise distances and angles between each possible pair of points\nwere calculated. An example of some of the distances on the face can be seen in Figure 4.1. The\nfeatures were ranked according to how well each one separated the classes in the training set, similar\nto d-prime. The most important features were chosen and used for gender classification. This method\nwas adapted to work within the partial face schema. Only points found within a region were used.\nThe only concession to full face was that all distances within each region were normalized by the\ninterocular distance. The distance between two points was calculated using the Euclidean distance\nmetric. The angle between two points was calculated by finding the inverse tangent between them.\nTable 4.1 shows the number of shape points used for each region and the length of the feature\nvectors. Since the number of points per region is small, all point pairs were considered important\n\nand no feature reduction was performed at this point.",
    "Page_68": "Figure 4.1: Example of shape feature calculation on a full face image.\n\n \n\nTable 4.1: Number of points used per region for shape features. Total feature vector length is 2 x (3)\nwhere n is the number of points per region.\n\n4.2 Experiment Setup\n\nThe experiments for this section will be performed on subsets of the FRGC and MORPH\ndatabases as previously described in Chapter 2. The points used in the feature extraction come\nfrom preprocessing performed on the color face images using the VeriLook SDK. Experiment sets\nremain the same from color experiments. Five runs of a stratified cross-validation experiment are\nrun for each region and classifier combination. Experiments are performed with and without PCA\nfeature reduction because of the small number of features present in each region. Eighty-four shape\n\nexperiments are run for each demographic and dataset.\n\n60",
    "Page_69": "4.3 Analysis\n\nLooking at the distance and angle features separately allows for several conclusions to be\nmade. With respect to gender, the mean male subject has larger distances, and thus larger features\nin the chin and nose regions. The eyes and mouth regions are more similar between the genders.\nEven with this difference, the distance between points overlaps for the genders. In ethnicity, the\nBlack class tends to have higher distances in the nose regions, indicating larger noses within this\nclass. However, as with the genders, the difference between ethnic classes are still small and have a\nhigh overlap. The angle features show no trend with respect to either gender or ethnicity.\n\nThe shape of the nose changes as an individual ages, with distances between points increasing\nslightly as the age of a subject increases. This agrees with research findings summarized by Albert\net al. [4] that the length and height of the nose increase with age. This trend is present in all\ndemographic groups, both gender and ethnicity, although it is not as smooth in the Female class.\nThe eyes, chin, and mouth regions show no consistent shape change with respect to age. Lip thickness\nlikely changes in the mouth according to Albert et al. [4], but the age change is lost in the variability\n\nof the mouth due to changes in expression over the experiment set.\n\n4.4 Gender\n\n4.4.1 Reliability\n\nFigure 4.2 shows the class-specific accuracies on the FRGC and MORPH datasets using\n1-NN classification. The gap between class-wise performances is smaller in FRGC, but the actual\nperformance is not much greater than chance. The higher resolution of the FRGC data allows\nfor better point localization during the face annotation phase. Feature reduction with these shape\nfeatures loses valuable information and actually makes performance on the Female class worse in\nsome instances. The most notable, and best performing, instances are the NOSE and NTIP regions,\nlikely due to the size difference between genders.\n\nFigure 4.3 shows the results of gender classification using the ANN and SVM classifiers.\nOnce again, feature reduction deteriorates performance. The nose regions seem to hold the most\ngender specific shape information for both MORPH and FRGC; however, the poor performance\n\nindicates that it is only a small amount. The shape features lack enough separation by gender to\n\n61",
    "Page_70": "100 100\n\no Ly o Ly\nol; ol;\n\nS 60 B Lo S 60 B Lo\n\n8 8\n\n3 3\n\nE 60 aa a 4 88 E 60\n\n2 2\n\nB 40 B 40\n\n= =\n\n8 8\n\nOo 20 Oo 20\n\nO“TEYEREYE NOSE NTIP ‘MOUTH CHIN O“TEYEREYE NOSE NTIP ‘MOUTH CHIN\n100 nip 100 nip\n\noh ah\n\n. a S a\n\n= 80 Leo z 80 Lex\n\n2 2\n\n5 5\n\nE 60 ae q E 60\n\n£ £\n\na a\n\na a\n\ng 4 g 4\n\n; ;\n\n8 8\n\n3 20 o 20\n\nOT LEYE REYE NOSE NTIP = MOUTH — CHIN OT LEYE REYE NOSE NTIP = MOUTH — CHIN\n\nFigure 4.2: Gender nearest neighbor results on shape features. Values graphed are the class-wise\naccuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature\nreduction. Bottom: PCA feature reduction keeping 95% of the variance.\n\n62",
    "Page_71": "100\n\n   \n\nANNs “0 ANN\nANN200 ANN200\n_ B ANNioo a B ANNioo\nze\" | | | j | j BSVMincsr = B SV Mirncar\n8 8\n5 5\nE 60 i E 60\n2 2\nB 40 | B 40\n= =\n8 8\nOo 20 Oo 20\nCTEYE REYE NOSE NTIP MOUTH CHIN Ov'LEYE-REYE NOSE NTIP MOUTH CHIN\n“0 @ ANNs “0 @ ANNs\nB ANN2 B ANN2\n_ B ANNioo a B ANNioo\nz* BSVMrinear = = B SVMrinear\n8 8\n5 5\nE so E so\n2 2\nB 2 40\n= =\n8 8\nOo 20 Oo 20\nOTEYE REYE NOSE NTIP MOUTH CHIN Ov'LEYE EYE NOSE NTIP MOUTH CHIN\n\nFigure 4.3: Gender ANN and SVM results on shape features. Values graphed are the class-wise\naccuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature\nreduction. Bottom: PCA feature reduction keeping 95% of the variance.\n\n63",
    "Page_72": "A ; NN\n\nFigure 4.4: Hard subjects in shape gender classification on MORPH images. Images are of subjects\nclassified incorrectly in over 98% of shape gender experiment runs.\n\nFRGC MORPH\nClassifier Male Female Average | Male Female Average\nAN Nio0 100.00 0.00 50.00 | 100.00 0.00 50.00\nSVMiinear | 81.12 74.63 77.88 - - -\nLy 70.70 59.22 64.96 | 87.45 25.62 56.54\n\n \n\n \n\nAN Nio0 100.00 00.00 50.00 | 95.54 34.42 64.98\nSV Miinear | 60.42 61.38 60.9 | 99.52 7.Al 53.47\nLy 67.09 55.64 61.36 | 87.45 26.23 56.84\n\n \n\nVeriLook 92.72 98.45 95.59 | 99.14 67.75 83.45\n\nTable 4.2: Gender performance using shape features and full face. Top section is without PCA,\nwhile the bottom section of the table is results using PCA for feature reduction. Percentage values\nrepresent the class-specific accuracies and the average class-specific accuracy for each experiment.\nVeriLook refrained from predicting gender on 2.75% and 0.89% of male and female images respec-\ntively in the FRGC dataset. In the MORPH dataset it predicted no gender on 0.5% and 11.26% of\nmale and female images.\n\n \n\n \n\ntrain a reliable classifier in most instances based upon the 0-100% performance in the MORPH\nexperiments. The FRGC experiments show a little more separation than MORPH, with correct\npredictions in both classes, for all regions.\n\nNo subjects were classified correctly in all shape experiments. The subjects and images\nthat performed the worst can be seen in Figure 4.4 being misclassified in at least 98% of the shape\nexperiments. The majority of the subjects are Black with one Native American and one White, but\nall the subjects are Female.\n\nTable 4.2 shows the results of gender classification using shape features in the full face. In\nFRGC, the highest performance of an individual region is 73% average class-wise accuracy with the\n\nNOSE region and SVM classifier. This is below shape classification on full face and definitely below\n\n64",
    "Page_73": "VeriLook classification. In MORPH, the results are even less favorable dropping to 55% with the\nMOUTH and one of the nearest neighbor classifiers. The face shape results are also low for MORPH.\nThis differs from what was seen in the work by Cao et al. [10]. Full face shape was able to achieve\ngender classification within 5-10% of appearance based methods. Therefore, the choosing of the best\nfeatures is an important part of the method. This is the step where the purposed method deviated,\n\neither choosing all features or using PCA for feature reduction.\n\n4.4.2 Age\n\nGender class-wise performance partitioned by age groups can be seen in Figure 4.5(a). In\nmost of the regions, the points used are stable, like the eye corners. Age should not influence the\nperformance of those regions, unless the features themselves change with age. Points in other regions,\nlike the mouth, are less rigid. It is possible that variability in these regions may be due to behavioral\nfactors, as well as age. For instance, it is possible that younger subjects smile more often in the\nexperiment set than older subjects.\n\nNo definite trend is noticed in the results with respect to age, except in the nose regions.\nThe Female class shows a decrease in performance as age increases. As discussed in the analysis of\nshape features, the samples in the Male class, on average, have larger noses than those in the Female\nclass; however, as an individual ages, the size of the nose increases. This means older female samples\nare more likely to overlap with the Male class and be classified incorrectly. The face results follow\nthe same trend for PCA reduced experiments. Other regions show little or inconsistent changes in\n\ngender classification with respect to age.\n\n4.5 Ethnicity\n\n4.5.1 Reliability\n\nResults of ethnicity classification using shape features can be seen in Figure 4.6. Once again,\nthere is a slight performance gain when excluding the feature reduction step. Performance is still\nvery low for both FRGC and MORPH. The NOSE and MOUTH regions seem to hold the most\nshape information related to ethnicity. For all experiments, one class performs well, which is the\n\nmost well represented class, but the performances of the other classes suffer. This indicates that the",
    "Page_74": "100 100\n@ 16-20\n\n21-25\n26-30\n80, B 31-35\ng n364 €\n8 B44 gy\n& © 4650 8\n60\nE asss &\nS mssco §\nFl ao §\n2° =\ng g\n20\n\n0\n\n \n\nLEE REYE NOSE NTIP. MOUTH CHIN\n\n(a) (b)\n\n \n\nMOUTH\n\n \n\n@ 16-20\nB 21-25\n@ 26-30\n31-35\n1 36-40\n41-45\n= 46-50\nS155\n@ 56-60\n@ 61-77\n\nFigure 4.5: Shape results by age for a) gender and b) ethnicity classification. Results shown are\n\nfrom the 1-NN experiments (L2) with no feature reduction.\n\n100\n\n \n\n \n\n& S 80 . oo\n8 8\n3 3\nE E 60\n2 2\nB 2 40\n= =\n8 8\n2 Oo 20\now LevE REYE NOSE NTIP MOUTH CHIN ow LeYE REYE NOSE\n100 oc 100\nol\ns B Leo s\n= =\ng g\n5 5\nE E\n& &\n3 3\na a\ng g\n5 5\n3S 3S\now LEYE REYE NOSE NTIP- MOUTH CHIN ow LEYE REYE NOSE\n\nNTIP\n\nNTIP\n\nMOUTH.\n\nMOUTH\n\nCHIN\n\n \n\nCHIN\n\naoe\nSN\n\n \n\nooo\nBS\n\n8\n\nFigure 4.6: Ethnic nearest neighbor results on shape features. Values graphed are the class-wise\naccuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature\n\nreduction. Bottom: PCA feature reduction keeping 95% of the variance.\n\n66",
    "Page_75": "Figure 4.7: Easy and hard subjects in shape ethnic classification on MORPH images. Subject on\nthe far left was classified correctly in all experiments. The subjects to the right are a subset of those\nmisclassified in all shape ethnicity experiments, 18 out of 69. From top to bottom the real ethnic\nlabels are Hispanic, Native American, and Asian. Subjects in the last column are Female; the rest\nare Male.\n\npartial face shape data for ethnicity classes overlaps and is not very useful for ethnicity classification.\n\nFigure 4.8 shows the results of ethnic classification using ANN and SVM classifiers. In most\ninstances, better performance is achieved using a 1-NN classifier, indicating small clusters spread\nout over the classification space. One class does really well with the ANN and SVM classifiers, and\nthe rest perform poorly, resulting in the outliers on the graphs. In FRGC this is the White class,\nin MORPH this is the Black class. Shape classification does improve on the next best class as the\nANN gets larger.\n\nOne subject, Black Male, was correctly classified in all MORPH shape ethnicity experiments.\nSixty-nine were misclassified in all of the experiments, some on multiple images. A subset of these\nsubjects can be seen in Figure 4.7. The majority of these subjects are in the Male class, but Female\nsubjects from each of the smaller classes are present as well. Half of the Asian class subjects are in\nthis set, as well as 70% of the Native American class. The rest of the subjects are Hispanic, but are\nonly responsible for approximately 6.5% of the subjects in that class. This supports the idea that\nthe smaller classes overlap a lot with one another and the larger classes.\n\nTables 4.3 and 4.4 show the results of ethnic classification on shape features on the whole\nface. In FRGC, the MOUTH region with the SVM classifier is the only one that approaches the\nperformance on the face. Results in the MORPH dataset improve slightly. The nose and mouth\n\nregions obtain average accuracies within 5% of the face results. While not the best representation,\n\n67",
    "Page_76": "Classifier | White Asian Hispanic Black Indian Average\nAN Nio0 100.00 0.00 0.00 0.00 0.00 20.00\nSV Myinear | 84.36 57.32 1.00 4.33 3.00 30.00\nLy 76.86 38.49 0.67 = 1.83 4.25 24.42\n\nAN Nio0 100.00 0.00 0.00 0.00 0.00 20.00\nSV Miinear | 73.31 52.23 1.67 5.83 5.75 27.76\nLe 75.45 33.29 3.33 1.17 7.00 24.05\n\n \n\nTable 4.3: Ethnic performance using shape features and full face, FRGC. Top section is without PCA,\nwhile the bottom section of the table is results using PCA for feature reduction. Percentage values\nrepresent the class-specific accuracies and the average class-specific accuracy for each experiment\n\n \n\n \n\nClassifier | White Asian Hispanic Black anaixe Average\nmerican\n\nAN N00 0.00 0.00 0.00 100.00 0.00 20.00\nSV Miinear 80.40 2.00 8.44 96.21 0.00 37.41\nLy 42.10 1.41 8.44 86.43 0.00 27.68\n\nAN Nio0 80.35 0.00 1.90 96.22 0.00 35.69\nSV Myinear | 81.05 0.00 0.00 96.63 0.00 35.54\nLz 42.63 0.77 8.91 86.46 0.00 27.75\n\n \n\nTable 4.4: Ethnic performance using shape features and full face, MORPH. Top section is without\nPCA, while the bottom section of the table is results using PCA for feature reduction. Percent-\nage values represent the class-specific accuracies and the average class-specific accuracy for each\nexperiment\n\nshape features on the nose and mouth can reach comparable performance to similar shape features\n\nover the whole face.\n\n4.5.2 Age\n\nEthnicity class-wise performance partitioned by age groups can be seen in Figure 4.5(b).\nPerformance on the best class, Black, showed an upward trend in the eye regions as the age of\nsubjects increased, suggesting that classification of ethnicity is easier on older subjects in the Black\nclass. This is opposite to the trend found in the MOUTH and CHIN regions. The decrease in\nperformance for the CHIN region could be due to the proclivity of the males in the class towards\ngrowing facial hair. This could possibly interfere with the accuracy of the facial annotation in that\nportion of the face and deteriorate performance. The nose regions perform similarly for all age\ngroups in this class. With the Black class having larger measurements in these regions to begin\nwith, an increase in the measurements would keep the separation between the classes.\n\nIn the White and Hispanic classes, the eyes seem fairly stable with respect to age while\n\nthe nose regions show a downward trend in these classes. An increase in the nose measurements\n\n68",
    "Page_77": "100\n\n \n\n@ ANNs\n\n \n\n \n\n \n\n' : B ANNow\na . @ ANNioo\n80 + .\né . : . B SVMyincor\n8\n5\nE 60\ng\nB40\n=\na\noO 2\ns LEYE REYE NOSE NTIP MOUTH CHIN\n100 Sas gees ist ae me Se\nae tt ‘ ann . B ANN,»\n. $ . B ANN2\nal B ANN\n80\né B SV Myincor\n8 .\n5\nE 60\ng\nB40\n=\na\noO 2\ns LEYE REYE NOSE NTIP MOUTH CHIN\n\nClass-wise performance (%)\n\nClass-wise performance (%)\n\n100\n\n100\n\n80\n\n60\n\n20\n\nLEYE\n\nLEYE\n\nREYE\n\n \n\nREYE\n\nNOSE\n\nNOSE\n\nNTIP\n\nNTIP\n\nMOUTH\n\nMOUTH\n\n \n\nCHIN\n\nCHIN\n\n@ ANNs\n\nB ANN\nAN Noo\n\nB SVMrinear\n\nB ANNs»\nB ANNo\n@ ANNioo\nB SVMiincar\n\nFigure 4.8: Ethnic ANN and SVM results on shape features. Values graphed are the class-wise\n\naccuracies over 5 runs of each experiment.\n\n69\n\n(L to R): FRGC, MORPH. Top: no PCA feature\nreduction. Bottom: PCA feature reduction keeping 95% of the variance.",
    "Page_78": "with these classes could begin to overlap with the Black class, decreasing performance. Performance\non the CHIN region for both these classes declines as age increases, likely due to facial hair and\nlandmark accuracy. The MOUTH region improves noticeably with age in the White class but is\ninconsistent with the Hispanic class. The performance of the MOUTH on both of these classes is\nstill markedly below the corresponding performance on the Black class.\n\nThe Asian and Native American classes are the worst performing classes with the Native\nAmerican class having very few correct classifications. In the Asian class, correct classifications\nmore likely in the age groups under 40 years of age. From 16-25, correct classifications could be\nfound in the eyes, nose, and mouth regions, but then the eyes and mouth became harder to classify,\nand just the achieved correct classification until subjects reach 40 years of age. This suggests that\nyounger Asian individuals can be classified by eye shape, but the nose becomes a more important\n\nethnic indicator as they age.\n\n4.6 Conclusions\n\nThese particular partial face shape features do not work well with gender and ethnic clas-\nsification. Performance is better with these features in higher quality images which allow for better\nlocalization of the points used. This indicates that there are some differences in shape between\ngender and ethnic classes. It is possible that these differences could be represented better with a\nmore sophisticated shape representation. In future work, a different shape representation, possibly\nActive Shape Models, will be investigated for use in partial face.\n\nThe shape features perform better for the most part without feature reduction. The size of\nthe original feature vectors in this category are much smaller than those used in color and texture\nexperiments. Feature reduction on these small feature vectors resulted in a loss of too much infor-\nmation for both gender and ethnic classifications, resulting in lower classification performance than\nthe original vectors. Baseline face experiments did not necessarily follow this trend. Some classifiers\nperformed better after PCA reduction was performed.\n\nThe most stable regions with respect to age for both gender and ethnicity are the eye and\nchin regions. The overall shape of the eye does not change much with age, neither does the shape of\nthe chin. The chin region has its own variance due to facial hair, but the small section of chin used\n\nfor shape here, does not vary much with age. These regions might be the most stable, but they are\n\n70",
    "Page_79": "not the best performing regions for the shape features investigated.\n\nThe nose regions show the best performance for both gender and ethnicity. These regions\nare also impacted the most by age. Males, on average, have larger noses than females which give\nthe best performance in gender classification. Between the ethnic classes there are also some size\ndifferences which allow for better classification. However, the nose grows slightly with age, which\ncan negatively impact both gender and ethnic performance as subjects get older. For future work,\nthe age of subjects should be taken into account when planning the training and testing sets.\n\nDifferent types of classifiers performed better between gender and ethnicity. Nearest neigh-\nbor classification performed better for ethnicity classification than the ANN and SVM classifiers,\nindicating a large overlap between the classes that the more sophisticated classifiers were unable\nto classify correctly. The SVM classifiers performed well in the FRGC dataset with gender classi-\nfication, but MORPH did not have enough separation to learn the Female class, always predicting\nMale. MORPH performed better with larger ANNs or nearest neighbor classification than SVM or\nANN with the smallest hidden layer.\n\nFor further experiments in the application section, the NOSE region will be used for only\nethnicity classification. Since shape region experiments performed better without feature reduction,\nPCA will not be performed on the nose shape features. The linear SVM classifier will be used to\nclassify this region in the hopes of obtaining the highest performance. The final category of features\ninvestigated for the reliability and age questions is texture. The following chapter will discuss the\n\nresults on texture classification.\n\n71",
    "Page_80": "Chapter 5\n\nTexture\n\nLocal texture can be found in the face in terms of skin texture, wrinkles, imperfections\nin the skin, and facial hair, including, but not limited to, eyebrows, mustaches, and beards. The\nstableness of texture depends on what is causing the specific texture. Skin texture will most likely\nbe stable on a day to day basis, but change over the years as wrinkles increase. Imperfections\nin the skin and facial hair can change more quickly. Three different local texture representations\nwill be investigated in this chapter: Histograms of Oriented Gradient, Local Binary Patterns, and\nLocal Phase Quantization. A brief description of the extraction method follows for each texture\n\nrepresentation.\n\n5.1 Feature Extraction Methods\n\n5.1.1 Histograms of Oriented Gradient\n\nOriginally proposed by Trigg and Dalal [18] for the detection of human pedestrians, His-\ntograms of Oriented Gradient (HOG) have been used for facial recognition purposes [20, 65]. The\nbasic idea of HOG is that local shape and appearance, or texture, can be characterized by the\ndistribution of the local image gradients. The image is divided into smaller regions, providing the\nlocalized area. The Prewitt convolution kernel is used to compute the image gradient. The gradient\n\nmagnitude, Gyz, and the gradient angle, G4, are computed by\n\nGy = (GX + GE and G4 = atan2(Gy, Gx).\n\n72",
    "Page_81": "Gx and Gy are the image gradients in the horizontal and vertical directions. The gradient orienta-\ntions, or angles, for each pixel are used to select the histogram bin and increment by the gradient\nmagnitude. For this work, the orientations are divided into 30° segments resulting in 12 bins and a\n\nfeature vector of length 12 per patch.\n\n5.1.2 Local Binary Patterns\n\nLocal Binary Patterns (LBP) were first introduced by Ojala et al. [53] to classify texture\npatterns. The LBP method looks at the neighborhood around each pixel in an image. The value\nof this feature representation is that it encodes different textures that can represent curved edges,\nspots, and even uniform areas. The texture for a specific pixel is represented by thresholding the\nintensity values of the neighboring pixels with the intensity value of the center pixel, given by the\n\nequation:\n\nv\n\n-1 1, ifa>0\nLBPp,r(gc) = 8(Jp — Je)2”, where s(x) =\n\n0 0, otherwise\n\n3\nIL\n\nP is the number of pixels in the neighborhood investigated along a circle of radius R which is centered\nat pixel c. g- and gp refer to the grayscale value for the given pixels. LBPp,p(gc) represents the\ntexture pattern at pixel g.. The texture patterns are accumulated into a histogram. The granularity\nof the histogram is dependent on the choices for P and R. For the LBP features used in this work\nthe values P = 8 and R = 2 were chosen. The “uniform” version of LBP was used which limits the\npatterns in the histogram to those with 2 or less changes between 0 and 1, resulting in a feature\nvector length of 59 per patch. Patterns that are not uniform are counted in the last bin of the\nhistogram. LBP was chosen having been used successfully for soft biometric classification using full\n\nfacial images [41, 75].\n\n5.1.3 Local Phase Quantization\n\nLocal Phase Quantization (LPQ) was recently proposed by Ojansivu et al. [54] as a descriptor\nfor texture which is robust to image blurring. Similar to HOG and LBP, the LPQ method looks\nat each pixel individually and accumulates the results into a histogram. The phase information is\nquantized and compiled into a histogram. By utilizing only the phase information, the method is\n\nalso not affected by uniform illumination changes. It has been used successfully for facial recognition\n\n73",
    "Page_82": "[2, 11, 12].\nThe LPQ feature extraction method first performs a Discrete Fourier Transform (DFT) on\n\nthe image, given by the equation,\n\nF(u,x) = S f(x — yearly,\nyENz\n\nwhere u is the frequency, x is the pixel location within the image, N, is a rectangular M x M\nneighborhood, and f(x) is the intensity value of a pixel in the image. Four frequencies are considered\nin this algorithm. The real and imaginary components of the frequencies are used to create a\ntransform matrix such that F, = Wf, where f, is a vector containing the pixels in N,, W is the\ntransform matrix, and F, is the transform coefficient vector.\n\nAfter further processing of F,, including decorrelation and whitening, the coefficient vector\nis quantized by thresholding the vector at 0. The LPQ code, corresponding to the histogram bin, is\n\ncalculated by\n8\nb= D792,\nj=l\n\nwhere gj is the thresholded value in F,. The feature vector for LPQ has 256 elements per patch.\n\n5.2 Experiment Setup\n\nThe experiments for this section will be performed on subsets of the FRGC and MORPH\ndatabases, using a grayscale version of the image. Experiment sets remain the same from color and\nshape experiments and preprocessing is performed as described in Chapter 2. Features are extracted\nusing the HOG, LBP, and LPQ methods. Gender and ethnicity classifications are performed on all\ntexture features using all classifiers. The total number of partial-face experiments in this chapter is\n126 for each dataset and demographic. Five runs of a stratified cross-validation experiment are run\nfor each feature, region, and classifier combination. Baseline face experiments are performed using\n\nLBP texture features.\n\n74",
    "Page_83": "5.3 Analysis\n\nFigures 5.1 show the average HOG features over each region for MORPH. Not much con-\nsistency exists across the regions. Even the eyes do not share similar texture indicating that texture\naround the eyes are independent. Issues may arise with the differences in texture between the re-\ngions. Texture may rely more on accurate localization of the facial features. If the region extraction\nis off, different texture may be present and not be classified correctly. These features also indicate\nthat the mouth and chin region hold different types of textures than the other regions, see bins 4-7.\n\nFigures 5.2 show the average LBP features over all the regions for MORPH. The last bin in\nthe histogram counts all the miscellaneous textures that are not counted in the rest of the bins. The\nFRGC features have twice the amount of miscellaneous textures as MORPH. This is most likely\ndue to the higher resolution of the FRGC images allowing texture to be extracted in greater detail.\nDifferences between regions in the rest of the bins are much smaller due to the content being spread\nover a greater number of bins. Other than magnitude, the shapes of the feature vectors are similar\nover both datasets. The texture features for the eyes are more similar than HOG, but several sections\nof the histogram are different, indicating that LBP texture is somewhat independent as well.\n\nThe LPQ feature vectors are much larger than both HOG and LBP, and will not visualize\nwell. With 256 bins in the histogram, the quantity in each bin is very small. Values are found in\nthe majority of bins, unlike many of the color space features.\n\nFigure 5.3 shows the differences between the regions for each feature, using the histogram\nintersection distance measure. Differences between the left eye and the other regions were similar to\ndifferences between the right eye and the rest of the face. The same is true for the nose and nose tip\nregions; therefore the graphs have been condensed to show pairs with the left eye, nose tip, mouth\nand chin regions. The nose and the nose tip are the most similar region still because of the overlap\npresent between the two. The largest differences in texture exist between the eye and chin regions.\nThese differences are larger for females on average, so it is not entirely due to facial hair. The Black\nclass has the smallest region differences for each texture representation, indicating texture is more\nstable around the face for that particular ethnic group. The region differences with texture features\nare larger than those reported in Section 3.4 indicating that texture is not as stable around the face\nas color.\n\nRegion differences show less change with respect to age for the Female class than the Male",
    "Page_84": "class. This indicates that texture across the face is more stable over the years for females than males.\nThis could be due to make-up and other facial care products that are more widely used by females\nwhich smooth skin texture. It could also be due to the variation in facial hair found in both classes.\nThe largest difference that also shows a large variance with respect to age for the Male class is the\neye to mouth comparison. Younger male faces show a larger difference in texture between these two\nregions than older males. This trend holds true for more than just the Male class and this region\ncomparison.\n\nIn the majority of the comparisons, if a variance is present with respect to age, the younger\nsubjects have larger differences than older subjects, indicating that texture is not as stable around\nthe face for younger subjects as it is for older subjects. The exception is present in some comparisons\non the lower face. In the Black class, the distances between the nose and mouth or chin regions is\nlarger for older subjects, and also in the White class in the mouth to chin comparison. In the Black\nclass, the largest difference with high variance due to age is the difference between texture in the eye\nand mouth regions. Combined with the face that this class also has the most stable texture around\nthe face, this suggests that wrinkles, especially in the eye region, show up later than in other classes.\nFor the White class, the corresponding difference is found between the eye and nose regions. The\nHispanic class has the most age variance between the same regions as both the Black and White\n\nclasses. The other ethnic classes show no discernible pattern with respect to age.\n\n5.4 Gender\n\n5.4.1 Reliability\n\nFigure 5.4 shows the results of 1-NN gender classification on the texture features for both\nFRGC and MORPH. FRGC shows there are differences between the genders based on texture\nwith some performances above 80%. The LPQ features perform the best out of the three texture\nrepresentations indicating that this feature representation encodes the most gender information.\nThese features are able to distinguish gender correctly at least 70% of the time in all but the nose\nregion. This region performed the worst in all three feature representations, indicating less gender\ntexture information can be found in the nose. The CHIN region was able to distinguish between the\ngenders the best. This is likely due to the presence of more facial hair, and thus different texture, in\n\nthe CHIN region of the Male class. The HOG features performed the worst, which is a result of the\n\n76",
    "Page_85": "Relative frequency\n°° o 28 °e 9\n§ € 5» & € € 8\n8 8 ££ 8S &€ &@ &\n\n2\n8\ng\n\n°\n°\n8\n\n \n\nBin number\n\nbidaunlaali\n\n \n\n7\n\nLEYE\nREYE\nNTIP\nNOSE\nMOUTH\nCHIN\n\nFigure 5.1: Average global feature vector for MORPH HOG features. These are the normalized his-\ntograms for each region. The x-axis corresponds to the bin in the histogram. The y-axis corresponds\n\nto the relative frequency of values found in each bin.\n\not ™@ LEYE\nm REYE\nm NTIP\nm NOSE\n0.08\" m™ MOUTH\n> = CHIN\n2\n&\n$\n© 0.06\n@\n$\n&\n2\n0.04\n0.02\n% 10 20 30\n\nBin number\n\n40\n\n50\n\nFigure 5.2: Average global feature vector for MORPH LBP features. These are the normalized his-\ntograms for each region. The x-axis corresponds to the bin in the histogram. The y-axis corresponds\n\nto the relative frequency of values found in each bin.\n\n77",
    "Page_86": "0.35\n\n= Male\n= Female\n\n   \n\nDifference\n\n \n\nye hip Ce Bie Oe me My\nre, Me, re, re, re, \"7p, \"Me\nRe ce\nre Mp NOs. un, My Ose. Cy,\n0.35 = Male\n= Female\na\nley, le, Ley, &\nEe Ye, mp ENog. © SO, va ry,\nrep Dn, Py Cay,\n= Male\n= Female\n£\na\nfy, & yy Gn Ge Mp Vp ny\nS, &, S, e, E \", We, 70 \"Ol\n“Fe Win Mog Moyy, Vn, \"Ose Moun Hy Hey\nNy\n\n \n\n  \n\n   \n\n      \n\n \n \n \n \n  \n  \n \n\n035 = White\n= Asian\n1 Hispanic\n03 = Black\n1 Indian\n0.25\n5 02\n5 01s\n01\n0.05\nOg, Se, Oe, Sey Se Ming Ming Mn Oey\nRee Vy Nos. Moy, Ty, Os. Moy, ny\nee Un, ty 8 ng, yy,\n0.35 = White\n® Asian\n= Hispanic\n03 = Black\n= Indian\n\nDifference\n\n“Oy, Ce, “Ere, “Ore, Gre Yn, Vn, Vr, Mo\n\nRee Vy Noo. Moun Oy NOs. \"Our, Om, Cy\n\n      \n\n= White\n= Asian\n= Hispanic\n\n \n\n0.25\n\nBlack\nIndian\n\nDifference\n\n   \n\n   \n\n       \n\n    \n\nnp,\n\nNip, “ip, “oy,\nOm,\n\nCie ley ey ey Oe,\nRey, My 05. Mou, \"Can,\nery,\n\nhe Wp Os. Cun,\n\n  \n \n\n‘ny,\n\nFigure 5.3: Mean difference between MORPH region-wide global texture vectors by demographic.\nDifferences shown are for HOG, LBP, and LPQ (top to bottom) using the Histogram Intersection\n\ndistance.\n\n78",
    "Page_87": "100\n\n8\n8\n\na\n\n40\n\nClass-wise performance (%)\n\n20\n\n100\n\n2\n\n40\n\nClass-wise performance (%6)\n\n20\n\n100\n\n8\n8\n\n60\n\n40\n\nClass-wise performance (%)\n\n20\n\n0\n\nLEYE\n\nLEYE\n\nLEYE\n\nREYE\n\nREYE\n\nREYE\n\nNOSE\n\nNOSE\n\nNOSE\n\nNTIP\n\noi bby aly\n\nNTIP\n\nNTIP\n\nMOUTH\n\nMOUTH\n\nMOUTH\n\nUte ile bal UB I\n\nCHIN\n\nCHIN\n\nCHIN\n\n2\nA\n\n00\n\nol,\n\nB Le\n\nClass-wise performance (%6) Class-wise performance (%)\n\nClass-wise performance (%)\n\n100\n\n20\n\n100\n\n3\n&\n\n20\n\n100\n\n20\n\n0\n\nLEYE\n\nREYE\n\nNOSE\n\nNTIP\n\n2\nA\n\nooo\nBS\n\nMOUTH CHIN\n\nol,\nal\n| | | | B Leo\n\nLEYE\n\nREYE\n\nNOSE\n\nNTIP\n\nMOUTH — CHIN\n\nol,\nal\n| | | B Le\n\nLEYE\n\nREYE\n\nNOSE\n\nNTIP\n\nMOUTH CHIN\n\nFigure 5.4: 1-NN gender classification on texture features over all regions. Values included in the\nbox-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom:\nHOG, LPQ, LPQ. L to R: FRGC, MORPH\n\n79",
    "Page_88": "texture not being consistent around the face. Small errors in localization result in different textures\nextracted, which leads to worse performance.\n\nThe performance on the MORPH dataset shows more overlap between the classes than is\nseen in the FRGC dataset. This is likely due to the lower quality of the MORPH dataset. The\ntexture cannot be extracted as reliably because it is not present in the same detail as in FRGC. The\nCHIN region does not perform noticeably better like it does in the FRGC dataset. Performance\non the Male class increases, but the performance of the Female class suffers for the L, distance\nmeasure. The performance of the HOG features in MORPH is much closer to the performance of\nthe other two representations than it is in FRGC. This could be a positive side effect of the lower\nresolution. Since the texture is not extracted in as much detail, the HOG features are more similar\nand not as dependent on the precise localization of the region.\n\nFigure 5.5 shows the results of gender classification on the texture features using the more\ncomplex classifiers, ANN and SVM. The results improve for both datasets, all regions, and all\nrepresentations. This suggests that there is a lot of overlap in the feature space, but differences exist\nthat set the classes apart, which the ANN and SVM classifiers are able to learn. The HOG features\nstill perform the worst in FRGC, and the gap between HOG and the other two representations are\nmore noticeable in MORPH with these classifiers. These results suggest that HOG is not the best\ntexture feature for encoding gender information.\n\nFigure 5.6 shows some of the easier and harder subjects to classify according to gender\nusing texture features. The majority of the subjects classified correctly in all texture experiments\nwere from the Black class. Some were from the White and Hispanic classes. All were from the\nMale class. The images that were misclassified in over 95% of texture gender experiments were all\nfrom subjects labeled Black Female. As previously seen in color classification results, one of the\nsubjects labeled Female is not female. Texture classification also predicted the class correctly, but\nwas marked incorrect since the subject was mislabeled.\n\nIn these results, the chin region still performs very well in the FRGC dataset, but it can also\nbe seen that the mouth and nose regions perform better than the other regions while looking at the\nMORPH dataset. The lack of texture detail found in MORPH seems to shift the importance from\nthe chin region to the mouth and nose regions. This indicates that the lower face regions hold more\ntexture information specific to gender than the upper face regions. The eye regions also perform\n\nvery well in FRGC, but the lack of texture detail in MORPH is more detrimental, putting their\n\n80",
    "Page_89": "100\n\n8\n\naged apa! gigg eget oped\n\n60\n\nClass-wise performance (96)\n\n20\n\nLEYE REYE NOSE NTIP\n\n100\n\ngga 9908\n\nvs sega 9960 gab\n\n60\n\nClass-wise performance (96)\n\n20\n\nLEYE REYE NOSE NTIP\n\n100\n\naegg 9988 song \"oeR tH88\n\n80\n\n60\n\nClass-wise performance (96)\n\n20\n\n0\n\nLEYE REYE NOSE NTIP\n\n@ ANNs\n\nMOUTH\n\nMOUTH\n\naa BF ANN 20\n@ ANNioo\nB SV Mrincar\nCHIN\n@ ANN;\n. 50\noe ANNoo\n@ ANNioo\nB SV Mrincar\nCHIN\naes; @ ANNs»,\n8 a ANNoo\n@ ANNioo\nB SV Mrincar\nCHIN\n\nMOUTH\n\nClass-wise performance (96) Class-wise performance (96)\n\nClass-wise performance (96)\n\n100\n\n8\n\n8\n\n&\n\n20\n\n100\n\n8\n\n8\n\n20\n\n100\n\n8\n\n20\n\n0\n\nLEYE\n\nLEYE\n\nLEYE\n\nREYE\n\nREYE\n\nREYE\n\nB ANNs»\nB ANNo\n@ ANNioo\nB SVMiincar\n\nMOUTH\n\nNOSE —NTIP ‘CHIN\n\nB ANNs»\nAN N20\n\n@ ANNioo\n\nB SVMiincar\n\nMOUTH\n\nNOSE —NTIP ‘CHIN\n\n@ ANNs\n\nB ANNox0\nANNioo\n\nB SVMrinear\n\nMOUTH\n\nNOSE —NTIP ‘CHIN\n\nFigure 5.5: ANN and SVM gender classification on texture features over all regions. Values included\n\nin the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to\nbottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH\n\n81",
    "Page_90": "Figure 5.6: Easy and hard subjects in texture gender classification on MORPH images. Subjects on\nthe left are a subset of those correctly classified in all texture experiments. Top to bottom: Black,\nWhite, and Hispanic. Subjects on the right those who were misclassified in over 95% of experiments.\nThe subject on the bottom was labeled incorrectly as Female.\n\nFRGC MORPH\nClassifier Male Female Average | Male Female Average\nAN Nio0 94.85 15.80 55.33 — — —\nSV Miinear | 92.89 91.02 91.96 ~ — ~\nLy 79.21 64.35 71.78 | 97.43 22.16 59.80\nVeriLook 92.72 98.45 95.59 | 99.14 67.75 83.45\n\n \n\nTable 5.1: Gender performance using LBP texture and full face. Percentage values represent the\nclass-specific accuracies and the average class-specific accuracy for each experiment. VeriLook pre-\ndicted no gender on 2.75% and 0.89% of male and female images respectively in the FRGC dataset.\nIn the MORPH dataset it predicted no gender on 0.5% and 11.26% of male and female images.\n\nperformance below the nose regions. This still follows what was seen in in previous work, the nose\nand eye regions perform among the best in previous texture experiments [47].\n\nTable 5.1 shows the results of the baseline face experiments using LBP texture and gender\nclassification. Texture on the CHIN region was able to achieve similar performance to the commercial\nclassification of the entire face in FRGC. The MOUTH region, as well as the NOSE region, was able\nto do the same in the MORPH dataset. Since these regions achieve similar performance to the\ncommercial results and not just the classification of the full face features, the gender information\n\nencoded in the texture of the MOUTH and CHIN seems to be sufficient for partial face classification.\n\n82",
    "Page_91": "5.4.2 Age\n\nThe results of gender classification on MORPH partitioned according to age can be seen\nin Figure 5.7. The classifier used in the experiments shown is the AN N50 classifier. These graphs\nindicate that the Male class is impacted less by changes in age than the Female class; however the\nmouth and chin regions do show variance in both classes with respect to age. The nose regions\nshow the least variance in classification accuracy in the LBP and LPQ features. The nose tip region\nshows less variance than the nose region, which makes sense, because the nose region overlaps the\neye regions which show the most variance with respect to age. The impact of age is seen the most\nseverely in the eye and chin regions for the Female class. Overall, the nose tip region is the least\nimpacted by age, but the mouth region still performs well over all age groups, even with variance\ncaused by age.\n\nIn full face experiments, little change is seen in the performance of the Male class according\nto age. The Female class shows a downward trend with respect to age, with the lowest performance\n\non subjects 41 to 50 years of age. After that performance improves again.\n\n5.5 Ethnicity\n\n5.5.1 Reliability\n\nThe results of nearest neighbor ethnic classification on both datasets can be seen in Fig-\nure 5.8. Compared to gender classification, the results are much more spread out. In both datasets,\none class performs rather well, with a high class-specific accuracy, but at least one or more of the\nclasses also has a class-specific accuracy below 10%. MORPH most often has at least two classes\nbelow 10%, Asian and Native American, and one more under 20%, Hispanic. The best performing\nclasses are still the most well-represented classes in the experiment set.\n\nFigure 5.9 shows the results of ethnic classification of the texture features using the ANN\nand SVM classifiers. FRGC experiments improve overall, but still have a very low median class-wise\naccuracy. MORPH experiments improve on the best performing classes, but decline slightly on the\nless well-represented classes. The exceptions to these are in the LBP features using a SVM classifier.\nAll but the CHIN region show a slight increase in the worst performing classes. In these experiments,\n\nMORPH results are likely to have the same two classes under 10%, but the other class, the Hispanic\n\n83",
    "Page_92": "1620\n80 :\nz\né 60 S\ng \"7\nzg\n2 0\n20\ne LEYE REYE NOSE NTIP MOUTH CHIN\n100\nle\n80 c\nzg\né 60\n<\nzg\n2 0\n20\ne LEYE REYE NOSE NTIP MOUTH CHIN\n100\n80 | | |\nz\n2 6\n<\nzg\n2 0\n20\ne LEYE REYE NOSE NTIP MOUTH CHIN\n\nFigure 5.7: Gender performance on texture features partitioned by age. From top to bottom the\n\ngraphs represent the HOG, LPB, and LPQ experiment results. The classifier used is AN N59 for\neach graph shown.\n\n84",
    "Page_93": "100\n\n100\n\nol, ol,\nal al\n= 80 : B Lew S 80 B Leg\n2 2\n: :\n— 60 — 60\né é\n2 2\n& 40 & 40\n3 3\n8 8\nSo 20 So 20\n2 LEYE REYE NOSE NTIP MOUTH CHIN 2 LEYE REYE NOSE NTIP MOUTH CHIN\n100 . 100 .\nal aL;\nB 80 = ° B 80 liad\n© ©\nz z\n5 5\nE 60 E 60\n£ £\n5 5\na a\nB40 B40\ni i\n8 8\nUv 20 Uv 20\n7 LEYE REYE NOSE NTIP MOUTH CHIN 7 LEYE REYE NOSE NTIP MOUTH CHIN\n100 : 100 . : 6\nal aL,\nz 80 a z 80 ; a\n2 2\n: :\n— 60 — 60\né é\n2 2\n& 40 & 40\n3 3\n8 8\nSo 20 So 20\n2 LEYE REYE NOSE NTIP MOUTH CHIN 2 LEYE REYE NOSE NTIP MOUTH CHIN\nFigure 5.8: 1-NN ethnic classification on texture features over all regions. Values included in the\n\n \n\nbox-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom:\nHOG, LPQ, LPQ. L to R: FRGC, MORPH\n\n85",
    "Page_94": "@ ANNs\nB ANN\n@ ANNioo\nB SV Mrincar\n\nClass-wise performance (%)\n\nLEYE REYE NOSE TIP MOUTH CHIN\n\n \n\na00 @ ANNs\n\nB ANNox0\nANNioo\nB SVMrinear\n\nClass-wise performance (%)\n3 2 2\n\n \n\nLEYE REYE NOSE TIP MOUTH CHIN\n\n@ ANNs\nANNo9\n\n@ ANNioo\n\nB SV Mrincar\n\nClass-wise performance (%)\n\n0\n\n \n\nLEYE REYE NOSE TIP MOUTH CHIN\n\nClass-wise performance (%) Class-wise performance (%)\n\nClass-wise performance (%)\n\n \n\n0\n\n@ ANNs\nB ANN\n@ ANNioo\nB SV Mrincar\n\nLEYE REYE NOSE TIP MOUTH CHIN\n\n@ ANNs\n\nB ANNo\nAN Nioo\n\nB SVMiincar\n\nLEYE  REYE NOSE TIP MOUTH CHIN\n\n@ ANNs\nANNo9\n\n@ ANNioo\n\nB SV Mrincar\n\nLEYE REYE NOSE NTIP\n\nMOUTH\n\nCHIN\n\nFigure 5.9: ANN and SVM ethnic classification on texture features over all regions. Values included\nin the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to\nbottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH\n\n86",
    "Page_95": "Figure 5.10: Easy and hard subjects in texture ethnic classification on MORPH images. Images to\nthe left are a subset of those correctly classified in all experiments. Images to the right are from\neach subject having an image that was never classified correctly. Most are from the Native American\nclass. The three images in the lower right-hand corner correspond to Hispanic subjects.\n\nClassifier | White Asian Hispanic Black Indian Average\n\n \n\nAN Nio0 99.54 12.84 0.33 0.33 0.00 22.61\nSV Miinear | 97-28 92.01 2.33 13.00 = 14.25 43.78\nLg 84.04 53.10 1.00 3.50 8.75 30.08\n\nTable 5.2: Ethnic performance using texture and full face, FRGC. Percentage values represent the\nclass-specific accuracies and the average class-specific accuracy for each experiment.\n\nclass, improves so that it more often has a class-specific accuracy above 20%.\n\nImages from 25 subjects were classified correctly in all texture ethnicity experiments. Sub-\njects with images classified incorrectly for all experiments were from the Native American and Asian\nclasses. Figure 5.10 shows images from some of these subjects. The hardest class to identify with\ntexture was the Native American class. All 13 subjects from that class have at least one image that\nwas never classified correctly. The easiest subjects to identify were Black subjects, specifically Male.\n\nTable 5.2 and 5.3 show the results of ethnic classification on the FRGC and MORPH datasets\nrespectively using LBP features and the full face. The FRGC eye and nose regions outperform the\n\nfull face experiments by at least 5% on the average class-wise accuracy. The same regions do not\n\nNative\n\n \n\nClassifier | White Asian Hispanic Black . Average\nAmerican\n\nAN Nio0 92.96 5.32 48.62 98.25 0.00 49.03\n\nLz 50.87 2.38 18.15 95.56 0.00 33.39\n\nTable 5.3: Ethnic performance using texture and full face, MORPH. Percentage values represent\nthe class-specific accuracies and the average class-specific accuracy for each experiment.\n\n87",
    "Page_96": "outperform MORPH face experiments, but come within 3-5% of full face results. This indicates\nthat some of the texture ethnic information is lost with the lower resolution, but that enough can\n\nbe found for comparable performance to full face experiments.\n\n5.5.2 Age\n\nEthnic results according to age can be seen in Figure 5.11. The results shown use the\nAN N%5p classifier. The graphs for each feature look similar, so only the graph for the LPQ features\nis shown. Overall the mean class-specific accuracy shows a downward trend with increasing age.\nThe jump in performance for the 41-45 and 61-77 classes is due to the face that not all ethnic classes\nwere present in these age groups and were omitted from the graph.\n\nThe White and Black classes show fairly consistent performance across the age groups\nwith very small declines present for some features and regions after 56 years of age. Class-specific\naccuracies for the Hispanic class peak around 46% in the 21-25 age group. Performance for that class\ndrops below 25% in the 41-45 age group and does not recover. This suggest that younger Hispanic\npartial faces are easier to classify according to ethnicity than older ones. Classification of the Asian\nclass is spotty, but most of the correct classifications fall within the first two age groups, 16-25. This\nalso indicates that younger Asian partial faces are easier to classifier according to ethnicity than\nolder faces in the same ethnic group.\n\nThe regions that seem the least affected by age are the nose regions. The most affected\nregion is the chin. However, in each of the regions, the Hispanic class is negatively affected by age,\nno matter the performance on the other classes. The results suggest that age affects different ethnic\ngroups differently on the various parts of the face.\n\nOverall, the face shows the same trends as the partial face experiments. Younger subjects\nare easier to classify, especially for the Asian and Hispanic classes. The Black class is the least\naffected by age with performance only dropping 8% from its highest when subjects reach 56 years\n\nof age.\n\n5.6 Conclusions\n\nResults from this chapter suggest that texture from partial face images can be used success-\n\nfully for gender and ethnic classification. Analysis shows that texture is not as stable around the\n\n88",
    "Page_97": "1B 26-30\n@ 31-35\n@ 36-40\n1B 41-45\n© 46-50\n60 w 51-55\n\n@ 56-60\n\n1m 61-77\n\n4 8 1620\n| , 2125\n\nClass-wise performance (%)\n\n \n\nLEVE REYE NOSE NTIP MOUTH CHIN\n\nFigure 5.11: Ethnic performance on texture features partitioned by age. Results shown are from the\nLPQ experiments using the AN Nso classifier.\n\nface as color. More texture gender information can be found in the lower regions of the face, while\nthe upper face holds more ethnic texture information. The various regions of the face are affected\ndifferently by age. The texture information in the nose is least affected by age while the texture\nin the chin is the most affected. The LPQ and LBP features achieve higher performance than the\nHOG features.\n\nThe best results for gender classification were achieved using the chin region with the LBP\nand LPQ features and the ANN classifiers on high quality images and the mouth region with LPQ\neatures and the linear SVM classifier with the MORPH images. The best average class-specific\naccuracy on FRGC images was 94% and the best on MORPH was 87%. The classifiers performed\n\netter on the Male class than the Female class indicating males have more distinct gender charac-\neristics than females. The MOUTH and CHIN regions were able to achieve comparable results to\null face classification using a commercial system.\n\nThe best results for ethnic classification were achieved using the left eye region with the\nLPQ features and the ANN classifiers on high quality images and the right eye region with LBP\neatures and the linear SVM classifier with the MORPH images. The best average class-specific\naccuracy was 52% on FRGC and 49% on MORPH. The eye regions performed well in both the\nLPQ and LBP features. With the ANN classifiers, the eyes performed just 2-3% under the best\n\nerformance on MORPH. The eye and nose regions were able to achieve comparable results to full\n\n \n\nace classification using similar features.\nThe effect of age seems to be present more by class than facial region. The Black and\n\nWhite classes were less impacted by age than the Hispanic class, but that might be due to the larger\n\n89",
    "Page_98": "number of samples present for each age group in these classes. Younger Hispanics are easier to\nclassify by ethnicity than older individuals. The same is true for the Asian class. The performance\nof the Male class overall holds steady over the different age groups while performance of the Female\nclass holds steady until the 41-45 age group. The mouth region improves slightly with age while the\nnose regions decline the least.\n\nSince one of the fusion methods will use the scores from ANN classification, the AN N00\nclassifier will be used for each region and feature combination. The REYE region with LPQ features\nwill be used for ethnic classification and the MOUTH with LBP for gender classification as the best\nperforming regions over all. The NOSE region with LPQ will also be used for both gender and\nethnicity as a more age stable option. This concludes the analysis of reliability and the impact of\nage on partial-face images. The following chapter covers the fusion experiments using partial-face\n\ngender and ethnicity classifications to filter a face biometric experiment.\n\n90",
    "Page_99": "Chapter 6\n\nApplication\n\nUsing the conclusions from the previous chapters, the experiments in this section will com-\nbine classification results with a typical face recognition or verification experiment in an effort to\nimprove performance. The features and regions that will be used were chosen from color, texture,\n\nand shape. The following experiments will be performed on the Pinellas dataset.\n\n6.1 Experiment Setup\n\nExperiments in this section require three sets of images: training, gallery, and probe sets.\nTable 6.1 shows the demographic breakdown of each of these sets. In combining classification results\nwith the performance of the VeriLook SDK face experiment, the Pinellas images had to pass the\nquality check within VeriLook. Images from previous FRGC and MORPH experiments did not\nhave to meet this criteria. VeriLook also recommends 50-75 pixels between the eye centers. The\naverage interocular distance for the Pinellas images used was 110.80. A maximum of four images\nwere selected for each subject. Subjects with 3-4 good images were used in the gallery and probe\nsets, while subjects with 1-2 good images were used in the training set. In the probe and gallery\nsets, the image with the highest age was placed in the probe set. Any subject with images labeled\nwith conflicting gender or ethnicity information was excluded from the lists. In order to create a\nmore balanced training set, many subjects and images were discarded from the candidate training\nlist. A limit of 10,000 subjects per gender and ethnicity class (i.e. White Male) was set. One image\n\nper subject was used. This limit affected both gender classes in the White and Black ethnicities.\n\n91",
    "Page_100": "Ethnicity Gender\nTraining Probe Gallery Training Probe Gallery\nWhite  20,000(1) —-62,025(1) ~—-62,025(2) | Male = 30,555(1)—71,383(1) —71,383(2)\nAsian 1,562(1) 738(1) 738(2) | Female 21,620(1) —37,208(1) ~—_37,208(2)\nHispanic —10,613(1) 7,059(1) 7,059(2)\nBlack _20,000(1) _ 38,769(1) _38,769(2)\nTotal = 52,175(1) ~—-108,591(1) = 108,591(2) 52,175(1) 108,591(1) —108,591(2)\n\n \n\nTable 6.1: Breakdown of subjects in the Pinellas experiment sets by gender and ethnicity. The\nnumber of images per subject in each class is given in parentheses.\n\nBecause of the relative small size of the Native American class, less than 200 subjects, and the poor\nperformance of the class in the MORPH experiments, subjects of this class were excluded from these\nexperiments. Out of 393,426 subjects with consistent gender and ethnicity labels, 52,175 were used\nfor training and 108,591 for classification and the biometric experiment.\n\nPrior to training and classification, PCA is performed on the texture and color features,\nkeeping 95% of the variance, similar to the previous experiments. The PCA projection for the\nclassification experiments is learned on the training set.\n\nTwo types of fusion will be investigated, using two levels, decision and score. In the first\nmethod, which can be described as a weighted sum decision fusion, the results of multiple classi-\nfications can be used in combination with the match scores from the recognition and verification\nexperiments. These classifications will determine if the match score is used or discarded. Each of\nhe probe classification results, p;, are provided to the experiment along with a weight, m; for each\nclassification set and a threshold, where i = 1...C and C is the number of classifications used. A\narticular match score, comparing probe j to gallery k is used when the following evaluates to true:\n> mi(pi(j) == gi(k)) > threshold. The value p;(j) is the predicted class of the j\" value in the\nrobe using the 7” classification. Similarly, the value g;(k) is the given class of the k*® value in the\nh\n\ngallery using the i‘ classification. Basically, this is a weighted sum of which classifications match.\n\nf enough match, the value is above the threshold, and the score is used. This equation can be used\n\n \n\no perform ‘AND’ and ‘OR’ decisions as well as equally weighted ‘7 out of k’. The second method\nis based on the scores generated by the classifier, one for each class. The maximum score is the\nclass given as the decision, but it is possible that other scores are very close. If any of the scores are\nwithin a given threshold of the maximum score, the corresponding class is added to the predicted\nlist. The match score will be used if the class of the gallery entry is found within the predicted list\n\nof the probe. Multiple classifiers may be used in this method as well.\n\n92",
    "Page_101": "Ethnicity Gender\nLabel | Region Feature Classifier Region Feature Classifier\n1 MOUTH LCH(RGB) ANNioo MOUTH LCH(RGB) AN Nioo\n2 NTIP LCH(RGB) ANN5 NTIP LCH(RGB) ANNio0\n3 REYE LPQ AN N00 MOUTH LBP AN Noo\n4 NOSE LPQ AN N00 NOSE LPQ AN Noo\n5 NOSE Shape SV Miinear | — — ~\n\n \n\n \n\n \n\n \n\nTable 6.2: Region/Feature/Classifier combinations chosen for application experiments.\n\n \n\nTable 6.2 lists the region/feature/classifier combinations that will be used in the experiments\nfor this chapter. Two combinations were chosen for both gender and ethnicity classification for\ntexture and color features. Shape features performed very poorly for gender classification, and so\nwill only be used for ethnicity classification in this chapter. The MOUTH region was chosen for color\nas the best performing region for both gender and ethnicity. The NTIP region was chosen for being\nmore stable with respect to age. The LCH features from the RGB color space perform the best over\nall color spaces investigated. The best texture region/feature combinations were the REYE/LPQ\nand MOUTH/LBP for ethnicity and gender classification respectively. The NOSE region with LPQ\nwas chosen for its performance across the age groups. All except the shape classifier will use an\nANN classifier to facilitate the score level fusion method. The ANN classification was close to linear\nSVM in the best cases, just a little lower. The shape classifier will use a linear SVM because all\nANN classification was poor, even in ethnicity classification.\n\nResults in this chapter will be reported following previous methods for classification, includ-\ning confusion matrices and box-plots of class-specific accuracies. Verification and recognition results\n\nwill be reported using DET and CMC plots as well as EER and Rank-1 values.\n\n6.2 Analysis\n\n6.2.1 Classification Results\n\nBoth the gallery and probe sets were classified according to the region/feature/classifier\ncombinations given in Table 6.2. The results from classification of the gallery set will be reported here\nto give an idea of how well the combinations performed on Pinellas data. Table 6.3 shows the results\nfor the combinations involving color, while Table 6.4 shows results using texture combinations. There\n\nare two main differences between MORPH and Pinellas ethnic classifications. Pinellas experiments\n\n93",
    "Page_102": "| White Asian Hispanic Black | White Asian Hispanic Black\n\n \n\n   \n\nWhite 45.58 0.95 A9.24 4.22 White 33.23 1.89 55.05 9.83\nAsian 12.13 4.20 72.76 10.91 Asian 3.66 14.16 59.55 22.63\nHispanic 5.06 0.34 88.95 5.65 Hispanic 2.94 0.96 86.99 9.11\nBlack 1.53 0.11 12.00 86.36 Black 1.85 0.51 24.08 73.56\n(a) le, average class-wise accuracy: 56.27%, overall (b) 2e, average class-wise accuracy: 51.99%, overall\naccuracy: 62.68%. accuracy 51.00%.\nMale Female Male Female\nMale 81.45 18.55 Male 84.94 15.06\nFemale | 24.01 75.99 Female | 35.10 64.90\n(c) lg, average class-wise accuracy: 78.72%, overall (d) 2g, average class-wise accuracy: 74.92%, overall\naccuracy: 79.58%. accuracy: 78.07%.\n\nTable 6.3: Confusion matrices from classifying the gallery set on the chosen color region/ feature/\nclassifier combinations. The row is the true class and the column is the predicted class. All entries\nare percentages. Class-specific accuracies are highlighted.\n\nonly utilize four ethnic classes, leaving out the Native American class. Since this class averaged\nclose to 0% in MORPH experiments, the Pinellas average class-wise accuracies will show an increase\nwithout that class. The other difference is in the class-specific accuracies for White and Hispanic.\nThe performance on the White class is reduced by almost half while performance on the Hispanic\nclass doubles. The biggest factor for the Hispanic class is likely the larger number of samples in\nthe training set. While not equal to the number samples in the White and Black classes, the\nHispanic training set is much closer to their proportions in Pinellas than MORPH. The decrease in\nperformance in the White class is most likely related due to overlap between the classes since a large\npercentage of the Hispanic class were misclassified as White in MORPH experiments.\n\nThe results of ethnic classification on the nose shape features can be seen in Table 6.5. The\naverage class-wise accuracy for this experiment is low, but it has a higher overall accuracy than other\nethnicity experiments. This is due to the higher performance on the White class, which accounts\nfor approximately 57% of the probe and gallery sets.\n\nGender classification on both texture and color combinations is similar to that found in the\nMORPH experiments. Performance on the Female class showed little change, but the performance\non the Male class dropped approximately 10%. Males are still correctly classified more often than\nFemales, just not as much.\n\nThe texture combinations still show the best performance when looking at average class-\nwise accuracies. Color and shape are sometimes able to gain higher overall accuracies based on the\n\ndistribution of classes in the set, but texture performs better for the majority of the classes.\n\n94",
    "Page_103": "| White Asian Hispanic Black | White Asian Hispanic Black\nWhite 50.13 0.32 43.34 6.21 White 42.50 0.50 50.19 6.81\nAsian 8.20 17.28 61.31 13.21 Asian 5.49 12.87 57.45 24.19\nHispanic 4.46 0.39 89.12 6.03 Hispanic 3.58 0.40 86.53 9.49\nBlack 1.93 0.10 14.00 83.97 Black 1.36 0.15 14.80 83.68\n\n(a) 3e, average class-wise accuracy: 60.12%, overall\naccuracy: 64.52%.\n\nMale\n87.64\n23.68\n\nFemale\n12.36\n76.32\n\n \n\nMale\nFemale\n\n(c) 3g, average class-wise accuracy: 81.98%, overall\naccuracy: 83.76%.\n\n(b) 4e, average class-wise accuracy: 56.40%, overall\naccuracy 59.86%.\n\nMale\n85.73\n21.94\n\nFemale\n14.27\n78.06\n\n \n\nMale\nFemale\n\n(d) 4g, average class-wise accuracy: 81.89%, overall\naccuracy: 83.10%.\n\nTable 6.4: Confusion matrices from classifying the gallery set on the chosen texture region/ feature/\nclassifier combinations. The row is the true class and the column is the predicted class. All entries\nare percentages. Class-specific accuracies are highlighted.\n\n \n\n| White Asian Hispanic Black\nWhite 74.70 0.00 5.78 19.52\nAsian 44,38 0.00 8.81 46.82\nHispanic | 42.51 0.00 11.63 45.86\nBlack 10.06 0.00 3.40 86.53\n\nTable 6.5: Confusion matrices from classifying the gallery set on the chosen shape combination,\n5e. The row is the true class and the column is the predicted class. All entries are percentages.\nClass-specific accuracies are highlighted. Average class-wise accuracy: 43.22%, overall accuracy:\n\n74.32%.",
    "Page_104": "20\n\n@ Training\n™ Gallery\n™@ Probe\n\n15\n=\nyn\nSo 10\noO\n£\n\n5\n\n  \n\n67.\n\nQ7. 165 2.5, 4 35 Sy To, Vm Stn, % 65.5 77.5 %9. 81.9 So. Io 1\n0 75 \"Ca? 39-399 336 Co. gy age Wesy Mo5¢ 6.5, M56 7, “M39 Cray Fr5¢ eg, M99 \"Og\n\nAge Ranges\n\nFigure 6.1: Age distribution of Pinellas experiment sets. Values graphed represent the percentage\nof images in each set found in the age ranges listed.\n\n6.2.2 Age\n\nFigure 6.1 shows the distribution of ages across the three image sets used for Pinellas\nexperiments. While compiling information on age, it was discovered that six images in the gallery\nset and one in the training set are labeled with negative ages. Each one is from a different subject\nand those in the gallery set have two corresponding images with a maximum age gap of 2 years\nbetween them. The images were stored with date of birth and date of image capture which were\nused to calculate the age. In some instances the correct date of birth was entered, but a default\nimage capture date instead of the actual date was used, January 1, 1900 for example. These cases\nresulted in a negative age. Figure 6.2 shows images from the gallery which had negative ages.\n\nImages with calculated ages less than sixteen were also found. While it is possible that the\nimages labeled with an age of fifteen are legitimate, it is highly unlikely that subjects age two to nine\nyears old are present in a database compiled from booking photographs. The image lists show 19\nimages of two to nine year-olds, 24 of ten to fourteen year-olds, and 50 of fifteen year-olds. Figure 6.3\nshows some examples of images that are associated with ages fifteen and under. At the other end\n\nof the scale, the image lists show 15 images of subjects greater than one-hundred years old. This is\n\n96",
    "Page_105": "Figure 6.2: Images from Pinellas with negative calculated ages.\n\n \n\nFigure 6.3: Pinellas images with calculated ages below 16. Calculated ages of the images on the top\nrow from L to R: 2 and 9; and on the bottom row: 13 and 15.\n\npossible; however, it is very unlikely that a subject would be in the database at both twenty-five and\none-hundred and five years of age, which is the case for one probe/gallery combination. Figure 6.4\nshows images from the probe and gallery sets with ages greater than one-hundred years old. These\nobvious errors are due to mistakes in the metadata which can be caused by mistyped or default\ninformation. It is possible that other errors in age occur in the middle ages, but these are not as\nobvious since those ages are well represented and have a high probability of naturally occurring in\nthe database.\n\nEach of the image sets, training, gallery, and probe, contain approximately the same distri-\nbution of ages. The training and gallery sets contain more younger subjects, but are within 5% of\n\nthe amounts in the probe set. The creation of the probe and gallery sets put the younger images in\n\n97",
    "Page_106": "Figure 6.4: Pinellas images with calculated ages over 100. Top row are images in the gallery set.\nBottom row consists of corresponding probe images (also with an age over 100).\n\nie 2 3e ae Se\n\nFigure 6.5: Performance of chosen region / feature combinations on the probe set according to age.\nResults are from gender (left) and ethnicity (right) classification.\n\n100 100\n\n   \n\nce 4)\n\n   \n\n‘lass-wise perform’\n\n \n\n° °\n\nthe gallery set which accounts for the discrepancy between those lists. The selection of the training\nset did not account for age.\n\nFigure 6.5 shows the performance of each of the region/feature/classifier combinations ac-\ncording to age for both gender and ethnicity classification. Gender classification shows more of an\nimpact with respect to age than ethnicity, with the easiest ages to classify falling between 30 and\n45. For most of the ethnic classes, younger subjects are easier to classify according to ethnicity than\nolder ones. The decline is the most pronounced in the Hispanic class. These findings agree with\n\nprevious observations on the MORPH experiments.\n\n6.2.3 Rank-1 Score vs Genuine Score\n\nThe match scores of the straight face experiment directly impact how much improvement\ncan be seen in the fusion experiments. The range of similarity scores for the straight whole-face\n\nexperiment was 0 to 10080. Figure 6.6 shows an enlarged section of the distribution of scores\n\n98",
    "Page_107": "between impostor and genuine comparisons. The area of overlap between impostor and genuine\nscores is of interest in this section.\n\nSeveral aspects were investigated for individuals that had a genuine match not at Rank-1 for\nthe straight whole-face experiment. The average score difference between the Rank-1 match and the\nhighest genuine match was 51.51, with a standard deviation of 102.31, which is not much considering\nthe range of scores present. Over 95% of the subjects had a difference of less than 200 between the\ngenuine and Rank-1 match. 174 of them actually had a difference of zero. These instances were ties\nthat were unaccounted for in the experiment. The system marked the lowest score as a match and\ndid not update the match unless a score was strictly less than the match it held.\n\nThe average genuine score for probes with an incorrect Rank-1 match was 78.58 with a\nmedian of 54 and a standard deviation of 68.44. The average Rank-1 match score was 130.09 with a\nmedian of 88 and a standard deviation of 133.40. The average rank of the genuine match was 1,777,\nbut with a median of 3, it seems highly skewed. The standard deviation was 12,368.48. The mean\ngenuine score over all the probes was 199.36, with an average Rank-1 score of 207.18. This supports\nwhat can be seen in Figure 6.6, that the lower the genuine score, the more likely it is for the probe\n\nto have an impostor score as a Rank-1 match.\n\n6.2.4 Ground Truth Fusion\n\nA baseline experiment was run with no soft biometric classifications used to filter the results.\nThis experiment, performed with the VeriLook system, obtained a Rank-1 performance of 84.83%\nand an EER of 3.27%. The straight facial experiment was also fused with the ground truth informa-\ntion for gender and ethnicity, both together and separately. This is the best performance possible if\nthe classifiers performed perfectly. These graphs can be seen in Figure 6.7. For Rank-1 performance\nthere is little change; the fused experiments increase the Rank-1 performance up to 85.29% for the\nexperiment where gender and ethnicity both match. The biggest increases occur when ethnicity\ninformation is utilized. The CMC curves reflect the same trends, with the fused experiments having\na steeper beginning than the straight experiment. The EERs increase with the fusion of both (AND)\ngender and ethnicity, increasing the most to 4.02%. This feels counter-intuitive. If obviously wrong\nscores are being excluded and all the genuine scores are being kept, it seems as if the performance\nshould get better. Table 6.6 gives the Rank-1 performance and EER for each experiment as well as\n\nthe total number of genuine and impostor scores used in the calculations.\n\n99",
    "Page_108": "2.5 T\nHl Impostor\n\nMl Genuine\n\nPercentage of scores\n\n \n\n \n\n200 250 300 350\nScore value\n\nFigure 6.6: Distribution between impostor and genuine scores for straight whole face experiment.\n\nIn verification experiments, the FAR and FRR are used to create the graphs and calculate\nthe EER. For these ground truth experiments, the FRR will stay constant since all genuine scores\nwere included. The FAR is what changes. In calculating the FAR, the number of impostor scores\nabove the threshold is divided by the total number of impostor scores. In this way the EER is\ndependent on the number of impostor scores. In Table 6.6, as the number of impostor scores used\nincreases, the EER decreases. Table 6.7 shows the average number of comparisons per probe for\neach of the baseline experiments. The greatest reduction, almost 75%, occurs when both gender and\nethnicity need to match, but a 24% reduction still occurs when either gender or ethnic class matches\nare included.\n\nFigure 6.6 shows the distribution between genuine and impostor scores for the facial verifi-\ncation and identification experiments. The majority of the impostor scores are grouped with scores\nbelow 50, while the genuine scores are more spread out. 70% of the impostor scores are actually\nzero. Less than 1% of the genuine scores are also zero. It is likely that the genuine scores of zero\ncontribute to the minimal increase in Rank-1 performance when using soft biometric information.\nEven if the candidate matches are narrowed to those matching in both gender and ethnicity, there\n\nis a good probability that one of the impostor scores will be greater than zero.\n\n100",
    "Page_109": "] Impostor Genuine Rank-1 EER\nMethod Attempts Attempts (%) (%)\nStraight 23,583,793,380 217,182 84.82 3.27\nGender 12,959,718,724 217,182 84.99 3.65\nEthnicity 10,800,803,040 217,182 85.12 3.68\nBoth (AND) 5,906,125,240 217,182 85.29 4.02\nBoth (OR) 17,854,396,524 217,182 84.85 3.49\n\nTable 6.6: Baseline performance details on facial recognition fusion experiments\n\nperformance and lowest EER are in bold.\n\n. Highest Rank-1\n\n \n\nMethod | Impostor (Reduction) Genuine\nStraight 217,180.00 (0%) 2.00\nGender 119,344.37 (45%) 2.00\nEthnicity 99,463.15 (54%) 2.00\nBoth (AND) 54,388.72 (75%) 2.00\nBoth (OR) 164,418.75 (24%) 2.00\n\nTable 6.7: Average comparisons per probe on baseline experiments. Percentage by which the number\nof impostor scores are reduced as compared to the straight experiment is given in parenthesis.\n\n— straight\n\n40 —+ gender\n—+ ethnic\n—+ both (AND)\n—+ both (OR)\n\n20\n\nMiss probability (in %)\n\nO1\n\n \n\n \n\n100,\n\n99.8}\n\n99.6)\n\no\ns\nFS\n\n \n\nCumulative Performance (%)\n\n— straight\n—+ gender\n— + ethnic\n\n—+ both (AND)\n— + both (OR)\n\n \n\n \n\n0.102 05 1 40\nFalse Alarm probability (in %)\n\n(a) Verification\n\nFigure 6.7: Baseline performance on identification and verification experiments.\n\n0 05 1 15 2 25\nRank x10*\n\n(b) Identification\n\nBaseline exper-\n\niments include the straight whole face experiment and fusion with ground truth gender and/or\nethnicity. DET is on the left and CMC is on the right.\n\n101",
    "Page_110": "A large percentage of the probe/gallery pairs, 91.6%, have an age gap of 0-4 years, with\n52.2% of the pairs actually having a gap of less than one year. Only 7.3% of the pairs have an age\ngap between 5 and 9 years. Of the rest, 1% of probe/gallery pairs have an age gap from 10-14 years,\nwhile only 0.1% have an age gap of 15 years or more. Some of these large age gaps stem from issues\nin the metadata mentioned earlier, such as a 25 year old with an image also appearing with an age\nof over 100. While the quality of the images is not as high of quality as found in FRGC, the images\nin these experiments had to meet the quality standards of the VeriLook SDK algorithm. The close\nage gap for most of the images and the multiple gallery entries, as well as the quality standards,\nwork together to make the problem easier than it could have been. It is possible that, by making\nthe original problem harder and changing any of those factors, more room for improvement using\n\nfusion can be seen.\n\n6.3. Results\n\nThere is always a risk in filtering the gallery before matching that the true match will be\nexcluded. That is why it is important to use multiple classifications to minimize the possibility of\nthe genuine matches getting filtered out. However, the more classifications used and/or the more\nlenient the rule, the more gallery entries will be included. What good is it to do all the work for\nclassification and yet still end up using the entire gallery due to a lenient filtering rule? It would be\nless work to just do all the gallery comparisons. Mixing the types of classification will help minimize\nthis. For example, there are two gender classifications for the probe and each predicts a different\ngender, an ‘OR’ classification rule will not rule out any of the gallery entries. But if an ethnicity\nclassification is added and the rule changed to ‘2 out of 3 (equal weights)’, some gallery entries would\nbe excluded based on the ethnicity classification. Still, having multiple gender classifications will\ndecrease the number of matches used, only on probes with differing classifications will all gallery\nentries be included. The following experiments will investigate using different numbers and types of\nclassification, both feature and soft biometric trait. The legends in the following graphs will use the\nlabels from Table 6.2 with an ‘e’ or a ‘g’ following to indicate ethnicity or gender for the classification\n\ntype. An indication of the rule used will also be included, such as ‘AND’ or ‘OR’.\n\n102",
    "Page_111": "Impostor Genuine Rank-1 EER\nMethod Attempts Attempts (%) (%)\nlg 12,707,142,872 171,434 67.46 3.40\n2g 13,069,745,244 165,812 65.02 3.39\n1g2g OR 16,174,486,048 198,884 77.85 3.40\nle 6,471,957,338 125,026 48.25 3.80\n2e 5,518,189,458 99,636 38.57 3.63\nle2e OR 7,961,731,784 138,652 53.59 3.68\n1g2gle2e OR | 18,694,632,076 209,012 81.58 3.40\n\n \n\nTable 6.8: Weighted sum fusion with color information. Highest Rank-1 performance and lowest\nEER are in bold.\n\nImpostor Genuine Rank-1 EER\nAttempts Attempts (%) (%)\n5e 10,451,692,626 159,468 61.92 3.88\nle5e OR | 12,374,469,974 190,666 74.49 3.50\n2e5e OR | 12,229,813,908 182,340 71.28 3.46\n3e5e OR | 12,420,118,146 190,800 74.53 3.48\n4de5e OR | 12,321,065,622 187,332 73.16 3.48\n\nMethod\n\n \n\nTable 6.9: Weighted sum fusion with shape and other information. Highest Rank-1 performance\nand lowest EER are in bold.\n\n6.3.1 Weighted Sum Decision Fusion\n\nTable 6.8 shows the results of filtering the gallery results using classifications based on color,\ncombinations 1 and 2, and the weighted sum decision rule. None of the experiments using just color\nclassifications were able to get too close to the results of the straight experiment. This indicates\nthat color alone is not what is needed to improve performance; however, since the color regions were\nnot the best performing regions from the MORPH dataset, this is not unexpected.\n\nIt is interesting to note that the performance in the verification experiments (EER) does\nnot change as dramatically as the performance in the recognition experiments (Rank-1). The CMC\ncurve is affected more by the exclusion of the genuine matches than the verification measurements.\nThe verification measurements include information about the number of impostor attempts, which\ncan outweigh the genuine attempt information.\n\nResults using ethnic shape classification combined with other ethnic classifications can be\nseen in Table 6.9. Shape ethnicity alone outperforms the color ethnicity experiments, but still does\nnot achieve comparable performance to the straight facial recognition experiment. Performance can\nbe traced back to good performance on the most prevalent class in the experiment set, White.\n\nTable 6.10 shows the results of the weighted sum fusion using texture classifications. Results\n\n103",
    "Page_112": "Impostor Genuine Rank-1 EER\n\n \n\n \n\n  \n\nMethod Attempts Attempts (%) (%)\n3g 2,843,138,128 192,678 75.54 3.54\n4g 2,716,977,840 178,866 70.30 3.46\n3e4e OR 5,415,754,464 210,044 += -82.19 3.49\n3e 6,893,394,536 129,712 50.30 3.67\nde 6,451,954,140 120,486 46.66 3.66\n3e4e OR 8,819,949,264 154,140 59.71 3.63\n\n \n\n3g4g3ede OR 8,436,420,630 215,202 84.09 3.44\n\nTable 6.10: Weighted sum fusion with texture information. Highest Rank-1 performance and lowest\nEER are in bold.\n\nImpostor Genuine Rank-1 EER\n\n \n\nMethod Attempts Attempts (%) (%)\n1g3g OR 5,522,684,680 207,358 81.17 3.47\nlg4g OR 5,742,372,896 203,056 79.57 3.44\n293g OR 6,197,310,236 209,090 81.87 3.44\n2ede OR 5,871,225,112 199,762 78.25 3.42\n1g2g3g OR 7,498,432,304 212,966 83.33 3.42\n1g2g4¢ OR 7,439,113,114 209,562 81.98 3.40\nlg3g4g OR 6,962,513,126 213,370 83.46 3.44\n2g3g4g OR 7,278,916,008 213,618 83.57 3.42\n\n \n\n1g2g3g4g OR | 18,227,744,400 215,076 84.12 3.40\n\nTable 6.11: Weighted sum fusion with mixed color and texture features. Highest Rank-1 performance\nand lowest EER are in bold.\n\n \n\nfrom these classifications are higher than those using color and shape, although shape outperformed\nthe single texture ethnic experiments. Using both gender and ethnicity texture classifications, these\nexperiments were able to come within 1% of the Rank-1 performance of the straight experiment.\nThese results further support the conclusion that the most gender and ethnicity information can be\nfound in texture features.\n\nThe results in Table 6.11 show results using a mixture of color and texture classifications,\neither all gender or all ethnicity. Since the ethnic classifiers do not perform as well as the gender\nclassifiers, fusion experiments using only ethnic classifications are unable to match the performance\nof the experiments utilizing gender classifications and are so omitted from the table. All four texture\nand color gender classifiers together are able to achieve a higher Rank-1 performance than any of the\nexperiments using just color or texture, but it is still below the Rank-1 performance of the straight\nexperiment.\n\nTaking the top performing classifiers from gender and ethnicity, 3g, 3g4g, 5e, and 3e5e,\n\ngender and ethnicity classifications from different feature types were used together. The results can\n\n104",
    "Page_113": "Impostor Genuine Rank-1 EER\nAttempts Attempts (%) (%)\n3g5e OR 17,526,217,284 209,666 81.93 3.45\n3g3e5e OR 18,434,508,896 213,634 83.48 3.44\n3g4g5e OR 18,997,060,648 214,982 84.00 3.42\n3g4g3e5e OR | 19,678,094,918 216,108 84.43 3.40\n\nMethod\n\n \n\nTable 6.12: Weighted sum fusion with mixed texture and shape classifications, best single and double\ntaken for gender and ethnicity. Highest Rank-1 performance and lowest EER are in bold.\n\nbe found in Table 6.12. The best result seen so far can be found in these experiments utilizing 3g4g\nand 3e5e. Replacing 4e with 5e from the all texture combination results in an improvement of less\nthan 0.5%.\n\nOne possible way to improve the results would be to compare the predicted class of the probe\nwith the predicted class of the gallery. This would allow someone who is consistently misclassified by\nthe classifier to have the genuine match score included. Using the predicted gallery class provides a\n5-7% improvement in Rank-1 when using color gender classifications and an improvement of 20-30%\nwhen using color ethnicity. When using both gender and ethnicity, the improvement shrinks to only\n3%. Texture shows an improvement of 2-3% on gender and approximately 20% on ethnicity, but less\nthan 1% when utilizing both. Shape experiments improve 8-10% when using the predicted gallery\nclasses. The results using the best classifiers improve less than 0.5%. The best Rank-1 results when\nusing predicted gallery is still 3g4g3e5e, 84.70%, followed by all gender classifiers, 84.69%, and all\ntexture, 84.66%. All these results are still 0.1% below the straight facial recognition system. The\n\nrelated CMC and DET plots can be seen in Figure 6.8.\n\n6.3.2 Score Fusion\n\nExperiments in this section have a slight advantage over those in the previous section. By\nusing the scores returned by the classifier instead of just the decision, the experiment has a little\nmore information with which to make the decision if the probe and gallery entry might be a match.\nTable 6.13 shows the results of a score-level fusion on color classifications to decide if match scores are\nused in the recognition experiment. Rank-1 scores are 2% higher using color gender and 5% higher\nwith color ethnicity than the corresponding weighted sum fusion experiments. However, using both\ncolor gender and ethnicity classifications, the Rank-1 performance still falls 2% below the straight\n\nface experiment. Table 6.14 shows the results using texture scores. Gender results improve less",
    "Page_114": "—straight\n40 : ——34g35e\nco 1234g\nMone oS. -— 34g34e\naN\nay >\n20 os z=\n$ 8\n< 5\n= 10 €\n= S\n3 =\nQs D\nS a\n5 é\ng 2 &\n¢ 3\n= —\n6\n0.5\n— straight\n0.2 —~34g35e\ni QBSf oreo renner desman nennenfinnaannneninfsnsmmnanneiuen 1234g\n“ ~~ 34g34e\n95)\n0.102 05 1 2 5 10 20 40 0 1 2 3 4 5\nFalse Alarm probability (in %) Rank x10°\n(a) Verification (b) Identification\n\nFigure 6.8: Best weighted sum fusion experiments compared to the straight experiment. DET is on\nthe left and CMC is on the right.\n\nthan 2% from weighted sum, while ethnicity shows an improvement of approximately 10%. Using\nboth gender and ethnicity improves less than 1% from the other fusion method, but the Rank-1\nperformance is within 1% of the straight face experiment. There are no shape experiments in this\nsection, because this fusion requires scores from an ANN classifier, and the shape classifier used a\nSVM classifier.\n\nExperiments mixing both gender and ethnicity classification using both color and texture\n\ninformation allow for a small increase as seen in Table 6.15. At this point the number of impostor\n\n \n\n \n\n \n\n \n\n \n\nMethod Impostor Genuine Rank-1 EER\n\n” Attempts Attempts (%) (%)\nlg 13,193,346,780 175,740 69.13 3.40\n2g 14,258,951,034 176,196 69.06 3.39\n1g2g OR 17,003,923,860 203,298 79.57 3.38\nle 7,716,306,302 137,362 53.01 3.72\n2e 6,746,179,502 113,470 43.89 3.62\nle2e OR 9,634,780,128 152,988 59.13 3.59\n1g2gle2e (G AND E) 6,934,431,200 144,566 56.03 3.76\n1g2gle2e (G OR E) 19,704,272,788 211,720 82.67 3.37\nAll (G OR E) predicted gallery | 22,587,493,941 217,153 84.81 3.30\n\nTable 6.13: Score fusion with color information. Highest Rank-1 performance and lowest EER are\nin bold.\n\n106",
    "Page_115": "Impostor Genuine Rank-1 EER\nMethod Attempts Attempts (%) (%)\n3g 14,303,998,630 192,596 75.58 3.43\n4g 14,422,806,168 192,442 75.51 3.45\n3g4g OR 17,163,555,578 211,572 82.81 3.42\n3e 9,047,946,474 155,040 60.26 3.56\nde 8,455,913,224 143,936 55.78 3.61\n3ede OR 11,400,418,864 177,956 69.16 3.70\n3g4g3ede (G AND E) 8,314,459,576 173,258 67.49 3.72\n3g4g3ede (G OR E) 20,249,514,866 216,270 84.49 3.37\nAll (G OR 5) predicted gallery | 22,383,990,928 216,920 84.83 3.30\n\nTable 6.14: Score fusion with texture information. Highest Rank-1 performance and lowest EER\nare in bold.\n\n \n\nImpostor Genuine Rank-1 EER\nMethod | Attempts Attempts (%) (%)\n3g4glesede 20,586,393 362 216,570 84.60 3.36\n1g3g4gle3ede | 21,032,866,890 216,826 84.70 3.35\n\n \n\nTable 6.15: Score fusion with mixed gender, ethnicity, color and texture classifications. Highest\nRank-1 performance and lowest EER are in bold.\n\nscores and genuine scores are nearing the numbers for the straight face experiment. As with weighted\nsum fusion, it is possible that matching to the predicted gallery would increase the Rank-1 perfor-\nmance by allowing more genuine scores to be included. Using all color combinations, the Rank-1\nperformance is just 0.01% under the performance of the straight experiment, but less than 5% of\nthe impostor scores are actually ruled out from the experiment. Using all texture combinations,\nthe Rank-1 performance improves to just 0.01% over the performance of the straight experiment.\nAgain, approximately 5% of the impostor scores are excluded. The CMC and DET graphs for these\nexperiments can be seen in Figure 6.9. The EERs are not as good for the fusion experiments, and\nthe performance on the CMC is very similar. So the improvement is very small and possibly not\n\nworth the cost of four partial face gender and ethnicity classifications.\n\n6.4 Conclusions\n\nWhile the gender and ethnicity information is useful in decreasing the number of impostor\nscores, in the proposed fusion experiments, multiple classifications on both the probe and gallery\nsets are needed to achieve a very slight improvement, 0.01% in Rank-1 and only a 5% reduction\n\nin comparisons. If the classifiers were perfect, the increase in Rank-1 in this particular scenario\n\n107",
    "Page_116": "100\n— straight\n40 ~~ 34g3de (G or E) predicted\nmS. ~----12g12e (G or E) predicted\nan ~~. 134g134e (G or E)\n\n \n\n20 ee,\n\n \n\n \n\n \n\n10\n\nMiss probability (in %)\nwn\nCumulative Performance (%)\n\n \n\n0.5\n\n— straight\n0.2 ~~ 34g34e (G or E) predicted\n\ncere 12g12e (G or E) predicted\n~~~ 134g134e (G or E)\n\n0.102 05 1 2 5 10 20 40 0 0.5 1 1.5 2 2.5\nFalse Alarm probability (in %) Rank x 10°\n\n0.1\n\n \n\n \n\n \n\n \n\n(a) Verification (b) Identification\n\nFigure 6.9: Best score fusion experiments. DET is on the left and CMC is on the right.\n\nwould be slightly larger, 0.47%, but the reduction of comparisons is much greater. Any of the\nregion/feature/classifier combinations by themselves were unable to improve Rank-1 recognition.\nThe small improvement in performance is not worth the cost multiple partial-face classifications in\nthis instance, but this may not be true in all cases. The score-level fusion allowed for better results\nwith the same or fewer classifications than the weighted sum decision fusion.\n\nIt is possible that some performance could be gained by making several changes to the\nproposed fusion methods. The first change was implemented by comparing the probe classification\nto the gallery classification instead of the ground truth information for the gallery subject. This\nresulted in a small gain in Rank-1 performance. Another possibility is to compare the probe classi-\nfication with both the gallery classification and the ground truth and combine those in a weighted\nsum. The weights could be individually learned in an authentication experiment, so that, for an\nindividual who is generally misclassified more weight is given to the classification comparison and\nfor an individual whose classification is haphazard, the weights could give equal consideration to\nboth. For a verification system, the weights would most likely be learned off-line from a training\nset or set as a system parameter. The final change would be to start with a partial face experiment\nas opposed to a whole face experiment. This would be more for the identification experiments than\n\nfor verification experiments. With less information to make the decision of who an individual is, the\n\n108",
    "Page_117": "soft biometric information might be able to leverage a larger gain in performance.\n\nImproving the classification accuracy would also help increase fusion experiment perfor-\nmance. In looking at the confusion matrices and misclassification rates for this section and the\nothers, a better representation of the Asian and White subjects is needed. Class-wise accuracy on\nthese classes is lower compared to the others. Key-point features as opposed to local-appearance\nbased features might be a better fit for these classes. The color and texture features used in this\nwork were extracted using local-appearance-based features. This would also play into using partial-\nface experiments. If feature extraction could be done without facial alignment, similar to the work\nby Liao and Jain [42], it would open up a lot more options for biometric applications. The system\nwould not need the whole face and it would not need specific points to reliably extract a certain part\nof the face. This would allow a more covert acquisition of facial images and help deal with occlusion\nand image quality. Another method to improve classification accuracy is to incorporate boosting\ninto the training algorithm so that the classifier spends more time learning how to classify difficult\nsubjects and less time on individuals that are easy to classify [21].\n\nThis concludes the discussion of fusion experiments performed using partial-face classifica-\ntions to filter a face experiment. The final chapter will summarize the conclusions made from all\n\nexperiments and discuss directions for future work.\n\n109",
    "Page_118": "Chapter 7\n\nConclusions and Future Work\n\n7.1 Reliability\n\nThe first question considered in this work was which parts of the face hold reliable gender\nand ethnicity information for machine-based classification. To the human eye, different parts of the\nface can have a feminine or masculine quality which can agree or disagree with a person’s identity.\nThe same can occur with ethnicity. Experiments were performed on two datasets, for both gender\nand ethnicity classification. Regions of the face considered were the left eye, right eye, nose tip,\nnose, mouth and chin.\n\nFrom 360 partial-face color experiments on the FRGC and MORPH datasets, the conclusion\nwas made that the mouth and eye regions provide the most gender information. The mouth region\nachieved similar performance to full face indicating the potential for as much gender information as\nthe whole face. From the same number of ethnicity classifications, the mouth and eye regions also\nprovide the most ethnicity information, again, comparable to face. The best region depends on the\nparticular ethnic group. For most it was the mouth region, but Asian class was erratic. The best\ncolor space for feature extraction was the RGB color space. The difference between this space and\nthe others was that RGB included color information in all three channels and did not compress it\ninto two. This agrees with what was seen in previous work, that the eye regions are useful for gender\nand ethnicity classification and that the RG channels from the RGB color space work well.\n\nPartial-face shape experiments for FRGC and MORPH numbered 168 for each trait, gender\n\nand ethnicity. Results suggest that partial-face shape is not reliable for either gender or ethnicity.\n\n110",
    "Page_119": "This is possibly due to feature representation and selection as opposed shape encoding no informa-\ntion. Full face results in previous work were able to achieve similar gender classification performance\nto texture features. Differences were seen between the gender and ethnic classes, but the overlap\nbetween them was too large for classifiers to adequately separate them. The best classifications used\nall features with no reduction. This is another indication that shape shows promise. It is possible\nthat a better representation or different feature selection could perform better.\n\nIn 252 partial-face texture gender experiments over the FRGC and MORPH datasets, it\nwas determined that texture features encode the most gender information in the lower face regions,\nnamely the mouth and the chin. These regions achieved performance comparable to the gender clas-\nsification of a commercial face system. In the corresponding ethnic experiments, it was determined\nthat the texture of the upper face, namely the eye regions, holds the most ethnicity information.\nThe eye and nose regions were able to achieve similar performance to the texture classification of the\nentire face. Previous work has shown most regions of the face to be useful for gender classification\nusing either texture or pixels, so these results agree with some and contradict others. Previous ethnic\nresearch has shown that the eye region holds discriminating ethnic information, which is confirmed\nin texture experiments.\n\nOver all 780 gender and 780 ethnicity experiments, texture features encode the most reliable\ngender and ethnicity information. Not all texture features are equal; however. The best texture\nrepresentations for this work were LBP and LPQ. The color features encode more soft biometric\ninformation than HOG features, but less than LBP and LPQ, indicating that it may be a useful\nfeature when texture cannot be accurately extracted. The best regions for gender classification are\nthe mouth. For ethnic classification, it is the eyes followed by the nose. The eyes perform well in\ngender classification as well. Region size does not seem to matter although the resolution of the\noriginal image does.\n\nEthnicity is a harder problem than gender classification. Not only does it have more possible\nclasses, the classes are open to interpretation by individual. An individual’s self-perception of\nethnicity does not always agree with anyone else’s or even society’s perception of their ethnic identity.\nThis makes classifications of this nature more difficult.\n\nWithin these experiments, no clear conclusion could be drawn in regards to the best clas-\nsification method. Both SVM and ANN classifiers had good performance and bad performance. It\n\nseemed to depend more on the feature than on the actual classifier. In most cases, both of these\n\nlil",
    "Page_120": "classifiers outperformed the k-NN classifier.\n\n7.2 Age\n\nThe second question considered in this work was the effect of age on the machine classifi-\ncation of gender and ethnicity using partial-face images. The human face changes as it ages, with\nskin becoming more coarse and gaining wrinkles. MORPH experiments were analyzed here to see if\nany region/feature combinations were less susceptible to changes in age than others.\n\nFrom the experiments in Chapters 3, 4, and 5, the conclusion was made that no region or\nfeature remained unaffected by changes in age. The regions including the nose were determined\nto be the the most stable for gender classification and fairly stable for ethnic classification. The\nchin region was stable for both as well, but had the lowest performance in ethnicity classification.\nEthnicity classification was less impacted by age than gender classification, which follows the trends\nseen in previous work classification involving the whole face. The impact of age on specific features\nwas dependent on the region. No clear conclusion could be drawn on the invariance of specific\nfeatures to age. Color features did seem less susceptible to changes in age than texture features, but\nwere still negative impacted by age.\n\nDifferent classes were affected differently by age. The Female class was more impacted by\nage than the Male class, especially in the eye regions. Younger subjects were easier to classify in\n\nmost cases, both gender and ethnicity.\n\n7.3 Application\n\nThe third question considered in this work was the extent to which partial-face gender\nand ethnicity classification could be used to improve the performance in a biometric application.\nExperiments were performed on one dataset not previously used in this work. Choices on the\nproposed methods were made based on the experiments in Chapters 3, 4, and 5. Two types of fusion\nwere investigated, one each at the score and decision levels.\n\nWhile the partial-face gender and ethnicity classifications were useful in decreasing the\nnumber of impostor scores, the proposed fusion methods gained little to no improvement in Rank-1\n\nperformance as compared to the straight face experiment. An improvement of 0.01% in Rank-1\n\n112",
    "Page_121": "performance using score fusion only excluded 5% of impostor scores. This contradicts the idea that\nthe soft biometric information used must be independent of the biometric modality, but follows\nthe trend that the more dependent the traits and the modality are, the smaller the improvement\nin biometric performance. Multiple partial-face classifications of both gender and ethnicity were\nneeded to gain the small improvement in Rank-1. The complexity and number of classifiers most\nlikely outweighs the reduction in comparisons achieved. Other experiments excluded more impostor\nscores, but this resulted in the exclusion of more genuine matches and decreased Rank-1 performance.\nUsing ground truth information only netted a small gain in Rank-1 performance, 0.47% and resulted\nin higher EERs.\n\nAs it stands, partial-face gender and ethnicity did not improve the performance of the\nchosen whole-face biometric application much. The conclusion was made that more improvement is\npossible, just not with the given whole-face application. Several changes were suggested to improve\nupon the methods given in this work, including using a partial-face recognition system, a single\ngallery experiment, basing the fusion on the comparison of the predicted probe class to the predicted\ngallery class, and changing how the soft biometric information is incorporated into the experiment.\nOther suggestions were based on improving the classification performance such as using key-point\nbased features instead of local-appearance and incorporating boosting algorithms into the training\n\nof the classifiers.\n\n7.4 Future Work\n\nFuture work directly tied to improving the proposed methods in this work can be divided\ninto two sections: improving the soft biometric classification and improving the performance of the\nfusion of soft biometric classifications with a biometric application. To improve the classification\nperformance, the use of boosting algorithms during training may prove useful. Also, both classifi-\ncation methods used in this work are machine-learning based, so trying a statistical classification\nmethod may provide more insight into the problem. Features used for classification were confined\nto local-appearance based texture and color features. Key-point features as well as shape features\nmight encode more reliable information for classification. To improve fusion performance, the base\nexperiment can be changed to a partial-face experiment and the fusion method changed from filtering\n\nto a feature to be compared.\n\n113",
    "Page_122": "There are several other topics that would be helpful to pursue but are not directly tied to\nimproving the proposed methods in this work. In the field of biometrics, there is literature on the\n‘biometric menagerie’ or Doddington’s Zoo’ [73]. This classification refers to a subject’s likelihood of\ncontributing to the FAR and FRR in an authentication problem. The categories are Sheep, Lambs,\nGoats, and Wolves. Subjects who are well separated from the others and rarely get rejected are\nlabeled as sheep, whereas subjects who are very hard to recognize and contribute to the FRR are\nclassified as goats. Lambs are subjects who overlap significantly with other users and contribute to\nthe FAR. Subjects classified as wolves are able to pass for other subjects and contribute to the FAR\nas well. It would be interesting to apply this theory to soft biometric classification. The categories\nwould not be quite the same, since it would be a classification problem instead of an authentication\nproblem, but it would be interesting to look at what characteristics of a particular demographic make\nit stand apart from the others or what characteristics that individuals within that demographic have\nthat are similar to another demographic. Categorizations of individuals would could help determine\nweights in an updated fusion scheme.\n\nIntrinsic to a soft biometric zoo problem, as well as to the soft biometric classification\nproblem, is the issue of the demographic labels themselves. Ethnic and gender identities are no\nlonger as black and white as they once were as cultures blend together and technologies advance.\nFor instance, someone’s self-perceived ethnic identity could be different from how others perceive\ntheir ethnic identity, which is different from their ethnic heritage, which might not fit into any labels\nthat a soft biometric system may have learned to classify. It would be interesting to do a study on\nthe perception of ethnic and gender identity to see if the results impacted the zoo classifications or\nprovided insight as to which labels soft biometric systems should learn.\n\nAn evaluation of partial-face soft biometric classification under varying image quality would\nalso be useful. Knowing the minimum quality requirements for a specified performance would provide\na guide for researchers to incorporate the soft biometric classifier into their research. Quality could\n\ninclude resolution, image blur, lighting, occlusion, and distortion.\n\n114",
    "Page_123": "Glossary\n\nk-NN k-Nearest Neighbor, simplistic machine learning based classifier.. 25, 36, 49, 112\n\nANN Artificial Neural Network , machine learning based classifier. 11, 27, 28, 36, 111\n\nAR face database collected by Aleix Martinez and Robert Benavente. 7, 8\n\nBioID face database. 7, 8\n\nBioSecure iris database. 7\n\nCAS-PEAL Chinese Academy of Sciences-Pose, Expression, Accessories, and Lighting,\nface database. 7-9\n\nCASIA Chinese Academy of Sciences Institute of Automation, iris database. 7, 8\n\nCMC Cumulative Match Curve, performance measure used in recognition/identification problems.\n30, 31, 93, 99, 103\n\nCMU-PIER iris database collected at Carnegie Mellon University. 7, 8\nconfusion matrix performance analysis used in classification problems. 28\nCV cross-validation, experiment technique that does not use specific training\n\nand testing sets. 30\n\nDET Detection Error Trade-off, performance measure used in verification/authentication problems.\n29, 93\n\nEER Equal Error Rate, place in a ROC or DET graph where FRR and FAR are equal. 29, 31, 93,\n99, 100, 113\n\nFAR False Accept Rate, percentage of false matches accepted at a given threshold in a verifica-\ntion/authentication problem. 29, 30, 100, 114\n\nFERET Face Recognition Technology, face database. 6-9\nFRGC Face Recognition Grand Challenge, face database. 7, 8, 15, 16, 18, 19, 23, 36, 60, 74, 102\n\nFRR False Reject Rate, percentage of true matches rejected at a given threshold in a verifica-\ntion/authentication problem. 29, 100, 114\n\nHOG Histograms of Oriented Gradient, texture feature representation. 10, 11, 72, 89, 111\n\n115",
    "Page_124": "LBP Local Binary Patterns, texture feature representation. 10, 11, 73, 89, 111\nLCH Local Color Histograms, color feature representation. 11, 32\n\nLDA Linear Discriminant Analysis, statistical analysis method for feature reduction which seeks\nto maximize between-class scatter while minimizing within-class scatter. 25\n\nLPQ Local Phase Quantization, texture feature representation. 11, 73, 74, 89, 111\n\nMBGC Multiple Biometric Grand Challenge, face and iris database. 7, 8\n\nMORPH Craniofacial Longitudinal MORPHological Face, face database. 8, 16-19, 23, 36, 60, 74\n\nPCA Principal Component Analysis, statistical analysis method typically used in biometrics for\nfeature reduction. 24, 25, 92\n\nPinellas face database. 8, 18, 19, 23, 91\n\nROC Receiver Operating Characteristic, performance measure used in verification/authentication\nproblems. 29-31\n\nSoftopia Japan face database available at:\nhttp://www.hoip.softopia.pref.gifu.jp/. 7,8\nSUMS Standford University Medical Student, face database. 7, 8\n\nSVM Support Vector Machine, machine learning based classifier. 11, 27, 36, 111\n\nUBIRIS iris database (noisy, visible wavelength). 7, 8\nUND face database collected by the University of Notre Dame. 7\n\nUPOL iris database collected by researchers at Palacky University Olomouc. 7, 8\n\nXM2VTS Extended Multi-Modal Verification for Teleservices and Security, face database\navailable at: http: //www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/. 6-9\n\n116",
    "Page_125": "Bibliography\n\n \n\n \n\n[10]\n\n(11)\n\nH. Abdi, D. Valentin, B. Edelman, and A. J. O’Toole. More about the difference between\nmen and women: evidence from linear neural networks and the principal-component approach.\nPerception, 24(5):539 — 562, 1995.\n\nT. Ahonen, E. Rahtu, V. Ojansivu, and J. Heikkila. Recognition of blurred faces using local\nphase quantization. In Proc. 19th Int. Conf. Pattern Recognition ICPR 2008, pages 1-4, 2008.\n\nR. Akbari and S. Mozaffari. Performance enhancement of PCA-based face recognition system\nvia gender classification method. In Proc. 6th Iranian Machine Vision and Image Processing\n(MVIP), pages 1-6, 2010.\n\nA. M. Albert, K. Ricanek, Jr., and E. Patterson. A review of the literature on the aging adult\nskull and face: Implications for forensic science research and applications. Forensic Science\nInternational, 172(1):1 — 9, 2007.\n\nY. Andreu and R. Mollineda. The role of face parts in gender recognition. In A. Campilho and\nM. Kamel, editors, Image Analysis and Recognition, volume 5112 of Lecture Notes in Computer\nScience, pages 945-954. Springer Berlin / Heidelberg, 2008. 10.1007/978-3-540-69812-8_94.\n\nR. Bolle, J. Connell, $. Pankanti, N. Ratha, and A. Senior. Guide to Biometrics. SpringerVerlag,\n2003.\n\nR. Bolle, J. Connell, S. Pankanti, N. Ratha, and A. Senior. The common biometrics. In Guide\nto Biometrics, Springer Professional Computing, pages 31-49. Springer New York, 2004.\n\nS. Buchala, N. Davey, R. Frank, T. Gale, M. Loomes, and W. Kanargard. Gender classification\nof face images: The role of global and feature-based information. In N. Pal, N. Kasabov,\nR. Mudi, S. Pal, and S. Parui, editors, Neural Information Processing, volume 3316 of Lecture\nNotes in Computer Science, pages 763-768. Springer Berlin / Heidelberg, 2004. 10.1007/978-\n3-540-30499-9_117.\n\nS. Buchala, N. Davey, R. J. Frank, and T. M. Gale. Dimensionality reduction of face images for\ngender classification. In Proc. 2nd Int Intelligent Systems IEEE Conf, volume 1, pages 88-93,\n2004.\n\nD. Cao, C. Chen, M. Piccirilli, D. Adjeroh, T. Bourlai, and A. Ross. Can facial metrology\npredict gender? In Biometrics (IJCB), 2011 International Joint Conference on, pages 1-8,\n2011.\n\nC. H. Chan, J. Kittler, N. Poh, T. Ahonen, and M. Pietikaéinen. (Multiscale) Local Phase Quan-\ntisation histogram discriminant analysis with score normalisation for robust face recognition.\nIn Proc. IEEE 12th Int Computer Vision Workshops (ICCV Workshops) Conf, pages 633-640,\n2009.\n\n117",
    "Page_126": "12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n \n\nC. H. Chan, M. Tahir, J. Kittler, and M. Pietikéinen. Multiscale local phase quantization for\nrobust component-based face recognition using kernel fusion of multiple descriptors. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, 35(5):1164-1177, 2013.\n\nC.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions\non Intelligent Systems and Technology, 2:27:1—-27:27, 2011. Software available at http://www.\ncsie.ntu.edu.tw/*cjlin/libsvm.\n\nS. R. Coleman and R. Grover. The anatomy of the aging face: Volume loss and changes in\n3-dimensional topography. Aesthetic Surgery Journal, 26(1, Supplement):S4 — $9, 2006.\n\nT. Cootes, G. Edwards, and C. Taylor. Active appearance models. In H. Burkhardt and\nB. Neumann, editors, Computer Vision - ECCV’98, volume 1407 of Lecture Notes in Computer\nScience, pages 484-498. Springer Berlin / Heidelberg, 1998. 10.1007/BFb0054760.\n\nG. W. Cottrell and J. Metcalfe. Empath: Face, emotion, and gender recognition using holons.\nIn Advances in Neural Information Processing Systems, volume 3, pages 564-571, 1991.\n\nA. A. Dahl. Heterochromia iridis symptoms, causes, treatments. Online: http://www.\nmedicinenet .com/heterochromia_iridis/page2.htm, 2013. Accessed November 2\n2014.\n\nN. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer\nVision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on,\nvolume 1, pages 886-893 vol. 1, 2005.\n\nM. Demirkus, K. Garg, and $. Guler. Automated person categorization for video surveillance\nusing soft biometrics. In Proceedings of SPIE, volume 7667, pages 76670P—76670P-—12, 2010.\n\nO. Déniz, G. Bueno, J. Salido, and F. D. la Torre. Face recognition using histograms of oriented\ngradients. Pattern Recognition Letters, 32(12):1598 — 1603, 2011.\n\nR. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, Inc., 2nd\nedition, 2001. Chapter 9.\n\nB. Edelman, D. Valentin, and H. Abdi. Sex classification of face areas: how well can a linear\nneural network predict human performance. Journal of Biological System, 6(3):241-264, 1998.\n\nFast artification neural network library (FANN). Online: http://leenissen.dk/fann/\nwp/. Accessed May 8, 2014.\n\nB. A. Golomb, D. T. Lawrence, and T. J. Sejnowski. Sexnet: A neural network identifies sex\nfrom human faces. In Proceedings of the 1990 conference on Advances in neural information\nprocessing systems 3, NIPS-3, pages 572-577, San Francisco, CA, USA, 1990. Morgan Kaufmann\nPublishers Inc.\n\nG. Guennebaud, B. Jacob, et al. Eigen v3. Online: http://eigen.tuxfamily.org, 2010.\nAccessed May 12, 2014.\n\nG. Guo, C. R. Dyer, Y. Fu, and T. S. Huang. Is gender recognition affected by age? In Proc.\nIEEE 12th Int Computer Vision Workshops (ICCV Workshops) Conf, pages 2032-2039, 2009.\n\nG. Guo and G. Mu. A study of large-scale ethnicity estimation with gender and age variations.\n\nIn Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition Workshops\n(CVPRW), pages 79-86, 2010.\n\n118",
    "Page_127": "28\n\n29\n\n30\n\n3l\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n \n\nS. Gutta, J. R. J. Huang, P. Jonathon, and H. Wechsler. Mixture of experts for classification\nof gender, ethnic origin, and pose of human faces. Neural Networks, IEEE Transactions on,\n11(4):948-960, 2000.\n\nS. Gutta, H. Wechsler, and P. J. Phillips. Gender and ethnic classification of face images. In\nProc. Third IEEE Int Automatic Face and Gesture Recognition Conf, pages 194-199, 1998.\n\nS. Hosoi, E. Takikawa, and M. Kawade. Ethnicity estimation with facial images. In Proc. Sixth\nIEEE Int Automatic Face and Gesture Recognition Conf, pages 195-200, 2004.\n\nY. Hu, J. Yan, and P. Shi. A fusion-based method for 3D facial gender classification. In Proc.\n2nd Int Computer and Automation Engineering (ICCAE) Conf, volume 5, pages 369-372, 2010.\n\nA. Jain, J. Huang, and S. Fang. Gender identification using frontal facial images. In Multimedia\nand Expo, 2005. ICME 2005. IEEE International Conference on, page 4 pp., July 2005.\n\nA. Jain, K. Nandakumar, X. Lu, and U. Park. Integrating faces, fingerprints, and soft biometric\ntraits for user recognition. In D. Maltoni and A. Jain, editors, Biometric Authentication, volume\n3087 of Lecture Notes in Computer Science, pages 259-269. Springer Berlin / Heidelberg, 2004.\n10.1007 /978-3-540-25976-3_24.\n\nA. K. Jain, S. C. Dass, and K. Nandakumar. Soft biometric traits for personal recognition\nsystems. In International conference on Biometric Authentication, pages 731-738, 2004.\n\nT. Kawano, K. Kato, and K. Yamamoto. A comparison of the gender differentiation capabil-\nity between facial parts. In Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th\nInternational Conference on, volume 1, pages 350 — 353 Vol.1, Aug 2004.\n\nT. Kawano, K. Kato, and K. Yamamoto. An analysis of the gender and age differentiation\nusing facial parts. In Proc. IEEE Int Systems, Man and Cybernetics Conf, volume 4, pages\n3432-3436, 2005.\n\nA. Lapedriza, M. Marin-Jimenez, and J. Vitria. Gender recognition in non controlled environ-\nments. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 3,\npages 834-837, 2006.\n\nA. Lapedriza, D. Masip, and J. Vitria. Are external face features useful for automatic face clas-\nsification? In Computer Vision and Pattern Recognition - Workshops, 2005. CVPR Workshops.\nIEEE Computer Society Conference on, page 151, June 2005.\n\nG. Li, G. Sun, and X. Zhang. Robust face recognition in low resolution and blurred image\nusing joint information in space and frequency. In J. Park, A. Zomaya, S.-S. Yeo, and S. Sahni,\neditors, Network and Parallel Computing, volume 7513 of Lecture Notes in Computer Science,\npages 616-624. Springer Berlin Heidelberg, 2012.\n\nY. Li, M. Savvides, and T. Chen. Investigating useful and distinguishing features around the\neyelash region. In Proc. 87th IEEE Applied Imagery Pattern Recognition Workshop AIPR 08,\npages 1-6, 2008.\n\nH.-C. Lian and B.-L. Lu. Multi-view gender classification using multi-resolution local binary\npatterns and support vector machines. International Journal of Neural Systems, 17(6):479-487,\n\n2007.\n\nS. Liao, A. Jain, and S. Li. Partial face recognition: Alignment-free approach. Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, 35(5):1193-1205, May 2013.\n\n119",
    "Page_128": "50\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n \n\nL. Lu and P. Shi. A novel fusion-based method for expression-invariant gender classification. In\nProc. IEEE Int. Conf. Acoustics, Speech and Signal Processing ICASSP 2009, pages 1065-1068,\n2009.\n\nL. Lu, Z. Xu, and P. Shi. Gender classification of facial images based on multiple facial regions.\nIn Proc. WRI World Congress Computer Science and Information Engineering, volume 6, pages\n48-52, 2009.\n\nX. Luand A. K. Jain. Ethnicity identification from face images. Proceedings of SPIE, 5404:114—\n123, 2004.\n\nJ. R. Lyle, P. E. Miller, S. J. Pundlik, and D. L. Woodard. Soft biometric classification using\nlocal appearance periocular region features. Pattern Recognition, 45(11):3877 — 3885, 2012.\n\nF. S. Manesh, M. Ghahramani, and Y. P. Tan. Facial part displacement effect on template-\nbased gender and ethnicity classification. In Proc. 11th Int Control Automation Robotics &\nVision (ICARCV) Conf, pages 1644-1649, 2010.\n\nJ. Merkow, B. Jou, and M. Savvides. An exploration of gender identification using only the\nperiocular region. In Biometrics: Theory Applications and Systems (BTAS), 2010 Fourth IEEE\nInternational Conference on, pages 1-5, Sept 2010.\n\nA. Mian, M. Bennamoun, and R. Owens. An efficient multimodal 2D-3D hybrid approach to\nautomatic face recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on,\n29(11):1927-1943, 2007.\n\nMinority Report. Dir. Steven Spielberg. Twentieth Century Fox Film Corporation, 2002. Film.\n\nMORPH — USIS. Online: http://www. faceaginggroup.com/morph/. Accessed Octo-\nber 17, 2013.\n\nS. Nissen. Neural networks made simple. Software Developer’s Journal, 2005. Online: http:\n//fann.sourceforge.net/fann_en.pdf. Accessed May 12, 2014.\n\nT. Ojala, M. Pietikaéinen, and T. Maienpaéa&. Multiresolution gray-scale and rotation invariant\ntexture classification with local binary patterns. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, 24(7):971-987, 2002.\n\nV. Ojansivu and J. Heikkilé. Blur insensitive texture classification using local phase quantiza-\ntion. In Proceedings of the 3rd international conference on Image and Signal Processing, ICISP\n08, pages 236-243, Berlin, Heidelberg, 2008. Springer-Verlag.\n\nO. Ozbudak, M. Kirci, Y. Cakir, and E. O. Giines. Effects of the facial and racial features on\ngender classification. In Proc. MELECON 2010 - 2010 15th IEEE Mediterranean Electrotech-\nnical Conf, pages 26-29, 2010.\n\nU. Park and A. K. Jain. Face matching and retrieval using soft biometrics. IEEE Transactions\non Information Forensics and Security, 5(3):406-415, 2010.\n\nP. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman, J. Marques, J. Min, and\nW. Worek. Overview of the face recognition grand challenge. In Computer Vision and Pattern\nRecognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 947—\n954 vol. 1, June 2005.\n\nP. Phillips, P. Flynn, T. Scruggs, K. Bowyer, and W. Worek. Preliminary face recognition\ngrand challenge results. In Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th\nInternational Conference on, pages 15-24, 2006.\n\n120",
    "Page_129": "59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n65\n\n66\n\n67\n\n68\n\n69\n\n70\n\n71\n\n72\n\n73\n\n \n\nX. Qiu, Z. Sun, and T. Tan. Global texture analysis of iris images for ethnic classification.\nIn D. Zhang and A. Jain, editors, Advances in Biometrics, volume 3832 of Lecture Notes in\nComputer Science, pages 411-418. Springer Berlin / Heidelberg, 2005. 10.1007/11608288_55.\n\nX. Qiu, Z. Sun, and T. Tan. Learning appearance primitives of iris images for ethnic classifica-\ntion. In Proc. IEEE Int. Conf. Image Processing ICIP 2007, volume 2, 2007.\n\nA. W. Rawls and K. Ricanek, Jr. MORPH: development and optimization of a longitudinal age\nprogression database. In Proceedings of the 2009 joint COST 2101 and 2102 international con-\nference on Biometric ID management and multimodal communication, BioID_MultiComm’09,\npages 17-24, Berlin, Heidelberg, 2009. Springer-Verlag.\n\nK. Ricanek and B. Barbour. What are soft biometrics and how can they be used? Computer,\n44(9):106-108, 2011.\n\nK. Ricanek and T. Tesafaye. MORPH: a longitudinal image database of normal adult age-\nprogression. In Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International\nConference on, pages 341-345, 2006.\n\nY. Saatci and C. Town. Cascaded classification of gender and facial expression using active\nappearance models. In Proc. 7th Int. Conf. Automatic Face and Gesture Recognition FGR\n2006, pages 393-398, 2006.\n\nW. Schwartz, H. Guo, and L. Davis. A robust and scalable approach to face identification. In\nK. Daniilidis, P. Maragos, and N. Paragios, editors, Computer Vision - ECCV 2010, volume\n6316 of Lecture Notes in Computer Science, pages 476-489. Springer Berlin / Heidelberg, 2010.\n10.1007/978-3-642-15567-3_35.\n\nC. Shan. Learning local binary patterns for gender classification on real-world face images.\nPattern Recognition Letters, 33(4):431 — 437, 2012. Intelligent Multimedia Interactivity.\n\nS. Tamura, H. Kawai, and H. Mitsumoto. Male/female identification from 8x6 very low resolu-\ntion face images by neural network. Pattern Recognition, 29(2):331 — 335, 1996.\n\nV. Thomas, N. V. Chawla, K. W. Bowyer, and P. J. Flynn. Learning to predict gender from iris\nimages. In Proc. First IEEE Int. Conf. Biometrics: Theory, Applications, and Systems BTAS\n2007, pages 1-5, 2007.\n\nM. Toews and T. Arbel. Detection, localization, and sex classification of faces from arbitrary\nviewpoints and under occlusion. Pattern Analysis and Machine Intelligence, IEEE Transactions\non, 31(9):1567 —1581, sept. 2009.\n\nM. Turk and A. Pentland. Face recognition using eigenfaces. In Computer Vision and Pattern\nRecognition, 1991. Proceedings CVPR ’91., IEEE Computer Society Conference on, pages 586—\n591, 1991.\n\nY. Wang, K. Ricanek, C. Chen, and Y. Chang. Gender classification from infants to seniors.\nIn Proc. Fourth IEEE Int Biometrics: Theory Applications and Systems (BTAS) Conf, pages\n1-6, 2010.\n\nL. Wiskott, J.-M. Fellous, N. Kriiger, and C. von der Malsburg. Face recognition and gender\ndetermination. In Proceedings of the International Workshop on Automatic Face and Gesture\nRecognition, pages 92-97, June 1995.\n\nM. Wittman, P. Davis, and P. Flynn. Empirical studies of the existence of the biometric\nmenagerie in the FRGC 2.0 color image corpus. In Computer Vision and Pattern Recognition\nWorkshop, 2006. CVPRW ’06. Conference on, pages 33-33, June 2006.\n\n121",
    "Page_130": "[74] D. Woodard, S. Pundlik, J. Lyle, and P. Miller. Periocular region appearance cues for biometric\nidentification. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE\nComputer Society Conference on, pages 162-169, 2010.\n\n[75] Z. Yang and H. Ai. Demographic classification with local binary patterns. In S.-W. Lee and\nS. Li, editors, Advances in Biometrics, volume 4642 of Lecture Notes in Computer Science,\npages 464-473. Springer Berlin / Heidelberg, 2007. 10.1007/978-3-540-74549-5_49.\n\n[76] R. Zewail, A. Elsafi, M. Saeb, and N. Hamdy. Soft and hard biometrics fusion for improved iden-\ntity verification. In Proc. 47th Midwest Symp. Circuits and Systems MWSCAS ’04, volume 1,\n2004.\n\n122"
}