{
    "Page_1": "ia bw\n—\n\n   \n\nREGULATING tC) J\nBIOMETRICS\n\nGlobal Approaches and\n\nUrgent Questions a\n\n  \n\naj” &F\n,\n\nEdited by Amba Kak September 2020",
    "Page_2": "REGULATING BIOMETRICS\n\nGlobal Aoproaches and Urgent Questions\n\nEdited by Amba Kak\nSeptember 2020\n\nCite as: Amba Kak, ed., “Regulating Biometrics: Global Approaches and Urgent Questions” Al Now\nInstitute, September 1 2020, https://ainowinstitute.org/regulatingbiometrics.html.\n\n \n\nACKNOWLEDGMENTS\n\n| would like to acknowledge and thank Luke Strathmann for his steadfast editorial support,\nwithout which this compendium would not have come together. Thanks also to Caren Litherland\nfor her meticulous copyediting. I’m immensely grateful to the authors of the chapters in this\ncompendium for their seamless collaboration, despite an unexpectedly challenging year in the\nmidst of a pandemic. I’m equally grateful, as always, to my colleagues Meredith Whittaker,\nAlejandro Calcajio, Theodora Dryer, Sarah Myers West, Varoon Mathur, and Inioluwa Deborah\nRaji for their detailed feedback and edits; and to Jason Schultz and Kate Crawford for their\nguidance on an earlier draft. A special thank you to Carly Kind (Ada Lovelace Institute), Ella\nJakubowska (EDRi), and Vidushi Marda (Article 19) for their generous feedback on the\nintroductory chapter.\n\nARTWORK\n\nThe images used on the cover and throughout this compendium are by Heather Dewey-Hagborg,\nVisiting Assistant Professor of Interactive Media at NYU Abu Dhabi and Artist Fellow at Al Now.\n\nIn How Do You See Me? Dewey-Hagborg developed custom software to produce a series of\nimages that are detected as “faces” or are recognized as her. Starting from primitive curves\nand gradients, images evolve to more strongly elicit the algorithmic detection and recognition\nresponse.\n\nWe see the face reduced to a white circle, laying bare the racial assumptions that underpin facial\ndetection technologies.\n\nAnd we see abstract shapes and patterns, images that seemingly bear no resemblance to faces,\nemerge as neighboring facial vectors to the artist’s own.\n\nThe outcome of these experiments is a series of images that give us a window into how we are\nseen by the opaque technologies of artificial intelligence and facial recognition.\n\nLearn more about the project at https://deweyhagborg.com/projects/how-do-you-see-me.\n\nThis work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License",
    "Page_3": "Regulating Biometrics: Global Approaches and Urgent Questions | 3\n\nNOTE FROM THE EDITOR\n\nAmid heightened public scrutiny, interest in regulating biometric technologies has grown across\nthe globe. Public advocacy has driven this scrutiny across globally dispersed and distinctive\nlocal political contexts. Common across these diverse movements is a growing sense that\nthese technologies are no longer inevitable, accompanied by questions as to whether they are\nnecessary at all. Advocates continue to remind developers, profiteers, and those using and\nregulating these biometric systems that the future course of these technologies must—and\nwill-be subject to greater democratic control. The next few years are poised to produce wide-\nranging legal regulation in many parts of the world that could alter the future course of these\ntechnologies. Addressing this moment of possibility, this compendium presents eight case\nstudies from academics, advocates, and policy experts offering a variety of perspectives and\nnational contexts. These expert contributors illuminate existing attempts to regulate biometric\nsystems, and reflect on the promise, and the limits, of the law.\n\nThe compendium begins with an introduction and a summary chapter that identifies key themes\nfrom existing legal approaches, and poses open questions for the future. These questions\nhighlight the critical research needed to inform ongoing national policy and advocacy efforts to\nregulate biometric recognition technologies.\n\nAINOW",
    "Page_4": "4 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nCONTENTS\n\nChapter 0.\nIntroduction 6\nAmba Kak\n\nChapter 1.\n\nThe State of Play and Open Questions for the Future 16\nTimeline of Legal Developments 42\nAmba Kak\n\nChapter 2.\nAustralian Identity-Matching Services Bill 44\nJake Goldenfein and Monique Mann\n\nChapter 3.\n\nThe Economy (and Regulatory Practice) That Biometrics Inspires:\n\nA Study of the Aadhaar Project 52\nNayantara Ranganathan\n\nChapter 4.\nA First Attempt at Regulating Biometric Data in the European Union 62\nEls Kindt\n\nChapter 5.\n\nReflecting on the International Committee of the Red Cross’s Biometric Policy:\nMinimizing Centralized Databases 70\nBen Hayes and Massimo Marelli\n\nChapter 6.\nPolicing Uses of Live Facial Recognition in the United Kingdom 78\nPeter Fussey and Daragh Murray\n\nChapter 7. A Taxonomy of Legislative Approaches to Face\nRecognition in the United States 86\nJameson Spivack and Clare Garvie\n\nChapter 8.\nBIPA: The Most Important Biometric Privacy Law in the US? 96\nWoodrow Hartzog\n\nChapter 9.\n\nBottom-Up Biometric Regulation: A Community's Response to\n\nUsing Face Surveillance in Schools 104\nStefanie Coyle and Rashida Richardson",
    "Page_5": "Regulating Biometrics: Global Approaches and Urgent Questions | 5",
    "Page_6": "6 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nIntroduction\n\nAmba Kak\n\nlthough the terminology varies,’ we use the phrase biometric recognition technologies\n\nto describe systems that “fix”? official identities to bodily, physiological, or behavioral\n\ntraits, providing new ways for individuals to identify themselves, and also to be identified\nor tracked. While fingerprints have the longest history as a marker of identity and continue to be\nused in a number of applications across the world, other bodily markers like face, voice, and iris or\nretina are proliferating, with significant research exploring their potential large-scale application.\nEmerging areas of interest in this field include using behavioral biometrics like gait (i.e., how\na person walks), keyboard keystroke patterns, and multimodal combinations of biometrics to\nidentify and potentially make inferences about individuals.*\n\nBeyond identifying people, these systems increasingly claim to be able to infer demographic\ncharacteristics, emotional states, and personality traits from bodily data. (This practice is\nsometimes referred to as “soft biometrics’® in technical literature.) In other words, there has\nbeen a change in questioning that historian Jane Caplan has summarized as a shift from “What\nperson is that?” to “What type of person is that?” Scholars have pointed to the fact that many\nof these systems that claim to detect interior characteristics from physical information are built\n\n1 The terms biometric recognition, identification, and processing are sometimes used interchangeably; other times, they are given more precise and\ndistinct definitions. We use the umbrella terms biometric systems, biometric technologies, or biometric recognition (which has broad cachet in\npolicy discourse) to cover the range of automated technologies that use biometric identifiers to identify, verify, or confirm a person's official identity.\nWe also highlight open questions about whether systems that produce other kinds of inferences from bodily data (beyond official identity) should\nbe included. This compendium does not analyze the regulation of DNA identifiers. While DNA is recognized as biometric information because of its\nability to uniquely identify individuals, it is generally regulated under separate genetic privacy laws rather than biometric privacy laws, and its use in\nthe criminal justice system has also been regulated under specific rules.\n\n2 See Aaron K. Martin and Edgar A. Whitley, “Fixing identity? Biometrics and the Tensions of Material Practices,” Media, Culture & Society 35, no. 1\n(2013): 52-60, https://doi.org/10.1177/0163443712464558.\n\n3 See Kelly A. Gates, Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance (New York: New York University Press, 2011),\n19. Gates refers to systems of biometric recognition “as an index or recorded visual trace of a specific person.”\n\n4 Riad |. Hammoud, Besma R. Abidi, Mongi A. Abidi, Face Biometrics for Personal Identification: Multi-Sensory Multi-Modal Systems (Berlin: Springer,\n2007). See also sections on gait recognition and multimodal biometrics in Global Biometric Authentication and Identification Market: Focus on\nModality (Face, Eye, Fingerprint, Palm, and Vein), Motility, Application, and Technology Trends Analysis and Forecast: 2018-2023, MarketResearch.\ncom, March 2019, https://www.marketresearch.com/BIS-Research-v401 1/Global-Biometric-Authentication-Identification-Focus-12342594/.\n\n5 Soft biometrics are defined as ancillary characteristics that provide some information, but not enough to identify a person. See Abdelgader\nAbdelwhab and Serestina Viriri, “A Survey on Soft Biometrics for Human Identification,’ in Machine Learning and Biometrics, ed. Jucheng Yang\net al. (London: IntechOpen, 2018), https://doi.org/10.5772/intechopen.76021. See also U. Park and A. K. Jain, “Face Matching and Retrieval\nUsing Soft Biometrics,’ IEEE Transactions on Information Forensics and Security 5, no. 3 (September 2010): 406-415, https://doi.org/10.1109/\nTIFS.2010.2049842. And see A. Dantcheva, “What Else Does Your Biometric Data Reveal? A Survey on Soft Biometrics,’ IEEE Transactions on\nInformation Forensics and Security 11, no. 3 (March 2016): 441-467, https://doi.org/10.1109/TIFS.2015.2480381\n\n6 Jane Caplan, “This or That Particular Person’: Protocols of Identification in Nineteenth-Century Europe,’ in Documenting Individual Identity: The\nDevelopment of State Practices in the Modern World, ed. Jane Caplan and John Torpey (Princeton: Princeton University Press, 2001). Cf. Jake\nGoldenfein, Facial Recognition Is Only the Beginning, Public Books, January 27, 2020, https://www.publicbooks.org/facial-recognition-is-only-the-\nbeginning/#fn-33473-10.",
    "Page_7": "Amba Kak | Introduction | 7\n\non debunked and racist scientific and cultural assumptions about who looks like what “type” of\nperson,’ and lead to demonstrated harms when applied in sensitive social domains like hiring or\neducation.®\n\nThe rapid expansion of the biometrics industry coincides with advancing technical methods\n\nand features and decreasing costs of hardware and software. Camera- and video-analytics\ntechnologies being produced today are designed to have higher resolution, the ability to work\nfrom greater distances, and night-vision sensors that create the conditions for live facial\nrecognition and real-time surveillance in public spaces.° Body-worn cameras that can attach to\nclothing or helmets have found a huge market among law enforcement agencies.'° And advanced\nvoice recorders that are able to pick up recordings from a greater distance are transforming voice\nrecognition into a tool that could enable persistent remote surveillance.\"\n\nMeanwhile, the ubiquity of face photographs and voice recordings tagged with people’s names\non the internet has greatly decreased the financial and technical resources required to create the\ndatabases that underpin face and voice recognition systems. Clearview Al provides an example.\nThe company was an inconspicuous start-up until it attracted global controversy in early 2020\nfor the three billion labeled face images (matched to names) it scraped from the web without\nconsent. The company then used these photos to market surveillance tools to a range of private\nand public actors, claiming that its system could pull up identity and other intimate information\nabout anyone whose image was in its database.'* Recent reporting demonstrates that Clearview\nAl is not unique. In July 2020, the German digital rights blog Netzpolitik uncovered a Polish\ncompany called PimEyes that creates a similar “face search engine” with a database of nine\nhundred million images scraped from the web.'? The magnitude of these companies’ systems,\nalong with their relative obscurity, demonstrates the way the market for biometric recognition\nsystems consists of a number of nontransparent vendors that sell their systems globally without\nany oversight or scrutiny.\"4\n\n7 In June 2020, the civil society collective Coalition for Critical Technology called for publishers to stop all publication of computational research\nclaiming to identify or predict “criminality” using biometric data. See Coalition for Critical Technology, “Abolish the #TechToPrisonPipeline,” Medium,\nJune 23, 2020, https://medium.com/@CoalitionForCriticalTechnology/abolish-the-techtoprisonpipeline-965b14366b16. See also Lisa Feldman\nBarrett, Ralph Adochs, and Stacy Marsella, “Emotional Expressions Reconsidered: Challenges to Inferring Emotion from Human Facial Movements,”\nPsychological Science in the Public Interest 20, no. 1 (July 2019): 1-68, https://doi.org/10.1177/1529100619832930; Zhimin Chen and David\nWhitney, “Tracking the Affective State of Unseen Persons,’ Proceedings of the National Academy of Sciences, February 5, 2019, https://www.\npnas.org/content/pnas/early/2019/02/26/18122501 16. full.pdf; Ruben van de Ven, “Choose How You Feel; You Have Seven Options,” Institute of\nNetwork Cultures, January 25, 2017, https://networkcultures.org/longform/2017/01/25/choose-how-you-feel-you-have-seven-options/; and Lauren\nRhue, “Racial Influence on Automated Perceptions of Emotions,” Race, Al, and Emotions, November 9, 2018, https://papers.ssrn.com/sol3/papers.\ncfm?abstract_id=3281765.\n\n8 See Jayne Williamson-Lee, “Amazon's A.|. Emotion-Recognition Software Confuses Expressions for Feelings,’ OneZero, Medium, October 28, 2019,\nhttps://onezero.medium.com/amazons-a-i-emotion-recognition-software-confuses-expressions-for-feelings-53e96007ca63; Fabio Fasoli and Peter\nHegarty. \"A Leader Doesn't Sound Lesbian!: The Impact of Sexual Orientation Vocal Cues on Heterosexual Persons’ First Impression and Hiring\nDecision,’ Psychology of Women Quarterly 44, no. 2 (June 2020): 234-55, https://doi.org/10.1177/0361684319891 168,\n\n9 See Jay Stanley, “The Dawn of Robot Surveillance: Al, Video Analytics, and Privacy,’ ACLU, June 17, 2019, https://www.aclu.org/sites/default/files/\nfield_document/061819-robot_surveillance.pdf. See also Kelly Gates, “Policing as Digital Platform,’ Surveillance & Society 17, no. 1/2 (2019), https://\njs.library.queensu.ca/index.php/surveillance-and-society/article/view/12940.\n\n10. See Vivian Hung, Steven Babin, and Jacqueline Coberly, “A Market Survey on Body Worn Camera Technologies,’ National Institute of Justice, Johns\nHopkins University Applied Physics Laboratory, November 2016, https://www.ncjrs.gov/pdffiles1/nij/grants/250381 pdf.\n\n11. Andreas Nautsch et al., “The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps towards a Common\nUnderstanding,” Proc. Interspeech, 2019, https://arxiv.org/abs/1907.03458.\n\n12 Kashmir Hill, “The Secretive Company That Might End Privacy as We Know It” New York Times, January 18, 2020, https://www.nytimes.\ncom/2020/01/18/technology/clearview-privacy-facial-recognition.html.\n\n13 See Daniel Laufer and Sebastian Mainek, “A Polish Company Is Abolishing Our Anonymity,’ NetzPolitik, July 10, 2020, https://netzpolitik.org/2020/\npimeyes-face-search-company-is-abolishing-our-anonymity/,\n\n14 See Dave Gershorm, “From RealPlayer to Toshiba, Tech Companies Cash in on the Facial Recognition Gold Rush,’ OneZero, Medium, June 2, 2020,\nhttps://onezero.medium.com/from-realplayer-to-toshiba-tech-companies-cash-in-on-the-facial-recognition-gold-rush-b40ab3e8f1e2.",
    "Page_8": "8 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nA range of mostly proprietary algorithmic processes enable vendors to transform these\ndatabases into biometric recognition systems capable of identifying individuals at a large scale.\nCreating such a system requires a combination of human and computational labor, as well as\n\na formidable technical, financial, and political infrastructure. Labeling and tagging biometric\ndata in order to make it searchable and to prepare it to feed into machine learning systems\nrequires significant, on-demand human labor power. There is no reliable way to create these\nsystems without such labeled data. At present, much of this data labeling work, often contingent\nand underpaid, is outsourced to firms across the world, with a high concentration in countries\nin the Global South.'° Using machine-learning techniques such as deep learning and readily\navailable neural network architectures, these large datasets of images are used by firms to train\nand calibrate computer models that are designed and optimized to predict “matches” within a\ndatabase, which in turn confirm or reveal identity.\"\n\nThe frenzied growth of biometrics into a global multibillion-dollar industry has not happened\norganically.” Powerful state and private actors promote the belief that these technologies are\neffective, necessary, and beneficial. Their core claim is that a strong connection exists between\nbodily traits and identity, and that biometric identifiers can be uniquely attributed to a particular\nindividual with a high degree of accuracy and continuity over time.'® This claim is naturalized in\nbiometric systems, as is the corollary belief that these digital technologies have lower chances of\nfraud compared to non-biometric and analog means of identification.\n\nThese claims of accuracy and efficiency are often taken as a given, and transposed onto broader\nsocietal and economic values like security, safety, and more efficient service delivery.'? While\nfingerprints have the longest history as a marker of identity and continue to be used in a number\nof applications across the world,”° other bodily markers like face, voice, and iris or retina are\nproliferating, with significant research exploring their potential large-scale application. Police\nagencies use data produced by facial recognition systems to identify suspects, make arrests,\nand confirm guilt or innocence through system matches.7! It is also being used as a tool to do ID\nchecks for those who lack identification documents,”* to monitor large events or public spaces\n\n15 See Mary L. Gray and Siddharth Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (New York: Houghton Mifflin\nHarcourt, 2019); and Sarah T. Roberts, Behind the Screen: Content Moderation in the Shadows of Social Media (New Haven: Yale University Press,\n2019)\n\n16 Neural network architectures like ResNet are widely available for training on individual datasets so developers can more quickly and efficiently build\ntheir own models. See Connor Shorten, “Introduction to ResNets,’ Towards Data Science, Medium, January 24, 2019, https://towardsdatascience.\ncom/introduction-to-resnets-c0a830a288a4.\n\n17 Chris Burt, “Global Biometrics Revenues to Approach $43B by 2025: Market Research Briefs,’ BiometricUpdate.com, November 28, 2019, https://\nwww.biometricupdate.com/20191 1/global-biometrics-revenues-to-approach-43b-by-2025-market-research-briefs.\n\n18 Sup. 3. Humans have always identified one another in part based on the way we look or sound. Kelly Gates explains how face recognition has\nproliferated in part because it digitized and automated an already existing documentary regime of face verification, where passport photos were\nroutinely affixed to all manner of government identification documents\n\n19 On the “securitization of identity,’ see generally Nikolas Rose, Powers of Freedom: Reframing Political Thought (Cambridge: Cambridge University\nPress, 1999).\n\n20 Simon A. Cole, Suspect Identities: A History of Fingerprinting and Criminal Identification (Cambridge, MA: Harvard University Press, 2001),\n\n21. See Tom Wilson and Madhumita Murgia, “Uganda Confirms Use of Huawei Facial Recognition Cameras,’ Financial Times, August 20, 2019, https://\nwww.ft.com/content/e20580de-c35f-11e9-a8e9-296ca66511¢9; see also Robert Muggah and Pedro Augusto Pereira, “Brazil's Risky Bet on Tech\nto Fight Crime,’ InSight Crime, February 19, 2020, https://www.insightcrime.org/news/analysis/brazil-risky-tech-fight-crime/; Vidushi Marda, “View:\nFrom Protests to Chai, Facial Recognition Is Creeping Up on Us,’ Economic Times, January 7, 2020, https://carnegieindia.org/2020/01/07/view-\nfrom-protests-to-chai-facial-recognition-is-creeping-up-on-us-pub-80708; and Clare Garvie, Alvaro Bedoya, and Jonathan Frankle, “The Perpetual\nLine-Up: Unregulated Police Face Recognition in America,’ Georgetown Law, Center on Privacy & Technology, October 18, 2016, https://www.\nperpetuallineup.org/; Jonathan Hillman and Maesea McCalpin, “Watching Huawei's ‘Safe Cities’ Center for Strategic & International Studies,\nNovember 4, 2019, https://www.csis.org/analysis/watching-huaweis-safe-cities.\n\n22 — Jennifer Valentino-DeVries, “How the Police Use Facial Recognition, and Where It Falls Short,” New York Times, January 12, 2020, https://www.\nnytimes.com/2020/01/12/technology/facial-recognition-police.htm|",
    "Page_9": "Amba Kak | Introduction | 9\n\nfor known criminals, and to surveil protests.”° Beyond face-based systems, a recent investigation\nrevealed that dozens of prisons across the US were creating voice-print databases of inmates and\napplying voice recognition to their phone communication to detect when particular voice prints\nappear, track call recipients of interest, and even to identify external people who were contacting\npeople in prison most often.*4* Meanwhile, amid an environment of heightened xenophobia\n\nand anti-immigrant political rhetoric, the use of biometrics is proliferating as a form of border\ncontrol technology.”° The rationale of security is by no means restricted to law and immigration\nenforcement. It has driven the use of these tools as access control technologies for workplaces,\nschools, and apartment complexes, where they automate identity verification and even evaluate\nbehavior to determine entry permissions.”°\n\nIn some ways, this growth and normalization of biometric recognition technology follows a similar\ntrajectory to the rapid growth of closed-circuit television (CCTV) use through the 2000s, despite\nno clear evidence that it was effective in controlling crime. Security systems are often installed as\nareaction to severe crimes, but without evidence that they would have prevented that crime in the\nfirst place. Indeed, research shows that the rapid proliferation of video surveillance followed from\n“crises, triggered by particular events such as, a child-kidnapping, a class-room murder, a terrorist\noutrage or rising concerns over crime.”””\n\nToday, governments across the world are the largest customer of the global biometrics\n\nindustry, sustaining and shaping its growth. The development of tools for this wide range of\ngovernment functions is typically outsourced to private firms that develop, market, and maintain\nthese systems. A 2019 market-research report says that the “government segment is the\nhighest revenue generating segment among all the applications of biometric authentication\n\nand identification.’*° Outside of security functions, governments are increasingly adopting\nbiometric identifiers as a routine part of service delivery, with the active support of international\ndevelopment institutions and donor agencies. Biometric IDs are promoted as a means to prevent\n\n23 “As Global Protests Continue, Facial Recognition Technology Must Be Banned,’ Amnesty International, June 11, 2020, https://www.amnesty.org/en/\nlatest/news/2020/06/usa-facial-recognition-ban/; Dave Gershgorn, “Facial Recognition Is Law Enforcement's Newest Weapon Against Protesters,’\nOneZero, Medium, June 3, 2020, https://onezero.medium.com/facial-recognition-is-law-enforcements-newest-weapon-against-protestors-\nc7a9760e46eb; Blake Schmidt, “Hong Kong Police Have Al Facial Recognition Tech—Are They Using It against Protesters?,’ October 22, 2019,\nhttps://www.bloomberg.com/news/articles/201 9-1 0-22/hong-kong-police-already-have-ai-tech-that-can-recognize-faces; Alexandra Ulmer and Zeba\nSiddiqui, “India’s Use of Facial Recognition Tech during Protests Causes Stir’ Reuters, February 17, 2020, https://www.reuters.com/article/us-india-\ncitizenship-protests-technology/indias-use-of-facial-recognition-tech-during-protests-causes-stir-idUSKBN20B0ZQ; Jameson Spivack, “Maryland's\nFace Recognition System Is One of the Most Invasive in the Nation” Baltimore Sun, March 9, 2020, https://www.baltimoresun.com/opinion/op-ed/\nbs-ed-op-0310-face-recognition-20200309-hg6jkfav2fdz3ccs55bvqjtnmu-story.html.\n\n24 George Joseph and Debbie Nathan, “Prisons across the US Are Quietly Building Databases of Incarcerated People's Voice Prints,’ Intercept, January\n30, 2019, https://theintercept.com/2019/01/30/prison-voice-prints-databases-securus/.\n\n25 Mark Latonero and Paula Kift, “On Digital Passages and Borders: Refugees and the New Infrastructure for Movement and Control,” Social Media +\nSociety 4, no. 1 (March 2018): 1-11, https://journals.sagepub.com/doi/full/10.1177/2056305118764432.\n\n26 Mark Maguire, “The Birth of Biometric Security,\" Anthropology Today 25, no. 2 (April 2009): 9-14, https://rai.onlinelibrary.wiley.com/doi/\nepdf/10.1111/j.1467-8322.2009.00654.x; see generally BiometricUpdate.com, Access Control, https://www.biometricupdate.com/biometric-news/\naccess-control-biometric-articles.\n\n27 — Clive Norris, Mike McCahill, and David Wood, “The Growth of CCTV: A Global Perspective on the International Diffusion of Video Surveillance in\nPublicly Accessible Space,’ Surveillance & Society 2, no. 2/3 (2004), https://doi.org/10.24908/ss.v2i2/3.3369.\n\n28 The development of tools for government functions is typically outsourced to private firms that develop, market, and maintain these systems. See,\ne.g., \"Global $52Bn Biometric Authentication & Identification Market, 2023: Focus on Modality, Motility, Application and Technology,” Business Wire,\nApril 10, 2019, https://www. businesswire.com/news/home/20190410005486/en/Global-52Bn-Biometric-Authentication-Identification-Market-2023,",
    "Page_10": "10 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nservice delivery fraud. Many of the ID systems are being rolled out in Global South countries—like\nin India, the Philippines, Kenya, and Brazil—and are not sector-specific, but are instead “general-\npurpose” IDs that construct a digital, biometric identity for each resident.”\n\nOutside of government, biometric recognition systems have been normalized as part of everyday\nexperiences, largely driven by the goal of preventing fraud. Biometric locks are now a staple\nfeature of many smartphones and laptops, and biometric profiles of customers offer a way to\nuniquely identify individuals across their transactions online or at ATMs. Biometrics are also being\npromoted as a novel and promising consumer advertising technology,*° where individuals can\nwalk through cameras in a shopping space and be offered personalized advertising or be verified\nfor loyalty programs seamlessly.*!\n\nThe last few years mark a critical juncture, perhaps even a turning point, in the trajectory\n\nof continued biometric expansion. Civil-society advocates have challenged the foundational\narguments made by companies and governments that produce and promote these technologies,\nhighlighting the tangible harms caused by their use. Mounting research demonstrates that these\nsystems perform poorly when used in real-life contexts,°2 even when the system meets narrow\nassessment standards that the industry relies on to back claims of accuracy.°* Even systems\nthat boast high accuracy rates have unevenly distributed errors. They perform less well on certain\ndemographics than on others,“ with particularly high failure rates for Black women, gender\nminorities, young and old people, members of the disabled community, and manual laborers.\nBeyond accuracy, research and civil society are also challenging the dominant discourses\n\n29 See Frank Hersey, “2019: A Critical Year for Biometrics and Digital ID in the Global South,’ BiometricUpdate.com, December 23, 2019, https://www.\nbiometricupdate.com/201912/2019-a-critical-year-for-biometrics-and-digital-id-in-the-global-south; and, for an analysis of several national biometric\nID projects, see Alice Munyua and Udbhav Tiwari, “What Could an ‘Open’ ID System Look Like?: Recommendations and Guardrails for National\nBiometric ID Projects,” Open Policy & Advocacy, January 20, 2020, https://blog.mozilla.org/netpolicy/2020/01/22/what-could-an-open-id-system-\nlook-like-recommendations-and-guardrails-for-national-biometric-id-projects/,\n\n30 See Joseph Turow, The Aisles Have Eyes: How Retailers Track Your Shopping, Strip Your Privacy, and Define Your Power (New Haven: Yale University\nPress, 2016). See also Robert Lee Angell and James R. Kraemer, “Using Biometric Data for a Customer to Improve Upsale Ad Cross-Sale of Items.\nUS Patent US9031858B2,\" filed September 26, 2007, and issued May 12, 2015, https://patents.google.com/patent/US9031858B2/en.\n\n31 Justin Lee, “Touché Launches Biometrics-Based Loyalty and Payment Platform,” BiometricUpdate.com, January 18, 2017, https://www.\nbiometricupdate.com/201701/touche-launches-biometrics-based-loyalty-and-payment-platform; Esther Fung, “Shopping Centers Exploring Facial\nRecognition in Brave New World of Retail,” Wall Street Journal, July 2, 2019, https://www.wsj.com/articles/shopping-centers-exploring-facial-\nrecognition-in-brave-new-world-of-retail-1 1562068802.\n\n32 See, for example, “Face Off: The Lawless Growth of Facial Recognition in UK Policing; Big Brother Watch, May 2018, https://bigbrotherwatch.org,\nuk/wp-content/uploads/2018/05/Face-Off-final-digital-1 pdf. “The Metropolitan Police has the worst record, with less than 2% accuracy of its\nautomated facial recognition ‘matches’ and over 98% of matches wrongly identifying innocent members of the public,’ the authors write. See also\nNIST, “NIST Study Evaluates Effects of Race, Age, Sex on Face Recognition Software; December 19, 2019, https://www.nist.gov/news-events/\nnews/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software. For error rates relating to biometric capture in India’s Aadhaar\nproject, see Anand Venkatanarayanan, “A Critical Examination of the State of Aadhaar 2018 Report by IDinsight,” Kaarana, Medium, May 22, 2018,\nhttps://medium.com/karana/a-critical-examination-of-the-state-of-aadhaar-2018-report-by-idinsight-ef751e24d6c5.\n\n33. Inioluwa Deborah Raji and Genevieve Fried, “About Face: A Survey of Facial Recognition Evaluation,” Meta-Evaluation workshop at AAAI Conference\non Artificial Intelligence, 2020.\n\n34 Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,’ Proceedings\nof Machine Learning Research 81 (2018):1-15, http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf; KS Krishnapriya et al.,\n“Characterizing the Variability in Face Recognition Accuracy Relative to Race,” Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (2019), https://arxiv.org/abs/1904.07325; Cynthia M. Cook et al., “Demographic Effects in Facial Recognition and Their\nDependence on Image Acquisition: An Evaluation of Eleven Commercial Systems,” IEEE Transactions on Biometrics, Behavior, and Identity Science\n1,no. 1 (Jan. 2019): 32-41, https://ieeexplore.ieee.org/document/8636231; Inioluwa Deborah Raji and Joy Buolamwini, “Actionable Auditing,\nInvestigating the Impact of Publicly Naming Biased Performance Results of Commercial Al Products,’ Proceedings of the Conf. on Artificial\nIntelligence, Ethics, and Society (2019), https://www.aies-conference.com/2019/wp-content/uploads/2019/01/AIES-19_paper_223.pdf; Morgan\nKlaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker, “How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial\nAnalysis Services,’ Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (November 2019): 1-33, https://doi.org/10.1145/3359246.\n\n35 Ursula Rao, “Biometric Bodies, or How to Make Electronic Fingerprinting Work in India,’ Body & Society 24, no. 3 (September 2018): 68-94, https://\ndoi.org/10.1177/1357034X18780983.",
    "Page_11": "Amba Kak | Introduction | 11\n\nof security, safety, and efficiency that have driven marketing and demand for these systems.\nAdvocates are increasingly asking for whom such systems provide safety and security. The\n\nclaim that biometric surveillance “makes communities safer” is heavily marketed but loosely\nbacked. Companies and governments make access to details on these systems and their use\ndifficult to obtain, but even so, there is strong evidence that these systems are being deployed\n\nin ways that harm historically marginalized people and communities. For example, in the US,\nthere have been multiple cases where facial recognition has resulted in misidentification of\nsuspects, including cases where facial recognition is used as primary evidence to determine\nguilt.2° This harm is compounded by the systematic denial of basic due process rights during trial,\nin which defendants are denied access to information about whether and how these systems\nwere used.°’ Even outside of law enforcement, there is no transparency at all when it comes\n\nto privately created “watch list” databases, which are likely being shared and institutionalized\nthrough their use at large-scale events, retail stores, and housing complexes. At a recent Taylor\nSwift concert, all attendees were subject to facial recognition without their knowledge or consent,\ncreating public debate around the lack of safeguards people would have recourse to if they were\nblacklisted unfairly by these systems.*®\n\nAs new applications of these technologies are created, so are new forms of pushback. Real-\n\ntime monitoring of protests with facial recognition (e.g., in Hong Kong,°? Delhi,“° Detroit,\" and\nBaltimore*’) has been met by fierce community opposition. This type of pervasive real-time\nsurveillance can potentially produce chilling effects on the democratic exercise of rights to\n\nfree speech and movement in public life. Organized tenants groups have contested the use of\nfacial recognition and other property technologies (“PropTech’) to control access to residential\nbuildings, arguing that they provide landlords with greater unaccountable control, and the ability to\nharass and surveil tenants.4? Meanwhile, coalitions between digital-rights organizations and social\nwelfare and accountability activists have challenged biometric ID schemes for social service\n\n \n\n36 Kashmir Hill, “Wrongfully Accused by an Algorithm,” New York Times, June 24, 2020 https://www.nytimes.com/2020/06/24/technology/facial-\nrecognition-arrest.html; Rashida Richardson, Jason M. Schultz, and Vincent M. Southerland, “Litigating Algorithms 2019 US Report: New Challenges\nto Government Use of Algorithmic Decision Systems,’ Al Now Institute, September 2019, https://ainowinstitute.org/litigatingalgorithms-2019-us,\npdf; Bob Van Voris, “Apple Face-Recognition Blamed by N.Y. Teen for False Arrest,’ Bloomberg, April 22, 2019, https://www.bloomberg.com/\nnews/articles/2019-04-22/apple-face-recognition-blamed-by-new-york-teen-for-false-arrest; Jeremy C. Fox, “Brown University Student Mistakenly\nIdentified as Sri Lanka Bombing Suspect,” Boston Globe, April 28, 2019, https://www.bostonglobe.com/metro/2019/04/28/brown-student-mistaken-\nidentified-sri-lanka-bombings-suspect/OhP2YwyYi4qrCEdxKZCpZM/story.html.\n\n37. For an analysis of criminal due process in the context of facial recognition, see Emma Lux, “Facing the Future: Facial Recognition Technology Under\nthe Confrontation Clause,” American Criminal Law Review 57, no. 0 (Winter 2020), https://www.law.georgetown.edu/american-criminal-law-review/\nsample-page/facing-the-future-facial-recognition-technology-under-the-confrontation-clause/.\n\n38 See Jay Stanley, “The Problem with Using Face Recognition on Fans at a Taylor Swift Concert,” ACLU, December 14, 2018, https://www.aclu.org/\nblog/privacy-technology/surveillance-technologies/problem-using-face-recognition-fans-taylor-swift; and Parmy Olson, “Facial Recognition’s Next\nBig Play: The Sports Stadium,’ Wall Street Journal, August 1, 2020, https://www.wsj.com/articles/facial-recognitions-next-big-play-the-sports-\nstadium-11596290400.\n\n39 Blake Schmidt, “Hong Kong Police Already Have Al Tech that Can Recognize Faces,’ Bloomberg, October 22, 2019, https://www.bloomberg.com/\nnews/articles/2019-10-22/hong-kong-police-already-have-ai-tech-that-can-recognize-faces.\n\n40 Alexandra Ulmer and Zeba Siddiqui, “India's Use of Facial Recognition Tech During Protests Causes Stir’ Reuters, February 17, 2020, https://www.\nreuters.com/article/us-india-citizenship-protests-technology/indias-use-of-facial-recognition-tech-during-protests-causes-stir-idUSKBN20B0ZQ.\n\n41 “Protesters Demand to Discontinue Facial Recognition Technology,” CBS Detroit, June 16, 2020, https://www.newsbreak.com/michigan/detroit/\nnews/OPLepn7b/protesters-demand-to-discontinue-facial-recognition-technology.\n\n42 — Spivack, “Maryland's Face Recognition System Is One of the Most Invasive in the Nation.”\n\n43 Tranae Moran, Fabian Rogers, and Mona Patel, “Tenants Against Facial Recognition,’ Al Now 2019 Symposium, October 2, 2019, https://\nainowinstitute.org/symposia/videos/tenants-against-facial-recognition.html; Erin McElroy, Meredith Whittaker, and Genevieve Fried, “COVID-19\nCrisis Capitalism Comes to Real Estate,’ Boston Review, May 7, 2020, http://bostonreview.net/class-inequality-science-nature/erin-mcelroy-\nmeredith-whittaker-genevieve-fried-covid-19-crisis.",
    "Page_12": "12 | Regulating Biometrics: Global Approaches and Urgent Questions\n\ndelivery on the basis of their impacts on privacy as well as the denial of basic entitlements due\nto technical or operational failures in these systems.“* Advocacy campaigns continue to question\nthe use of facial recognition at airports, as well as the reuse of driver's licenses and other civilian\nbiometric databases for immigration enforcement and private investigation purposes.*°\n\nWhile public advocacy is increasing in many parts of the world, and each campaign has its unique\ncharacteristics related to local political and economic contexts, what unites the current wave of\npushback is the insistence that these technologies are not inevitable. Questioning technological\ninevitability has become a popular refrain, and reminds those acquiring, promoting, and regulating\nthese systems that the future course of these technologies must and will be subject to greater\ndemocratic control.\n\nCalls for regulation include demands to introduce new laws (e.g., like data-protection\nframeworks); to reform and update existing laws (e.g., laws that currently only regulate\nfingerprints and DNA use in the criminal process); to pause these systems; or to outright ban\ntheir use. In Kenya and India, there have been demands to pass data-protection laws amid the\nrollout of large-scale biometric ID projects without such laws in place.*° Parliamentarians and\ngovernment officials in the UK’? and a government-appointed advisory group in Scotland have\nacknowledged the need for a broad regulatory framework for biometric use, alongside the need to\nupdate existing laws that only apply to fingerprint and DNA biometrics.*® The clearest pushback\non the idea that these technologies are inevitable has come in the form of advocacy championing\ncomplete bans or moratoria on the use of these systems, irrespective of context.*? Similarly, while\n\n44 See Jamaican Supreme Court Decision, Julian Robinson v. Attorney General of Jamaica [2019] JMFC Full 04, https://supremecourt.gov.jm/sites/\ndefault/files/judgments/Robinson%2C%20Julian%20v%20Attorney%20General%200f%20Jamaica.pdf; Indian Supreme Court decision, K. S.\nPuttaswamy v. Union of India, Supreme Court of India, Writ Petition (Civil) No. 494 of 2012, https://indiankanoon.org/doc/127517806/; see also\n#WhyID, “An Open Letter to the Leaders of International Development Banks, the United Nations, International Aid Organisations, Funding Agencies,\nand National Governments,’ Access Now, https://www.accessnow.org/whyid-letter/.\n\n45 See Project South, “Georgia Department of Driver's Services Colludes with Immigration and Customs Enforcement and Law Enforcement Agencies,”\n2020, https://projectsouth.org/wp-content/uploads/2020/03/GA-DDS-ICE-Fact-Sheet-.pdf; Joseph Cox, “DMVs Are Selling Your Data to Private\nInvestigators,’ Vice, September 6, 2019, https://www.vice.com/en_us/article/43kxzq/dmvs-selling-data-private-investigators-making-millions-of-\ndollars; Drew Harwell and Erin Cox, \"ICE Has Run Facial-Recognition Searches on Millions of Maryland Drivers,” Washington Post, February 26,\n\n2020 https://www.washingtonpost.com/technology/2020/02/26/ice-has-run-facial-recognition-searches-millions-maryland-drivers/; Sushovan\nSircar, “Selling Vehicle Owners’ Data as ‘Public Good’, Govt Earns Rs 65 Cr’ Quint, July 10, 2015, https://www.thequint.com/news/india/ministry-of-\ntransport-and-highways-rs-65-crore-driving-license-vehicle-registration-bulk-data-sale; “Opposition to Face Recognition Software in Airports Due to\nIneffectiveness and Privacy Concerns,’ ACLU, n.d., https://www.aclu.org/other/opposition-face-recognition-software-airports-due-ineffectiveness-\nand-privacy-concerns\n\n46 See, e.g., Christine Mungai, “Kenya's Huduma: Data Commodification and Government Tyranny,’ Al Jazeera, August 6, 2019, https://www.aljazeera\ncom/indepth/opinion/kenya-huduma-data-commodification-government-tyranny-190806134307370.html; Vrinda Bhandari, “Why Amend the\nAadhaar Act without First Passing a Data Protection Bill?” The Wire, January 4, 2019, https://thewire.in/law/aadhaar-act-amendment-data-\nprotection\n\n47 “The Future of Biometrics,’ UK Parliament Post, February 6, 2019, https://www.parliament.uk/documents/post/Future%200f%20Biometrics_\nnotes%20from%20briefing%20event_final.pdf; Claire Cohen, “Public Expect Police to Be Using Facial Recognition Technology after Seeing It in Spy\nThrillers Like James Bond, Says Cressida Dick,” Telegraph, June 3, 2019, https://www.telegraph.co.uk/news/2019/06/03/public-expect-police-using-\nfacial-recognition-technology-seeing/\n\n48 Scottish Government, “Independent Advisory Group on the Use of Biometric Data in Scotland,” March 2018, https://www.gov.scot/binaries/\ncontent/documents/govscot/publications/independent-report/201 8/03/report-independent-advisory-group-use-biometric-data-scotland/\ndocuments/00533063-pdf/00533063-pdf/govscot%3Adocument/00533063.pdf.\n\n49 See generally Melina Sebastian, “Normalizing Resistance: Saying No to Facial Recognition Technology,’ Feminist Media Studies 20, no. 4 (May 2020):\n594-597; Ban Facial Recognition, https://www.banfacialrecognition.com/; Big Brother Watch, Stop Facial Recognition, n.d., https://bigbrotherwatch.\norg.uk/campaigns/stop-facial-recognition/; Urvashi Aneja and Angelina Chamauh, “We Need to Ban Facial Recognition Altogether, Not Just\nRegulate Its Use,’ Tandem Research India, January 19, 2020, https://tandemresearch.org/publications/we-need-to-ban-facial-recognition-altogether-\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nnot-just-regulate-its-use.",
    "Page_13": "Amba Kak | Introduction | 13\n\na recent Indian Supreme Court decision eventually upheld the constitutionality of the country’s\nbiometric ID project, a dissenting opinion from one of the judges also made clear that it’s not too\nlate to turn back, ordering that “all such data be destroyed.”*°\n\nAdvocacy and the threat of regulation have spurred companies to act proactively to mitigate,\nand potentially undercut or postpone, demands for prohibition or strict regulation. Microsoft and\nAmazon have released calculated public statements in support of facial recognition regulation.®!\nMore recently, IBM, Microsoft, Amazon, and others committed to pause their use of these\ntechnologies, citing disproportionate harms to people of color amid widespread antiracist Black\nLives Matter mobilization in the US and around the globe. Activists responded by reminding\nlegislators that these voluntary gestures were not nearly enough: “Facial recognition, like\nAmerican policing as we know it, must go.”*°\n\nAmid heightened public scrutiny, interest in regulating biometric technologies has grown\nsignificantly. The degree of openness to legislating technology varies, and for some countries\nregulation is not a realistic or appropriate intervention at all. Yet in many parts of the world,\n\nthe next few years do seem poised to produce wide ranging regulation and with that, offer the\npossibility to alter the future course of biometric technologies. This compendium responds to\nthis environment of possibility, compiling detailed case studies of existing attempts to regulate\nbiometric systems that post emergent and open questions for the future.\n\n50 Justice Chandrachur (dissenting opinion) in K. S. Puttaswamy v. Union of India , Supreme Court of India, Writ Petition (Civil) No. 494 of 2012,\nhttps://indiankanoon.org/doc/127517806/; see also Ashok Kini, “Jamaican SC Quotes Justice Chandrachud's Dissent to Strike Down Aadhaar-Like\nProgramme,’ The Wire, April 13, 2019, https://thewire.in/law/jamaica-supreme-court-aadhaar-justice-chandrachud\n\n51 Often these companies publicly champion the need for some “regulation” but simultaneously lobby against moratoria and bans.\n\n52 Kate Kaye, “IBM, Microsoft, and Amazon's Face Recognition Bans Don't Go Far Enough,’ Fast Company, June 13, 2020, https://www.fastcompany.\ncom/90516450/ibm-microsoft-and-amazons-face-recognition-bans-dont-go-far-enough.\n\n53 Malkia Devich-Cyril, \"Defund Facial Recognigion,’ Atlantic, July 5, 2020, https://www.theatlantic.com/technology/archive/2020/07/defund-facial-\nrecognition/613771/.",
    "Page_14": "14 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThis collection of eight essays from diverse contributors covers widely divergent contexts:\n\nAustralian Identity-Matching Services Bill: Jake Goldenfein (Melbourne Law Schoo!) and\nMonique Mann (Deakin University) track the institutional and political maneuvers that resulted in\nAustralia’s ambitious centralized facial recognition program (“The Capability”). They draw lessons\nfrom what they term the “futility of regulatory oversight.”\n\nThe Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar\nProject: Nayantara Ranganathan (lawyer and independent researcher, India) explains how law and\npolicy around India’s Biometric ID (“Aadhaar”) project eventually served to construct biometric\ndata as a resource for value extraction by private companies. She explores how regulation was\ninfluenced by the logics and cultures of the project it sought to regulate.\n\nA First Attempt at Regulating Biometric Data in the European Union: E/s Kindt (KU Leuven)\nprovides a detailed account of the European Union's General Data Protection Regulation (GDPR)\napproach to regulating biometric data. As many countries are set to implement similarly worded\nnational laws, she warns of potential loopholes and highlights key areas for reform.\n\nReflecting on the International Committee of the Red Cross’s Biometric Policy: Minimizing\nCentralized Databases: Ben Hayes (AWO Agency, Consultant legal advisor to the International\nCommittee of the Red Cross [ICRC]) and Massimo Marelli (Head of the ICRC Data Protection Office)\nexplain ICRC’s decision-making process as they formulated the institution's first biometrics policy\nin the context of humanitarian assistance, with a focus on minimizing the creation of databases\nand risks to vulnerable populations.\n\nPolicing Uses of Live Facial Recognition in the United Kingdom: Peter Fussey (University of Essex)\nand Daragh Murray (University of Essex), lead authors of the independent empirical review of the\nLondon Metropolitan Police's trial of Live Facial Recognition (LFR), explain how existing legal norms\nand regulatory tools fell short, with broader lessons for the regulation of LFR in the UK and elsewhere.\n\nA Taxonomy of Legislative Approaches to Face Recognition in the United States: Jameson\nSpivack and Clare Garvie (Georgetown Center on Privacy and Technology) write about the dozens\n\nof bans and moratoria legislation on police use of facial recognition in the US, providing a detailed\ntaxonomy that goes beyond these broad categories, and lessons learned from their implementation.\n\nBIPA: The Most Important Biometric Privacy Law in the US? Woodrow Hartzog (Northeastern\nUniversity) explores the promise and pitfalls of the State of Illinois’ Biometric Information Privacy\nAct (BIPA) and, more broadly, of the private right of action model. He questions the inevitable\nlimits of a law that is centered on notice and consent.\n\nBottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in\nSchools: Stefanie Coyle (NYCLU) and Rashida Richardson (Rutgers University, Al Now Institute, NYU)\nexamine the controversial move by a school district in Lockport, New York, to implement a facial\nand object recognition system. They highlight the community-driven response that incited a national\ndebate and led to statewide legislation regulating the use of biometric technologies in schools.",
    "Page_15": "Amba Kak | Introduction | 15",
    "Page_16": "16 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThe State of Play and\nOpen Questions for\nthe Future\n\nAmba Kak\n\nhis chapter synthesizes broad trends and open questions for the regulation of biometric\n\nsystems. We draw insights primarily from the essays in this compendium to surface\n\nlessons from existing legal approaches across multiple countries and contexts. Beyond\nanalysis of the current state of play, we pose open questions about where regulation needs\nrevision, or reimagination. We explore the rapidly evolving policy conversation around new\nkinds of regulatory interventions but also, crucially, the limits of the law in capturing or resolving\nconcerns about these technologies.\n\nRegulation of biometric systems has largely been through data-protection laws. Biometric data\n\nis typically designated as an especially sensitive category of personal data and is regulated\nthrough a series of restrictions on the collection, retention, and disclosure of such data.' The 2016\nEuropean Union's General Data Protection law (GDPR) is emblematic of this approach, and there\nare currently over 140 countries with national data-protection laws that cover private- and public-\nsector use of data.? The United States lacks a comprehensive federal data privacy regulation\nsimilar to the GDPR, but state laws like the 2018 Illinois Biometric Information Privacy Act (BIPA)\n\n1 This compendium does not analyze the regulation of DNA identifiers. While DNA is recognized as biometric information because of its ability to\nuniquely identify individuals, it is generally regulated under separate genetic privacy laws rather than biometric privacy laws, and its use in the\ncriminal justice system has also been regulated under specific rules.\n\n2 Graham Greenleaf and Bertil Cottier, “2020 Ends a Decade of 62 New Data Privacy Laws,’ Privacy Laws & Business International Report 163, no. 24-\n26 (January 29, 2020), https://ssrn.com/abstract=3572611. (According to this research, the count was at 142 at the end of 2019.)",
    "Page_17": "Amba Kak | The State of Play and Open Questions for the Future | 17\n\nfollow a similar data-protection approach to regulating biometric data.? Key elements of this\napproach are also included in laws that establish and govern biometric ID systems like India’s\n2016 Aadhaar Act,’ Australia’s 2019 Identity Services Matching Bill,> and Kenya's 2019 Huduma\nNamba bill.° Section 1 (“The Data-Protection Lens”) examines these approaches to regulating\nbiometrics, highlighting key concerns that have become apparent through their implementation.\n\nWhile data-protection laws have made fundamental shifts in the way companies and government\napproach the collection, retention, and use of personal data, there are clear limitations on their\nability to address the full spectrum of potential harms produced by new forms of data-driven\ntechnology, like biometric identification and analysis. Their focus on individual (rather than group)\nconceptions of harm fails to meaningfully address questions of discrimination and algorithmic\nprofiling.’ The focus on data as the object of regulation has also sometimes obscured the\nbroader challenges to social and institutional practices that these systems and platforms exert\non society, in which imperfect but established methods of accountability, contestation, and\ndemocratic decision-making are undercut by the introduction of opaque automated technology.®\nIn contrast, there has been a flurry of legislation, mostly in the United States, that bans the use\n\nof these systems in particular sectors, across certain uses, or for lengths of time until there is\namore participatory and deliberative process of decision-making in place. Sector-specific rules\nhave also emerged, like those that address the harms of biometric systems in criminal justice\n\nor employment or education domains. Sections 2 and 3 of this chapter track these emergent\nconcerns and legal approaches.\n\n \n\n3 Illinois Biometric Privacy Act, 740 Ill. Comp. Stat. Ann. 14/15. Texas and Washington have passed similar biometric privacy laws (see Tex. Bus.\n& Com. Code §503.001; Wash. Rev. Code Ann. §19.375.020). Proposals like the Florida Biometric Privacy Act, Bill S.1385 in Massachusetts, and\nNew York Biometric Privacy Act NY SB 1203 in New York are also explicitly modeled after BIPA. For other examples of a data privacy approach\nto biometric data, see California Consumer Privacy Act of 2018 (CCPA) [1798.100 - 1798.199]; N.Y. 2019 Stop Hacks and Improve Electronic Data\nSecurity (SHIELD) Act; N.Y. Lab. Law §201 (prohibiting fingerprinting as a condition of employment); Arkansas Code §4-110-103(7). On August 4\n2020, as this compendium was going into print, the National Biometric Privacy Act was introduced by Senators Bernie Sanders and Jeff Merkley\nalong similar lines to BIPA. See The National Law Review, “National Biometric Information Privacy Act, Proposed by Sens. Jeff Merkley and Bernie\nSanders’, The National Law Review, August 5, 2020 https://www.natlawreview.com/article/national-biometric-information-privacy-act-proposed-\nsens-jeff-merkley-and-bernie\n\n4 Ministry of Law and Justice (Legislative Department), Aadhaar (Targeted Delivery of Financial and Other Subsidies, Benefits and Services) Act, 2016,\nPub. L. No. 18 of 2016, https://uidai.gov.in/images/the_aadhaar_act_2016.pdf.\n\n5 Parliament of Australia, “Identity-Matching Services Bill 2019 (Cth),’ https://www.aph.gov.au/Parliamentary_Business/Bills_Legislation/Bills_Search_\nResults/Result?b|d=r6387.\n\n6 The Huduma Bill, 2019, https://www.ict.go.ke/wp-content/uploads/2019/07/12-07-2019-The-Huduma-Bill-2019-2. pdf.\n\n7 See generally Martin Tisné, “The Data Delusion: Protecting Individual Data Isn't Enough When the Harm Is Collective,” Stanford Cyber Policy Center,\nn.d., https://fsi-live.s3.us-west-1.amazonaws.com/s3fs-public/the_data_delusion_formatted-v3.pdf; Linnet Taylor, Luciano Floridi, and Bart van\nder Sloot, eds., Group Privacy: New Challenges of Data Technologies (Cham: Springer, 2016), https://www.springer.com/gp/book/9783319466064;\nBrent Mittelstadt, “From Individual to Group Privacy in Big Data Analytics,’ Philosophy & Technology 30, no. 4 (February 2017): 475-494, https://link.\nspringer.com/article/10.1007/s13347-017-0253-7.\n\n8 See generally Amba Kak and Rashida Richardson, “Artificial Intelligence Policies Must Focus on Impact and Accountability,’ CIG! Online, May 1, 2020,\nhttps://www.cigionline.org/articles/artificial-intelligence-policies-must-focus-impact-and-accountability.",
    "Page_18": "18 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThe following is a summary of the questions that this compendium raises, pointing to research,\nregulation, and community engagement that will be needed to inform ongoing national policy and\nadvocacy efforts:\n\n1. The Data-Protection Lens\n\nHow should regulation define “biometric data”?\n\nWhy have data protection laws had limited effectiveness in curbing the expansion of\nbiometric surveillance infrastructure by government?\n\ns meaningful notice and consent possible in the context of biometric systems? What are\nhe limitations of a consent-based approach and what supplements or alternatives might be\nrequired?\n\n2. Beyond Privacy: Accuracy, Discrimination, Human Review, and Due Process\n\nHow should regulatory frameworks address concerns about accuracy and non-\ndiscrimination in biometric systems?\n\nTo what extent should regulation rely on standards of performance and accuracy set by\nechnical standards-setting bodies?\n\n \n\nDoes requiring “meaningful human review\" of biometric recognition systems ensure\noversight and accountability?\n\nShould regulatory frameworks create a risk-based classification between “identification” and\n“verification” uses of biometric recognition?\n\n+ What are the potential risks of a permissive regulatory approach to verification?\n\nWhat kinds of due process safeguards are required for law enforcement use of biometric\nrecognition?\n\n* Should law enforcement have access to these systems to begin with?\nAre systems that process bodily data for purposes beyond establishing individual\n\nidentity, like making inferences around emotional state, personality traits, or demographic\ncharacteristics covered under existing biometric regulation?\n\n* Should such systems be permitted at all, given their contested scientific foundations\nand mounting evidence of harm?\n\n3. Emerging Regulatory Tools and Enforcement Mechanisms\n\nWhat different types of “bans” and moratoria have been passed in the US over the past few\nyears?\n\n* How can moratoria conditions be strengthened to ensure that eventual legislative or\ndeliberative processes are robust?\n\nHow will bans and moratoria on government use impact the private development and\nproduction of biometric systems?\n\nWhat regulatory tools can be used to create public transparency around the development,\npurchase, and use of biometric recognition tools?\n\nWhat role can community-led advocacy play in shaping the priorities and impact of\nregulation?",
    "Page_19": "Amba Kak | The State of Play and Open Questions for the Future | 19\n\nSECTION 1. THE DATA-PROTECTION LENS\n\nHow should regulation define “biometric data”?\n\nUnder the dominant data-protection approach to regulating biometric systems, meeting\nthe definition of “biometric data” has been the threshold condition for legal protections to\napply. Recent regulatory attempts move away from this with “systems” rather than data\nas the object of regulation.\n\nIn laws that establish and regulate biometric ID systems, the definition of biometric data\nhas typically been left open-ended to allow governments to add or change the types of\nbiometrics collected under these projects.\n\nIn defining biometric data and systems, the law not only reflects but also entrenches certain\nperceptions about the stability and accuracy of biometrics as an identification technology. For\nexample, the GDPR states that biometric data is bodily, physiological, and behavioral data that\n“allow or confirm the unique identification of that natural person,’ while the Illinois BIPA provides\nan exhaustive list of identifiers that count as biometric data and requires that they are “used to\nidentify an individual.”\"° These foundational beliefs about the ability of biometric data to uniquely\nidentify an individual are not stable and are today highly contested. Research has demonstrated\nvulnerabilities as people age, and the inaccuracies that creep in when these systems are used\nto identify people of color, young and old people, manual laborers, those who speak English\n\nwith a non-native accent, and many other demographic and phenotypic subgroups.\"' Biometric\nregulation does not interrogate these questions, but simply takes these claims of accuracy and\nequivalence to real identity as given.\n\nIn data-protection laws, fulfilling the definition of “biometric data” or “biometric information” is the\nthreshold condition for legal protections to apply. It also determines the stage (for, e.g., collection,\nprocessing, storage, and use) at which these protections are activated. When part of a broader\npersonal data-protection law like the GDPR, such definitions usually work to distinguish biometric\ndata from other kinds of personal data in order to offer special or stricter levels of protection. In\nlaws like BIPA, which is solely focused on biometric data, the definition determines the scope of\nthe legislation as a whole. Laws that establish government biometric ID projects, on the other\nhand, have tended toward an expansive definition that allows agencies to expand on the kinds of\nbiometric data they can collect. The Kenyan draft law'? and the Indian Aadhaar legislation’ list a\nseries of identifiers that are currently collected under the project but allow the government to add\nto these categories of data collected at will.\n\n9 Article 4(14), GDPR\n\n10 Section 10, BIPA.\n\n11 See footnotes 34 and 35 of this compendium’s Introduction\n\n12 The Huduma Namba Bill, 2019 defines biometric data as follows: “(B)iometric data’ includes fingerprint, hand geometry, earlobe geometry, retina\nand iris patterns, toe impression, voice waves, blood typing, photograph, or such other biological attributes of an individual obtained by way of\nbiometrics.”\n\n13. The Aadhaar Act, 2016 defines biometric information as follows: “biometric information’ means photograph, fingerprint, iris scan, or such other\nbiological attributes of an individual as may be specified by regulations.”",
    "Page_20": "20 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nAs legislation moves beyond traditional data privacy and security concerns to questions of\naccountability around whether or how to use these systems, and who is liable if these systems\nfail, some recent bills shift the focus from “data” to “systems.” For example, recent US legislation\nthat restricts the use of these technologies does not define biometric data at all, and instead\nfocuses on “face recognition systems’ or “services, or face/biometric “surveillance systems”\nas the object of regulation.'® The definitions of these terms emphasize the eventual uses or\nintentions that drive the application of such systems in social contexts (such as surveillance,\nidentification, verification, or tracking).\n\nThe legal definition of biometric data is usually restricted to data that has been\ntechnically processed for use in an algorithmic system by specifying a particular\ndigital representation (e.g., ‘template’ or ‘print’). The definition often explicitly excludes\nphotographs and voice recordings and creates a loophole around foundational stages\nwhen data is collected, processed, and stored.\n\nThe definition of biometric data has generally been restricted to mean a technically defined digital\nrepresentation of bodily traits that have already been processed for machine or algorithmic\n\nanalysis. This is suggested by semi-technical terms like “templates,” “geometry,” “prints,’\"® or, in the\n\nGDPR, data that has already been subject to “specific technical processing.” Terms like “template”\nrefer to the initial stage of algorithmic processing where data is extracted from, say, an image or\nvoice recording. Modern machine learning systems do not need “all” of the data, but instead rely\non extracting meaningful subparts from voice or image data, which can then be easily compared\nto existing “templates” in a database.\" This is the logic that leads to photographs of faces being\nexpressly excluded from the definition of biometric data in the BIPA'® and the GDPR.'°\n\n14 Recent US legislation uses terms like “facial recognition systems,’ as in the City of Boston Ordinance Banning Face Surveillance Technology, https://\nwww.eff.org/document/ordinance-banning-face-surveillance-technology-boston; “facial recognition services,’ as in the Washington Senate Bill\n6280, is defined as “technology that analyzes facial features and is used by a state or local government agency for the identification, verification, or\npersistent tracking.” See SB 6280 (2019-20), https://app.leg.wa.gov/billsummary?BillNumber=62808Year=201 9&lnitiative=false.\n\n15 The phrase “face surveillance systems” appears in S.4084 (Facial Recognition and Biometric Technology Moratorium Act) introduced in June 2020,\nhttps://www.markey.senate.gov/news/press-releases/senators-markey-and-merkley-and-reps-jayapal-pressley-to-introduce-legislation-to-ban-\ngovernment-use-of-facial-recognition-other-biometric-technology. The term “biometric surveillance systems” is used in the California A.B. 1215\nthat bans face recognition on body-worn cameras; the bill refers to “any computer software or application that performs facial recognition or other\nbiometric surveillance.”\n\n16 See, for example, the way biometric data is defined in BIPA and other state biometric privacy laws, which specify “facial geometry,’ “voice prints,” and\n“fingerprints.”\n\n17 __ Kelly Gates, “Introduction: Experimenting with the Face,’ in Our Biometric Future (New York: New York University Press, 2011).\n\n18 Several defendants sued under BIPA have unsuccessfully argued before the courts that the specific exclusion of photographs means that\ninformation derived from photographs should also be excluded. In Rivera v. Google, Inc. (238 F. Supp. 3d 1088, 1095 (N.D. Ill. 2017), Google argued\nthat its facial templates were derived from photographs, and therefore excluded from BIPA's definition of biometric information, but the court held\nthat templates were still biometric identifiers, since BIPA does not qualify the definition of biometric identifiers based on how they were derived. See\nMatthew T. Hays, “Technology Defendants Continue to Test Whether the Illinois BIPA Law Can Cope with Modern Facial Recognition Technology,’\nFirewall, December 6, 2019, https://www.thefirewall-blog.com/2019/12/technology-defendants-continue-to-test-whether-the-illinois-bipa-law-can-\ncope-with-modern-facial-recognition-technology/.\n\n19 Recital 51 of the GDPR notes that “[t]he processing of photographs should not systematically be considered to be processing of special categories\nof personal data as they are covered by the definition of biometric data only when processed through a specific technical means allowing the\nunique identification or authentication of a natural person.”",
    "Page_21": "Amba Kak | The State of Play and Open Questions for the Future | 21\n\nThis narrow technical definition of biometric data creates a set of troubling loopholes. In her\nchapter, Els Kindt explains that the exclusion of photographs, voice recordings, or other forms\nof so called “raw” biometric data adversely limits the impact of the GDPR. She argues that\nheightened protections, like explicit consent, are foregone in the initial stage of data collection\nand storage (such as when a photo is uploaded to a social media site) and that use of such data\nwithout consent is often permitted by particular exceptions for law enforcement agencies after\nsuch data has been collected.\n\nThe exclusion of photographs and voice recording is also troubling given the realities of how\ncommercial and government surveillance systems are developed and deployed today. The\nharvesting of face images matched to individual names from the web is a common method used\nto create face-name databases. These databases are the foundation of sophisticated and covert\nsurveillance tools created by private firms, who often do so in secret and proceed with almost no\noversight.2° The same covert surveillance practices are emerging with voice recordings.”\"\n\nThe definition of biometric data offered in the California Consumer Protection Act (CCPA) of 2018\nstands apart from existing definitions and could be instructive as a way to close this loophole.\nRather than the current representation of the data, CCPA’s definition focuses on the ability to\nextract an identifier template that can be algorithmically processed in order to determine whether\nit falls within the scope of the law.\n\nWhy have data-protection laws had limited effectiveness in\ncurbing the expansion of biometric surveillance infrastructure by\ngovernment?\n\nPrinciples of data minimization and purpose limitation have rarely been applied to\nchallenge the creation or expansion of biometric systems. Rather than an evidence-based\nscrutiny of the link between the means and the ends, the broad rationale of security and\nefficiency in service delivery has usually served to enable rather than restrict the use of\nbiometric systems.\n\n20 See Daniel Laufer and Sebastian Mainek, “A Polish Company Is Abolishing Our Anonymity,’ NetzPolitik, July 10, 2020, https://netzpolitik.org/2020/\npimeyes-face-search-company-is-abolishing-our-anonymity/; and Louise Matsakis, “Scraping the Web Is a Powerful Tool. Clearview Al Abused It,”\nWired, January 25, 2020, https://www.wired.com/story/clearview-ai-scraping-web/.\n\n21 Jeremy Kirk, “Hey Alexa. Is This My Voice or a Recording?,’ BankInfoSecurity, July 6, 2020, https://www.bankinfosecurity.com/hey-alexa-this-my-\nvoice-or-recording-a-14562; George Joseph and Debbie Nathan, “Prisons across the U.S. Are Quietly Building Databases of Incarcerated People's\nVoice Prints,’ Intercept, January 30, 2019, https://theintercept.com/2019/01/30/prison-voice-prints-databases-securus/.\n\n22 See Section 3(e) of the CCPA of 2018: “Biometric information includes, but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm,\nvein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted...”\n(emphasis mine)",
    "Page_22": "22 | Regulating Biometrics: Global Approaches and Urgent Questions\n\necessity and proportionality are common legal principles in international human rights law\n\nand reflected in a number of data-protection laws across the world.” They require that any\ninfringement of privacy or data-protection rights be necessary and strike the appropriate balance\nbetween the means used and the intended objective. The proportionality principle is also central\n0 constitutional privacy case law across the world, and while there are regional differences,\nhese tests generally involve a balancing exercise where the right to privacy is balanced against a\ncompeting right or public interest.**\n\nn data-protection regulation, these principles are reflected in the types of data categories that\nare collected,” how the data can be used,”° and how long it can be stored.?” Under the GDPR and\nsimilar data-protection laws, the “data minimization’ provision in Article 5 requires that entities\nimit personal data collection to that which is “adequate, relevant and limited to what is necessary\nin relation to the purposes for which they are processed.” For law enforcement agencies, the\n\nData Protection Law Enforcement Directive (DP LED) requires a higher standard of whether that\nbiometric data collection is “strictly necessary.\"\n\n \n\nTaken seriously, these provisions question whether the collection of biometric data is necessary\nin the first place.2? For example, the Swedish Data Protection Authority outlawed the use of facial\nrecognition in schools on the grounds that its use for attendance was a disproportionate means\nto achieve this goal when far less intrusive means exist.°° The French Data Protection Authority\n(Commission Nationale de I'iInformatique et des Libertés, or CNIL) and the regional court of\nMarseille also ruled similarly to declare the trial of facial recognition attendance systems illegal in\nFrance.%\n\n \n\nIn Ben Hayes and Massimo Marelli's chapter, they explain how the International Committee of the\nRed Cross (ICRC) applied data-protection proportionality principles to the use of biometrics for aid\ndistribution to people in need of humanitarian assistance. While the ICRC eventually determined\nthat there was a “legitimate interest’ in using biometric systems for this purpose, they limited the\nuse to a “token-based system’ (i.e., a card on which people’s biometric data is securely stored).\nThe ICRC decided not to collect, retain, or further process people’s biometric data, and therefore\nnot to establish a biometric database. If people want to withdraw or delete their biometric data,\nthey can either return the card or destroy it themselves.\n\n23 See Privacy International, “Towards International Principles on Communications Surveillance,’ November 20, 2012, https://privacyinternational.org/\nblog/1360/towards-international-principles-communications-surveillance. The article refers to a meeting of experts in Brussels in October 2012. See\nalso European Data Protection Supervisor, “Necessity & Proportionality,’ n.d., https://edps.europa.eu/data-protection/our-work/subjects/necessity-\nproportionality_en. See generally Charlotte Bagger Tranberg, “Proportionality and Data Protection in the Case Law of the European Court of Justice,”\nInternational Data Privacy Law 1, no. 4 (November 2011): 239-248, https://doi.org/10.1093/idpl/ipr01 5.\n\n24 See generally Alec Stone Sweet and Jud Mathews, “Proportionality Balancing and Global Constitutionalism,’ Columbia Journal of Transnational Law\n47, no. 72 (2008-09): 112.\n\n25 See Article 5(c), GDPR on “data minimization,’ and Article 9, GDPR on processing of special categories of personal data\n\n26 See Article 5(b), GDPR on “purpose limitation.”\n\n27 See Article 5(e), GDPR on “storage limitation.”\n\n28 See Article 10, DP LED on processing of “sensitive categories” of personal data.\n\n29 See Els Kindt, “Biometric Applications and the Data Protection Legislation: The Legal Review and the Proportionality Test,’ Datenschutz und\nDatensicherheit 31, no. 3 (2007):166-170, https://www.law.kuleuven.be/citip/en/archive/copy_of_publications/880dud3-2007-1662f90.pdf. See also\nYue Liu, “The Principle of Proportionality in Biometrics: Case Studies from Norway,’ Computer Law & Security Review 25, no. 3 (December 2009):\n237-250.\n\n30 Sofia Edvardsen, “How to Interpret Sweden's First GDPR Fine on Facial Recognition in School, IAPP. August 27, 2019, https://iapp.org/news/a/how-\nto-interpret-swedens-first-gdpr-fine-on-facial-recognition-in-school/.\n\n31 EDRi, “Ban Biometric Mass Surveillance,’ May 13, 2020, https://edri.org/wp-content/uploads/2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf.",
    "Page_23": "Amba Kak | The State of Play and Open Questions for the Future | 23\n\nUnfortunately, the application of these principles to challenge the creation of biometric systems\nand databases is rare, especially during the key initial or “pilot” stages before these systems are\nbuilt and used.** More often than not, inquiries into “necessity” are structured to enable rather\nthan restrict the use of biometric systems. Even where data minimization principles exist, the\nnotoriously broad but powerful rationale of “efficiency” or “law and order” and “national security”\nserve to grant most government uses of biometrics a free pass without any evidence-based\nscrutiny of the relationship between means and ends.*? As noted in the European Digital Rights\n(EDRi) 2020 report on biometric mass surveillance, this uneven application of the law can also be\nattributed to the European Union's inadequately resourced and politically disempowered National\nData Protection Authorities. On the other hand, in countries that still lack data-protection laws\nand data-protection authorities (DPAs), when biometric ID projects have faced constitutional\nchallenges in the Court, the proportionality test is often overlooked in favor of broad claims\naround the efficiency of biometric service delivery systems, with scant analysis of alternative, less\nrights-infringing means to achieve that goal.°*\n\nLegal principles of “purpose limitation” are often ineffective given the broader political\nand institutional trends working to dissolve boundaries between civilian, criminal, and\nimmigration biometric databases. Driver's license face databases are a key site for this\nkind of “function creep” and require urgent policy intervention.\n\nThe “purpose limitation” principle restricts the use of data for purposes beyond what it was\noriginally collected for; a specified purpose must not be used for another “incompatible” purpose.\nYet pervasive “security” imperatives often blur the boundaries between criminal, welfare,\n\nand immigration processes and, consequently, obfuscate what is perceived and understood\n\nas a “compatible” purpose. Under the US federal Secure Communities program (S-COMM),\n\nstates submit fingerprints of arrestees to criminal as well as immigration databases, allowing\nImmigration and Customs Enforcement (ICE) to access this information.*® ICE has also requested\nface recognition searches of driver's license databases in multiple states in the US.% In Australia,\nthe Home Affairs department has been centralizing state driver's license face databases to use\nfor broader policing and law enforcement purposes.” India’s biometric ID project Aadhaar is\n\n32 See EDRi, “Evidence on Biometrics and Fundamental Rights,’ July 2020 (submitted to the European Commission consultation and on file with the\nauthor) for a list of projects, including multiple case studies from Europe that were not properly assessed due to the claim that they were in the\n“experimental” or “pilot” stage.\n\n33. In the European context, see Fundamental Rights Agency (FRA), “Facial Recognition Technology: Fundamental Rights Considerations in the Context\nof Law Enforcement’ 2020, https://fra.europa.eu/sites/default/files/fra_uploads/fra-2019-facial-recognition-technology-focus-paper-1_en.pdf. The\nauthors note than “[a]n objective of general interest—such as crime prevention or public security—is not, in itself, sufficient to justify an interference”\nwith fundamental rights, meaning that the Law Enforcement Directive's data protections must apply.\n\n34 See Mariyan Kamil, “The Aadhaar Judgment and the Constitution — Il: On Proportionality,” Indian Constitutional Law and Philosophy, September 30,\n2018, https://indconlawphil.wordpress.com/2018/09/30/the-aadhaar-judgment-and-the-constitution-ii-on-proportionality-guest-post/.\n\n35 As aresult of this, anyone arrested for a state crime (even if they were never charged or were wrongly arrested) is vulnerable to deportation or\ndetention. See Jennifer Lynch, “From Fingerprints to DNA: Biometric Data Collection in U.S. Immigrant Communities and Beyond,’ American\nImmigration Council, May 23, 2012, https://www.americanimmigrationcouncil.org/research/fingerprints-dna-biometric-data-collection-us-\nimmigrant-communities-and-beyond; see also ACLU, “Secure Communities (‘S-Comm’),\" n.d., https://www.aclu.org/other/secure-communities-s-\ncomm\n\n36 Harrison Rudolph, “ICE Searches of State Driver's License Databases,’ Center on Privacy & Technology at Georgetown Law, Medium, July 8, 2019,\nhttps://medium.com/center-on-privacy-technology/ice-searches-of-state-drivers-license-databases-4891a97d3e19.\n\n37. See Jake Goldenfein and Monique Mann, “Australian Identity-Matching Services Bill” in this compendium.",
    "Page_24": "24 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nwidely known as a welfare delivery system, yet government officials may use the data for national\nsecurity purposes in limited circumstances,® and the National Crime Bureau has publicly stated\ntheir desire to use the system for criminal investigations.*° These systems are structured to evade\nand remove the purpose limitations on data use.\n\nThe failure of proportionality safeguards is also borne out in the context of centralized\nbiometric ID systems where legislation has frequently been introduced only after these\nsystems are developed, and in some cases after they're already deployed and in use.\n\nLarge-scale biometric ID projects that span welfare, criminal, and immigration contexts have\ntypically been implemented as technocratic exercises driven by executive agencies, often with\nthe glaring absence of law. Even as advocacy efforts focus on demanding legal frameworks to\nensure legislative and public scrutiny, legislation often comes too little, too late. For one, many\nprojects do not receive proper scrutiny or are passed through extraordinary measures that forgo\nscrutiny altogether.*° In other cases, weak procedural safeguards are proposed, but the broader\ncentralization of power in a few agencies remains unchallenged.\n\nIn Jake Goldenfein and Monique Mann's chapter, they argue that the Australian Identity Services\nBill provided the Home Affairs department with authorization to become the central node (“the\nhub”) through which all identity and suspect identification requests would be routed. They\nconclude that “a true proportionality analysis” might have questioned whether a centralized facial\nrecognition database was in fact necessary to address the stated purpose of curbing identity\nfraud, but in reality “this framing is operationalized in ways that enable continuing expansion of\nsurveillance systems.”\n\nThe mere existence of procedural safeguards like data security or consent can obscure the root\nof the problem, only serving to legitimize the continued existence of these systems. When faced\nwith existential threats, like the potential of being invalidated by the highest courts, data-privacy\nrules have repeatedly been held up as an adequate safeguard against the concerns raised, leading\nto widespread skepticism about the role these laws play.*' In the case of India’s nationwide\nbiometric ID project (Aadhaar), legislation authorizing and regulating the project came nearly a\ndecade after biometric data collection began. This massive delay is even more concerning given\nthe absence of a data-privacy law that applied to government agencies. In her contribution to this\ncompendium, Nayantara Ranganathan challenges foundational assumptions about the role of the\n\n38. Vrinda Bhandari and Renuka Sane, “A Critique of the Aadhaar Legal Framework,” National Law School of India Review 31, no. 4 (2019):1-23.\n\n39 Aman Sharma, “Cannot Share Aadhaar Biometric Data for Crime Investigations,’ Economic Times, June 22, 2018, https://economictimes. indiatimes.\ncom/news/politics-and-nation/cannot-share-aadhaar-biometric-data-for-crime-investigations-uidai/articleshow/64700379.cms.\n\n40 See Nayantara Ranganathan’s chapter in this compendium, “The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the\nAadhaar Project,’ in which she describes the truncated and legally dubious passage of the Aadhaar as a “money bill.\" See also ADC, “ADC Files an\nAction of Unconstitutionality before GCBA after the Introduction of Face Recognition System,’ November 6, 2019, https://adc.org.ar/en/2019/11/06/\nadc-files-an-action-of-unconstitutionality-before-gcba-after-the-introduction-of-face-recognition-system/.\n\n41 See commentary on the Kenyan data-protection law by Rasna Warah, “Data Protection in the Age of Huduma Namba: Who Will Benefit?,” Elephant,\nNovember 29, 2019, https://www.theelephant.info/op-eds/2019/11/29/data-protection-in-the-age-of-huduma-namba-who-will-benefit/; and see\nPraavita, “Can the Aadhaar Act and a Data Protection Act Coexist?,” The Wire, July 30, 2018, https://thewire.in/law/can-the-aadhaar-act-and-a-data-\nprotection-act-coexist.",
    "Page_25": "Amba Kak | The State of Play and Open Questions for the Future | 25\n\nlaw in relation to these projects, characterizing regulation as a legitimizing force that reflects the\ninterests of the powerful actors that drive these systems. She argues that Aachaar’s regulation\nfunctioned to “consolidate the developments of the first seven years of the project, and also\npresented a revisionist history of the actual goals of the project, obscuring the stakes for private\ninterests...[MJany of the problems with Aadhaar should not be understood as failures of law or\nregulation, but products of law and regulation.”\n\nIs meaningful notice and consent possible in the context of biometric\nsystems? What are the limitations of a consent-based approach and\nwhat supplements or alternatives might be required?\n\nGiven the predominance of the data-protection approach, notice and consent has\n\nbeen a cornerstone of biometric regulation, yet the well-documented limitations of this\nmodel underscore the need for additional necessity and proportionality limits even after\nconsent has been obtained. Recent Al legislation also requires broader “explainability”\nrequirements as a core component of meaningful notice.\n\nWhile notice and consent has traditionally been the cornerstone of data-protection and privacy\napproaches globally, its limitations have been laid bare in recent years, leading to skepticism\nabout (if not outright rejection of) the idealized conception of “individual control.’ In their chapter,\nBen Hayes and Massimo Marelli explain why the Red Cross removed consent as a legal “ground\nof processing’*’ in emergency humanitarian contexts where, the authors argue, consent can never\nbe assumed to be “freely given.”\n\nAt the same time, the individual's right to refuse or revoke permission for the collection or use of\ntheir data has been an important tool in challenging biometric systems like live facial recognition\nin public spaces that are designed to evade such active permission. As described in Woodrow\nHartzog's chapter, under BIPA, the failure to obtain consent from individuals before using their\nbiometric data has led to several successful lawsuits against some of the largest tech companies\nin the world and is the basis for the lawsuit recently launched against Clearview Al.“4\n\n42 Fora rejection of the idea of privacy as “control,” see generally Ruth Gavison, “Privacy and the Limits of Law,’ Yale Law Journal 89, no. 3 (January\n1980): 421-471. See also Woodrow Hartzog, “The Case Against Idealising Control,” European Data Protection Law Review 4, no. 4 (2018): 423-432.\n\n43. Grounds of processing is a legal term of art popularized by the GDPR. It refers to a number of legal justifications, of which at least one must be\nmet in order to “process” (i.e., collect, store, use, etc.) personal data. Grounds of processing include consent, performance of a contract, legitimate\ninterests of a business, and so on.\n\n44 Woodrow Hartzog, “BIPA: The Most Important Biometric Privacy Law in the US?,\" in this compendium",
    "Page_26": "26 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nIn the GDPR, consent is supplemented by several general limits of proportionality and necessity*®\nthat hold irrespective of whether consent is obtained. By contrast, US state laws like BIPA focus\non notice and consent with few additional restrictions on collection or use beyond the prohibition\nagainst selling biometric data for profit and limits on retention.\n\nAs Hartzog concludes, BIPA has done “very little to bring about the kind of structural change and\nsubstantive limits necessary.” For one, he explains how most of us are simply “not capable of\nmeaningfully exercising our agency over modern data practices” and argues that BIPA provides\nlittle protection from the “post-permission risks” of biometric technologies.*° This underscores the\nneed for additional transparency and accountability, including bright-line restrictions alongside a\nrobust notice and consent regime.\n\nEmerging regulatory approaches for algorithmic or Al systems include a broader understanding\nof notice that goes beyond simply informing the individual that algorithmic tools are being\n\nused. These newer approaches also take into account how these systems work, the context in\nwhich these systems are used, and what criteria are informing algorithmic decisions. This broad\nscope will be especially valuable in regulating biometric systems that serve purposes beyond\nidentification and verification. The Illinois Artificial Intelligence Video Interview Act is an example\nof a notice provision tailored to the specific context of job interviews; it requires that all job\napplicants be informed when Al systems used to assess their performance as a candidate are\ndeployed during interviews. In addition to this, it requires that each applicant be provided clear\ninformation about “how the artificial intelligence works and what general types of characteristics\nit uses to evaluate applicants.” Whether such explanations are possible, and whether they can\nwork to inform meaningful choices on the part of job seekers given the power dynamics at work\nin the context of a job interview, have yet to be seen.\n\n45 See Article 5 GDPR including principles of data minimization, collection limitation, purpose limitation, storage limitation principles.\n46 Ibid\n47 See Section 5, “Artificial Intelligence Video Interview Act,’ http://www.ilga.gov/legislation/publicacts/fulltext.asp?Name=101-0260.",
    "Page_27": "Amba Kak | The State of Play and Open Questions for the Future | 27\n\nSECTION 2. BEYOND PRIVACY: ACCURACY,\nDISCRIMINATION, HUMAN REVIEW, AND DUE\nPROCESS\n\nHow should regulatory frameworks address concerns about accuracy\nand non-discrimination in biometric systems?\n\nTo what extent should regulation rely on standards of performance\nand accuracy set by technical standards-setting bodies?\n\nWhile accuracy and discrimination concerns are at the forefront of public debate,\ncorresponding legal protections have been rare in existing regulatory frameworks.\nHowever, recent legislation and advocacy efforts in the US have mandated accuracy and\nnondiscrimination audits for facial recognition systems, going as far as to require such\naudits as a condition for lifting a moratorium on use.\n\nWhile technical standards (e.g., NIST’s Face Recognition Vendor Test) are evolving to\naccount for bias and inaccuracy, they generally underperform in “real-life” contexts and are\nlimited in their ability to address the broader discriminatory impact of these systems as\nthey are applied in practice. If such standards are positioned as the sole check on facial\nrecognition systems, they could function to obfuscate, rather than mitigate, harm.\n\nAccuracy and “error rates” metrics are a staple of the mainstream conversations around\nbiometrics and are used as a tool in the machine learning field to compare systems and assess\nprogress. Accuracy claims have been a simple way for those developing, marketing, and\napplying these systems to “prove” effectiveness, and to demonstrate that automation offers an\nimprovement over manual processes. In the past two years, however, the same facial recognition\nsystems that boast high accuracy rates according to such narrow metrics have been shown\n\nto perform less well when accuracy rates are stratified across demographics like age, race,\ngender, and disability.“* “Errors” in these systems are not evenly distributed, and reflect historical\n\n48 Joy Buolamwini and Timnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,’ Proceedings\nof Machine Learning Research 81 (2018):1-15, http://proceedings.mlr.press/v81/buolamwini1 8a/buolamwini18a.pdf; KS Krishnapriya et al.,\n“Characterizing the Variability in Face Recognition Accuracy Relative to Race,” Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (2019), https://arxiv.org/abs/1904.07325; Cynthia M. Cook et al., “Demographic Effects in Facial Recognition and Their\nDependence on Image Acquisition: An Evaluation of Eleven Commercial Systems,” IEEE Transactions on Biometrics, Behavior, and Identity Science\n1,no. 1 (Jan. 2019): 32-41, https://ieeexplore.ieee.org/document/8636231; Inioluwa Deborah Raji and Joy Buolamwini, “Actionable Auditing,\nInvestigating the Impact of Publicly Naming Biased Performance Results of Commercial Al Products,’ Proceedings of the Conf. on Artificial\nIntelligence, Ethics, and Society (2019), https://www.aies-conference.com/2019/wp-content/uploads/2019/01/AIES-19_paper_223.pdf; Morgan\nKlaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker, “How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial\nAnalysis Services,’ Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (November 2019): 1-33, https://doi.org/10.1145/3359246.",
    "Page_28": "28 | Regulating Biometrics: Global Approaches and Urgent Questions\n\npatterns of racism, gender bias, and ableist discrimination. To remedy this problem, researchers\nhave called for auditing on accuracy across specific demographic and phenotypic subgroups,\naccompanied by measures that can close performance gaps where they arise.”\n\nTo accomplish such audits, many are turning to technical standard-setting bodies that set\nbenchmarks for accuracy, performance and safety. Auditing protocols like the National Institute\nof Standards and Technology (NIST) 2019 Face Recognition Vendor Test (part three) evaluate\nwhether the algorithm performs differently across different demographics in the dataset.\n\nRegulators and lawmakers have also begun to take notice, calling for audits by technical\nstandards-setting bodies that set benchmarks for accuracy, performance, and safety. In March\n2020, the UK Equality and Human Rights Commission called to suspend the use of facial\nrecognition in England and Wales until discrimination against protected groups has been\nindependently scrutinized. Recent legislation in the US includes accuracy and nondiscrimination\naudits as a condition for the use of facial recognition. The Washington State Bill SB 6280, passed\nin March 2020, requires that face recognition companies cooperate to allow for independent\ntesting for “accuracy and unfair performance” across subgroups including race, skin tone,\nethnicity, gender, age, or disability status. If independent testing reveals “material unfair\nperformance differences,” companies are required to rectify the issues within ninety days. Another\nproposed federal bill (S.2878: the Facial Recognition Technology Warrant Act of 2019) requires\nfederal law enforcement agencies to work with NIST®? to establish testing systems to ensure\nconsistent accuracy across gender, age, and ethnicity.\n\nWhile these standards are a step in the right direction, it would be premature to rely on them to\nassess performance, and they do not adequately capture the broader discriminatory impacts\nthese systems might have when they are used. First, researchers and advocacy organizations\nhave found that many of the systems that “pass” current benchmark evaluations continue\n\nto underperform in real-life contexts.°' Additionally, there is currently no standard practice to\ndocument and communicate the histories and limits of benchmarking datasets, and thus no way\nto determine their applicability to a particular system or suitability for a given context.\n\nMoreover, creating a solely technical threshold to judge discriminatory impact can distort the\nbiased practical implementation of these technologies and their weaponization against specific\ngroups. For example, facial recognition systems are deployed disproportionately in minority\ncommunities, so even the most accurate systems will be discriminatory. They also “run the risk of\nproviding ‘checkbox certification; allowing vendors and companies to assert that their technology\nis safe and fair without accounting for how it will be used, or its fitness for a given context.”\n\n49 See Raji and Buolamwini, “Actionable Auditing,” and Buolamwini et al., Gender Shades, MIT Media Lab, http://gendershades.org,\n\n50 NIST is anon-regulatory federal agency within the US Department of Commerce. Its mission is to promote US innovation and industrial\ncompetitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve quality of\nlife. Auditing protocols like the NIST 2019 Face Recognition Vendor Test (part three) evaluate whether the algorithm performs differently across\ndifferent demographics in the dataset.\n\n51 See Inioluwa Deborah Raji and Genevieve Fried, “About Face: A Survey of Facial Recognition Evaluation,’ Meta-Evaluation workshop at\nAAAI Conference on Artificial Intelligence (forthcoming, 2020); Pete Fussey and Daragh Murray, “Independent Report on the London\nMetropolitan Police Service's Trial of Live Facial Recognition Technology,” The Human Rights, Big Data and Technology Project, July 2019,\nhttps://48ba3m4eh2bf2sksp43rq8kk-wpengine.netdna-ssl.com/wp-content/uploads/2019/07/London-Met-Police-Trial-of-Facial-Recognition-Tech-\nReport.pdf.\n\n52 Written Testimony of Meredith Whittaker, US House of Representatives Committee on Oversight and Reform, “Facial Recognition Technology (Part\nIll): Ensuring Commercial Transparency & Accuracy, January 15, 2020, https://oversight.house.gov/sites/democrats.oversight.house.gov/files/\ndocuments/WRITTEN%20testimony%20-%20MW%20oversight.pdf/",
    "Page_29": "Amba Kak | The State of Play and Open Questions for the Future | 29\n\nDoes requiring “meaningful human review” of biometric recognition\nsystems ensure oversight and accountability?\n\nRecent legislation includes provisions that mandate ‘meaningful human intervention”\nin the results of biometric systems. However, a large body of research suggests that\nthe people who review the results of biometric systems overwhelmingly overestimate\ncredibility, and often respond inaccurately and with bias.\n\n“Human intervention” in automated decisions has gained considerable acceptance as a legal\napproach to provide a meaningful check on the potential harms these systems represent. Article\n22 of the GDPR, for example, includes a restriction on “solely automated decisions,” and requires\nhuman intervention when automated systems impact “legal or similarly significant” decisions\nabout people's lives. The recently passed and heavily criticized®* Washington State facial\nrecognition law similarly includes provisions for “meaningful human review\" and periodic officer\ntraining as conditions for the use of biometric technology. Human review is defined in terms of\n“review or oversight by one or more individuals...who have the authority to alter the decision under\nreview.”*\n\nHowever, a large body of research demonstrates that human intervention in these systems does\nnot address major concerns about transparency or control. Individuals who review results are\noften unable to accurately evaluate the quality or fairness of the outputs, and often respond to\npredictions in biased and inaccurate ways.°° The ACLU has pointed to the imprecisely defined\nnotion of meaningful human review as “deeply flawed” given its vague definition. They maintain\nthat it should not become a rubber stamp that allows for the use of facial recognition or similar\nsystems in sensitive social domains like welfare and criminal justice.\n\nIn their chapter, Peter Fussey and Daragh Murray show that human operators who asses live\nfacial recognition “matches” often defer to the algorithm’s output, despite the known inaccuracy\nof such output—a phenomenon referred to as “automation bias.” In their research, they found that\n“humans overwhelmingly overestimated the credibility of the system.”*” The Indian government\nestablished a system of “manual overrides” to address the issue of biometric errors that lead to\n\n53 Jennifer Lee, \"We Need a Face Surveillance Moratorium, Not Weak Regulations: Concerns about SB 6280,’ ACLU, March 31, 2020, https://www.aclu-\nwa.org/story/we-need-face-surveillance-moratorium-not-weak-regulations-concerns-about-sb-6280.\n\n54 — Section 2(7), Washington Senate Bill 6280, http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20Legislature/6280-S.\nPL.pdf?q=20200331083729.\n\n55 See Ben Green and Yiling Chen, “Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,’ January 2019, https://\nwww.benzevgreen.com/wp-content/uploads/2019/02/19-fat.pdf; Ben Green and Yiling Chen, “The Principles and Limits of Algorithm-in-the-Loop\nDecision Making,’ November 2019, https://www.benzevgreen.com/wp-content/uploads/2019/09/19-cscw.pdf; Megan Stevenson, “Assessing\nRisk Assessment in Action,” Minnesota Law Review 103, no. 303 (2018), https://dx.doi.org/10.2139/ssrn.3016088; Berkeley J. Dietvorst, Joseph P.\nSimmons, and Cade Massey, “Algorithm Aversion:People Erroneously Avoid Algorithms after Seeing Them Err,” Journal of Experimental Psychology\n144, no. 1 (February 2015): 114-126, https://psycnet.apa.org/fulltext/2014-48748-001.html; Amirhossein Kiani et al., “Impact of a Deep Learning\nAssistant on the Histopathologic Classification of Liver Cancer,’ npj Digital Medicine 3, no. 23 (2020), https://doi.org/10.1038/s41746-020-0232-8.\n\n56 Lee, \"We Need a Face Surveillance Moratorium.”\n\n57 See Peter Fussey and Daragh Murray, “Policing Uses of Live Facial Recognition in the United Kingdom,’ in this compendium.",
    "Page_30": "30 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nexclusion from government benefits and systems.*°® However, studies suggest that even these\nlegal norms did not always govern the behavior of those operating the biometric systems on\nthe ground. Those managing these systems often failed to exercise this option and refused\npeople access to services because of “incorrect’ (or rather complete lack of) human intention in\novercoming technological failure.”\n\nAn open question for future legal approaches is how to incentivize and ensure real capacity\nfor human oversight. This would include an assessment of the gaps in knowledge, biases,\nor inefficiencies that limit accountability and prevent human operators from assessing or\nanticipating problems with these systems.\n\nShould regulatory frameworks create a risk-based classification\nbetween “identification” and “verification” uses of biometric\nrecognition?\n\nWhat are the potential risks of a permissive regulatory approach to\nverification?\n\nRecent official policy documents in the EU suggest that “verification” (1:1) is an inherently\nless risky use compared to identification (1:n) in terms of accuracy, data security\nvulnerabilities, and the capacity for meaningful consent.\n\nHowever, any broad-brush permissive approach to verification in the law should be\navoided. Even if participation in a verification system is with knowledge, these systems\nmight not afford individual's real choice when they act as gatekeepers to access essential\nspaces or services.\n\nThe distinction between verification and identification is often described in terms of the technical\nshorthand 1:1 versus 1:n. 1:1 verification (or authentication) aims to determine whether people\nare who they claim to be through a one-to-one match that queries biometric information (e.g.,\n\na facial scan by a smartphone) against the data that the person has previously provided (e.g.,\nthe person stores their photograph on the phone when they first purchase it).®° Identification,\n\nor 1:n, is a more technically involved process that compares the biometric information of an\n\n58 Ronald Abraham et al., “State of Aadhaar Report 2017-18,\" IDinsight, May 2018, https://staticl.squarespace.com/\nstatic/S5b7cc54eec4eb7d25f7af2be/t/Sbbd2874c830256 1 862f03d4/1539123330295/Statetof+AadhaartReport_2017-18.pdf.\n\n59 See Bidisha Chaudhuri, “Paradoxes of Intermediation in Aadhaar: Human Making of a Digital Infrastructure,’ Journal of South Asian Studies 42\n(2019):572-587, https://doi.org/10.1080/00856401.2019.1598671\n\n60 See Stan Z. Li and Anil K. Jain, Handbook of Face Recognition (New York: Springer, 2005), 1-15.",
    "Page_31": "Amba Kak | The State of Play and Open Questions for the Future | 31\n\nunknown person against a database of many people's biometric data. An algorithm determines\nif the person is represented in the database and who they might be. Some identification systems\nprovide a number of “similar faces” that meet a specified confidence or accuracy threshold.°'\n\nRecent official policy documents” as well as data-protection authorities in the EUS suggest\n\nthat verification is an inherently less risky use of biometrics in terms of accuracy, data security\nvulnerabilities, and the capacity for meaningful consent. Biometric locks on phones are a\ncommon example used to demonstrate these claims, and San Francisco recently amended its\nfacial recognition moratorium to allow employees to use biometric lock features on government-\nissued cell phones.™ By contrast, some of the most controversial reported cases of facial\nrecognition largely pertain to identification (1:n) systems like live facial recognition (LFR), which\nhas a record of high error rates.\n\nThese accounts of verifications often link or even conflate the claim of higher accuracy with\nmeaningful consent. The claim is that with verification systems, people are willing to present\ntheir biometrics in a “cooperative” way (like a frontal face with eyes open), whereas with\nidentification, people could be unaware of being identified, which increases the error rates.®° Any\ngeneral assumption that verification systems involve the active and targeted participation of the\nindividual, however, rests on shaky foundations. While these systems might have higher accuracy\nrates than identification systems, they are still predictive and not immune to the same kinds of\nerrors and biases across lines of race, gender, and other demographic traits. More importantly,\neven if participation is done with volition and knowledge, these systems might not afford\nindividuals real choice when they act as gatekeepers to access to essential spaces and services.\nThis became a focal point in the opposition against biometric ID systems in India and Kenya,® as\nwell as in the use of biometric systems in humanitarian contexts.%\n\n61 Ibid\n\n62 See Luana Pascu, “New EU Al Strategy Puts Remote Biometric Identification in ‘High-Risk’ Category,’ BiometricUpdate.com, February 19, 2020,\nhttps://www.biometricupdate.com/202002/new-eu-ai-strategy-puts-remote-biometric-identification-in-high-risk-category; Paul de Hert and Koen\nChristianen, “Progress Report on the Application of the Principles of Convention 108 to the Collection and Processing of Biometric Data,’ Tilburg\nInstitute for Law, Technology, and Society, April 2013, https://rm.coe.int/progress-report-on-the-application-of-the-principles-of-convention-\n108/1680744d81; see also, e.g., Article 29—Data Protection Working Party, “Working Document on Biometrics (WP 80),’ 2003, https://ec.europa.\neu/justice/article-29/documentation/opinion-recommendation/files/2003/wp80_en.pdf; “Opinion 02/2012 on Facial Recognition in Online and\nMobile Services (WP192),’ March 23, 2012, https://www.pdpjournals.com/docs/87997.pdf; and “Opinion 3/2012 on Developments in Biometric\nTechnologies (WP193),’ April 2012, https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2012/wp193_en.pdf; and\nsee CNIL, “Facial Recognition: For a Debate Living Up to the Challenges,” December 19, 2019, https://www.cnil.fr/en/facial-recognition-debate-living-\nchallenges.\n\n63 See French data-protection authority CNIL, Communication central storage fingerprint, 2007; and cf. Els Kindt, Privacy and Data Protection Issues of\nBiometric Applications: A Comparative Legal Analysis (Dordrecht: Springer, 2013), 540.\n\n64 Tim Cushing, “San Francisco Amends Facial Recognition Ban after Realizing City Employees Could No Longer Use Smartphones,” Techdirt,\nDecember 20, 2019, https://www.techdirt.com/articles/20191219/18253743605/san-francisco-amends-facial-recognition-ban-after-realizing-city-\nemployees-could-no-longer-use-smartphones.shtml.\n\n65 See Li and Jain, Handbook of Face Recognition, 1-15.\n\n66 Abdi Latif Dahir and Carlos Mureithi, “Kenya's High Court Delays National Biometric ID Program,’ New York Times, January 31, 2020, https://www.\nnytimes.com/2020/01/31/world/africa/kenya-biometric-ID-registry.html; see also reportage on the farcical nature of “consent camps’ for Aadhaar\ndiscussed during the People's Tribunal on Aadhaar-related Issues, February 28, 2020, https://threadreaderapp.com/thread/1233608762604154880.\nhtm.\n\n67 Petra Molnar, “The Contested Technologies That Manage Migration,” CIGI Online, December 14, 2018, https://www.cigionline.org/articles/contested-\ntechnologies-manage-migration:",
    "Page_32": "32 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nProponents of permissive approaches to verification typically argue that these systems\n\ninvolve local data storage, which minimizes the data security risks that come with centralized\ndatabases. However, access control at borders, airports, and buildings often centralize biometric\nauthentication systems for access to services, and many of these systems maintain centralized\nstorage and authentication records. Risks associated with biometric use are certainly\ncontextual, but any broad-brush permissive approach to verification in the law should be avoided,\nespecially where it can create loopholes that allow more harmful implementations of verification.\n\nWhat kinds of due process safeguards are required for law\nenforcement use of biometric recognition?\n\nShould law enforcement have access to these systems to begin with?\n\nOutside of a complete ban on law enforcement use, recent regulatory approaches have\nfocused on strengthening due process safeguards. This includes requiring warrants\nfor ongoing surveillance, restricting the use of facial recognition to serious crimes, and\nensuring defendants get meaningful access to biometric evidence that is used against\nthem.\n\nWhile facial recognition has received special regulatory attention, these tools should be\nunderstood as part of a broader set of algorithmic police surveillance tools, including\ndrone surveillance, license plate recognition, and predictive policing.\n\nThe use of biometric technologies in policing raises a range of legal issues, many of which have\nbeen debated and litigated over the years in the context of fingerprinting and DNA.® These include\nthe conditions under which biometric data can be taken (whether it should be at arrest or upon\nconviction), and the circumstances under which it should be deleted from such databases (for\nexample, if a person is never convicted or if a conviction is overturned). The increasing shift to\nuse of face and voice identifiers has exacerbated some of these existing concerns and created\nnew ones. Indeed, law enforcement use of facial recognition has been the subject of intense\npublic and regulatory scrutiny recently. These systems have misidentified people and been\ndisproportionately used to target communities of color. Moreover, the vast majority of cases\ninvolving face recognition searches are not disclosed, depriving defendants of the ability to\nchallenge evidence that could determine their fate in criminal trials.”°\n\n68 For an enumeration of concerns with centralized or centrally linked biometric ID infrastructures, see Access Now, #WhyID campaign, 2019, https://\nwww.accessnow.org/whyid/,\n\n69 See Robyn Caplan et al., \"Data & Civil Rights: Biometric Technologies in Policing,” Data & Society, October 27, 2015, https://datasociety.net/library/\ndata-civil-rights-biometric-technologies-in-policing/; Brandon L Garrett, \"DNA and Due Process,’ Fordham Law Review 78, no. 6 (2010): 2919-2960;\nElizabeth N. Jones, “Spit and Acquit’: Legal and Practical Ramifications of the DA's DNA Gathering Program,’ Orange County Lawyer Magazine 51, no\n9 (September 2009), http://papers.ssrn.com/sol3/papers.cfm?abstractid=1809997.\n\n70 See section on Florida case involving FACES facial recognition system used in the case against Willie E. Lynch in Rashida Richardson, Jason M.\nSchultz, and Vincent M. Southerland, “Litigating Algorithms 2019 US Report: New Challenges to Government Use of Algorithmic Decision Systems,”\nAl Now Institute, September 2019, https://ainowinstitute.org/litigatingalgorithms-2019-us. pdf.",
    "Page_33": "Amba Kak | The State of Play and Open Questions for the Future | 33\n\nOutside of complete bans, there are multiple proposals that seek to regulate different aspects\n\nof law enforcement use. In their chapter, Jameson Spivack and Clare Garvie outline emergent\nregulatory approaches in the US that focus on limiting the use of facial recognition. Some\nlimitations are based on the seriousness of the crime (e.g., only for violent felonies), while others\nban the use in conjunction with body cameras or drones. There are also bills that would require a\ncourt order to run facial recognition searches,”' as well as one that would require that defendants\nhave access to source code and other information necessary to exercise their due process rights\nwhen algorithms are used to analyze evidence in their case.”\n\nWhile facial recognition has received special regulatory attention, these tools should be\nunderstood as part of a broader set of algorithmic surveillance tools, including drone surveillance,\nlicense plate recognition, and predictive policing.”* These systems raise similar challenges for\nestablished principles around procedural fairness, such as notice, hearing, the disclosure of\nevidence, establishing reasons for decisions, and the ability to challenge these decisions.\n\nLaw enforcement use of live facial recognition (LFR) has been the subject of intense\npublic and regulatory scrutiny. Advocacy demands range from requiring a specific\nauthorizing law to calls to ban law enforcement use of LFR altogether.\n\nLive facial recognition systems in public spaces are particularly controversial. Typically, cameras\nare deployed at a fixed location and the list of people who are identified is communicated to law\nenforcement officers on the ground.’”* Despite LFR’s implications for privacy, criminal due process,\nand freedom of speech or expression, these tools have largely been rolled out without undergoing\npublic and parliamentary scrutiny.\n\nIn their chapter, Peter Fussey and Daragh Murray describe London's expansive LFR program,’\nand discuss how the London Metropolitan Police successfully argued before the High Court\nthat LFR was part of their inherent powers, and thus did not need new legislation to explicitly\nauthorize its use.’”° The case is on appeal, but one of the factors that contributed to the Court's\ndecision was the notion that LFR was not “invasive” technology and therefore did not require\nspecial sanction. Buenos Aires has also conducted an expansive LFR program.” In this case, the\nmunicipal government pushed through a resolution with truncated processes that authorized\nthe use of these systems with minimal safeguards. Advocacy organizations have challenged the\nconstitutionality of this ordinance.”\n\n71 See, e.g., the proposed Facial Recognition Technology Warrant Act Of 2019, https://www.coons.senate.gov/imo/media/doc/FRTWA%200ne-\nPager%20FinalFinal.pdf.\n\n72 “H.R. 4368: Justice in Forensic Algorithms Act of 2019,” https://www.congress.gov/bill/116th-congress/house-bill/4368/text.\n\n73 See Jay Stanley, “The Dawn of Robot Surveillance: Al, Video Analytics, and Privacy,’ ACLU, 2019, https://www.aclu.org/report/dawn-robot-\nsurveillance.\n\n74 LFR refers to facial recognition that is “always on,’ identifying people in real time as they move through public and private space.\n\n75 Metropolitan Police UK, “Live Facial Recognition,’ n.d., https://www.met.police.uk/advice/advice-and-information/facial-recognition/live-facial-\nrecognition/.\n\n76 — R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, 4 September 2019, para. 78. (‘AFR Locate” is South Wales\nPolice's nomenclature for LFR.)\n\n77 Dave Gershorn, “The U.S. Fears Live Facial Recognition. In Buenos Aires, It's a Fact of Life,” OneZero, Medium, March 4, 2020, https://onezero.\nmedium.com/the-u-s-fears-live-facial-recognition-in-buenos-aires-its-a-fact-of-life-5201 9eff454d.\n\n78 ADC, “ADC Files an Action of Unconstitutionality before GCBA”",
    "Page_34": "34 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nIncreasingly, privacy advocates are calling for a complete ban on LFR, viewing it as incompatible\nwith fundamental rights and entailing risks that cannot be mitigated through procedural\nsafeguards.\n\nAre systems that process bodily data for purposes beyond establishing\nindividual identity, like making inferences around emotional state,\npersonality traits, or demographic characteristics, covered under\nexisting biometric regulation?\n\nShould such systems be permitted at all, given their contested\nscientific foundations and mounting evidence of harm?\n\nSince many emotion recognition and personality prediction systems rely on face and\nvoice data that could be used to identify an individual (even if that is not its current\npurpose), these systems could fulfill the definitional threshold of data-protection laws like\nthe GDPR. Many recent moratorium bills in the US include systems that infer “emotion,\nassociations, activities, or the location of an individual.”\n\nHowever, many organizations are calling to ban these systems altogether given\ndiscredited scientific foundations and mounting evidence of harm.\n\nIt is unclear whether existing biometric regulation will apply to systems where the primary\npurpose is to infer emotional states, interior characteristics, or identities like gender, race,\nethnicity, and age.” The fact that these systems rely on face or voice data that could be used to\nconfirm or establish an individual's identity (even if that is not its current purpose) could mean\nthat these systems fulfill the definitional threshold of biometric data under data-protection\n\nlaws like the GDPR. The European Digital Rights Initiative (EDRi) has also argued that biometric\nprocessing under the GDPR should be interpreted to include “detection of appearance, inferred\nbehavior, predicted emotions or other personal characteristics.”°°\n\n79 Some technical literature uses the term “soft biometrics” to define the process of “categorizing information about bodily traits where a person may\nnot be identified in the process.” See U. Park and A. K. Jain, “Face Matching and Retrieval Using Soft Biometrics,” IEEE Transactions on Information\nForensics and Security 5, no. 3 (September 2010): 406-415, https://doi.org/10.1109/TIFS.2010.2049842; and see A. Dantcheva, “What Else\nDoes Your Biometric Data Reveal? A Survey on Soft Biometrics,’ IEEE Transactions on Information Forensics and Security 11, no. 3 (March 2016)\n441-467, https://doi.org/10.1109/TIFS.2015.2480381\n\n80 Sarah Chander, “Recommendations for a Fundamental Rights-Based Artificial Intelligence Regulation,” EDRi, June 4, 2020, https://edri.org/wp-\ncontent/uploads/2020/06/Al_EDRiRecommendations.pdf.",
    "Page_35": "Amba Kak | The State of Play and Open Questions for the Future | 35\n\nMany recent moratorium bills in the US include systems that use facial data for broader\ninferences, such as inferring “emotion, associations, activities, or the location of an individual.”*!\nThe 2019 moratorium bills introduced in New York® and Washington®? include any automated\nprocess by which characteristics of a person's face are analyzed to determine “the person's\nsentiment, state of mind, or other propensities including, but not limited to, the person's level\nof dangerousness.” In specific contexts, these systems will require additional norms around\nexplainability or transparency about how inferences are made, such as in the Illinois Al\nVideoconferencing Act 2019, which regulates the use of these tools in hiring.\n\n81 Eg., Bill S.1385/H.1538. See ACLU Massachusetts, “Face Surveillance Moratorium,’ n.d., https://www.aclum.org/en/legislation/face-surveillance-\nmoratorium. See also Ed Markey, “Senators Markey and Merkley, and Reps. Jayapal, Pressley to Introduce Legislation to Ban Government Use\nof Facial Recognition, Other Biometric Technology,’ June 25, 2020, https://www.markey.senate.gov/news/press-releases/senators-markey-and-\nmerkley-and-reps-jayapal-pressley-to-introduce-legislation-to-ban-government-use-of-facial-recognition-other-biometric-technology; and Cory\nBooker, “Booker Introduces Bill Banning Facial Recognition Technology in Public Housing,” November 1, 2019, https://www.booker.senate.gov/news/\npress/booker-introduces-bill-banning-facial-recognition-technology-in-public-housing\n\n82 Bill A6787D, New York https://www.nysenate.gov/legislation/bills/2019/a6787.\n\n83 Bill HB 2856, Washington, https://app.leg.wa.gov/billsummary?BillNumber=2856&Year=201 9&lnitiative=false.",
    "Page_36": "36 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nSECTION 3. EMERGING REGULATORY TOOLS AND\nENFORCEMENT MECHANISMS\n\nWhat are the different types of “bans” and moratoria that have been\npassed in the US over the last few years?\n\nHow can moratoria conditions be strengthened to ensure that eventual\nlegislative or deliberative processes are robust?\n\nHow will bans and moratoria on government use impact the private\ndevelopment and production of biometric systems?\n\nOver the past few years, a wave of municipal legislation has sought to ban government\nuse of facial recognition in the US, and some states have also proposed similar bills. Many\nof these bans focus on law enforcement use. While some large tech companies have\ncome out in favor of regulation, they have consistently pushed back against bans, often\nfavoring much less stringent approaches.\n\nThe term “moratorium” is shorthand for a range of regulatory interventions with varying\nconditions for when the restrictions would be lifted—from straightforward time-bound\ngoals for drafting and authorizing legislation to the establishment of deliberative,\nconsultative processes. Some moratorium bills prescribe specific conditions to ensure\nthe quality of the legislation and meaningful community participation in any deliberative\nprocess.\n\nMany cities and states in the US have recently introduced legislation that bans government use of\nfacial recognition, with a primary focus on law enforcement use. These legislative interventions\nhave played an outsize role in shaping the regulatory landscape by introducing a complete\nprohibition as a regulatory option against which other, less strict interventions will be compared.\nAs Jameson Spivack and Clare Garvie point out in their chapter, advocates have been critical\n\nof weaker regulatory bills for “using up available political capital’ and potentially undercutting\ndemands for bans in the future.®°\n\n84 In their contribution to this compendium, Jameson Spivack and Clare Garvie track this legislative activity, noting that “[als of July 2020, the following\nmunicipalities had banned face recognition: Alameda, California; Berkeley, California; Boston, Massachusetts; Brookline, Massachusetts; Cambridge,\nMassachusetts; Easthampton, Massachusetts; Northampton, Massachusetts; Oakland, California; San Francisco, California; and Somerville,\nMassachusetts. A number of states proposed bans on face recognition during the 2019-2020 legislative session: Nebraska, New Hampshire, New\nYork, and Vermont.”\n\n85 See Spivack and Garvie, “A Taxonomy of Legislative Approaches to Face Recognition in the United States,” in this compendium",
    "Page_37": "Amba Kak | The State of Play and Open Questions for the Future | 37\n\nSome of the largest technology companies that develop and sell these systems to law\nenforcement have been deeply engaged in these legislative processes, often publicly championing\nthe need for some “regulation” but simultaneously lobbying against moratoria and bans. For\nexample, Microsoft celebrated Washington State’s SB 6280 (“Finally, progress on regulating facial\nrecognition,’ Brad Smith, the company's general counsel, announced), only to face questions and\ncriticisms about their involvement in pushing through a law that was considered weak by many\norganizations, and that effectively undercut a potential ban on government use.®°\n\nMoratoria and bans are often used interchangeably, yet Spivack and Garvie argue that this\nshorthand conceals a wide spectrum of regulatory interventions. Moratoria, in particular, contain\na range of approaches that vary widely in terms of strictness and the conditions for lifting\nrestrictions. While some moratoria stop all use of face recognition for a predetermined time,\nthere is a risk that the legislature fails to act before the period is over and facial recognition use\nrecommences without any further legislative intervention. On the other hand, directive moratoria\nban the use of facial recognition until a law is passed and/or a statutory body (e.g, a task force or\ncommittee) is formed to submit recommendations for what to include in the law.\n\nMoratoria can work to fast-track a deliberative or legislative process where one might not\notherwise have been possible. While this is welcome, it is also eventually susceptible to the\nvested public and private interests that will push for weak or no legislation. There is a risk that a\ntask force created by these laws “may not be representative of affected communities; may lack\nauthority; or may be inadequately funded.”®? Some moratoria do more to prevent weak regulation\nthan others. A 2019 Massachusetts law sets minimum requirements for what future legislation\nshould achieve, including data privacy safeguards, auditing requirements, and protection for civil\nliberties. Similarly, the recently passed Washington State law specifies that the legislative task\nforce be comprised of “advocacy organizations that represent consumers or protected classes\nof communities historically impacted by surveillance technologies including, but not limited\n\nto, African American, Hispanic American, Native American, and Asian American communities,\nreligious minorities, protest and activist groups, and other vulnerable communities.”®\n\n86 See Dave Gershgorn, “A Microsoft Employee Literally Wrote Washington's Facial Recognition Law,’ OneZero, Medium, April 3, 2020, https://onezero.\nmedium.com/a-microsoft-employee-literally-wrote-washingtons-facial-recognition-legislation-aab950396927; and see Lee, “We Need a Face\nSurveillance Moratorium.”\n\n87 See Spivack and Garvie, “A Taxonomy of Legislative Approaches’; see also Rashida Richardson, ed., “Confronting Black Boxes: A Shadow Report of\nthe New York City Automated Decision System Task Force,’ Al Now Institute, December 2019, https://ainowinstitute.org/ads-shadowreport-2019.\nhtml.\n\n88 Section 10, Washington Senate Bill 6280, http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20Legislature/6280-S.\nPL.pdf?q=20200331083729",
    "Page_38": "38 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nWhat regulatory tools can be used to create public transparency\naround the development, purchase, and use of biometric recognition\ntools?\n\nTransparency around the development, purchase, and use of biometric recognition tools\nremains a key barrier to creating public awareness and enforcing existing regulation.\nRecent advocacy demands include mandatory impact assessments, public notice\ncomment periods, and publicly accessible registries of vendors and uses.\n\nEnforcing existing regulations has been a challenge in part because the development, purchase,\nand use of biometric tools is often shrouded in secrecy, driven by private firms that have no duty\nto reveal such “proprietary information.” Once these tools are built, the purchase and subsequent\n\nimplementation\nare often delibe'\ninvestigations h\nbiometric recog\nthe Clearview A\n\nby government, particularly law enforcement agencies, proceeds in ways that\nrately hidden from the public. Yet as scrutiny of Clearview Al and subsequent\n\nave made clear, there are hundreds of globally distributed vendors selling\n\nnition technology without people's knowledge or explicit consent. It was only when\n| story broke that lawsuits were filed under Illinois BIPA, prompting quick action\n\nfrom the company that had violated informed consent requirements when scraping millions of\n\nface images of\n\nIn recent years,\nas early in the p\n\nthe web.®?\n\nprivacy advocates have demanded regulatory tools that ensure transparency\nrocess as possible. Many of these policies target government use to ensure\n\nthat there is public notice and consultation before these tools are acquired and implemented.\n\nFor example, in\nprotest against\n\nJune 2020, after years of civil society advocacy, and in the context of sustained\nanti-Black police brutality, New York City passed The POST Act, a law that\n\nwould require the New York Police Department (NYPD) to issue a surveillance impact and use\npolicy about any surveillance technology in use (including biometric recognition tech).°° This\nassessment would include information about capabilities, processes and guidelines, and any\n\nsafeguards and\n\nsecurity measures in place. In the EU, advocacy organizations like Access\n\nNow and Algorithm Watch have called for a mandatory disclosure scheme for all Al systems\n\n \n\nused in the pub\nassessment.*!\n\nic sector, in conjunction with a mandatory human rights or algorithmic impact\n\nAdvocates have also demanded that regulation should ensure that external researchers and\nauditors have access to algorithmic systems in order to understand their workings, as well as the\n\ndesign choices\nengage the pub\nsoftware toolch\n\n89 Even these faced t\ndataset due to the\n90\n\n2020, https://www.\n\n91\n\nand incentives that informed their development and commercialization, and to\nlic and impacted communities in the process. Meaningful access includes making\nains and APIs open to auditing by third parties.\n\nhe barrier of establishing legal standing because it was difficult to confirm that an Illinois resident was in fact part of Clearview's\nlack of publicly available information. See ACLU v. Clearview Al, https://www.aclu.org/cases/aclu-v-clearview-ai\n\nThe surveillance impact and use policy would first be released in draft form for review by the public. See STOP Spying, POST Act, signed July 7,\n\nstopspying.org/post-act.\n\nAccess Now, “Access Now's Submission to the Consultation on the ‘White Paper on Artificial Intelligence—a European Approach to Excellence and\n\nTrust,’ May 2020, https://www.accessnow.org/cms/assets/uploads/2020/05/EU-white-paper-consultation AccessNow_May2020.pdf.",
    "Page_39": "Amba Kak | The State of Play and Open Questions for the Future | 39\n\nWhile advocates continue to push for more transparency, some laws have already enacted certain\nchecks and balances. The GDPR currently has provisions for data-protection impact assessments\n(DPIA) and “privacy by design” assessments that kick in when there is any “large-scale” processing\nof biometric data and also in cases of surveillance in publicly accessible spaces. In theory, these\noffer a robust assessment of the rights implications of the use of these systems, including\nfundamental questions about necessity and proportionality. However, as Els Kindt notes in her\nchapter, DPIAs have been challenging to implement in practice, with wide variations across\ndifferent member countries of the EU. Moreover, the predominant focus on data-protection\nconcerns can leave out inquiries about accuracy or discriminatory impact. Recent proposals\naround algorithmic impact assessments (AIAs) are structured to include this broader range of\nconcerns and ensure the participation of directly impacted communities in the risk-identification\nprocess.°\n\nWhile transparency and accountability measures have gained momentum, procurement contracts\nwith third-party vendors can inhibit the government's ability to comply.°? Government procurement\nof biometric and other forms of Al systems is often confidential due to trade secrecy or other\nintellectual property claims. When challenged, governments have denied any knowledge or ability\nto explain and remedy the problems created by these systems. Recent advocacy by civil society\norganizations and certain city governments in Europe focuses on including standard contractual\nclauses in these contracts that include waivers to trade secrecy, non-disclosure agreements, or\nother confidentiality clauses, as well as terms that ensure the process of procurement involves\nopen bidding and public notice.%*\n\nWhat role can community-led advocacy play in shaping the priorities\nand impact of regulation?\n\nCommunity advocacy to regulate biometrics is growing, playing a crucial role in surfacing\nevidence of harm, and shaping the rights and protections that policy interventions\neventually offer.\n\nAdvocacy and mobilization against the use of biometric systems have taken many forms. While\ntraditional digital rights or privacy groups remain active, over the past few years, directly impacted\ncommunities have also organized to push back against these systems based on their lived\nexperiences of harm.\n\n92 See Al Now's detailed AIA framework that public agencies can draw from when implementing AlAs: Dillon Reisman et al., “Algorithmic Impact\nAssessments: A Practical Framework for Public Agency Accountability,’ Al Now Institute, April 2018, https://ainowinstitute.org/aiareport2018.pdf.\nThe Canadian government's Algorithmic Impact Assessment tool is also a useful template for regulatory agencies; see Government of Canada,\nAIA, 2019, https://canada-ca.github.io/digital-playbook-guide-numerique/views-vues/automated-decision-automatise/en/algorithmic-impact-\nassessment.html. ICO's draft auditing framework for Al systems also has helpful guidance on how to document risks, manage inevitable trade-offs,\nand increase reflexivity at every stage of ADS procurement or development.\n\n93 See Houston Federation of Teachers v. Houston Independent School District and Ark. Dep't of Human Servs. v. Ledgerwood cases in Richardson,\nSchultz, and Southerland, “Litigating Algorithms 2019 US Report.”\n\n94 AI Now Institute, City of Amsterdam, City of Helsinki, Mozilla, and Nesta, “Using Procurement Instruments to Ensure Trustworthy Al,” June 15, 2020,\nhttps://foundation.mozilla.org/en/blog/using-procurement-instruments-ensure-trustworthy-ai/,",
    "Page_40": "40 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nIn India, a coalition of privacy groups and grassroots welfare activists formed to publicly protest\nand legally challenge the Aadhaar biometric ID project.°° Against the broad claims of efficiency\nby the government, the coalition surfaced specific examples of exclusion due to the technical\nand bureaucratic failures of the system. In their chapter, Stefanie Coyle and Rashida Richardson\nrecount the community-driven advocacy in Lockport, New York, where a group of parents\norganized against the school district’s decision to purchase and deploy facial recognition in\nschools. Eventually, Coyle and Richardson note, “[plarents shifted the discourse from debating\nwhether the biometric surveillance system was necessary to focus on the real harms posed to\nstudents if the school district decided to move forward.” As a result of that advocacy, the New\nYork State Senate introduced a moratorium bill that “mirrors the concerns raised by residents in\nthe community and advocates across the state and country.”\n\nLarge-scale biometric projects are often promoted in terms of lofty claims about security,\naccuracy, and efficiency. Community advocacy, particularly on the part of those directly impacted\nby these systems, has been critical in surfacing key questions like: Efficiency for whom? (In)\nsecurity for whom? Those required to live under biometric surveillance possess an expertise that\ncannot be gained by examining these systems at a technical or policy level. There is no way to\nguarantee the just use of these technologies without centering the experiences of those affected\nby their use. Recent attempts demonstrate how community interventions can be structured, for\nexample, the “Citizen Biometric Councils” run by the Ada Lovelace Institute in the UK,°° and the\nNew York City ADS Task Force “Shadow Report” prepared by a civil society coalition with detailed\nrecommendations to ensure community engagement is meaningful and equitable.°” Ultimately,\nthis underscores the importance of community deliberation to the processes that decide whether\nthese systems are used, but also to the kinds of rights and protections that policy interventions\neventually offer.\n\n95 See Rethink Aadhaar, https://rethinkaadhaar.in/,\n\n96 See Ada Lovelace Institute, “Citizen's Biometric Council\" https://www.adalovelaceinstitute.org/our-work/identities-liberties/citizens-biometrics-\ncouncil/\n\n97 Rashida Richardson, ed., “Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force”, Al Now\nInstitute, 2019, https://ainowinstitute.org/ads-shadowreport-2019.html.",
    "Page_41": "Amba Kak | The State of Play and Open Questions for the Future | 41",
    "Page_42": "42 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nTIMELINE OF LEGAL DEVELOPMENTS\n\nThis timeline tracks the key legal and regulatory developments analyzed in this compendium. The\nspecific chapters where they are discussed are noted below.\n\nOctober 2008 April 2018\n\nUnited States European Union\nIllinois Biometric Information Privacy Act (BIPA) ‘ . , aoe\nenacted (See Chapter 8) f General Data Protection Regulation (provisions on\n\n‘.. biometric data) enacted (See Chapter 4)\n\nData Protection Law Enforcement Directive enacted\n\nMarch 2016 (See Chapter 4)\n\nIndia\n> Aadhaar Act enacted (See Chapter 3) September 2018\n\nIndia\n+ Indian Supreme Court restricts private use of\n\nAadhaar Biometric ID system (See Chapter 3)\n\n \n\n2016 2017 2018\n\nApril 2019 August 2019\n\n_. Jamaica + International Committee of Red Cross\n\nJamaican Supreme Court rules biometric ID system = ICRC assembly adopts Biometrics Policy (See\nunconstitutional (See Chapter 7) Chapter 5)\n\nMay 2019\n. September 2019\nUnited States\n\nSan Francisco ban on government use of facial United States\nrecognition technology passed (See Chapter 7) California Body Camera Accountability Act\n(A.B 1215) (moratorium on existing use of face\nJune 2019 recognition on body-worn cameras till 2023) passed\nUnited States (See Chapter 7)\n\nSomerville, MA ban on government use of facial 5 United Kingdom\nrecognition technology (See Chapter 7) 3 UK High Court finds Live Facial Recognition\npermissible, rules out need for new authorizing\nJuly 2019 legislation (See Chapter 6)\n\nUnited States United States\nOakland, CA ban on government use of facial Justice in Forensic Algorithms Act of 2019 (HR\nrecognition technology (See Chapter 7) 4368) introduced (See Chapter 1)\n\nAustralia\nIdentity Service Matching Bill introduced (See\nChapter 2)\n\nKenya\nHuduma Bill (legal authorization for NMIMS project)\nintroduced (See Chapter 1)",
    "Page_43": "October 2019\n\ni\n”~\n\n—\nt\n\nUnited States\nNo Biometric Barriers to Housing Act (S 2689)\nintroduced (See Chapter 1)\n\nAustralia\nIdentity Service Matching Bill rejected by Australian\nParliament (See Chapter 2)\n\nUnited States\nBerkeley, CA ban on government use of facial\nrecognition technology passed (See Chapter 7)\n\nArgentina\nConstitutional challenge to Buenos Aires Live Facial\nRecognition project (See Chapter 1)\n\nAmba Kak | The State of Play and Open Questions for the Future | 43\n\nNovember 2019\n\nUnited States\nThe Facial Recognition Technology Warrant Act of\n2019 (S 2878) introduced (See Chapter 1)\n\nDecember 2019\n\nUnited States\nNorthampton, MA ban on government use of facial\nrecognition technology passed (See Chapter 7)\n\nUnited States\n\nAlameda, CA ban on government use of facial\nrecognition technology passed (See Chapter 7)\nUnited States\n\nBrookline, MA ban on government use of facial\nrecognition technology passed (See Chapter 7)\n\n \n\nJanuary 2020\n\nUnited States\nCambridge, MA ban on police use of facial\nrecognition technology passed (See Chapter 7)\n\nKenya\nKenyan High Court suspends NMIMS biometric ID\nproject (See Chapter 1)\n\nUnited States\nCalifornia Consumer Privacy Act (provisions on\nbiometric data) enacted (See Chapter 1)\n\nFebruary 2020\n\nUnited States\nSpringfield, MA moratorium on government use of\nfacial recognition technology passed (See Chapter 7)\n\nMarch 2020\n\nUnited States\n\nWashington SB 6280 (regulates government use\nof facial recognition technology) passed (See\nChapter 7)\n\nJune 2020\n\nUnited States\nFacial Recognition & Biometric Technologies\nMoratorium Bill S 4084 introduced (See Chapter 7)\n\nUnited States\n\nNew York Public Oversight of Surveillance\nTechnology (POST) Act (Int 0487-2018) passed (See\nChapter 1)\n\nJuly 2020\n\nUnited States\nNew York Senate Bill S5140B (regulating biometric\ntechnologies in school) passed (See Chapter 9)\n\nAugust 2020\n\nUnited States\nNational Biometric Privacy Act (S __) introduced\n(See Chapter 1)",
    "Page_44": "44 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nAustralian Identity-Matching\nServices Bill\n\nJake Goldenfein (Melbourne Law School)\nMonique Mann (Deakin University)\n\nto make facial recognition technology more widely available to civil and policing agencies.\n\nThese efforts, part of a long-term and continuing expansion of surveillance powers by the\nAustralian federal government, have culminated in a new biometric identity-information system.\nFederal authorities have argued that facial recognition technology is useful for law enforcement\nand preventing identity fraud, but to achieve those benefits, they have combined civil and criminal,\nas well as state and federal, identity systems into a powerful intelligence apparatus controlled by\na single government department: the Australian Department of Home Affairs.\n\nS ince 2017, the Australian federal government has pushed for political and legal changes\n\nHome Affairs was created in 2017 through a merger of the Department of Immigration and\nBorder Protection and the Australian Border Protection Service. As a result of the merger, Home\nAffairs assumed multiple policing and intelligence competencies from the Attorney General's\nDepartment (AGD), including those related to national security, immigration, organized crime,\ncybersecurity, and public safety policing. Home Affairs also took over control and operation of the\nnational identity-matching services, which included the one-to-one facial recognition verification\nsystem known as the “Face Verification Service” (FVS).'\n\n1 One-to-one verification means that an image is submitted along with a stated identity, and the system responds with a “yes” or a “no.” The purpose is\nto prevent identity fraud by ensuring an individual presenting to an agency is who they claim to be.",
    "Page_45": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 45\n\nThe Australian government has been developing the institutional, technical, and legal architecture\nfor facial recognition capabilities for several years,? culminating in the 2019 federal Identity-\nMatching Services Bill. The original bill was rejected, however, for a lack of privacy protection and\noversight, and is presently being redrafted. The new bill will likely increase parliamentary oversight\nof the system and the amount of necessary reporting, but will not challenge the fundamental\ninstitutional changes that are already underway, such as the aggregation of civil and criminal\nsystems, or increased control of state-level civic data within a federal intelligence system.\n\nAlthough governments have always had the function of identifying their citizens,* they have not\nalways linked those identities to intelligence dossiers or made them available to law enforcement\nagencies. Indeed, the intermingling of civil and criminal identity systems has been the concern of\nhuman rights jurisprudence for some time.° Biometrics are of particular concern to the linkage\n\nof criminal and civil systems, and surveillance more generally, because they act as a conduit\nbetween an individual's physical presence and digital databases, thus amplifying surveillance\ncapacities. By advancing a centralized identity matching system, Australia is pushing beyond the\nlimits of legitimate state function.\n\nBIOMETRICS DEVELOPMENT IN AUSTRALIA\n\nAustralia has collected biometric information, including images for facial recognition, since at\nleast 2007. This began with border-protection agencies collecting information from noncitizens,\nsuch as people caught fishing illegally in Australian waters, and eventually from visa applicants.\n\nIt has progressively expanded to include information collected from Australian citizens, both at\nthe border and through civic licensing agencies.® States have also used biometric systems for\nmatching against their police information holdings (i.e., mug shot databases) since at least 2009.’\n\nThe 2007 Intergovernmental Agreement to a National Identity Security Strategy® proposed the\ndevelopment of a national biometric interoperability framework,’ which was launched in 2012.'°\nPlans for a further national facial biometric matching “Capability” to enable cross-jurisdictional\nsharing of identity information, the precursor to the identity matching system operated by Home\nAffairs, were announced in 2014.\"\n\n2 See, e.g., Australian Government, Department of Home Affairs, “Agreement to a National Identity Security Strategy,” April 2007, https://www.\nhomeaffairs.gov.au/criminal-justice/files/inter-gov-agreement-national-identity-security-strategy.pdf.\n\n3 Identity-Matching Services Bill 2019 (Cth). The note (Cth) indicates that this is a commonwealth or federal bill. The Identity-Matching Services Bill\nwas first introduced in February 2018, but did not progress through parliament and lapsed in April 2019. It was reintroduced in July 2019.\n\n4 See, e.g., Markus Dirk Dubber, The Police Power: Patriarchy and the Foundations of American Government (New York: Columbia University Press,\n2005)\n\n5 See, e.g., Jake Goldenfein, Monitoring Laws: Profiling and Identity in the World State (Cambridge: Cambridge University Press, 2019).\nDean Wilson, “Australian Biometrics and Global Surveillance,’ International Criminal Justice Review 17, no. 3 (September 2007): 207-219.\n\n7 Parliament of Australia, “CrimTrac Overview 2009” (direct download, PDF), https://www.aph.gov.au/DocumentStore.ashx?id=dd60984f-33e2-4836-\n85a4-690052ca7914.\n\n8 Australian Government, Department of Home Affairs, “An Agreement to a National Identity Security Strategy,’ April 2007, https://www.homeaffairs.\ngov.au/criminal-justice/files/inter-gov-agreement-national-identity-security-strategy.pdf.\n\n9 Australian Government, Department of Home Affairs, “A National Biometric Interoperability Framework for Government in Australia,’ n.d., https://\nwww.homeaffairs.gov.au/criminal-justice/files/national-biometric-interoperability-framework-for-government-in-australia.pdf.\n\n10 Attorney-General's Department, National Identity Security Strategy 2012, Canberra, 2013.\n\n11 Law, Crime and Community Safety Council, Communique, COAG Meeting, Canberra, October 3, 2014, https://parlinfo.aph.gov.au/parlinfo/search/\ndisplay/display.w3p;query=|d:%22media/pressrel/3523779%22",
    "Page_46": "46 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThe one-to-one face verification system (FVS) that Home Affairs took over from the Attorney-\nGeneral's Department (AGD) began operating in 2016, but only included passport images held\n\nby the federal Department of Foreign Affairs and Trade (DFAT).'2 Given the uptake of driver’s\nlicenses in the general population and the ambition for a national system, the policy goal has long\nbeen to integrate state-controlled driver's license images into a general database for policing and\nintelligence.'® Efforts by federal entities to access driver's license images have been, however,\nfrustrated by state privacy laws, which prohibit providing federal agencies direct access to their\ndatabases.\" The result has been limited and complex arrangements for cross-jurisdictional\ninformation sharing. This began to change, however, with the 2017 Intergovernmental Agreement\non Identity Matching Services (IGA)''—the precursor to the Identity-Matching Services Bill-and\nthe corresponding formation of the Department of Home Affairs, with its very broad federal\npolicing and intelligence remit.\n\nCENTRALIZATION OF IDENTITY DATABASES\n\nIn 2017, the Australian states agreed multilaterally to enable federal access to their identity data\nunder the auspices of the IGA. Some states made explicit the value they saw in the system,\n\nwith the Queensland Minister for Police noting the value that one-to-many facial recognition\nwould contribute to enhanced security at the Commonwealth Games.'® Other states were more\nreluctant, raising the alarm about possible contravention of state-level human rights protections,\nand suggesting that there were inadequate protections for civil liberties.\"”\n\nNonetheless, the IGA established the framework for a data-sharing regime, gave immunity from\nstate-level privacy laws, and introduced new identity-matching services, including a one-to-many\nfacial identification service (FIS) to complement the FVS. Such systems are the primary facial\nrecognition tool used in policing in Australia. The system allows for law enforcement, national\nsecurity, and related entities at state and federal level to run queries through the technical\ninfrastructure of a host agency: originally the AGD, and then the Department of Home Affairs.\nImportantly, while the IGA introduced a technical architecture for information sharing, it left\ncontrol over identity databases with the states.'®\n\n12 See Allie Coyne, “Australia’s New Facial Verification System Goes Live,’ T News, November 16, 2016, https://www.itnews.com.au/news/\naustralias-new-facial-verification-system-goes-live-441484. That federal system was populated by passport photos, which in 2010-2011 covered\napproximately 48 percent of the Australian population (Department of Foreign Affairs and Trade (Cth), “Program 2.2: Passport Services,’ Annual\nReport 2010-2011, https://nla.gov.au/nla.obj-990174440/view?partid=nla.obj-99433421 9#page/n161/mode/1up) and presently covers about 57.9\npercent of the population (https://www.passports.gov.au/2019-passport-facts).\n\n13. Monique Mann and Marcus Smith, “Automated Facial Recognition Technology: Recent Developments and Strengthening Oversight,” UNSW Law\nJournal 40, no. 1 (2017): 121-145.\n\n14 See, e.g., the Parliament of the Commonwealth of Australia, “Identity Matching Services Bill 2019, Explanatory Memorandum,’ describing Clause 19\nof the Bill. An exception is the NSW Roads and Maritime Services, which provides access to the Australian Security Intelligence Organisation (ASIO)\nand the Australian Federal Police (AFP) for the purposes of investigating terrorism offenses.\n\n15 Council of Australian Governments, “Intergovernmental Agreement on Identity Matching Services,’ October 5, 2017, https://www.coag.gov.au/sites/\ndefault/files/agreements/iga-identity-matching-services.pdf.\n\n16 Mark Ryan, “Queensland Leads Nation to Strengthen Security Measures,” Queensland Government, The Queensland Cabinet and Ministerial\nDirectory, March 7, 2018, http://statements.qld.gov.au/Statement/2018/3/7/queensland-leads-nation-to-strengthen-security-measures.\n\n17 See,e.g., Adam Cary, “Biometrically Opposed: Victoria Queries Peter Dutton over Facial Recognition Scheme,” Sydney Morning Herald, May 2, 2018,\nhttps://www.smh.com.au/politics/federal/biometrically-opposed-victoria-queries-peter-dutton-over-facial-recognition-scheme-20180502-p4zcvs\nhtml.\n\n18 Note that the IGA architecture replicates, and was perhaps inspired by, the FBI's Next Generation Identity system, launched in 2014. See FBI, Next\nGeneration Identification (NGI), https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi.",
    "Page_47": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 47\n\nA few months later, the government introduced the Identity-Matching Services Bill, which\nostensibly legislated for the IGA. In reality, however, the bill went significantly further, shifting the\nsystem from one that facilitated information sharing into one that enabled the aggregation and\ncentralization of identity information in the Department of Home Affairs.\n\nThis increased centralization is in no way integral to satisfying the objectives of the system, at\nleast as publicly stated. The bill’s explanatory memorandum, for instance, outlined the primary\ngoal as preventing fraud and identity theft (described as an enabler of organized crime and\nterrorism), but not to build an intelligence apparatus.'? Despite the limited technical capacity\nnecessary to achieve that stated objective, the system specified in the bill would fold state-level\ntransport authorities’ data and images into the data-intensive apparatuses of federal security and\nintelligence agencies.\n\nThe centralizing dimensions of the system architecture become apparent when looking closely at\nthe differences between the IGA and the bill. Beyond addressing identity fraud, we suggest these\nchanges reveal the true underlying political rationalities and motivations for establishing this\nnational facial recognition system as a radical shift in identity data governance arrangements.\n\nLEGAL CONCENTRATION OF POWER\n\nThe Identity-Matching Services Bill sought to establish Home Affairs as the “hub” through\n\nwhich government identity-verification and law enforcement suspect-identification requests are\nprocessed, establishing Home Affairs as the central point of information processing across the\n\npublic sector and for law enforcement agencies. But there were meaningful departures from the\nsystem described in the bill and the 2017 IGA.\n\nThe IGA outlined two technical architectures: 1) The National Driver License Facial Recognition\nSolution (FRS), a biometric identity image database; and 2) the “interoperability hub,’ a\ncommunications system for processing and routing data access requests from agencies around\nAustralia.\n\nIn the IGA, the FRS was described as a federated database system, in which state-level data\nwould be partitioned, and state agencies could control the conditions of access. Databases\nwould be linked through Home Affairs, which would operate the facial recognition technology that\nperforms identity matching. The FRS was described as retaining only biometric identity templates\nand no other identity or personal data. The IGA stipulated that the host agency (initially the AGD,\nbut subsequently Home Affairs) could not view, modify, or update information in partitioned\nfederated databases containing state-level information. However, the bill only prescribed that\nHome Affairs could not modify or update that data; in other words, it could still view it.2° In fact,\n\n19 The Australian Government IDMatch home page, for example, promotes “Identity Matching Services that help verify and protect your identity”\n(https://www.idmatch.gov.au). See also the Parliament of the Commonwealth of Australia, “Identity Matching Services Bill 2019, Explanatory\nMemorandum,” https://parlinfo.aph.gov.au/parllnfo/download/legislation/ems/r6387_ems_f8e7bb62-e2bd-420b-8597-8881422b4b8f/upload_\npdf/713695.pdf.fileType=application%2Fpdf.\n\n20 Sup. 11. See IGA clause 6.16.",
    "Page_48": "48 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nthe legislation clarified that Home Affairs could collect, effectively without limit, information\nflowing through the systems for satisfaction of its “community safety” purposes, which include\nlaw enforcement, national security, community safety, protective security, and road safety, along\nwith identity verification. The bill effectively vested control over the databases of driver's license\nimages squarely within Home Affairs, and enabled unrestrained collection of information.\n\nWith respect to the “interoperability hub,” the IGA described it as a “router” through which agencies\naround the country could request and transmit information to one another. That is, it could be\nused for “relaying electronic communications between bodies and persons for the purposes of\nrequesting and providing identity-matching services.” Rather than simply routing information\n\nfrom place to place, however, the bill enabled Home Affairs to collect data flowing through the\nhub whenever an agency used an identification, verification, or information sharing service, both\nfor the sake of operating that database,?' as well as for its identity and community protection\nactivities.** The bill thus enhanced the legal capacity of Home Affairs from an infrastructure\nprovider into a data aggregator.\n\nOther important elements of the bill gave greater power than envisaged to the Department of\nHome Affairs. For instance, the bill enabled the Minister for Home Affairs to expand the powers\nunder the regime without parliamentary oversight. Furthermore, the identity information that\ncould be collected through those systems was far broader than anticipated by the IGA,”? including\ninformation held by agencies that is about or associated with the identity document.\n\nIt is difficult to identify a single rationale that may have motivated the changes between the\n\nIGA and the Identity-Matching Services Bill. New technological affordances associated with\nfacial recognition may have animated interest in developing a comprehensive national system,\nespecially considering international trends. The institutional culture and political power of the\nDepartment of Home Affairs may also have made centralization and the use of civil documents\nin intelligence investigation more feasible. Indeed, its participation in forms of intelligence work\nand political policing connects it to a policing tradition that has always involved information\naggregation, not necessarily in line with traditional liberal political limits.** That expansion of\npolitical and technological power is also consistent with Home Affairs’ broad portfolio.\n\nAustralia lacks enforceable human rights protections at the federal level (though some states\nhave their own independent human rights protections), which raises a number of issues and\nconcerns with the centralization of data and surveillance capabilities within federal agencies.\nUnder the Australian Constitution,?° crime control and criminal justice are a competency of\nthe states, not the federal government. Policing agencies are historically restricted to identity\nmatching against data in local policing information systems (such as mug shots), which\n\n21 Sup. 3. See § 17 (2)\n\n22 Sup. 3. See § 17(2)(b); note that the purposes for which Home Affairs can collect data flowing through the interoperability hub is not clear in the\nlegislation because it is split over two provisions. However, it has been interpreted to mean collection is permitted for the broader range of purposes\n(Bills Digest).\n\n23 Inthe Bill, § 5; in the IGA, clause 3.1\n\n24 See, e.g., Bernard Porter, The Origins of the Vigilante State: The London Metropolitan Police Special Branch before the First World War (London\nWeidenfeld and Nicolson, 1987)\n\n25 Commonwealth of Australia Constitution Act 1900 (Cth).",
    "Page_49": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 49\n\nhave comprehensive rules and limits on retention.%° As Home Affairs moves to aggregate and\ncentralize biometric data, it is violating privacy norms by way of “scope creep,’ i.e., generating data\nfor one government purpose (e.g., licensing drivers), and using it for another (e.g., policing or other\npunitive applications).\n\nPJCIS REJECTS THE BILL\n\nUltimately, the Identity-Matching Services Bill did not pass parliamentary scrutiny and was\nrejected by the Parliamentary Joint Committee on Intelligence and Security (PUCIS). But the\nspecific issues that led to its rejection are unlikely to halt the system's development. In fact,\nthe rejection can be interpreted as an endorsement of the general system and the resultant\ncentralization, subject to privacy and accountability “tweaking.”\n\nWhen the bill reached the PJCIS, it was rejected largely due to concerns that it would grant too\nmuch executive authority to the Department of Home Affairs, meaning that the Minister for Home\nAffairs could change rules without legislative oversight.2”? The PUCIS also echoed the fears of\nprivacy advocates around the possibility of a real-time, facial recognition-powered CCTV mass\nsurveillance system which could end anonymity in public and stifle political action like protesting.\nThe report also noted accountability issues like the absence of judicial warrant requirements, and\nthe lack of a dedicated biometric oversight body (both of which exist in the United Kingdom).\n\nOn a broader level, the PUCIS expressed anxieties around the system not being proportionate\nto the issues it purported to solve, or sufficiently privacy-protective. But those concerns were\nconnected to possible problematic “uses” of the system, not the broader structural issues of\ndata centralization or the aggregation of civil and criminal identity databases. Instead, there\nwas general approval that this type of data sharing would occur subject to a binding legislative\nframework rather than through creative interpretations of law enforcement and security\nexemptions to privacy laws.”\n\n26 Jake Goldenfein, “Police Photography and Privacy: Identity, Stigma, and Reasonable Expectation,’ University of New South Wales Law Journal 36, no.\n1 (2013): 256-279\n\n27 See Parliament of Australia, “Review of Identity-Matching Services Bill 2019 and the Australian Passports Amendment (Identity-Matching Services)\nBill 2019,\" n.d., https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/Intelligence_and_Security/Identity-Matching2019\n\n28 See the “Parliamentary Joint Committee on Intelligence and Security” (https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/\nIntelligence_and_Security), the “Advisory Report on the Identity-Matching Services Bill 2019 and the Australian Passport Amendment (Identity-\nmatching Services) Bill 2019” (https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/Intelligence_and_Security/Identity-\nMatching2019/Report), and “A Workable Identity-Matching Regime” (https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/\nIntelligence_and_Security/Identity-Matching2019/Report/section?id=committees %2Freportjnt%2F024343%2F 27805). Specifically, the PUCIS\nargued that the Identity-Matching Bill is designed to “permit all levels of government and the private sector unprecedented access to Australian\ncitizens’ private biometric information in the form of a facial image” and that “given the significance of these measures, the Committee considers\nit preferable that privacy oversight and safeguards are established and set out in this enabling legislation rather than only being provided in\nsupplementary agreements or arrangements.”",
    "Page_50": "50 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThe PJCIS accordingly recommended redrafting the bill to make its function and purpose\n\nclearer to the ordinary reader, reduce Ministerial rule-making power, fund a biometric oversight\ncommission, and require more comprehensive reporting.”? The PJCIS did not, however, completely\nreject the bill, the use of facial recognition technology, or the new data governance arrangements\nthat would power the system.\n\nFUTILITY OF AUSTRALIAN REGULATORY OVERSIGHT\n\nThe Identity-Matching Services Bill is presently being redrafted, with the new text yet to be\nreleased. Nonetheless, the states continue to upload identity images to the system in anticipation\nof the law passing and the system developing along similar lines. One reason political review\n\nhas failed to meaningfully challenge the general structure of the identity matching and facial\nrecognition system is that the debate, especially as expressed in the PUCIS report, has taken up a\n“orivacy versus security” framing. International human rights law requires that state surveillance\nbe “reasonable” and “proportionate,” and this language clearly influenced the PCJIS.\n\nUnder a human rights framework, to legitimately limit fundamental freedoms like privacy, a\nsurveillance intervention must be directly related to, and the least restrictive measure for, the\n“necessary” purpose pursued. A true proportionality analysis might question whether such\ndramatic data governance rearrangements are necessary to address the stated purpose of\nidentity fraud. In reality, however, this framing is operationalized in ways that enable continuing\nexpansion of surveillance systems, especially in nations like Australia, where it is not backed up by\nactionable protections.\n\nWhen privacy is pitched against security, the benefits of centralization and surveillance\ntechnology to purposes like identity fraud are taken as given, and the question becomes: Which\ncivil liberties are we willing to curtail or limit in exchange? Blanket data sharing for policing and\nintelligence agencies is thus readily accepted and normalized as a necessary response to crime\nand insecurity, subject to privacy balancing intended to curtail its most abusive and authoritarian\ndimensions.*° That framing fails to address the reality that the system fundamentally eliminates\nthe need for the largest policing and intelligence apparatus in the country to justify its access\n\nto personal data that was previously distributed to the states. This goes beyond agencies using\nbiometrics for their democratically constituted civic purposes (e.g., driver's licenses), and beyond\nthe stated intention of the bill (e.g., detecting identity fraud). By pushing this bill forward, Home\nAffairs is promoting facial recognition technology as a necessary solution to identity crime, while\nsidelining concerns around the institutional and data governance rearrangements that it claims\nare necessary for its introduction.\n\n29 It should be noted that there are oversight bodies responsible for Commonwealth law enforcement agencies under the Law Enforcement Integrity\nCommissioner Act 2006 (Cth) that established the Commonwealth Integrity Commissioner and the Australian Commission for Law Enforcement\nIntegrity, which has jurisdiction over all Commonwealth law enforcement agencies (including those responsible for the facial biometrics matching\nsystem).\n\n30 See, for example, Monique Mann et al., “The limits of (Digital) Constitutionalism? Exploring the Privacy-Security (Im)balance in Australia,”\nInternational Communication Gazette 80, no. 4 (2018), 369-384.",
    "Page_51": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 51\n\nFrom this position, it becomes impossible to challenge the construction of the surveillance\nsystem, or to fight the technical or institutional architecture, in any meaningful way. The\ninstitutional momentum also makes resisting significant data governance rearrangements\ndifficult. One recent positive development, however, has been the Australian Human Rights\nCommissioner calling for a moratorium on the use of facial recognition technology as part of\nthe Technology and Human Rights Project, which mirrors some international trends.*' However,\nit is uncertain what impact this will have on the design, development, and eventual deployment\nof facial recognition technology in Australia, especially considering the extent to which the\ninfrastructure is already in place.\n\nFinally, technologies like Clearview Al, which has aggregated billions of identified images\n\nfrom the public web, complicate how to parse these developments.*? Private providers, not\nconstrained in the same way, can undermine relevant privacy protections or political processes\nby secretly selling surveillance services to government, while using their own privately operated\ninfrastructure. When governments procure those services, they bypass whatever regulatory or\nfinancial obstacles might have prevented or limited those developments by the state itself. To that\nend, it is at least admirable that the Australian identity matching regime will be implemented in\nlaw, subject to democratic process and parliamentary oversight. Nonetheless, even when that is\nthe case, the purposes expressed to justify new facial recognition implementations for the sake\nof those democratic processes appear not to tell the full story. It remains imperative to identify\nand address the institutional realignments and data governance reconfigurations connected to\ntechnologies like facial recognition and not be distracted by any single new surveillance capacity.\n\n31 Australian Human Rights Commission, “Human Rights and Technology Discussion Paper,” December 2019, https://www.humanrights.gov.au/our-\nwork/rights-and-freedoms/publications/human-rights-and-technology-discussion-paper-2019\n\n32. “Australian police agencies initially denied they were using the service. The denial held until a list of Clearview Al's customers was stolen and\ndisseminated, revealing users from the Australian Federal Police as well as the state police in Queensland, Victoria and South Australia.” See Jake\nGoldenfein, “Australian Police Are Using the Clearview Al Facial Recognition Technology with No Accountability,’ The Conversation, March 4, 2020,\nhttps://theconversation.com/australian-police-are-using-the-clearview-ai-facial-recognition-system-with-no-accountability-132667.",
    "Page_52": "52 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThe Economy (and Regulatory\nPractice) That Biometrics\nInspires: A Study of the\nAadhaar Project\n\nNayantara Ranganathan (lawyer and independent researcher, India)\n\nhe Government of India launched the Aadhaar biometric identity project in 2009 with\n\nthe aim of providing identification for all residents.’ The project called for a centralized\n\ndatabase that would store biometric information (fingerprints, iris scans, and photographs)\nfor every individual resident in India, indexed alongside their demographic information and a\nunique twelve-digit “Aadhaar” number. India’s now-dissolved Planning Commission formed the\nUnique Identification Authority of India (UIDAI) to plan the project, as well as implement and\nperform regulatory functions. The scale and ambitions of the project are matched only by the\nlong and rich history of resistance to it. Economists, technologists, people’s movements, and\nconcerned citizens have questioned the amplified surveillance dangers, indignities from exclusion\ndue to failures in biometric identification systems, and lack of institutional accountability.? The\nproject proceeded without any legal framework to govern it for seven years after its inception\n(government use of data in India is still not governed by any dedicated law).\n\n1 Government of India, Planning Commission, “Notification No. A-43011/02/2009-Admn.|,’ January 28, 2009, https://uidai.gov.in/images/\nnotification_28_jan_2009.pdf (last accessed on July 15, 2020). UIDAI was set up under the chairmanship of one of the foremost industry leaders of\nthe Indian IT sector, Nandan Nilekani\n\n2 Ibid.\n\n3 For some critiques by technologists, see, for example, Rethink Aadhaar, https://rethinkaadhaar.in; and the Medium site Kaarana, https://medium\ncom/karana. For a compilation of dissenting notes by various authors, see Reetika Khera, ed., Dissent on Aadhaar: Big Data Meets Big Brother\n(Hyderabad: Orient Blackswan, 2019).",
    "Page_53": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 53\n\nResponding to the glaring lack of accountability raised by public advocacy and litigation, the\nIndian government passed the Aadhaar (Targeted Delivery of Financial and Other Subsidies,\nBenefits and Services) Act* in 2016, with negligible public or parliamentary debate.° While the\n\nlaw provided some procedural safeguards around biometric data security and consent, the law\nshould be understood as a part of a broader institutional and economic project to instrumentalize\nbiometric information in the service of the data economy. This essay explores the continuing\nlegal and regulatory complicity in constructing data as a resource for value extraction, and how\nregulatory practice mimics the logics and cultures of the technologies it seeks to regulate.\n\nMAKING DATA MARKET-READY\n\nThe law goes to great lengths to sustain the idea of biometric data as signifying truth, supporting\nand maintaining an infrastructure that is foundational for the data economy.\n\nThe Truth about Biometrics\n\nEarly planning documents of the Aadhaar project refer to biometrics as a fundamental identity,\nwhile older forms of identification based on demographic information are considered “surrogates\nof identity.”° Yet biometric information is also a class of media, offering representations of bodily\nattributes captured at a particular moment in time under specific material conditions, and of\n\nno greater epistemic caliber. However, when coupled with the moral timbre of truth, biometric\ninformation can perform the important function of instituting people as data points within\ndatabases. This allows datafication of flows like cash exchanges or road traffic to be easily\nmapped onto signifiers of “real” people within databases, making these newly captured and latent\ndataflows more meaningful and profitable.\n\nIn the Aadhaar project, high-resolution photographs of people's irises and fingerprints were\ncollected at the time of enrollment into the database, along with standard photographs of faces.’\nAn equivalence between media artifacts captured about a person and their true identity might\n\n4 Hereinafter called the Aadhaar Act, or simply Aadhaar. For the text of the act, see the Ministry of Law and Justice, “The Aadhaar (Targeted Delivery\nof Financial and Other Subsidies, Benefits and Services) Act,” 2016, March 26, 2016, https://uidai.gov.in/images/targeted_delivery_of_financial_and_\nother_subsidies_benefits_and_services_13072016.pdf.\n\n5 See Ujwala Uppaluri, “The Aadhaar Programme Violates Democratic Process and Constitutional Rights,’ Caravan, April 4, 2017, https://\ncaravanmagazine.in/vantage/aadhaar-violates-democratic-process-constitutional-rights. See also Software Freedom Law Center (SFLC), “How\nParliament Debated the Aardhaar Bill, 2016,\" March 19, 2016, https://sflc.in/how-parliament-debated-aadhaar-bill-2016.\n\n6 UIDAI, “Role of Biometric Technology in Aadhaar Enrollment,’ January 21, 2012, http://www.dematerialisedid.com/PDFs/role_of_biometric_\ntechnology_in_aadhaar_jan21_2012.pdf. See also UIDAI, “Basic Knowledge of UIDAI and Aadhaar, Module 1; March 16, 2015, https://uidai.gov.in/\nimages/training/module_1_basic_knowledge_of_uidai_and_aadhaar_16032015.pdf.\n\n7 UIDAI, “Aadhaar Enrollment Process,” https://uidai.gov.in/contact-support/have-any-question/296-faqs/enrolment-update/aadhaar-enrolment-\nprocess.html",
    "Page_54": "54 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nbe common in popular parlance, but with Aadhaar, such an equivalence was crystallized in law.®\nYet this equivalence is neither natural nor logical. The jump from the material facts of these\nrepresentational media to their revelatory quality is a tactical one that several actors in the data\neconomy are invested in maintaining. Aadhaar intends to act as both a unique and ubiquitous?\nsignifier, offering itself as part of an “identity layer” that may then be used as a foundation for the\ndatafication of realms like finance, taxation, healthcare, and education.'°\n\nGrooming Uniqueness as Truth\n\nFor practical as well as ethical reasons, the use of biometrics as a stand-in for unassailable\n\ntruth about people is suspect.'' But law and regulation have worked to prop up this fiction and\nattach market value to it, through the legally defined processes of “deduplication,” the mandatory\nupdating of biometrics information, and the reputational coupling of demographic and biometrics\ndata.\n\nDeduplication: At the time of enrollment in Aadhaar, all biometrics information is checked against\nevery other entry in the database.'* Deduplication is often seen as a best practice in biometrics\nenrollment, but its role in solidifying assumptions about the nature or suitability of biometrics for\npurposes of identification or authentication is not equally recognized. Deduplication confirms the\nuniqueness of each entry's biometric information, within the database and for the limited purpose\nof the database.\n\nUpdating biometrics information and technology: While uniqueness is architected through\ndeduplication, fidelity of the media at the time of enrollment to the biological attributes of\nindividuals cannot be sustained for reasons like fingerprints and irises changing over time,\n\nas well as several types of fraud.'? Rather than questioning the wisdom of using biometric\ninformation as a fundamental identity and an authentication key, the law uses minor fixes while\nstill equating biometric information with biological attributes. The law gives UIDAI powers to\n\n8 Erasure of the fact of mediation surfaces in the definition of “biometric information.” Notice that in the definition of “biometric information’ in Section\n2(g) of the Act, there is a slippage or equivalence between media (e.g., a photograph) and the subject (e.g., a fingerprint). In other words, there is a\nslippage and equivalence between biological attributes and their representation that is captured in the machines. Section 2(g) states that “biometric\ninformation means photograph, fingerprint, iris scan, or such other biological attributes of an individual as may be specified by regulations.” This\nerasure is again reiterated in the definition of “core biometric information’ in Section 2(j): “core biometric information’ means fingerprint, iris scan, or\nsuch other biological attribute of an individual as may be specified by regulation.” A definition not making this erasure might read as follows: “core\nbiometric information’ means fingerprint, iris scan, or such other representations of biological attributes of an individual as may be specified by\nregulation.”\n\n9 Usha Ramanathan, “Aadhaar—From Welfare to Profit,’ in Dissent on Aadhaar: Big Data Meets Big Brother, ed. Reetika Khera (Hyderabad: Orient\nBlackswan, 2019), 178\n\n10 See “Basic Knowledge of UIDAI and Aadhaar, Module 1,” https://uidai.gov.in/images/training/module_1_basic_knowledge_of_uidai_and_\naadhaar_16032015.pdf.\n\n11 Fora discussion of the issues surrounding use of biometrics as a stand-in for truth about people, e.g., with creating a “self-referential system’ that\nis unconcerned with the realities of aging, machine quality, and fraud, see Nishant Shah, “Identity and Identification: The Individual in the Time of\nNetworked Governance,’ Socio-Legal Review 11, no. 2 (2015): 22-40, http://docs.manupatra.in/newsline/articles/Upload/D47CF36C-C409-45BF-\n8AE6-659D7B6281FB.pdf; on the harms of treating bodies as data, see Anja Kovacs, “When Our Bodies Become Data, Where Does That Leave Us?,”\nDeep Dives, May 28, 2020, https://deepdives.in/when-our-bodies-become-data-where-does-that-leave-us-906674f6a969.\n\n12 — UIDAI, “Features of Aadhaar\" https://uidai.gov.in/my-aadhaar/about-your-aadhaar/features-of-aadhaar.html\n\n13. The only kind of fraud that biometrics protects against is the enrollment of the same person more than once.",
    "Page_55": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 55\n\nrequire Aadhaar holders to update their biometric information from time to time, at their own cost,\n“to ensure continued accuracy” and not, say, to correct the inevitable deterioration of fidelity of\nbiometric information.'* The law even anticipates, supports, and relies on ever-better biometrics\ntechnologies, bridging any imagined distance between a thing and its representation.'°\n\nLending truth to demographic data: Biometrics’ reputation of truth, and its resultant market\nvalue, are also transposed onto the corresponding demographic information (like name, gender,\naddress) and the unique twelve-digit Aadhaar number generated.'® However, such demographic\ndata does not benefit from the same heightened data-security protections,\" is unverified, and is\nunaudited.\n\nKEEPING DATA MARKET-READY\n\nAadhaar has been described as “a government programme run with the energy of a private sector\nstart-up,\"\"® and is emblematic of the close cooperation between private actors and UIDAI. As a\nresult of these close ties, regulation of the Aadhaar project enacts itself as cybernetic feedback\nloops that are constantly adapting to unfavorable changes, optimized toward keeping an\ninfrastructural building block of the data economy alive.\n\nAadhaar as a Building Block\n\nFrom the outset, the UIDAI envisioned Aadhaar as an identity “platform”: an infrastructure that\nwould provide authentication and verification services, and satisfy a necessary precondition for\nthe data economy to thrive.'® Indeed, the law emphasizes the importance of Aadhaar as a source\nof identification for the marginalized, and to enable efficient and targeted welfare delivery.°\n\n14 — See Section 6 of the Aadhaar Act (https://uidai.gov.in/images/targeted_delivery_of_financial_and_other_subsidies_benefits_and_services_13072016.\npdf). The Act places the responsibility of such updates with the Aadhaar number holder.\n\n15 The Aadhaar Act anticipates and privileges the proliferation of biometrics technologies by including an expansive definition of biometrics; see\nSection 2(g) of the Act. Additionally, the Act also reserves the power of UIDAI to promote research and development for advancement in biometrics\nand related areas; see Section 23(2)(q) of the Act.\n\n16 The Aadhaar number is used for various purposes, including bank verification, despite the low quality of data and its unverified and unaudited\nnature.\n\n17. On the authentication of the Aadhaar number, see Section 8 of the Aadhaar Act; on the restriction on sharing information, see Section 29; on\nbiometric information deemed to be sensitive personal information, see Section 30. Conversely, where concerns around the Aadhaar project have\narisen, they are allayed by a false sense of safety provided for the biometric information, while the associated demographic information fills any\ngaps created by the withdrawal of biometrics.\n\n18 Viral Shah, “Like Narendra Modi, Nandan Nilekani Too Understands the Transformative Power of Technology,’ India Today, September 16, 2017,\nhttps://www.indiatoday.in/magazine/news-makers/story/20170925-pm-narendra-modi-nandan-nilekani-aadhaar-ekyc-gst-artificial-intelligen\nce-1044702-201 7-09-16.\n\n19 The Biometrics Standard Committee set up by the UIDAI in its report as far back as December 2009 stated that the UIDAI would “create a\nplatform to first collect identity details of residents, and subsequently perform identity authentication services that can be used by government\nand commercial service providers.” See UIDAI, “Biometrics Design Standards for UID Applications,’ December 2009, https://archive.org/details/\nBiometricsStandardsCommitteeReport/mode/2up. See also Ramanathan, “Aadhaar—From Welfare to Profit,” 177.\n\n20 See Krishnadas Rajagopal, “Centre's Aadhaar Affidavit in Supreme Court: ‘Welfare of Masses Trumps Privacy of Elite.” The Hindu, June 9, 2017,\nhttps://www.thehindu.com/news/national/centres-aadhaar-affidavit-in-supreme-court-welfare-of- masses-trumps-privacy-of-elite/article18951798.\nece; and see the Preamble of The Aadhaar Act.",
    "Page_56": "56 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nHowever, Aadhaar’s market function has been understated; early documents indicate that the\nproject was preoccupied with its role of instituting people as data points within existing and new\ndatabases.?!\n\nAt the outset, this authentication infrastructure took the form of application programming\ninterfaces (APIs)? for use by government agencies and third parties, for verification and\nauthentication of identity information.” Such an instrumentalization of the Aadhaar database not\nonly drastically reduced the costs of performing door-to-door verification required of banking and\ntelecom service providers, but also held the promise of entirely new use cases for businesses.”*\n\nThese APIs are part of “India Stack,” a growing set of APIs built by a group of self-styled\nvolunteers called India Software Products Roundtable (iSPIRT), or Product Nation.7° iSPIRT\ndesigns and builds these APIs for use by government entities and businesses alike, in the process\ncreating novel opportunities for value extraction from data flows and populating the Aadhaar\necosystem.\n\nAadhaar Integration with Cooperation of Sectoral Regulatory\nInstitutions\n\nWith a strong need for identity verification, the finance sector was the first to fully embrace\nAadhaar. With the close cooperation of UIDAI and iSPIRT, institutions within the finance sector?®\nled efforts to build technology products forming a cashless layer atop the Aadhaar identity layer.\n\nThese products allowed banks to use the Aadhaar number to make remittances”’ or to authorize\nAadhaar-linked bank accounts to transact through biometric authentication,” and allowed\n\nfirms to query the Aadhaar database to verify and onboard customers.”? With these Aadhaar\nintegrations into legacy banking services in place, NPCI launched a payments system that\nintroduced interoperability between different payments and settlements systems through the\n\n21 Consider, for example, the orientation of the UIDAI Security Policy and Framework for UIDAI Authentication, which provided mandatory and\nrecommendatory security considerations to Authentication User Agencies (AUA), Authentication Service Agencies (ASA), Devices, etc. This\ndocument, speaking directly to security and privacy concerns, which are traditionally welcome as areas of regulation, primarily deals with network\nsecurity concerns like distributed denial of service (DDOS) attacks that protect the conditions that are critical for the authentication infrastructure\nto run seamlessly. While no doubt this is required, the policy is entirely unconcerned with simpler and more commonplace risks that have proven\nto affect individuals to disastrous effect. For example, people who were not used to treating erstwhile ID cards as private information continued\nto share their Aadhaar numbers and related information with no hesitation, sometimes compromising their economic security. The mandate to\nguard against security risks that such socially grounded identification cultures bring is not something that any of the regulatory agencies concern\nthemselves with.\n\n22 “An APlis a set of definitions and protocols for building and integrating application software...APIs let a product or service communicate with other\nproducts and services without having to know how they're implemented.” See “What Is an API?” RedHat, https://www.redhat.com/en/topics/api/\nwhat-are-application-programming-interfaces.\n\n23 Aadhaar Authentication API: returns “yes/no” responses to queries seeking authentication of biometric or demographic data. Aadhaar electronic\nKnow-Your-Customer (eKYC) API: returns demographic information in response to queries.\n\n24 Aman Sharma, “The Private Sector Can Use Aadhaar Authentication Too: UIDAI,\" Economic Times, April 5, 2016, https://economictimes.indiatimes.\ncom/news/politics-and-nation/private-sector-can-use-aadhaar-authentication-too-uidai/articleshow/51691531.cms.\n\n25 See iSPIRT, https://ispirt.in/\n\n26 E.g., the National Payments Corporation of India (NPCI); and the central bank and finance regulator, Reserve Bank of India\n\n27 See Aadhaar Payments Bridge System, a new batch processing system implemented by NPCI.\n\n28 See Aadhaar Enabled Payment System:\n\n29 See Aadhaar-based Biometric Authentication and electronic Know-Your-Customer norms.",
    "Page_57": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 57\n\nintroduction of Aadhaar biometric authentication, among others.*° In practical terms, private\nfirms could now build payment-related products and users could easily make payments through\ntheir smartphones. As a cohesive suite of technology “platforms,” these products and switches*\"\nenabled the creation, capture, and monetization of data flows in finance.°?\n\nUIDAI and its private-sector financial partners planned for the interoperability between Aadhaar\nand financial tools from the start,°° and conflicts of interest were notable. People associated with\nbuilding the cashless layer went on to launch startups that created novel ways of payment-data\nmonetization. Venture capitalists associated with the cashless layer went on to back these very\nstartups.*4\n\nNevertheless, the government and financial sector have argued that this ecosystem is a boon for\nfinancial inclusion.*> As a testament to its value, Nandan Nilekani notes that securing a loan has\nnow become as simple as having “a richer digital footprint.”°°\n\nHowever, these narratives recast complex sociopolitical issues like lack of access to banking\nas individual journeys of competition for artificially scarce resources, to be won by participating\nand winning in the data economy. These interventions are far from actually addressing issues\nof financial inclusion.*” While the financial sector led the efforts to monetize Aadhaar, many\nother industries continue to follow suit (e.g, with “technology stacks” for healthcare, lending,\ntelemedicine, and agriculture).°*\n\n30 See “United Payments Interface,” February 2015, https://archive.vn/xZEWO#selection-3321.29-3327.29.\n\n31. Aswitch handles authentication and communication between issuing and acquiring banks.\n\n32. Alandscaping study of companies built on top of India Stack recorded at least 150 startups doing background verification, digital lending, and\ndigital wallets as far back as 2018. Bharat Inclusion Fund, “Startups building on IndiaStack: A Landscaping Study,’ Medium, August 23, 2018, https://\nmedium.com/bharatinclusion/startups-building-on-indiastack-a-landscaping-study-a77344b51d19\n\n33. UIDAI provided blueprints for how its architecture may be used for financial-sector commercial products to the Reserve Bank of India\n(RBI). See “Report of Task Force on an Aadhaar-Enabled Unified Payment Infrastructure,” February 2012, https://archive.org/details/\nreporttaskforceaadhaarpaymentinfra. Indeed, RBI leadership in charge of developing standards for payments and settlements included industry\nplayers behind Aadhaar. See Anuj Srivas, “Exclusive: How the RBI Forced National Payments Body to Hire Government Favourite as CEO,’ The Wire,\nFebruary 14, 2018, https://thewire.in/business/rbi-npci-digital-india. The committee set up for “deepening digital payments” was helmed by the first\nchairperson of the UIDAI, Nandan Nilekani. See Aria Thaker, “Behind RBI's Digital Payments Panel, a Controversial Firm's Shadow, Conflict of Interest\nAllegations,’ Scroll.in, January 10, 2019, https://scroll.in/article/908802/behind-rbis-digital-payments-panel-a-controversial-firms-shadow-conflict-of-\ninterest-allegations.\n\n34 Aria Thaker, “The New Oil: Aadhaar's Mixing of Public Risk and Private Profit Caravan, April 30, 2018, https://caravanmagazine.in/reportage/\naadhaar-mixing-public-risk-private-profit.\n\n35 See Suprita Anupam, “Nandan Nilekani on Creating the Architecture for India’s Digital Future,” Inc42, April 24, 2019, https://inc42.com/features/\nnandan-nilekani-on-aadhaar-digital-india-kyc-gst-upi-payments-fastag/. See also ProductNation/iSPIRT, “Nandan Nilekani: Identity, Payments,\n\nData Empowerment 2019,\" SlideShare, December 9, 2019, https://www.slideshare.net/ProductNation/nandan-nilekani-identity-payments-data-\nempowerment-2019; and ITU News, “Aadhaar: India’s Route to Digital Financial Inclusion,’ June 26, 2017, https://news.itu.int/aadhaar-indias-route-\nto-financial-inclusion/; and Ronald Abraham et al., “State of Aadhaar Report 2016-17, Chapter 4: Financial Inclusion,’ Omidyar Network, May 2017,\nhttps://static1 .squarespace.com/static/S5b/7cc54eec4eb7d25f7af2be/t/5bc535e324a694e7994fcf0c/1539651143095/State-of-Aadhaar-Ch4-\nFinancial-Inclusion.pdf.\n\n36 See ProductNation/iSPIRT, “Nandan Nilekani: Identity, Payments, Data Empowerment 2019.”\n\n37. For example, according to economist and author M. S. Sriram, issues of identity, deduplication, and authentication were not the most significant\nbarriers to financial inclusion. See Sriram, “Moving Beyond Aadhaar: Identity for Inclusion,’ Economic & Political Weekly 49, no. 28 (July 12, 2014),\nhttps://www.epw.in/journal/2014/28/special-articles/identity-inclusion.htm| (paywall)\n\n38 For healthcare, see “National Health Stack: Strategy and Approach,” July 2018, https://niti.gov.in/writereaddata/files/document_publication/\nNHS-Strategy-and-Approach-Document-for-consultation.pdf; and Seema Singh and Arundhati Ramanathan, “The Elite VC-Founder Club Riding\nAarogya Setu to Telemed Domination,’ The Ken, May 18, 2020, https://the-ken.com/story/the-elite-vc-founder-club-riding-aarogya-setu-to-telemed-\ndomination/. On loans for MSME, see Arundhati Ramanathan, “Sahay, India's Fintech Disruption Sequel,” The Ken, May 8, 2020, https://the-ken.com/\nstory/sahay-indias-fintech-disruption-sequel/.",
    "Page_58": "58 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nAgile Regulation Keeps the Ecosystem Alive\n\nAs private-sector use of Aadhaar took off, many harms materialized®? and several entities\nsubmitted petitions to challenge the law.*° Even as the Supreme Court struck down private-\nsector uses of Aadhaar in 2018,’ and dealt an existential blow to entire sectors”? built with its\naffordances, in practice this did not ultimately limit companies from using Aadhaar for private\ngain.\n\nAs if seeing the Supreme Court's verdict as a procedural complication and not a principled\nopposition to private use, the Ministry of Law and Justice introduced an ordinance amending\nthe Aachaar Act and other finance laws to keep authentication possibilities alive by introducing\n“offline verification” and “alternative virtual identity.’“° This allowed Aadhaar number holders to\nproduce digitally signed copies of their Aadhaar acknowledgement letter by producing a QR\ncode or .xml file downloaded from the UIDAI website.** Despite these shoddy and dangerous\naccommodations, businesses were still disgruntled, as the ease and low costs of verification\nwere nevertheless affected.*®\n\nIn response, the Central Government issued a note to allow private entities to use Aadhaar-based\nverification facilities upon the fulfillment of certain conditions, and at the discretion of UIDAI and\nthe appropriate regulator.*° With this cue, the finance-sector regulator allowed the use of Aadhaar\nfor opening bank accounts,’” and UIDAI allowed private firms to regain access to Electronic Know\nYour Customer (eKYC) authentication.”®\n\n39 E.g., through the profiling of blue-collar workers, or fraudulent uses of data. See Usha Ramanathan, “The Future Is Here: A Private Company Claims\nIt Can Use Aadhaar to Profile People,’ Scroll.in, March 16, 2016, https://scroll.in/article/805201/the-future-is-here-a-private-company-claims-to-have-\n\n \n\naccess-to-your-aadhaar-data; and “UIDAI Suspends Airtel, Airtel Payments Bank's e-KYC License over Aadhaar Misuse,’ Economic Times, December\n16, 2017, https://economictimes.indiatimes.com/news/politics-and-nation/uidai-suspends-airtel-airtel-payments-banks-e-kyc-licence-over-aadhaar-\n\n \n\nmisuse/articleshow/62096832.cms.\n\n40 For example, activists challenged the linking of Aadhaar to bank accounts and mobile numbers. See Laxmi Prasanna, “New Petition in Apex Court\nChallenges Linking Aadhaar with Bank Account and Phones,’ Times of India, October 19, 2017, https://timesofindia.indiatimes.com/india/new-\npetition-in-apex-court-challenges-linking-aadhaar-with-bank-account-and-phones/articleshow/61145283.cms. See also Anoo Bhuyan, “Aadhaar\nIsn't Just about Privacy. There Are 30 Challenges the Govt Is Facing in Supreme Court,” The Wire, January 18, 2018, https://thewire.in/government/\naadhaar-privacy-government-supreme-court.\n\n41 Justice K. S. Puttaswamy and Another v. Union of India and Others, Writ Petition (Civil) No. 494 of 2012, https://main.sci.gov.in/\nsupremecourt/2012/35071/35071_2012_Judgement_26-Sep-2018.pdf.\n\n42 Komal Gupta, “Aadhaar Verdict Puts Fintech Firms in a Spot,” Livemint, September 28, 2018, https://www.livemint.com/Politics/\nglGcFQMgHR146zXfPkGqjO/Aadhaar-verdict-puts-fintech-firms-in-a-spot.html.\n\n43 Anita Baid, “RBI Amends KYC Master Directions: Aadhaar to Be Officially Valid Document Now,’ Moneylife, May 31, 2019, https://www.moneylife.in/\narticle/rbi-amends-kyc-master-directions-aadhaar-to-be-officially-valid-document-now/57317.html.\n\n44 Ministry of Law and Justice, “The Aadhaar and Other Laws (Amendment) Ordinance, 2019,” https://uidai.gov.in/images/news/Ordinance_Aadhaar_\namendment_07032019.pdf. See also UIDAI, “Secure QR Code Specification,’ March 2019, https://uidai.gov.in/images/resource/User_manulal_\nQR_Code_15032019.pdf. Note that the supposed security features of a digital identity linked to biometrics is undone when the artifact of proof of\nidentity becomes part of an .xml file.\n\n45 Pratik Bhakta, “India's Fintech Companies Struggle for an Alternative to Aadhaar,” Economic Times, December 21, 2018, https://economictimes.\nindiatimes.com/small-biz/startups/features/indias-fintech-companies-struggle-for-an-alternative-to-aadhaar/articleshow/67186586.cms.\n\n46 Pratik Bhakta, “Soon, Non-Banking Companies May Verify via eKYC,’ Economic Times, May 17, 2019, https://economictimes.indiatimes.com/\nindustry/banking/finance/banking/soon-non-banking-companies-may-verify-via-ekyc/articleshow/69366383.cms.\n\n47 “Banks Can Use Aadhaar for KYC with Customer's Consent: RBI,’ May 29, 2019, https://economictimes.indiatimes.com/industry/banking/finance/\nbanking/banks-can-use-aadhaar-for-kyc-with-customers-consent-rbi/articleshow/69568435.cms.\n\n48 This was based on a creative interpretation of the opinion of the attorney general. For example, authentication functions were allowed for purposes\nof welfare delivery. UIDAI applied this as if products using Aadhaar-enabled Payments System (AePS) could access it, since AePS might be used in\nthe course of welfare delivery.",
    "Page_59": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 59\n\nREMAKING REGULATION IN TECHNOLOGY'S IMAGE\n\nRegulatory practice surrounding Aadhaar indicates that regulation is becoming beholden to the\nsame values, managerial styles, procedural cadence, interests, and language of communication\nas the applications of technologies it seeks to regulate.\n\nRegulation as Public Relations and Marketing\n\nFor the first seven years of its existence, Aadhaar had little oversight and was shaped by UIDAI,\n\na body preoccupied with the market importance of Aadhaar. Even after the passage of the\n\nlaw, regulation and technology development have worked hand-in-hand to create and maintain\nthe conditions for use of the biometric data by private companies, to the artificial exclusion of\nsocioeconomic concerns.*? Regulations not only consolidated the developments of the first seven\nyears of the project, but also presented a revisionist history of the actual goals of the project,\nobscuring the stakes for private interests.® For this and other reasons, many of the problems\nwith Aadhaar should not be understood as failures of law or regulation, but as products of law and\nregulation.\n\nWhile law and regulation were meant to address the risks of Aadhaar, the instruments uncritically\n\nadopted disingenuous jargon like “financial inclusion,” “innovation,” and “efficiency.” What was\nrighteously proclaimed by UIDAI as public buy-in for the project owed some credit to incentives\nprovided to enrollment agencies,’ as well as expertise drawn from “multiple areas of marketing,\ncreative communication, research, understanding of past social marketing efforts, media\nchannels, branding and positioning.”\n\nRegulation as Technology Product\n\nThe private sector's direction and influence in the development and adoption of technology\nprojects has a key feature of anticipating concerns around data use, and making data protection\nitself a product, feature, and layer.\n\n49 Law and regulation of the finance sector, for example, creates an artificial distinction between civil and political rights (often framed in the narrow\nlanguage of privacy) and economic imperatives (generalized benefits for the country). See also Nandan Nilekani, “Data to the People: India’s\nInclusive Internet,” Foreign Affairs, September/October 2018, https://uidai.gov.in/images/news/Data-to-the-people-Nandan-Nilekani-foreign-affairs.\npdf.\n\n50 See Ramanathan, “The Future Is Here: A Private Company Claims It Can Use Aadhaar to Profile People,” Scroll. in.\n\n51 Anand Venkatanarayanan, “How Trustworthy Are the Entries in the Aadhaar Database?” MediaNama, September 28, 2017, https://www.medianama\ncom/2017/09/223-how-safe-is-the-aadhaar-database/\n\n52 UIDAI, “Aadhaar Awareness and Communications Strategy Advisory Council Order” February 17, 2010, https://archive.org/details/\nUIDAIMediaAwarenessAdvisoryCouncil/page/n1/mode/2up. See also conflicts of interest within initiatives like ID4D. Transnational interests like the\nWorld Bank's ID4D initiative, pushing digital identification in the language of rights to developing countries, even as its composition reveals shocking\nconflicts of interests, including investors in fintech and related data economy businesses, venture capitalists as well as Nandan Nilekani himself.\nSee also Anandita Thakur and Karan Saini, “Selling Aadhaar: What the UIDAI's Advertisements Don't Tell You,’ The Wire, August 23, 2018, https://\nthewire.in/rights/aadhaar-advertisements-identity-citizenship-rights. “If the advertisements espoused by the UIDAI were to be believed, the prospect\nof biometric failures and internet connectivity issues do not even figure into the day-to-day business of the coercive practice of making Aadhaar an\nunsubstitutable instrument of citizen life in India.”",
    "Page_60": "60 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nIn the case of Aadhaar and data governance in India, the private-sector group building India\nStack took it upon itself to “innovate” around encoding data-protection safeguards (e.g., through\n“consent” and “transparency”) within the technology ecosystem and to solve for data protection.\nThis maneuver simultaneously tries to foreclose demands for a data-protection law (which India\ndoes not have) and, more importantly, distracts from broader questions about whether such\ndatafication is at all necessary and who benefits from it, making the present trajectory seem\ninevitable.\n\nConsent: Arguably one of the biggest issues with Aadhaar has been its coercive nature and\nabsolute disregard for consent, which has continued to be an issue even after courts have\nattempted to intervene.*® Perhaps learning from the problems caused by the pesky need for\nconsent, India Stack evolved a “consent layer’™ consisting of two products: Account Aggregator\nand Data Empowerment and Protection Architecture (DEPA).°° The former is an entity legally\ninstituted by the Reserve Bank of India, which is tasked with consolidating, organizing, and\nretrieving data about a customer's different types of financial arrangements, including mutual\nfunds and insurance schemes. The latter aims to provide “a modern privacy data sharing\nframework’ and introduces convenience into the process of sharing personal data in exchange\nfor finance, healthcare, and other services by building an interface for the purpose. The contents\nof this layer effectively make consent a bureaucratic formality and logistical complication to be\nsimplified by technology, obscuring the instrumentalization of people's lives toward value creation\nfor private firms.\n\nThe consent-related products blindside the need to consider whether such datafication is at\nall necessary, or what the subsequent terms of use of this data might be, ultimately cornering\nregulation into becoming a mimicry of the direction the market for data takes.\n\nTransparency: The Aadhaar project documents are littered with references to the importance\n\nof transparency. One of the main sources of proactive disclosure about Aadhaar is the UIDAI\ndashboard,°° where monthly data about enrollments, updates, and authentication are maintained.\nHowever, this data is a far cry from the granularity or consistency of useful information that\npeople have been demanding for a long time,’” like the number of failed biometric authentications.\n\nThe transparency-related artifacts use aesthetic devices like dashboards, data visualizations, and\nsocial media campaigns that have little substance and remain inert to demands for meaningful\ninformation.\n\n53 Anuj Srivas, “Aadhaar Moves Forward as Ministries Navigate SC Order and Public Backlash,’ The Wire, September 20, 2016, https://thewire.in/\ngovernment/aadhaar-supreme-court-compliance.\n\n54 Jayadevan PK, “Consent, the Final Layer in India’s Ambitious Data Regime, Falling in Place,’ Factor Daily, September 5, 2017, https://factordaily.com/\nconsent-architecture-indiastack/.\n\n55 See IndiaStack, “About Data Empowerment and Protection Architecture (DEPA), https://www.indiastack.org/depa/.\n\n56 See UIDAI, Aadhaar Dashboard, https://uidai.gov.in/aadhaar_dashboard/.\n\n57 See Gus Hosein and Edgar Whitley, “Identity and Development: Questioning Aadhaar’s Digital Credentials” in Dissent on Aadhaar.",
    "Page_61": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 61\n\nRegulation as Optimization\n\nThe regulatory framework around Aachaar has been perennially agile and adaptive to the needs\nof the data economy. Within the broader vision for technology-enabled governance, agencies are\nencouraged to roll out projects “as soon as possible, and iterated rapidly, rather than waiting to roll\nout a perfect system.”®®\n\nBesides aligning regulatory priorities with the workflows and cultures of technology firms, there is\na push for regulatory practice to adopt the same logics (prediction, optimization) as technology\ndirections within the industry. For example, Nandan Nilekani argues that the market is a perfectly\nresponsive system: “Digital systems enable early-warning systems and more precise regulatory\ninterventions, e.g., for managing loan defaults.”®?\n\nCONCLUSION\n\nThe data economy relies on instituting individuals as data points within databases. Law and\nregulation around Aadhaar cooperate to create the perfect conditions under which this might\n\nbe possible: architecting biometric information as truth, and facilitating its use, integration, and\nmaintenance within other systems. Even then, law and regulation maintain a depoliticized reading\nof economic enrichment from data, and a false dichotomy between questions of rights and\nquestions of enrichment.\n\nInstead of treating biometric information simply as data to be guarded, law and regulation should\nreckon with the entire range of powerful market interests that the networked subject kicks into\nmotion, as well as regulation’s own malleability in the face of these forces.\n\n58 Ministry of Finance, “Report of the Technology Advisory Group for Unique Projects,” January 31, 2011, https://www.finmin.nic.in/sites/default/files/\nTAGUP_Report.pdf.\n59 See ProductNation/iSPIRT, “Nandan Nilekani: Identity, Payments, Data Empowerment 2019.”",
    "Page_62": "62 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nA First Attempt at Regulating\nBiometric Data in the\nEuropean Union\n\nEls Kindt (KU Leuven)\n\nINTRODUCTION\n\nn 2004, the European Union (“Union”) enacted legislation that obligated Member States (“MS”)\n\nto store facial images and fingerprints in citizens’ passports and travel documents.’ Around\n\nthe same time, the Union set up large-scale databases containing the biometric data of\nasylum and visa seekers and an information system for protecting the Schengen Area.? It wasn't\nlong before this spilled over into public and private entities, which began using biometric data for\ncrowd control, access control in the workplace, and monitoring in schools. While acknowledging\nthat the use of biometric technology has many potential benefits, the Council of Europe warned\nthat biometric data should be considered as “sensitive” data that presents risks, because it\ncontains information about health and race, has the ability to identify people, can make it easier to\nlink records, and is irrevocable.®\n\n1 EU Regulation No 2252/2004, December 13, 2004.\n\n2 Consider, for example, Eurodac, the Visa Information System (VIS), and the Schengen Information System (currently SIS II), which all emerged after\n2000. Biometric data remains central to the Union's information systems, including the European Travel Information and Authorisation System\n(ETIAS), the Entry/Exit System (EES) (Regulation No 2017/2226), and the European Criminal Records Information System for Third-Country\nNationals (ECRIS-TCN), as is clear from the recent interoperability framework (Regulation No 2019/817 and Regulation No2019/818).\n\n3 See Council of Europe, Progress Report on the Application of the Principles of Convention 108 to the Collection and Processing of Biometric Data\n(Strasbourg: 2005), and the updated Progress Report of 2013, T-PD(2013)06, https://rm.coe.int/progress-report-on-the-application-of-the-principles-\nof-convention-108/1680744d81. The Council of Europe adopted the European Convention on Human Rights in 1950, and the Convention No.108\non data protection in 1981, as revised in 2018 (Convention No. 108+). The Council of Europe consists of forty-seven Member States and is distinct\nfrom the European Union.",
    "Page_63": "Els Kindt | A First Attempt at Regulating Biometric Data in the European Union | 63\n\nDespite the risks, the general data-protection framework and most national legislation did not\ncontain specific provisions on biometric data use and processing,’ and guidance remained\nlimited while these technologies were being developed.® To address these gaps, some national\nsupervisory data protection authorities (SAs) developed frameworks for biometric use.® As part of\nthese frameworks, SAs have focused on the sensitive nature of the data, the risks of maintaining\ndatabases, and the possibility of “function creep.” The SAs also focused on whether the use of\nbiometrics was proportionate to the legitimate aim sought to be achieved (i.e., the “proportionality\nprinciple’), leaving much room for discretionary policy considerations and unpredictable\noutcomes when applying the proportionality principle.®\n\nIt was against this backdrop that the Union introduced the General Data Protection Regulation\n2016/679 (GDPR) in 2016. The regulation is directly applicable in Member States and includes\nprovisions for both public and private biometric data processing. The Union also introduced\nDirective 2016/680 (Data Protection Law Enforcement Directive, or DP LED), which applies\nspecifically to personal data processing for the prevention, detection, investigation, or prosecution\nof crime by law enforcement authorities (LEAs).\n\nTHE EU’S REGULATORY APPROACH TO BIOMETRIC\nDATA PROCESSING\n\nBoth the GDPR and DP LED provide, for the first time, a definition of biometric data: “personal data\nresulting from specific technical processing relating to the physical, physiological or behavioural\ncharacteristics of a natural person, which allow or confirm the unique identification of that natural\n\n4 See Directive 95/46 and Framework Decision 2008/977/JHA. “Processing” is understood very broadly, and is defined as “any operation or set of\noperations . .. whether or not by automated means, such as the collection, recording, organization, structuring, storage, adaptation or alteration,\nretrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure\nor destruction” (Article 4(2), GDPR). Only a few Member States introduced specific legal provisions, e.g., France, Article 25 and 27 Act No. 78-17.\n\n5 See e.g, the Article 29 WP, Working Document on Biometrics 2003 (WP 80), Opinion 2/2012 on facial recognition in online and mobile services\n(WP192), and Opinion 3/2012 on developments in biometric technologies (WP193).\n\n6 This is done by advising, adopting opinions, and issuing guidelines, authorizations, restrictions, and bans. See, e.g., for France, Claire Gayel, “The\nPrinciple of Proportionality Applied to Biometrics in France: Review of Ten Years of CNIL's Deliberations,” Computer Law & Security Review 32, no. 3\n(June 2016), 450-461, https://doi.org/10.1016/j.clsr.2016.01.013\n\n7 This can happen, for example, when an agency uses the data for something other than its original purpose (e.g., for law enforcement purposes).\nSee also CNIL, “Communication de la CNIL relative a la mise en oeuvre de dispositifs de reconnaissance par empreinte digitale avec stockage dans\nune base de données,” Communication central storage fingerprint, December 28, 2007, https://www.cnil.fr/sites/default/files/typo/document/\nCommunication-biometrie.pdf.\n\n8 The proportionality principle is an important principle in data-protection legislation. It requires that the processing is lawful and the data adequate\nand relevant and not excessive for the purpose specified. When interfering with human rights, the proportionality principle requires in addition\na three-step test: that there is accessible and sufficiently certain law allowing the interference (‘rule of law’); a legitimate aim; and necessity in a\ndemocratic society. For assessing the latter, one needs to determine whether (1) the interference answers a “pressing social need,” (2) the argued\nreasons for deploying the interference are relevant and sufficient, and last but not least (3) whether all of this, in particular the interference (in our\ncase the use of biometric technology), is in proportion with the legitimate aim pursued. As there remained confusion about the need for double\nreview and because there was also lack of clarity about the three-pronged approach, this resulted in divergent and unpredictable outcomes when\napplying the proportionality principles and in broad “margins of appreciation.” See also E. Kindt, Privacy and Data Protection Issues of Biometric\nApplications: A Comparative Legal Analysis (Dordrecht: Springer, 2013), 403 et seq. and 621 et seq",
    "Page_64": "64 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nperson, such as facial images or dactyloscopic [fingerprint] data.”° A particularly noteworthy\naspect is the “specific technical processing”'’® component, which effectively excludes “raw” data\nstored and retained in databases (e.g., of facial images captured on CCTV, voice recordings,\n\nor fingerprints),'' or when published on a website or social network. The GDPR accounts also\nmention that the “processing of photographs should not systematically be considered to be\nprocessing of special categories of personal data...\"'* Video footage of an individual is also not\nconsidered biometric data as long as it has not been specifically technically processed in order to\ncontribute to the identification of the individual.'*\n\nWhile the GDPR states that “processing of biometric data for the purposes of uniquely identifying”\nis prohibited,'* there are many exceptions to this prohibition, including when the data “are\nmanifestly made public” or if processing is “necessary for reasons of substantial public interest,\non the basis of Union or Member State law which shall be proportionate to the aim pursued,\nrespect the essence of the right to data protection and provide for suitable and specific measures\nto safeguard the fundamental rights and the interests of the data subject.”\"® Because the\nexceptions remain vague (e.g., “substantial public interest”) and are numerous,\" the GDPR still\nallows the processing of biometric data in many circumstances, including those where people\ngive explicit consent.'” Finally, the GDPR specifies that Member States may maintain or introduce\nfurther conditions or limitations.\"®\n\n9 Article 4(14) GDPR and Article 3(13) Directive 2016/680. The technical process is likely to be understood as a biometric technical processing\nThe original definition in the EU Commission's GDPR proposal of January 25, 2012 COM(2012) 11 final and in the European Parliament's position\nin its first reading of April 13, 2014 was broader: “biometric data’ means any [personal data] relating to the physical, physiological, or behavioural\ncharacteristics of an individual which allow their unique identification, such as facial images, or dactyloscopic data” (Article 4(11)). Experts view this\ndefinition as narrow and contrary to a general understanding of biometric data. For examples, see the ISO/IEC 2382-37 Information Technology—\nVocabulary—Part 37: Biometrics (2017), where “biometric data’ (3.3.6) includes both biometric samples (analog or digital representations of\nbiometric characteristics), hence the initial or “raw” data, and the technically processed data thereof. See also EES, where biometric data is\ndefined as including images: “biometric data’ means fingerprint data and facial image” (Regulation 2017/2226, article 3.1 (18)). On the biometric\nterminology and possible confusion, see also Catherine Jasserand, “Legal Nature of Biometric Data: From ‘Generic’ Personal Data to Sensitive Data,’\nEDPL 2, no. 3 (2016): 297-311, https://doi.org/10.21552/EDPL/2016/3/6.\n\n10 Added by the Council of the Union, composed of the heads of the Member States and governments. See Council doc. 15395/14, December 19,\n2014, https://www.statewatch.org/media/documents/news/2014/dec/eu-council-dp-reg-1 5395-14.pdf. This modification was requested and added\nto the initially proposed definition and finally adopted. On the origin of this modification, see E. J. Kindt, “Having Yes, Using No? About the New Legal\nRegime for Biometric Data,’ Computer Law & Security Review 34, no. 3 (June 2018):523-538, https://doi.org/10.1016/j.clsr.2017.11.004. This article\nalso contains a graphic showing what counts as biometric data and not, and which legal provisions apply.\n\n11 For example, the collection of facial images by governments to issue identity documents, and their storage in databases.\n\n12 Rec. 51 GDPR. See also EDPB, Guidelines 3/2019 on Processing of Personal Data through Video Devices, on Video Surveillance, January 29, 2020, §\n74 (\"EDPB Guidelines 3/2019 on video devices’).\n\n13 Ibid.\n\n14 Article 9.1 GDPR. The general prohibition was an amendment requested by the European parliament (EP) to the original proposal of the EU\nCommission. The processing of biometric data was hereby hence added to the list of special categories of data. EP first reading, T7-0212/2014,\nMarch 12, 2014. This followed the 2012 suggestions of the Council of Europe's Consultative Committee working on the modernization of\nConvention No. 108. Compare with Article 6.1 of Convention 108+ of the Council of Europe. The words “for purposes of uniquely identifying” were\nadded later during the trilogue in 2016. See Council position, 05419/1/2016, April 8, 2016.\n\n15 There are ten explicit exemptions. See Article 9.2 GDPR, “Processing of Special Categories of Personal Data,’ https://www.privacy-regulation.eu/en/\narticle-9-processing-of-special-categories-of-personal-data-GDPR.htm.\n\n16 The exception “personal data which are manifestly made public by the data subject” is also much debated. See also, e.g., EDPB Guidelines 3/2019\non video devices, § 70\n\n17 For example, banks may rely on biometric data for financial account access if their customers explicitly agree.\n\n18 Article 9.4 GDPR. For example, the Netherlands adopted a law allowing biometric data processing if necessary for “authentication or security\npurposes.” Article 29 Dutch GDPR implementation Act of May 16, 2018. The Dutch SA however seems to apply this in a strict manner: see the\ndecision of the Dutch SA (Autoriteit Persoonsgegevens), December 4, 2019, imposing a fine of 725,000 euros for unlawful fingerprinting of\nemployees for access control (appeal pending), available at https://autoriteitpersoonsgegevens.nl/sites/default/files/atoms/files/onderzoek_\nvingerafdrukken_personeel.pdf (in Dutch).",
    "Page_65": "Els Kindt | A First Attempt at Regulating Biometric Data in the European Union | 65\n\nUnder the DP LED, LEAs do not face a prohibition and may process biometric data to uniquely\nidentify people where strictly necessary, subject to appropriate safeguards, and only in three\nsituations: if authorized by law, to protect vital interests, or where the processing relates to data\nmanifestly made public by the data subject.'? While the DP LED has data processing restrictions\nonly if there is “specific technical processing,’ LEAs may collect data (e.g., facial images or voice\nrecordings) without biometric specific limitations imposed by the DP LED.\n\nIn cases where new technologies lead to processing that is “likely to result in a high risk” or in\ncase of large-scale processing of special categories of personal data, the GDPR and DP LED\nrequire entities to conduct Data Protection Impact Assessments (DPIA). A DPIA is also required\nfor systematic monitoring of a publicly accessible area on a large scale.*° DPIAs mandate entities\nto conduct a comprehensive assessment of the risks of processing, as well as of the necessity\nand proportionality of the technology.*' In some cases, private or public entities will have to ask\nthe SA for prior consultation and authorization.” Furthermore, if the biometric data processing\ninterferes with fundamental human rights and freedoms, including the right to privacy and the\nright to personal data protection, the fundamental rights framework shall be applied as well.?°\n\nThe following sections outline the key learnings from these regulatory attempts, discuss their\neffectiveness, and highlight learnings for future regulation.\n\nASSESSMENT AND EFFECTS OF THE REGULATORY\nCHOICES\n\nImpact of Definitional Choices\n\nSince the GDPR and DP LED definitions of biometric data require “specific technical processing,” the\ncollection and storage of data like facial images or voice recordings do not receive more or stricter\nprotection than any other personal data, such as the requirement of explicit consent or necessity and\n\n19 See Article 10 Directive 2016/680. Note that in the two last situations, the need for an authorizing law doesn’t seem to be required. For “data\nmanifestly made public by the data subject,’ this is meant to cover social media.\n\n20 Article 35 GDPR and Article 27 Directive 2016/680. This DPIA requirement was part of the original EU Commission's GDPR proposal of January 25,\n2012 COM(2012) 11 final\n\n21. While the DPIA requirement adds important responsibility (and liability) for assessing the risks, necessity, and proportionality of biometric systems,\npost-GDPR experience already shows that such assessment is in general very difficult to conduct in practice. For the French SA's guidance, see\nCNIL, “The Open Source PIA Software Helps to Carry out Data Protection Impact Assessment\" and its updates, June 25, 2019, https://www.cnil.fr/fr/\nnode/23992\n\n22 See, e.g., CNIL (French SA), “Délibération no. 2019-001;’ January 10, 2019, https://www.cnil.fr/sites/default/files/atoms/files/deliberation-2019-\n001-10-01-2019-reglement-type-controle-dacces-biometrique.pdf. The document discusses the processing of employee biometric data for access\ncontrol to premises, devices, and apps at work, which requires such DPIA, Article 11\n\n23 See Kindt, Privacy and Data Protection Issues, 570 et seq; see also supra note 8 on the proportionality principle. The relevant fundamental rights\nof the European Convention on Human Rights and of the EU Charter that could be affected by biometric technology include, besides the right to\nprivacy and data protection, the right to freedom of expression and of free movement, non-discrimination, and the right to assembly. In relation to\nLFR and LEAs, see FRA, “Facial Recognition Technology: Fundamental Rights Considerations in the Context of Law Enforcement, November 27,\n2019, https://fra.europa.eu/en/publication/2019/facial-recognition-technology-fundamental-rights-considerations-context-law. See also Pete Fussey\nand Daragh Murray, “Independent Report on the London Metropolitan Police Service's Trial of Live Facial Recognition Technology,’ The Human\nRights, Big Data and Technology Project, July 2019. National traditions interpreting these fundamental rights must also be taken into account,\nadding complexity to the matter.",
    "Page_66": "66 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nthe need for law as required under Article 9.2 GDPR. It is the use of the data, rather than its sensitive\nnature or its ability to enable identification, that determines when data becomes biometric.”\n\nBecause of the way the definition was written, the risks of biometric data collection are not\ncovered. Data that should get special protection does not, because it is not currently being\nprocessed. This is particularly concerning because later use, particularly by law enforcement\nagencies, may be less transparent or restricted, and the data could be used without any notice\nto the individuals concerned or to the public.*° Under the permissions granted under the GDPR\nand the DP LED, this implies that companies and government can collect large databases of\nimages (e.g., similar to the information collected by Clearview), which might later be used for law\nenforcement purposes.”°\n\nFinally, the definition is not in line with the European Court of Human Rights case law, which has\nrepeatedly stated that the practice of capturing, collecting, and storing unique human characteristics\nin databases interferes with the right to respect for private life.2” Such interference was confirmed\nfor facial images in Gaughran v. The United Kingdom, where the Court took facial recognition and\nfacial mapping techniques into account, and “found that the retention of the applicant's DNA profile,\nfingerprints and photograph amounted to an interference with his private life.’\n\nAn appropriate definition should offer legal protections to unique human characteristics that are fit\nfor identification purposes or could be used by automated processes, and regulation should also\nrestrict the storage of this data in databases.” An alternative definition of biometric data could be:\n“all personal data (a) relating directly or indirectly to unique or distinctive biological or behavioural\ncharacteristics of human beings and (b) used or fit for use by automated means (c) for purposes of\nidentification, identity verification, or verification of a claim of living natural persons.’°\n\nLack of Clarity around Biometric “Prohibition” and Sweeping\nExceptions\n\nThe law should take into account how different biometric systems function, and these functionalities\nshould be regulated depending on how the data is processed. For example, while prohibitions on use\nand processing are outlined in the law, Article 9.1 GDPR does not distinguish between one-to-one (1:1)\nbiometrics comparisons (i.e., verification), and one-to-many (1:n) comparisons (i.e., identification).*!\n\n24 The definition of biometric data does not include so-called “soft” biometrics, such as emotions, since they usually do not allow for identification or\nidentity verification.\n\n25 Article 23 GDPR allows Union or MS law to restrict the rights of data subjects, including the right to information, e.g., to protect public security. See\nalso Article 13.3 Directive 2016/680.\n\n26 For example, LEAs could use FR technology combined with social media profiles; or see the online dating investigation tool offered by Socialcatfish:\ncom, which has commercialized social media and dating profile data.\n\n27 ECtHR, S. and Marper 2008, § 86; ECtHR, M.K. v France 2013, § 26; see also Cons. const. (France) no. 2012-652, March 22, 2012 (Lo/ protection de\nlidentité), § 6\n\n28  ECtHR, Gaughran v. The United Kingdom 2020, §70.\n\n29 This should come first and in addition to a prohibition to use for identification purposes, except for precise limited exceptions determined by law.\n\n30 See also Kindt, 2013, Privacy and Data Protection Issues 144 et seq. and 851 et seq\n\n31. See and compare the wording of the prohibition with the definition of article 4(14) GDPR, which refers to the two functionalities (\"which allow or\nconfirm the unique identification’): article 9 GDPR forbids only “biometric data for the purpose of uniquely identifying,\" leaving it uncertain if this\nprohibition also includes processing for purposes of confirming identification (verification).",
    "Page_67": "Els Kindt | A First Attempt at Regulating Biometric Data in the European Union | 67\n\nMeanwhile, the Council of Europe and SAs have stated that biometric verification contains less risk\nthan biometric identification because no database is needed.°?\n\nOn the other hand, one-to-many comparisons (i.e., identification) introduce additional risks,\nincluding the large-scale collection and storage of biometric information in databases, probability-\nbased matching (which raises concerns about accuracy and false positives), and privacy-\nsurveillance concerns. Because the GDPR and DP LED do not differentiate between the two\nfunctionalities, there is legal uncertainty for companies that want to invest in biometric verification\ntechnologies and privacy-enhancing methods.*° Appropriate regulation should meaningfully\naddress the relative risks of each functionality, discouraging or banning those that pose real\n\nrisks, and potentially encouraging those that have the potential to offer real privacy and security\nprotections.\n\nFinally, the broad exceptions and overall vagueness of the law leaves the door open for\nspecifically risky uses of biometric data like live facial recognition (LFR). The GDPR exceptions\n\nare general, and include language allowing biometric data processing for “reasons of substantial\npublic interest” based on law. Because of the way this and other exceptions are worded, it remains\nunclear whether these serve as a legal basis that authorizes public or private entities to deploy\nLFR (e.g., at large stadium events).5* The GDPR and DP LED alone will not resolve these questions,\nand additional specific EU and national laws are needed.*°\n\nCONCLUSION\n\nThe GDPR and DP LED approaches to defining biometric data exclude the collection of so-called\n“raw” data like facial images, yet protection is most important at the initial stage of the creation\nof biometric systems and infrastructures. The GDPR and DP LED also deviate from Europe's\nhuman rights case law and its own approach to data “processing,” which is that data protection\nshould start at the collection stage. A comprehensive legal framework should also aim to restrict\n\n32. CoE, Progress Report 2013 (supra note 3), 58 (recommendation 7, as set out in the CoE report of 2005); Article 29 WP, WP 80 (supra note 5), 11\n(Conclusion). One shall hence keep in mind that it is precisely the use of databases against a general public in public places (or in places accessible\nto the public, such as shops) and the identification functionality that pose the most risk, e.g., of surveillance or of unwanted identification. For\nexample, verification could use local storage and strict safeguards that offer increased security for people trying to access phones or bank\naccounts, e.g., by local comparison of a facial image locally stored in a protected template form under the individual's control, e.g., on a smartphone,\nfor controlling access to a payment application. Verification and identification have also been rightly distinguished by data protection authorities\nsuch as the French SA: see CNIL, “Communication central storage fingerprint,\" December 28, 2007, 5-6.\n\n33. Such methods exist, in particular template-protection methods, permitting pseudonymous, revocable, and unlinkable biometric identifiers. See also\nCoE, Progress Report 2013 (supra note 3), 30-31 and Kindt, Privacy and Data Protection Issues, 801-807. Because of the data-protection-by-design\nobligation, such methods are very important.\n\n34 See Danish SA, “Tilladelse til behandling af biometriske data ved brug af automatisk ansigtsgenkendelse ved indgange pa Brondby Stadion,’ May 24,\n2019, https://www.datatilsynet.dk/tilsyn-og-afgoerelser/tilladelser/2019/maj/tilladelse-til-behandling-af-biometriske-data-ved-brug-af-automatisk-\nansigtsgenkendelse-ved-indgange-paa-broendby-stadion/.\n\n35 Any interference with fundamental rights and freedoms requires a law that shall be sufficiently precise and certain (foreseeability) and accessible,\nin order to exclude arbitrariness. This is especially important for technology because “the technology available for use is continually becoming\nmore sophisticated” (ECtHR, Kruslin, 1990, §33, on voice recording in criminal proceedings). In COVID-19 times, public controllers and LEAs may\nalso be tempted to deploy LFR for controlling movement restrictions. Because of the risks posed for fundamental rights, the EU Commission\nrecently launched a debate about possibly additional legislation for remote biometric identification: see EU Commission, White Paper on Artificial\nIntelligence: a European Approach to Excellence and Trust, February 19, 2020, https://ec.europa.eu/info/publications/white-paper-artificial-\nintelligence-european-approach-excellence-and-trust_en.",
    "Page_68": "68 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nany biometric data storage in databases, and should offer clear guidance as to any undesirable\nor forbidden biometric identification, unless allowed under strict legal conditions, and biometric\nverification solutions, under precise conditions. More precise laws around police collection and\nuse of such data and policing techniques are needed, in addition to a strict interpretation of the\nnecessity and proportionality tests as they apply to law enforcement use.\n\nApart from stronger legal and procedural safeguards under the GDPR and DP LED, and\nenhanced consideration of the fundamental rights’ three-steps test, policymakers should adopt\nspecial regulation to strengthen and reinforce fundamental rights. These could include bans or\nmoratoria against particular uses of biometric technology like LFR unless strictly necessary and\nproportionate for substantial public interests described in law. This is crucial, especially if LFR\ndirectly contradicts and affects the essence of fundamental rights, such as the right to peaceful\nassembly, which should not be left to case-by-case assessment.\n\nAs other states or countries look to the Union for guidance around regulating biometric data\ncollection and use, this chapter has aimed to highlight the challenges posed by uncritically\nadopting the text of the GDPR and DP LED. For any future legislation, it will be important to\nrecognize the risks and functionalities of biometric data systems, starting from the collection\nand storage of the data, not just during its processing or use, and to reconsider broadly worded\nexceptions that provide loopholes for companies, governments, and authorities to exploit.",
    "Page_69": "Els Kindt | A First Attempt at Regulating Biometric Data in the European Union | 69",
    "Page_70": "70 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nReflecting on the International\nCommittee of the Red Cross’s\nBiometric Policy: Minimizing\nCentralized Databases\n\nBen Hayes (AWO agency, Consultant legal advisor to the ICRC)\nMassimo Marelli (Head of the ICRC Data Protection Office)\n\nvulnerable people in the world, providing humanitarian assistance to populations affected\n\nby armed conflict and other situations of violence.' Like many other humanitarian\norganizations, the ICRC is exploring new technologies to support its operations and beneficiaries.\nAs part of its digital transformation agenda, the ICRC developed a Biometrics Policy (“the Policy”)\nthat both facilitates the responsible use of biometrics and addresses data-protection challenges.\nICRC adopted the Policy in August 2019,? which recognizes the legitimacy and value of using\nbiometrics to support its programmatic and operational objectives while also ruling out the\ncreation of any central, biometric databases in the short term. This article discusses some of the\nfactors brought to bear on the decision-making process we went through as an institution.°\n\nT he International Committee of the Red Cross (ICRC) works with some of the most\n\n1 International Committee of the Red Cross, “The ICRC’s Mandate and Mission,’ https://www.icrc.org/en/mandate-and-mission.\n2 International Committee of the Red Cross, “The ICRC Biometrics Policy,’ October 16, 2019, https://www.icre.org/en/document/icrc-biometrics-\npolicy.\n\n \n\n3 This article builds on Ben Hayes and Massimo Marelli, “Facilitating innovation, ensuring protection: the ICRC Biometrics Policy,’ ICRC, Humanitarian\nLaw & Policy, October 18, 2019, https://blogs.icrc.org/law-and-policy/2019/10/18/innovation-protection-icre-biometrics-policy.",
    "Page_71": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 71\n\nBIOMETRICS IN THE HUMANITARIAN SECTOR\n\nThe ICRC works in more than ninety countries and is part of a global humanitarian network\n\nof over eighty million people. It provides healthcare, food, basic shelter, clothing, access to\neducation, employment, and assistance to detained persons, and also helps restore family\n\ninks by reuniting separated persons and finding missing persons. To address the logistical\nchallenges of protection and assistance programs, some humanitarian organizations use\nbiometric identification systems to enroll people in humanitarian programs and verify their identity\nwhen providing services or assistance. The primary justification for this use is that recipients of\nhumanitarian assistance frequently lack identity documents, which poses a challenge if they need\no be identifiable.\n\nHumanitarian organizations have intensely debated when and how people “need” to be\nidentifiable, and the legitimacy of using biometrics to perform that function.® On one side,\ncontinuity of healthcare and some forms of humanitarian assistance clearly need people to be\nidentifiable (e.g., for provision of travel documents or financial services). For example, the United\nations Refugee Agency (UNHCR) has a clear mandate to identify refugees and asylum seekers,\nand to provide them with identity documents? (though it has been heavily criticized for deploying\nbiometrics’). However, most humanitarian organizations do not have a formal mandate to\nprovide people with an identity or supporting documentation. They have primarily developed and\nimplemented biometric ID systems because of the perceived efficacy and accountability gains\nsuch systems provide.®\n\n \n\nWhile existing ID cards, social security numbers, and other documents may be used by\nhumanitarian organizations to check or verify an individual's identity, these cannot be\nunequivocally associated with a single individual in the way that a biometric ID can. Biometric\ndatabases can also be used to prevent the same individual from registering in an aid program\nmore than once, which is attractive for humanitarian organizations that are concerned about\nindividuals or families obtaining more assistance than has been earmarked for them.? Indeed,\nbiometrics have played an increasingly large role in the scaling up of cash-transfer programs\n(CTPs).'° For financial service providers that are obligated to verify the identity of account holders\n\nICRC, “The International Red Cross and Red Crescent Movement,” https://www.icrc.org/en/who-we-are/movement.\n\n5 See, for example, “Head to Head: Biometrics and Aid”, The New Humanitarian, July 17, 2019, https://www.thenewhumanitarian.org/\nopinion/2019/07/17/head-head-biometrics-and-aid; and Katja Lindskov Jacobsen, Kristin Bergtora Sandvik, and Sean Martin McDonald,\n“Humanitarian Experimentation,’ ICRC, Humanitarian Law & Policy, November 28, 2017, https://blogs.icrce.org/law-and-policy/2017/11/28/\nhumanitarian-experimentation/.\n\n6 United Nations High Commissioner for Refugees, “Note on the Mandate of the High Commissioner for Refugees and His Office,’ Refworld, October\n2018, https://www.refworld.org/docid/5268c9474.html. Note: The ICRC also issues emergency travel documents, albeit very few by comparison.\n\n7 See for example Chris Burt, “UNHCR Reaches 7.2M Biometric Records but Critics Express Concern,’ Biometric Update, June 24, 2019, https://www.\nbiometricupdate.com/201906/unher-reaches-7-2m-biometric-records-but-critics-express-concern\n\n8 The Engine Room and Oxfam, “Biometrics in the Humanitarian Sector” March 2018: https://www.theengineroom.org/wp-content/uploads/2018/03/\nEngine-Room-Oxfam-Biometrics-Review. pdf.\n\n9 Laura Gordon, “Risk and Humanitarian Cash Transfer Programming: Background Note for the High Level Panel on Humanitarian Cash Transfers,”\nOverseas Development Institute, May 2015, https://www.odi.org/sites/odi.org.uk/files/odi-assets/publications-opinion-files/9727.pdf.\n\n10 See, for example, World Bank Group, “Guidelines for |ID4D Diagnostics,’ 2018, http://documents1 .worldbank.org/curated/en/370121518449921710/\nGuidelines-for-ID4D-Diagnostics.pdf. Cash and other forms of direct financial disbursement are widely viewed as providing beneficiaries of\nhumanitarian programs with more dignity and autonomy than food parcels and other disbursed goods, but donors are concerned that these\nprograms are more susceptible to fraud and abuse.",
    "Page_72": "72 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nand cash recipients, biometric data could offer a simple and straightforward way to meet multiple\noperational needs and legal obligations.\"\n\nThese are crucial issues for humanitarian staff, who want operations to be as efficient as\npossible, and to ensure that scarce humanitarian services and assistance are provided to\nintended recipients. There is also implicit pressure to use biometrics from donors, which\nincreasingly demand “end-to-end auditability” (allowing the tracking of humanitarian funds from\ndonor to recipient) and make funding contingent on anti-fraud and accountability processes. All\nof this has contributed to a tangible impetus for humanitarian organizations to use biometrics for\nbeneficiary registration and aid distribution. And why not, if everyone else is doing it?\n\nRISKS AND CONCERNS\n\nConcerns about the use of biometrics in the humanitarian sector are well known, but are often\noverlooked.'* Biometric data are unique, immutable, and create a permanently identifiable record\nfor individuals in vulnerable humanitarian contexts who may not want to be identifiable forever.\nThe creation of a permanent biometric record underpins concern that this record could increase\nthe risk of harm to the persons concerned in the event it was subsequently accessed by or\nprovided to the regime or non-State actor they had fled.\n\nBiometrics constitute particularly sensitive data'? due to the potential for reuse or misuse, as\nwell as “function creep,’ i.e., the possibility that biometrics may be used in a new way, separate\nfrom the original purpose and without the understanding or consent of the affected individuals.\nFor example, biometrics could be shared with non-humanitarian organizations or governments\nfor non-humanitarian purposes, such as security and migration control.'* This is particularly\nconcerning when biometric identity management systems are developed during a crisis or\n\n11. These assumptions also dovetail with the UN's Sustainable Development Agenda, which mandates the provision of legal identity to all and targets\nincreased financial inclusion, tacitly encouraging States and the financial sector to predicate both on a biometric identity. See, for example,\nSustainable Development Goal (SDG) target 16.9: “By 2030, provide legal identity for all, including birth registration: Promote just, peaceful and\ninclusive societies.” Financial inclusion is a target for eight of the seventeen SDGs. United Nations, Department of Economic and Social Affairs,\nSustainable Development, “The 17 Goals,’ https://sdgs.un.org/goals.\n\n12 — See, for example, Gus Hosein and Carly Nyst, “Aiding Surveillance,’ Privacy International, October 2013, https://privacyinternational.org/report/841/\naiding-surveillance. See also Katja Lindskov Jacobsen, “On Humanitarian Refugee Biometrics and New Forms of Intervention,’ Journal of\nIntervention and Statebuilding 11, no. 4 (2017): 529-551, https://doi.org/10.1080/17502977.2017.1347856.\n\n13 The General Data Protection Regulation (EU) 2016/679 (GDPR), for example, introduces a general prohibition against the processing of biometric\ndata unless, inter alia, the data subject has given their “explicit consent” (something which is problematic in a humanitarian context, as discussed\nfurther below); the processing is subject to a specific law or legal agreement; the processing is necessary to protect the vital interests of data\nsubjects who are physically or legally incapable of giving consent; or where the processing is necessary for reasons of public interest and subject to\nadequate measures to protect the interests and safeguard the fundamental rights of the data subject (Article 9). The recently adopted “Modernised\nCoE Convention 108+\" on data protection broadly adopts the same approach to biometric data as the GDPR by classifying them as “sensitive data”\nand imposing core restrictions and conditions on their processing. The African Union Convention on Cybersecurity and Personal Data Protection\nalso imposes restrictions on the processing of biometric data.\n\n14 Affected populations have expressed serious concerns about the use of biometrics and potential access to the data by non-humanitarian\norganizations. See, for example, Aziz El Yaakoubi and Lisa Barrington, “Yemen's Houthis and WFP Dispute Aid Control as Millions Starve,”\n\nReuters, June 4, 2019, https://www.reuters.com/article/us-yemen-security-wfp/yemens-houthis-and-wfp-dispute-aid-control-as-millions-starve-\nidUSKCN1T51Y0; “Rohingya Refugees Protest, Strike Against Smart ID Cards Issued in Bangladesh Camps,” Radio Free Asia, October 26, 2018,\nAttps://www.rfa.org/english/news/myanmar/rohingya-refugees-protest-strike-1 1262018154627.html; and “Over 2,500 Burundi Refugees in Congo\nSeek Shelter in Rwanda,’ Voice of Africa News, March 8, 2018, https://www.voanews.com/africa/over-2500-burundi-refugees-congo-seek-shelter-\nrwanda",
    "Page_73": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 73\n\nemergency, where data could be used in ways that recipients of humanitarian assistance do\nnot want, understand, or consent to. Humanitarian databases may, for example, be integrated\nor made interoperable with other social registries or national ID systems run by development\nor government partners. Technology may also advance to allow biometric profiles to be used\nto ascertain additional information about the data subject—for example regarding their health,\nethnicity, or genetic makeup.\n\nStates have shown increasing interest in biometrics to monitor the movement of populations and\nidentify security “threats.” In December 2017, the UN Security Council called for the enhanced\n\nuse of biometric ID systems to identify terrorist suspects, mandating all UN Member States to\n“develop and implement systems to collect biometric data, which could include fingerprints,\nphotographs, facial recognition, and other relevant identifying biometric data, in order to\nresponsibly and properly identify terrorists, including foreign terrorist fighters, in compliance\n\nwith domestic law and international human rights law.’ Some humanitarian organizations\n\nhave already come under pressure from States to disclose biometric data for non-humanitarian\npurposes, though these requests are generally not in the public domain. Organizations are also\nvulnerable to cyber-operations by State and non-State actors seeking unauthorized access to their\ndata.'©\n\nBiometric data use was a central theme at the 33rd International Conference of the Red Cross\nand Red Crescent, held in December 2019.\"” To safeguard the independence, neutrality, and trust\nin humanitarian organizations, the Conference adopted a landmark resolution on “restoring family\nlinks while respecting privacy.”'® Founded on the principle of purpose limitation, the resolution\n“urges States and the Movement to cooperate to ensure that personal data is not requested or\nused for purposes incompatible with the humanitarian nature of the work of the Movement.”\n\n \n\nRATIONALIZING BIOMETRICS AT THE ICRC\n\nPrior to the adoption of its biometrics policy, the ICRC was already employing biometrics in limited\nuse cases, for example in forensics and the restoration of family links, and by putting fingerprints\non the travel documents it issues (but not into any database). In addition to using DNA profiling\n\n15 UN Security Council Resolution 2396, adopted December 21, 2017 under Chapter VII of the UN Charter on “Foreign Terrorist Fighters.” As the UN\nSpecial Rapporteur for the Protection and Promotion of Human Rights While Countering Terrorism has stated, the biometrics mandate provided by\nthe Security Council is “deeply concerning” because the Resolution does not contain any explicit reference to constitutional or legislative protections\nfor privacy or data protection. See Fionnuala Ni Aoldin, “The UN Security Council, Global Watch Lists, Biometrics, and the Threat to the Rule of Law,’\nJust Security, January 17,2018, https://www.justsecurity.org/51075/security-council-global-watch-lists-biometrics/.\n\n16 Massimo Marelli, “Hacking Humanitarians: Moving towards a Humanitarian Cybersecurity Strategy,’ ICRC, Humanitarian Law & Policy, January 16,\n2020, https://blogs.icrc.org/law-and-policy/2020/01/16/hacking-humanitarians-cybersecurity-strategy/.\n\n17 International Federation of Red Cross and Red Crescent Societies, 33rd International Conference, 2019, https://rereconference.org/about/33rd-\ninternational-conference/\n\n18 Reuniting families separated by conflict and disaster is a core activity of the International Red Cross and Red Crescent Movement globally. See\n“Restoring Family Links While Respecting Privacy, including as it Relates to Personal Data Protection” ( 331C/19/R4), 33rd International Conference\nof the Red Cross and Red Crescent, December 9-12, 2019, https://rercconference.org/app/uploads/2019/12/331C-R4-RFL-CLEAN_ADOPTED_\nen.pdf.\n\n19 Ibid., Article 11",
    "Page_74": "74 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nto help identify human remains to determine the fate of the missing, the ICRC is exploring facial\nrecognition technology to locate persons sought by family members following separation due to\nhumanitarian emergencies.”°\n\nThis is part of a broader ICRC strategy to transform and adapt its humanitarian response\n\nby seizing the opportunities that new technologies offer its operations and beneficiaries.\nManaging the attendant risks is central to this digital transformation agenda.*' Early in 2018,\nfollowing significant interest in expanding biometric data use, the ICRC Directorate requested an\nassessment of the operational, ethical, and reputational risks involved, as well as an institution-\nwide policy that would facilitate both innovation and data protection.\n\nICRC developed the policy over an eighteen-month period that included extensive research,\nanalysis, consultation, and reflection. ICRC reviewed all scenarios in which the ICRC processed or\nconsidered the use of biometrics, evaluated the “legitimate basis” and specific purposes for the\nprocessing, and identified organizational, technical, and legal safeguards. Although the ICRC is\nnot bound by national or regional data-protection law, it has adopted similar rules that require it to\nidentify a legitimate basis (equivalent to a legal basis) for all of its data-processing activities.”\n\nIn some cases, ICRC’s rationale for biometric data use was straightforward: for instance, when\nused with specific objectives associated with its international mandate and where particular\nobjectives cannot be realized without using biometrics. Examples include using DNA to determine\nthe fate or whereabouts of the missing, or using facial recognition to match missing and\n\nsought persons in its work on restoring family links.”? In these cases, the ICRC processes the\nbiometric data as a matter of “public interest.’** Subject to appropriate safeguards, biometric\n\ndata processing provides the ICRC with tools that greatly enhance its capacity to implement its\nmandate with respect to persons separated or missing in humanitarian emergencies.\n\nOther cases are much more challenging: for example, when the potential use case involves\nbiometrics for beneficiary management and aid distribution, where requiring the identification of\nindividuals may not be viewed as an integral part of an ICRC mandate-based activity. Because the\npurpose is primarily efficiency, and aid can be (and long has been) distributed without the need\nfor biometrics, the ICRC determined that the “legitimate interest” of using a biometric identity-\nmanagement system did not outweigh the potential concerns over rights and freedoms. This\nbalancing test is typical of data-protection laws (e.g., as in GDPR), whenever a data controller\nrelies on their own interests as a basis for processing.*>\n\n20 See “Rewards and Risks in Humanitarian Al: An Example,” ICRC, Inspired, September 6, 2019, https://blogs.icre.org/inspired/2019/09/06/\nhumanitarian-artificial-intelligence/.\n\n21 In addition to “doing no harm,’ ICRC maintains principles of impartiality, neutrality, and independence. The protection of personal data that could\nbe misused or whose disclosure could put its beneficiaries at risk is an integral means of ensuring these principles are upheld. See ICRC, “The\nFundamental Principles of the International Red Cross and Red Crescent Movement,’ https://www.icrc.org/sites/default/files/topic/file_plus_\nlist/4046-the_fundamental_principles_of_the_international_red_cross_and_red_crescent_movement.pdf.\n\n22 See ICRC, “Rules on Personal Data Protection,’ (\"ICRC Rules’), https://www.icrc.org/en/publication/4261-icrc-rules-on-personal-data-protection. The\nrules were adopted by the Directorate of the ICRC on February 24, 2015 (updated on November 10, 2015), and updated and adopted by the ICRC.\nAssembly on December 19, 2019.\n\n23 ICRC, Restoring Family Links, https://familylinks.icrc.org/en/Pages/home.aspx.\n\n24 ICRC Rules, Article 1\n\n25 ICRC Rules, Article 1; GDPR, Article 6.",
    "Page_75": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 75\n\nAfter careful consideration, ICRC concluded that it was possible to leverage the efficiency and\neffectiveness gains of biometric authentication, as well as end-to-end accountability in its aid\ndistributions, while also minimizing the risks to its beneficiaries. This balance rests on using\nbiometric data in beneficiary registration and verification, and limiting the processing to a token-\nbased system. In practice, this means that beneficiaries could be issued a card on which their\nbiometric data is securely stored, but that the ICRC will not collect, retain, or further process their\nbiometric data (and therefore not establish a biometric database).\n\nThe token/card could be used to verify beneficiaries during aid distributions to ensure that the\n\naid reaches those individuals for whom it has been earmarked, but no other use will be possible.\nIf the beneficiary wants to withdraw or delete their biometric data, they may return or destroy the\ncard. If authorities seek to compel humanitarian organizations in a particular country to hand over\nthe biometric data of beneficiaries, the ICRC will not face such pressure because it will not have\nthe data.\n\nKEY FEATURES OF THE POLICY\n\nAdopted by the ICRC Assembly in August 2019, the ICRC Biometrics Policy sets forth staff and\nprogram roles and responsibilities,”° the legitimate basis for processing biometric data by the\nICRC,”’ the specific purposes and use cases for which the use of biometrics is authorized, 7° and\nthe types of biometric data that may be processed by the ICRC.” Specifically, it allows the ICRC\nto:\n\ne include the fingerprints of the holder on travel documents issued by the ICRC to persons\nwho have no valid identity papers, enabling them to return to their country of origin or\nhabitual residence or to go to a country which is willing to receive them;\n\ne use biometric identification systems to restrict access to strictly confidential information\nand/or mission-critical resources such as servers and control rooms in ICRC premises;\n\ne use fingerprints, facial scans, and DNA to identify human remains recovered from disaster\nor conflict zones or in connection with other situations of violence;\n\ne use digitized photographs for the purposes of tracing and clarifying the fate of separated or\nmissing persons;\n\ne use biometric data to ascertain the identity or fate of specific individuals in the course of\ninvestigations related to the abduction of, or attacks upon, ICRC staff members;\n\n*« onacase-by-case basis, where it has been determined that it is in the best interest of the\npersons concerned, collect biological reference samples for the purposes of DNA profiling\nto facilitate family reunification or to determine the fate of a missing person; and\n\ne use biometrics to provide beneficiaries with a token-based verification credential such as a\ncard that can be used to verify their receipt of those services, where the token is held solely\nby the Data Subject.\n\n26 ICRC Biometrics Policy, Article 4.\n27 Ibid., Article 5.\n28 Ibid., Article 6.\n29 Ibid., Article 7.",
    "Page_76": "76 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nThere are additional caveats:\n\ne Theuse of fingerprints for travel documents remains limited to ink prints on hard-copy\ndocuments (with no further biometric processing by the ICRC permitted).\n\ne Delegations may not use biometrics for routine premises control (only specific assets that\nrequire a high level of security and where profiling is limited to staff authorized to access\nthem).\n\ne« DNAprofiling for family reunion purposes is strictly limited to cases where proof that two\npersons are actually related is required under national law or policy.\n\nThe Policy also expressly rules out the creation of biometric databases with respect to the\nauthorized use cases. Finally, where ICRC programs or delegations wish to process biometric\ndata pursuant to an authorized use case, they must first conduct a data-protection impact\nassessment and ensure that detailed data protection by design and by default requirements are\nimplemented as the process or system is developed.\n\nThe Biometrics Policy also addresses some other common data-protection challenges, including\n“consent,” which humanitarian organizations have traditionally sought from the people who use\ntheir services or receive assistance. In some contexts, like medical treatment, these processes\nhave been quite robust. In others, however, people have routinely signed “consent forms”\n\nor provided a thumbprint in lieu of a signature (e.g., for those unable to write; as part of its\nbiometrics review, the ICRC is also putting an end to this practice). “Informed consent” in data\nprocessing is subject to high standards: the ICRC Rules on Personal Data Protection require\n“freely given, specific, informed indication of his or her wishes by which a Data Subject signals\nagreement to the Processing of Personal Data relating to him or her.”’!\n\nWhile the ICRC is firmly committed to transparency, it does not believe that consent provides a\nlegally valid basis for data processing in many emergency situations. Consent to data processing\ncannot be regarded as valid if the individual has no real choice: for example, where the provision\nof aid is effectively dependent on the provision of personal information, and consent is therefore\nunlikely to be “freely given.” In addition, power imbalances may imply no real “choice,” and\nindividuals may be induced to accept what is proposed by a humanitarian organization. Where\nbiometrics are concerned, it is extremely difficult to ensure that consent is genuinely “informed,”\nsince affected populations may not be able to fully comprehend the technology, information\nflows, risks, or benefits that underpin biometric data processing.\n\nThe Biometrics Policy requires that the ICRC explain the basis and purpose of data processing\nto its beneficiaries, including any data-sharing arrangements, regardless of the basis for the\nprocessing.** The ICRC also seeks to ensure that beneficiaries have the opportunity to ask\nquestions and object if they wish, particularly where data may be shared with third parties.°? If\npeople do not want to provide their biometric or other personal data, or share their data with\n\n30 Ibid., Articles 10 and 11\n\n31 ICRC Rules, Definitions: “Consent”\n\n32 ICRC Biometrics Policy, Article 18. This is in line with the ICRC Rules on Personal Data Protection.\n33 Ibid. Article 18.4.",
    "Page_77": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 77\n\npartners, the ICRC will respect their wishes.** The ICRC will only use biometric data where it\nenhances the capacity of the organization to implement its humanitarian mandate.*°\n\nFinally, under no circumstances will the ICRC share biometric data with third parties, including\nauthorities, that may use them for non-humanitarian purposes.°° Even where exclusively\nhumanitarian grounds for sharing biometric data can be identified, strict conditions must be\nsatisfied before ICRC will transfer any data.%’\n\nThe ICRC will review the Biometrics Policy at least every three years,* including the decision\nnot to establish biometric databases for the purposes of identity management. ICRC will review\ndevelopments around the availability, security, cost, effectiveness, and impact of biometric\ntechnology, and may amend the Policy to widen the scope for using biometrics, or to introduce\nnew safeguards.\n\nLESSONS LEARNED\n\nDuring its deliberations, the ICRC considered the option of not adopting a biometrics policy\n\nand leaving decisions about how and when to use these data to programs, operations, and\ndelegations in the field. This option was rejected as “high risk on the basis that it could undermine,\ninter alia, the rights of the ICRC’s beneficiaries, the ‘do no harm’ principle, and ICRC’s reputation.”\nWhile the internal organizational debates have been challenging, the Policy has provided much\nneeded clarity and operating procedures for staff who were struggling to balance the perceived\nbenefits and risks of specific uses.\n\nICRC consulted internal staff and external stakeholders in order to answer questions around\noperational needs, data-protection requirements, technology options, ethics, and risk appetite.\nCase-by-case assessment of the existing and possible use cases was fundamental in shaping\nthe ICRC Biometrics Policy. However, ICRC faced many challenges because it was already using\nbiometrics, and the new Policy could have led to changes in practice or prohibitions against\ncertain processing options or operations. Finally, the ICRC Biometrics Policy benefited from\nconsiderable dialogue and investment in innovative compromises such as the token-based\nsolution, which might not have been achieved through a less coherent or constructive exercise.\nAs biometric data use-case law and data-protection enforcement actions continue to expand, the\nneed for humanitarian organizations to develop proactive policies only becomes more important.\n\n34 — Ibid., Articles 19 and 20.\n35 Ibid. Article 6.1\n36 Ibid. Article 14.\n37 Ibid. Article 15.\n38 Ibid., Article 21",
    "Page_78": "78 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nPolicing Uses of Live Facial\nRecognition in the United\nKingdom\n\nPeter Fussey (University of Essex)\nDaragh Murray (University of Essex)\n\nBACKGROUND TO THE USE OF FACIAL RECOGNITION\nIN THE UK\n\nondon has a long history of trialing advanced surveillance technology. Police agencies first\ninstalled closed-circuit television (CCTV) cameras in the city in 1953, and until recently\nLondon likely had more CCTV cameras per person than any country in the world.’ The\ncity deployed one of the world’s first automatic license plate recognition (ALPR) systems in the\nmid-1990s, and has since introduced crowd-modeling video analytics to survey its mass transit\nsystems.” London was also one of the first cities in the world to trial facial recognition (FR) in\nthe east of the city during the late 1990s, although technological limitations at the time led to its\nabandonment.?\n\n1 Pete Fussey, “Beyond Liberty, Beyond Security: The Politics of Public Surveillance,’ British Politics 3, no. 1 (April 2008): 120-135. See also Jess\nYoung, “A History of CCRV Surveillance in Britain, SWNS, January 22, 2018, https://stories.swns.com/news/history-cctv-surveillance-britain-93449/,\nLondon remains the most CCTV-heavy city outside of China.\n\n2 Pete Fussey, “Observing Potentiality in the Global City: Surveillance and Counterterrorism in London,” International Criminal Justice Review 17, no. 3\n(September 1, 2007): 171-192.\n\n3 Pete Fussey, “Eastern Promise? East London Transformations and the State of Surveillance; Information Polity, 17, no. 1 (January 2012): 21-34.",
    "Page_79": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 79\n\nWith rapid advancements in FR technology, the Metropolitan Police Service (MPS) conducted\na series of ten live facial recognition (LFR) test deployments between 2016 and 2019, moving\nto operational deployments in early 2020.* South Wales Police have also been using LFR since\n2017, mostly at large concerts, festivals, and sporting events.° Both constabularies deploy LFR\nby installing temporary cameras at a fixed geographic location? for a fixed time period.’ Police\ngenerally mount the cameras on an LFR van with a control center used to monitor the live LFR\nfeeds and to communicate with officers on the ground. LFR cameras scan the faces of all\nindividuals passing through their field of vision, and then officers check the resultant biometric\nprofiles against a watch list containing persons of interest. To date, police have only deployed\nLFR technology in this standalone manner, and have not, for example, integrated it into existing\ninfrastructure, such as CCTV networks.®\n\nPolice use of LFR has resulted in significant controversy, with a number of human rights and civil\nsociety organizations leading opposition against LFR deployments. Many of these organizations\nhave initiated advocacy campaigns calling for either a moratorium on the use of LFR,? or an\noutright prohibition.'° South Wales Police’s use of LFR is currently subject to legal challenge, and\nan initial hearing before the Court of Appeal took place in June 2020.\"\n\nIn order to examine issues relating to operational effectiveness and human rights compliance,\nthe MPS invited the authors to provide an independent academic report on the last six LFR\n\ntest deployments.'* We conducted ethnographic observations from beginning to end of each\ndeployment, of pre-deployment police briefings and post-deployment debriefings, and of a range\nof other planning meetings. We also held interviews with key stakeholders and analyzed large\nquantities of MPS internal documents. In this piece, we draw on this research to explore three\nkey themes relating to the regulatory regime: 1) the legal requirement for an authorizing law\n\nfor LFR; 2) the inability and failure of existing institutions and laws to meaningfully restrict this\ntechnology; and 3) the operational considerations unique to LFR. Our focus is on working toward\nhuman rights compliance. A key element not addressed in this piece is the “necessity” of police\nLFR deployments. However, this consideration only comes into play if an appropriate legal basis\nexists.\n\n4 Having moved out of the “test” phase, the MPS now has authority to deploy LFR on the basis of operational intelligence. For further information,\nsee Metropolitan Police, “Live Facial Recognition,’ n.d., https://www.met.police.uk/advice/advice-and-information/facial-recognition/live-facial-\nrecognition/.\n\n5 For more information, see South Wales Police, \"Facial Recognition Helps South Wales Police Become Smarter, Creating a Safer and Connected\n\nCommunity,’ n.d., http://afr.south-wales.police.uk. The list of deployments is available at https://afr.south-wales.police.uk/wp-content/\n\nuploads/2020/04/All-Deployments.pdf.\n\nA “fixed location\" might be a city square, the entrance to an underground tube station, or a football match, for example.\n\nThis is typically a number of hours; to date, no deployments have lasted longer than a day.\n\nAlthough this is becoming increasingly technologically feasible, it is unlikely that these more advanced LFR deployments will occur in the short term.\n\nSee, for example, Carly Kind, “Biometrics and Facial Recognition Technology—Where Next?\" Ada Lovelace Institute, July 2, 2019, https://www.\n\nadalovelaceinstitute.org/biometrics-and-facial-recognition-technology-where-next/.\n\n10 See, for example, Liberty, Resist Facial Recognition, https://www.libertyhumanrights.org.uk/campaign/resist-facial-recognition/; and Big Brother\nWatch, Stop Facial Recognition, https://bigbrotherwatch.org.uk/campaigns/stop-facial-recognition/.\n\n11. The complaint was brought by Ed Bridges, who believes he was subject to facial recognition processing at a peaceful anti-arms trade protest, and\nwhile Christmas shopping. See Liberty, “Liberty Client Takes on Police in Ground-Breaking Facial Recognition Challenge—Hearing Opens Today, May\n21, 2019, https://www.libertyhumanrights.org.uk/issue/liberty-client-takes-on-police-in-ground-breaking-facial-recognition-challenge-hearing-opens-\ntoday/.\n\n12. Peter Fussey and Daragh Murray, “Independent Report on the London Metropolitan Police Service's Trial of Live Facial Recognition Technology,’\nUniversity of Essex Human Rights Centre, section 2,1.1, July 9, 2019, http://repository.essex.ac.uk/24946/.",
    "Page_80": "80 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nPolice LFR deployments directly engage / interfere with several distinct human rights protections.\nThe right to privacy of all individuals passing through a camera’s field of vision (and thus subject\nto biometric processing) is directly engaged. Additional, discrete right-to-privacy issues are\n\nraised by any retention or analysis of the resultant footage.'? The use of LFR may also engage\ndiscrimination laws as a result of the technology's biases.\" Importantly, the deployment of LFR\ntechnology may generate a chilling effect, whereby individuals refrain from lawfully exercising\ntheir democratic rights due to a fear of the consequences that may follow.'® This may harm a\nnumber of rights, including the right to freedom of expression, the right to freedom of assembly\nand association, and the right to freedom of religion.'®\n\nSignificantly, the UK’s Human Rights Act 1998 (implementing the European Convention on Human\nRights'”), requires that any interference with a right be “in accordance with the law.” As such, any\nmeasure interfering with human rights protections must have a legal basis, and that legal basis\nmust be of sufficient quality to protect against arbitrary rights interferences. Key in this regard\n\nis the foreseeability of the law.'® If a measure fails to satisfy the “in accordance with the law”\nrequirement, it is unlawful in and of itself.\n\nTHE COMMON LAW AS A LEGAL BASIS FOR LIVE\nFACIAL RECOGNITION\n\nUnited Kingdom common law establishes the core common law principles for police: protecting\nlife and property, preserving order, preventing the commission of offenses, and bringing\noffenders to justice.’ Although no legislation exists that explicitly authorizes police use of LFR,\nthe government has claimed that these common-law powers provide sufficient implicit legal\nauthorization to satisfy the “in accordance with the law’ test.\n\nIn Bridges v. South Wales Police, the UK High Court agreed with the Government,” indicating that\nthe common law establishes sufficient legal basis for LFR.2' This judgment is currently subject to\nappeal, and this finding is a key point of contention.\n\n13 R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 59.\n\n14 For further discussion on indirect discrimination, see D.H. and Others v. the Czech Republic, Judgment ECtHR, App. No. 57325/00, November 13,\n2007, para. 184.\n\n15. The precise contours of any chilling effect are contested, but research points to its existence. See Jon Penney, “Chilling Effects: Online Surveillance\nand Wikipedia Use,” Berkeley Technology Law Journal 31, no. 1 (2016): 117; see also Elizabeth Stoycheff, “Under Surveillance: Examining Facebook's\nSpiral of Silence Effects in the Wake of NSA Internet Monitoring,’ Journalism & Mass Communication Quarterly 93, no. 2 (2016): 296-311; for\na general discussion, see Daragh Murray and Pete Fussey, “Bulk Surveillance in the Digital Age: Rethinking the Human Rights Law Approach to\nBulk Monitoring of Communications Data,\" Israel Law Review 52, no. 1 (March 2019): 31-60. For a discussion of the chilling effect as it applies to\njournalists, see Centro Europa 7 S.R.L. and Di Stefano v. Italy, Judgment, European Court of Human Rights, App. No. 38433/09, June 7, 2012, para.\n129.\n\n16 For amore in-depth discussion of potential human rights harms, see Fussey and Murray, “Independent Report,’ section 2,1.2, http://repository.essex\nac.uk/24946/,\n\n17 See, e.g., Shimovolos v. Russia, Judgment, ECtHR, App. No. 30194/09, June 21, 2011, para. 67\n\n18 Catt v. the United Kingdom, Judgment, ECtHR, App. No. 43514/15, January 24, 2019, para. 94.\n\n19 See, for example, Metropolitan Police, “Live Facial Recognition, (FR) MPS Legal Mandate’ p. 5, July 23, 2018, https://www.statewatch.org/media/\ndocuments/news/2018/dec/uk-live-facial-recognition-Ifr-mps-legal-mandate.pdf.\n\n20 — R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 78: “For these reasons, we consider\nthe police's common law powers to be ‘amply sufficient’ in relation to the use of AFR Locate. The police do not need new express statutory powers\nfor this purpose.” (\"AFR Locate” is South Wales Police's nomenclature for LFR.)\n\n21 R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 78.",
    "Page_81": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 81\n\nAt the heart of the matter is the fact that police powers under the common law are expressed in\nbroad terms. The common law is inappropriately vague and, for example, does not delimit the\ncircumstances in which a particular measure may be deployed, such that those circumstances\nare foreseeable, thereby protecting against arbitrary rights interference. Relying on the common\nlaw to provide a legal basis for LFR therefore arguably fails to satisfy the “in accordance with the\nlaw” requirement established under human rights law, and presents a clear risk of arbitrariness.?\n\nA key reason for the High Court's conclusion that the common law was a sufficient legal basis\n\nfor LFR, and that new statutory powers were not required, was the classification of LFR as a\nnonintrusive means of obtaining information,”? and as “no more intrusive than the use of CCTV in\nthe streets.“ This is clearly contentious: it appears inconsistent with common understandings of\nthe surveillance capacity inherent in LFR, and has been challenged by a number of key figures in\nthe UK. It also appears inconsistent with the High Court's own finding that—as a form of biometric\nprocessing—LFR engaged the right to privacy of all individuals passing through an LFR camera’s\nfield of vision.2®\n\nConcerns regarding the arbitrary exercise of powers mean that reliance on the common law to\nprovide the legal basis for the use of LFR is likely to be incompatible with the UK’s obligations\nunder the Human Rights Act or European Convention on Human Rights. The Bridges line of cases\nwill provide further guidance in this regard. However, irrespective of the outcomes of these cases,\nestablishing an explicit legal and regulatory basis for the use of LFR would provide much needed\nclarity, both for the public and for the police.\n\nOTHER LAWS, LEGISLATIONS, AND AGENCIES THAT\nAPPLY TO LFR\n\nPolice documentation and political debate have consistently referred to the oversight roles of the\nmultiple data-protection and surveillance-related authorities in the UK.2° These include the UK's\ndata-protection authority, the Information Commissioner's Office (ICO); the Surveillance Camera\nCommissioner; the Biometrics Commissioner; and the Investigatory Powers Commissioner's\nOffice. While these agencies have contributed to the debate, each body is narrowly relevant\n\nto a specific aspect of LFR and, critically, they do not have explicit authorization to limit LFR\ndeployments. Indeed, while many of these regulatory bodies are heralded as a safeguard to\npromote appropriate use, their mandates do not provide meaningful oversight. This is explained in\nthe following table:\n\n22 This concern is equally applicable vis-a-vis the regulation of LFR deployments. The human rights law tests regarding clarity, foreseeability, and\nprotection against arbitrariness are equally applicable in this regard. This conclusion is supported by relevant case law. See, for example, S and\nMarper v. United Kingdom, Judgment, ECtHR, App. Nos. 30562/04 & 30566/04, December 4, 2008, para. 99.\n\n23 R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 74.\n\n24 — R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 75.\n\n25 — This finding distinguishes LFR as more invasive than CCTV. See R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No\nC0/4085/2018, September 4, 2019, paras. 59, 62.\n\n26 As noted above, these considerations are irrelevant if the “in accordance with the law” requirement is not satisfied",
    "Page_82": "Authority\n\n82 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nRole\n\nApplication to LFR\n\n \n\nThe Information\nCommissioner's\nOffice (ICO)\n\nThe Surveillance\nCamera\nCommissioner\n\nThe Biometrics\nCommissioner\n\nThe Investigatory\nPowers\nCommissioner's\nOffice\n\nOversees issues relating to data\nprotection in the UK, particularly\nthe Data Protection Act 2018\nand the General Data Protection\nRegulation.” In 2017, they\npublished “In the Picture: A Data\nProtection Code of Practice\n\nfor Surveillance Cameras\n\nand Personal Information,”\nwhich provided best practices\nfor automated recognition\ntechnologies.”®\n\nEstablished by the Protection of\nFreedoms Act 2012 to oversee\nthe use of closed-circuit television\nsystems (CCTV). 7°\n\nEstablished by the Protection of\nFreedoms Act 2012 to oversee\nretention and use of biometric\ninformation.*'\n\nEstablished under the\nInvestigatory Powers Act 2016,\nhas authority to oversee covert\npolice deployments.°?\n\nAlthough important, data protection\nlaw cannot adequately address the\nbroad range of potential human\n\nrights harms brought about by police\nLFR deployments. It does not, for\nexample, fully address issues relating\nto whether the use of LFR is necessary\nor proportionate. As such, the impact\nof the ICO on the overall LFR debate is\nrelatively limited.\n\nWhile they are primarily focused\n\non CCTV systems, and LFR is\nimplemented through standalone video\nsystems, they have published guidance\non police use of LFR.°°\n\nThe Biometrics Commissioner's role is\nrestricted in statute to fingerprints and\nDNA data, and so does not extend to\nLFR. The Commissioner has published\nseveral statements questioning the\nuse of LFR and has said that “we need\nproper governance of new biometric\ntechnologies such as LFR through\nlegislation.”S?\n\nAs currently deployed, the principal\nuses of LFR by police in the UK are not\nclassified as covert. This may change\ngoing forward.\n\n27 See, further, the Information Commissioner's Office (ICO), https://ico.org.uk/about-the-ico/\n28 ICO, “In the Picture: A Date Protection Code of Practice for Surveillance Cameras and Personal Information,’ Version 1.2, June 9, 2017, https://ico.\norg.uk/media/1542/cctv-code-of-practice.pdf.\n\n29\n\n30\n\n31\n\n32\n\n33\n\nProtection of Freedoms Act ref. See, further, the Surveillance Camera Commissioner, https://www.gov.uk/government/organisations/surveillance-\ncamera-commissioner/about.\n\nSee, for example, the Surveillance Camera Commissioner's Code of Practice, June 2013, https://www.gov.uk/government/publications/surveillance-\ncamera-code-of-practice; and “The Police Use of Automated Facial Recognition Technology with Surveillance Camera Systems,” Section 33\nProtection of Freedoms Act 2012, March 2019, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/\nfile/786392/AFR_police_guidance_of_PoFA_V1_March_2019.pdf.\n\nProtection of Freedoms Act ref. See, further, Office of the Biometrics Commissioner, https://www.gov.uk/government/organisations/biometrics-\ncommissioner/about.\n\nSee, for example, GOV.UK, “Automated Facial Recognition,” September 10, 2019, https://www.gov.uk/government/news/automated-facial-\nrecognition, https://www.gov.uk/government/news/biometrics-commissioner-on-the-police-use-of-live-facial-recognition.\n\nInvestigatory Powers Act 2016, https://www.legislation.gov.uk/ukpga/2016/25/contents/enacted",
    "Page_83": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 83\n\nAs it stands, police use of LFR in the UK is not subject to adequate oversight or meaningful\nregulation. This must urgently be addressed.\n\nISSUES ARISING IN THE CONTEXT OF POLICE LIVE\nFACIAL RECOGNITION DEPLOYMENTS\n\nThis section examines a number of issues arising in the context of LFR deployments, including\nwatch lists, the “presumption to intervene” and associated deficits in effective human oversight,\nhow accuracy is determined, and potential discrimination. These operational elements illustrate\nthe uncertainty associated with LFR deployments, contesting police claims of utility, and highlight\nproblems arising from the absence of appropriate regulation.\n\nOperational Considerations\n\nMeasuring LFR performance is complex and includes both partial and instrumental use of\nstatistics. Some technical evaluations compare the number of false matches to an estimate\n\nof the total number of individuals passing through an LFR camera’s field of vision during a\n\ngiven deployment. These numbers are widely cited by supporters of LFR, yet they offer only a\ntiny ratio of numbers of faces scanned to those correctly or incorrectly matched.* A variation\nof this approach was adopted by the MPS in a recently published evaluation of their LFR trial\ndeployments,®° leading to widely publicized claims that the technology was “70% effective.”°°\nHowever, such claims often conflate two different forms of data, merging “blue list” data (where\nvolunteers are sent past the cameras to measure their effectiveness—the measure used to\nsupport the claim of 70 percent effectiveness) and live data (camera performance when there is\nno certainty about whether suspects will walk past the cameras).\n\nAnother shortcoming of this methodology is the way it de-emphasizes the impact of LFR on\nthose flagged by the technology by contextualizing their experience against larger quantities of\ndata that are arguably less relevant. This makes this measure less suitable for understanding\nthe individual rights-based interferences brought by LFR. Other measures of LFR performance\ncompare how often a human operator discards a computer-suggested alert.°” One challenge of\nthis approach is the potential for readers to conflate human and computer decision-making: a\nhuman might decide the LFR system is wrong, regardless of the veracity of the computational\ndecision.\n\n34 See comments by Baroness Williams of Trafford regarding “a one in 4,500 chance of triggering a false alert’ House of Lords, January 27, 2020,\nhttps://www.theyworkforyou.com/lords/?id=2020-01-27a.1300.2&p=12902.\n\n35 National Physical Laboratory and Metropolitan Police Service, “Metropolitan Police Service Live Facial Recognition Trials,’ February 2020, https://\nwww.met.police.uk/SysSiteAssets/media/downloads/central/advice/met/facial-recognition/met-evaluation-report.pdf.\n\n36 Vikram Dodd, “Met Police to Begin Using Live Facial Recognition Cameras in London,’ Guardian, January 24, 2020, https://www.theguardian.com/\ntechnology/2020/jan/24/met-police-begin-using-live-facial-recognition-cameras.\n\n37. Bethan Davies, Martin Innes, and Andrew Dawson, An Evaluation of South Wales Police's Use of Automated Facial Recognition (Cardiff: Universities’\nPolice Science Institute, Crime and Security Research Institute, Cardiff University, 2018).",
    "Page_84": "84 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nWe designed our independent academic review of the MPS system to address the above\nchallenges, focusing on human rights considerations and protection against arbitrary rights\ninterferences. The research used the same statistics as the MPS study above, and asked two\nstraightforward questions to determine accuracy and examine the role of human oversight:\n\na. When anLFR system matches someone to the watch list, how often is it verifiably correct???\n\nb. To what extent do human adjudicators consider LFR matches to be credible? To understand if\na computer match is correct, it needs to be tested against something—in this case, an identity\ncheck of the suspect.\n\nFor (a), our research found that out of forty-two computer-generated LFR matches, eight were\nverifiably correct (19.05 percent). For (b), human adjudicators judged twenty-six out of forty-two\nmatches were sufficiently credible to apprehend the matched individual (61.91 percent), meaning\nthat humans overwhelmingly overestimated the credibility of the system. Four of these matched\nindividuals were lost in the crowd. The remaining fourteen were incorrectly matched by the LFR\nsystem.\n\nTwo conclusions can be drawn from this. First, there is a “presumption to intervene” on behalf of\nhuman operators assessing the credibility of LFR matches. Second, this tendency of deference to\nthe algorithm exists despite the computer being either incorrect or not verifiably correct in a large\nmajority of cases.\n\nThese conclusions hold relevance for considerations over the form of human adjudication taking\nplace around LFR systems. Policy emphasizes the importance of “the human in the loop” as\n\na safeguard against algorithmic-induced harms. That human adjudication takes place is not\n\nin question, however. The issue at stake is the form it takes, and the degree of critical human\nscrutiny applied. Moreover, a presumption to intervene suggests LFR frames and structures\nsuspicion ahead of human engagement with the technology.\n\nA final question is whether LFR is discriminatory. UK police forces have made repeated claims\nthat LFR technology is nondiscriminatory in terms of racial characteristics.4° However, this is\n\na complex issue and covers both the capability of the technology in identifying faces from a\nrange of ethnic groups and the composition of databases of suspects (watch lists). It is difficult\nfor law enforcement agencies to undertake an analysis of sufficient scope to support definitive\nconclusions. For example, the US National Institute of Standards and Technology (NIST)\nreviewed 189 facial recognition algorithms and revealed marked “demographic differentials”\n\nin the performance of facial recognition algorithms across different ethnicities.’ Accordingly,\nclaims made in the technical evaluation of the MPS LFR scheme that “differences in FR algorithm\nperformance due to ethnicity are not statistically significant’** may arise simply because the total\nnumber of matches themselves are not statistically significant.\n\n38 Sup. 35.\n\n39 In other words, could it be definitively concluded that the individual identified by LFR matched the individual on the watch list, such as by means of a\nsubsequent identity check?\n\n40 See, for example, public statements by Metropolitan Police Commissioner Dame Cressida Dick, RUSI Annual Security Lecture, London, February 24,\n2020. “The tech we are deploying is proven not to have an ethnic bias.” Available here: https://rusi.org/event/rusi-annual-security-lecture.\n\n41 Patrick Grother, Mei Ngan, and Kayee Hanaoka, “Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects,’ NIST, December 2019, https://\ndoi.org/10.6028/NIST.IR.8280.\n\n42 National Physical Laboratory and Metropolitan Police Service, “Metropolitan Police Service Live Facial Recognition Trials,” p.4.",
    "Page_85": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 85\n\nAccording to the MPS statistics, twenty-eight people were engaged by a police officer after being\nmatched by LFR systems across their ten test deployments. We contend that it is impossible to\nmake robust and definitive conclusions over demographic disparities from such small numbers.\nConcerns over this issue have been most recently articulated in March 2020 in calls by Great\nBritain's Equality and Human Rights Commission to suspend the use of facial recognition in\nEngland and Wales until its impact has been independently scrutinized.\n\nCONCLUSION\n\nThis piece highlights three key concerns. First, the legal basis underpinning LFR is inappropriately\nvague, negatively affecting foreseeability, and arguably failing to meet the “in accordance with the\nlaw” test established by human rights law. Second, although a number of UK regulatory bodies\nengage in this area, there is no dedicated body with authority to limit or effectively oversee LFR\ndeployments. Third, operational realities contest police claims of LFR’s utility, the effectiveness of\nhuman oversight, and discriminatory outcomes. These highlight the practical consequences and\nharms arising in the absence of appropriate legal or regulatory frameworks.\n\n43 Equality and Human Rights Commission, “Facial Recognition Technology and Predictive Policing Algorithms Out-pacing the Law,’ March 12, 2020,\nhttps://www.equalityhumanrights.com/en/our-work/news/facial-recognition-technology-and-predictive-policing-algorithms-out-pacing-law.",
    "Page_86": "86 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nA Taxonomy of Legislative\nApproaches to Face Recognition\nin the United States\n\nJameson Spivack (Georgetown Center on Privacy and Technology)\nClare Garvie (Georgetown Center on Privacy and Technology)\n\nINTRODUCTION: POLICE FACE RECOGNITION IN THE\nUNITED STATES\n\nn December 25, 2015, Florida resident Willie Allen Lynch was arrested for selling fifty\ndollars’ worth of crack cocaine to two undercover Jacksonville sheriffs three months\nearlier. The only thing tying Mr. Lynch to the crime was a face recognition search\ncomparing photographs the officers had taken of the drug sale to the county's mugshot database.\nThe search returned five possible matches—Mr. Lynch and four other suspects. Mr. Lynch and\nhis defense attorney were given no information about the use of face recognition: its accuracy,\npotential biases, or even a list of the other possible suspects. Despite this, Mr. Lynch, who\nmaintains his innocence, was sentenced to eight years in prison.'\n\n1 See Lynch v. State, 260 So. 3d 1166 (Fla. Dist. Ct. App. 2018). For an overview of how face recognition was used in the case, see Lynch v. State, No.\nSC2019-0298 (Fla. Sup. Ct. 2019), Amici Curiae Brief in Support of Petitioner, available at https://www.aclu.org/sites/default/files/field_document/\nflorida_face_recognition_amici_brief.pdf. The lower court's decision was affirmed on appeal, and the State Supreme Court determined it did not have\njurisdiction to hear the case. Lynch v. State, SC2019-0298 (Fla. Sup. Ct. 2019).",
    "Page_87": "Jameson Spivack & Clare Garvie | A Taxonomy of Legislative Approaches to Face Recognition in the United States | 87\n\nPolice use of face recognition is pervasive, affects most Americans, and, until very recently,\n\nhas persisted under a widespread lack of transparency, oversight, and rules governing its use.”\nPolice departments across the United States have deployed face recognition technology in\nthousands of criminal investigations since as early as 2001.° At least one agency has also used\nface recognition to identify protesters,* and by 2016, one quarter of the nearly eighteen thousand\nagencies across the country had access to a face recognition system.*® Because thirty-one states\nallow police searches of DMV databases, more than half of all American adults can be identified\nthrough police face recognition simply by having a driver's license.® Many police departments\nhave also used Clearview Al's face recognition service, which has amassed a database of an\nadditional three billion images scraped from Facebook, Instagram, Twitter, Venmo, YouTube, and\nelsewhere.’\n\nIn 2016, the Government Accountability Office (GAO) published an extensive report on the use of\nface recognition by the FBI.° It made recommendations to increase transparency, enhance privacy\nprotections, and better test the accuracy of their systems to guard against misidentification. This\nand many other reports have highlighted unique risks posed by police face recognition use:\n\n10\n11\n\nFace recognition poses a threat to privacy. Under the Fourth Amendment of the US\nConstitution, the right to privacy extends beyond the home, protecting “reasonable\nexpectations of privacy” in some public settings and activities.° Face recognition gives police\nthe power to conduct identity-based surveillance and the ability to scan and identify groups\n\nof people in secret, as well as to track someone's whereabouts through a network of security\ncameras. Without a warrant, this power may violate the Fourth Amendment, interpreted in the\nSupreme Court's 2018 decision in Carpenter v. United States as including a right to privacy\n\nin our movements across time and space.\"° The enrollment of most American adults into\nbiometric databases used in criminal investigations represents an unprecedented expansion of\nlaw enforcement access to personal data, to which the American public did not consent.\"\n\nFor an overview of the state of face recognition and laws governing its use, see Clare Garvie, Alvaro M. Bedoya, and Jonathan Frankle, “The\nPerpetual Line-Up: Unregulated Face Recognition in America,’ Georgetown Law Center on Privacy & Technology, (October 18, 2016): 25, 35, https://\nwww.perpetuallineup.org/report.\n\nSee Pinellas County Sheriff ‘s Office, Florida's Facial Recognition Network (Mar. 26, 2014), available at https://drive.google.com/file/d/OB-\nMxWJPOZmePX1 QwTjltQkdVXOU/view?usp=sharing (indicating 2001 as the start date for the Sheriff Office's system).\n\nGeofeedia, Baltimore County Police Department and Geofeedia Partner to Protect the Public During Freddie Gray Riots (obtained by ACLU Northern\nCalifornia Oct. 11, 2016), available at https://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf.\n\nSee supra note 2, at 25. This is a conservative estimate—the actual number is likely much higher. Prior to being terminated by the Attorney General's\nOffice, all law enforcement agencies in the country were able to request searches of the Vermont driver's license face recognition system. See\nACLU Demands Immediate End to DMV Facial Recognition Program, ACLU-VT (May 24, 2017), www.acluvt.org/en/press-releases/aclu-demands-\nimmediate-end-dmv-facial-recognition-program.\n\nSee Statement of Clare Garvie, Senior Associate, Center on Privacy & Technology at Georgetown Law before the U.S. House of Representatives\nCommittee on Oversight and Reform (May 22, 2019), 5, available at https://docs.house.gov/meetings/GO/GO00/20190522/109521/HHRG-116-\nG000-Wstate-GarvieC-20190522. pdf.\n\nSee Kashmir Hill, “The Secretive Company That Might End Privacy as We Know It,” New York Times, January 18, 2020, https://www.nytimes.\ncom/2020/01/18/technology/clearview-privacy-facial-recognition.html. Former Center technologist Jonathan Frankle cautioned against just\nsuch a tool in 2016. See Jonathan Frankle, “How Russia's New Facial Recognition App Could End Anonymity,’ Atlantic, May 23, 2016, https://www.\ntheatlantic.com/technology/archive/2016/05/find-face/483962/\n\nGovernment Accountability Office (GAO), “Face Recognition Technology: FBI Should Better Ensure Privacy and Accuracy,” May 2016, https://www.\ngao.gov/assets/680/677098.pdf.\n\nU.S. Const. Amend. IV; Katz v. United States, 389 U.S. 347 (1967).\n\nCarpenter v. United States, 138 S. Ct. 2206, 2217 (2018).\n\nSee supra note 6, at 5-7.",
    "Page_88": "88 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nFace recognition risks having a chilling effect on free speech. The First Amendment of\n\nthe US Constitution protects the right to free speech, assembly, and association.’ As law\nenforcement agencies themselves have cautioned, face recognition surveillance has the\npotential to “make people feel extremely uncomfortable, cause people to alter their behavior,\nand lead to self-censorship and inhibition’—chilling our ability to participate in constitutionally\nprotected activities.'°\n\nSearches may lead to misidentifications. While the algorithms behind face recognition have\nimproved significantly since 2001, misidentification is still a major issue. Low-quality images,\nedited photos, and unreliable inputs such as forensic sketches and “celebrity lookalikes”\nincrease the odds that the wrong person will be investigated, arrested, and charged with a\ncrime they did not commit.'*\n\nFace recognition may have a disparate impact on communities of color. Communities\n\nof color are disproportionately enrolled in face recognition databases and targeted by\nsurveillance.'> In San Diego, for example, police have used face recognition technology and\nlicense-plate readers up to two and a half times more on people of color than expected by\npopulation statistics.'® The technology performs less accurately on people of color, meaning\nthe risks of the face recognition police use, and the mistakes it may make, will not be\ndistributed equally.\n\nThe failure to disclose a face recognition search may deprive a defendant of due process.\nThe risks of misidentification and bias are not mitigated by a fair, transparent court process.\nFace recognition searches produce evidence that speaks directly to a defendant's guilt or\ninnocence. Per the constitutional right to due process and the Supreme Court's decision in\nBrady v. Maryland, evidence must be turned over to the defense.'” Yet as in Mr. Lynch's case,\nand indeed the vast majority of cases involving a face recognition search, this information is\nnot disclosed.'®\n\nIn response to growing concern over the risks that the use of unregulated police face recognition\nposes to our civil rights and liberties, legislators have begun introducing—and passing—face\nrecognition bans, moratoria, and regulatory bills.'°\n\n12\n13\n\nU.S. Const. Amend. |\n\nInternational Justice and Public Safety Network (Nlets), “Privacy Impact Assessment Report for the Utilization of Facial Recognition Technologies to\nIdentify Subjects in the Field,” June 30, 2011, 2, https://www.eff.org/files/2013/11/07/09_-_facial_recognition_pia_report_final_v2_2.pdf.\n\nFor a discussion of how face recognition is used in practice and its associated risks, see Clare Garvie, “Garbage In, Garbage Out: Face Recognition\non Flawed Data,” Georgetown Law Center on Privacy & Technology, May 16, 2019, https://www.flawedfacedata.com.\n\nSee supra note 2 at 56 (describing disproportionately high arrest rates of black Americans); see Grother, Ngan, & Hanoaka, Face Recognition Vendor\nTest (FRVT) Part 3: Demographic Effects, Nat'l Institute of Standards and Technology (NIST) (Dec. 2019), https://nvlpubs.nist.gov/nistpubs/ir/2019/\nNIST.IR.8280.pdf (\"We found empirical evidence for the existence of demographic differentials in the majority of contemporary face recognition\nalgorithms that we evaluated.’\n\nSee, e.g., Automated Regional Justice Information System, San Diego's Privacy Policy Development: Efforts & Lessons Learned, 11, available at\nhttps://drive.google.com/file/d/1ZR2jjiLcBMUKnHTRk1ZC248NbFUGNRww/view?us p=sharing (indicating that black Americans were 1.5-2.5 times\nmore likely to be the targets of police use of licence-plate readers and face recognition technology).\n\nBrady v. Maryland, 373 U.S. 83, 87 (1963) (holding that the suppression of evidence that is material to the guilt or innocence of the accused violates\nhis due process rights under the Fourteenth Amendment).\n\nMost law enforcement agencies consider face recognition searches to produce “investigative leads” only, not probable cause to make an arrest. But\nin practice, face recognition matches are often not independently corroborated through additional investigative steps before an arrest is made. See\nsup. note 14.\n\nThis represents the general trend currently; some states introduced bills earlier. See, e.g., MD H.B. 1148 (2017).",
    "Page_89": "Jameson Spivack & Clare Garvie | A Taxonomy of Legislative Approaches to Face Recognition in the United States | 89\n\nPROPOSED AND ENACTED LEGISLATION\n\nGenerally, there have been three legislative approaches to regulating face recognition in the United\nStates: complete bans, moratoria, and regulatory bills. Moratoria can be further broken down into\ntwo types: time-bound moratoria, which “pause” face recognition use for a set amount of time;\nand directive moratoria, which “pause” face recognition use and require legislative action—such\n\nas a task force or express statutory authorization—to supersede the moratoria. Most of these\n\nbills have covered all government use of face recognition, with particular attention given to limits\nplaced on police use. This section focuses on police use as well.\n\nType of legislation\n\nWhat it does\n\nExamples\n\n \n\nBan\n\nComplete shutdown of all\nace recognition use\n\nEnacted: San Francisco, CA;2°\nCambridge, MA?!\n\nProposed: Nebraska’?\n\nMoratorium: time-bound Face recognition use paused\n\nor a set amount of time\n\nEnacted: Springfield, MA?\n\nProposed: Maryland”\n\nMoratorium: directive Face recognition use paused,\nrequires legislative action to\n\nsupersede\n\nProposed: Massachusetts”°\n\nRegulatory bill Regulates specific elements Enacted:\nof face recognition, along\na spectrum from narrowly\n\nocused to broader\n\nCalifornia: prohibited in conjunction\nwith police body-worn cameras”°\n(narrower)\n\n \n\nWashington: regulates numerous\nelements” (broader)\n\n20 See Kate Conger, Richard Fausset, and Serge F. Kovaleski, “San Francisco Bans Facial Recognition Technology,’ New York Times, May 14, 2019,\nhttps://www.nytimes.com/2019/05/14/us/facial-recognition-ban-san-francisco.htm|\n\n21. See Jackson Cote, “Cambridge Bans Facial Recognition Technology, Becoming Fourth Community in Massachusetts to Do So,’ MassLive,\nFebruary 27, 2020, https://www.masslive.com/news/2020/01/cambridge-bans-facial-recognition-technology-becoming-fourth-community-in-\nmassachusetts-to-do-so.html.\n\n22 See LB1091, “Adopt the Face Surveillance Privacy Act,’ Nebraska Unicameral Legislature, available at https://www.nebraskalegislature.gov/bills/\nview_bill.php?Document|D=41387.\n\n23. See Jackson Cote, “Springfield City Council Passes Facial Recognition Moratorium,’ MassLive, February 25, 2020, https://www.masslive.com/\nspringfield/2020/02/springfield-city-council-passes-facial-recognition-moratorium.html\n\n24 MD S.B.857 (2020), available at http://mgaleg.maryland.gov/2020RS/bills/sb/sb0857F.pdf.\n\n25 MAS.B. 1385 (2019), available at https://malegislature.gov/Bills/191/S1385.\n\n26 CAA. 1215 (2019) (prohibited only until Jan. 1, 2023), available at https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_\nid=201920200AB1215.\n\n27 WA Engrossed. Subst. S.B. 6280 (2020), available at http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20\nLegislature/6280-S.PL.pdf.",
    "Page_90": "90 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nA. Bans\n\nThe strongest legislative response is to ban the use and acquisition of the technology completely.\nBans can focus on state use of face recognition, commercial or private sector use, or both. To\ndate, only local municipal governments have implemented bans, concentrated in towns and\ncities in California and Massachusetts. As of July 2020, the following municipalities had banned\nface recognition: Alameda, California; Berkeley, California; Boston, Massachusetts; Brookline,\nMassachusetts; Cambridge, Massachusetts; Easthampton, Massachusetts; Northampton,\nMassachusetts; Oakland, California; San Francisco, California; and Somerville, Massachusetts.7°\nA number of states proposed bans on face recognition during the 2019-2020 legislative session:\nNebraska, New Hampshire, New York, and Vermont.”\n\nCity governments have passed bans following robust public dialogue about the risks and benefits\nof face recognition technology. They represent what is possible with a transparent, democratic\nprocess, and the power of proactive localities. In the words of the San Francisco city supervisor\nwho sponsored the ban: “We have an outsize responsibility to regulate the excesses of technology\nprecisely because they are headquartered here.” It is unclear at this point, however, whether face\nrecognition bans will take hold at the local, state, or federal level. Some jurisdictions may also\n\nfind the bans to be unintentionally overbroad, restricting uses of the technology deemed to be\nnecessary or uncontroversial.°!\n\nB. Moratoria\n\nAnother strong measure that a legislature can take is to place a moratorium on the technology,\nwhich has two forms: time-bound and directive.\n\n28 See Peter Hegarty, “East Bay City Becomes Latest to Ban Use of Facial Recognition Technology,” East Bay Times, December 18, 2019, https://www.\neastbaytimes.com/2019/12/18/east-bay-city-becomes-latest-to-ban-use-of-facial-recognition-technology; see Tom McKay, “Berkeley Becomes\nFourth U.S. City to Ban Face Recognition in Unanimous Vote,” Gizmodo, October 16, 2019, https://gizmodo.com/berkeley-becomes-fourth-u-s-city-\nto-ban-face-recogniti-1839087651; see Nik DeCosta-Klipa, “Boston City Council Unanimously Passes Ban on Facial Recognition Technology,’ Boston.\ncom, June 24, 2020, https://www.boston.com/news/local-news/2020/06/24/boston-face-recognition-technology-ban; see ACLU of Massachusetts,\n“Brookline Bans Municipal Use of Face Surveillance,” December 11, 2019, https://www.aclum.org/en/news/brookline-bans-municipal-use-face-\nsurveillance; see sup. note 20; see Michael Connors, “Easthampton Bans Facial Recognition Technology,” Daily Hampshire Gazette, July 3, 2020,\nhttps://www.gazettenet.com/Easthampton-City-Council-passes-ordinance-banning-facial-recognition-survaillance-technology-35048140; see\nJackson Cote, “Northampton Bans Facial Recognition Technology, Becoming Third Community in Massachusetts to Do So,’ MassLive, February 27,\n2020, https://www.masslive.com/news/2019/12/northampton-bans-facial-recognition-technology-becoming-third-community-in-massachusetts-\nto-do-so.html; see CBS SF, “Oakland Officials Take Steps Towards Banning City Use of Facial Recognition Tech,’ July 16, 2019, https://sanfrancisco.\ncbslocal.com/2019/07/16/oakland-officials-take-step-towards-banning-city-use-of-facial-recognition-tech; see sup. note 20; see Alex Newman,\n“Somerville Bans Facial Recognition Technology,’ Patch, June 28, 2019, https://patch.com/massachusetts/somerville/somerville-bans-facial-\nrecognition-technology.\n\n29 NEL.B. 1091 (2020), available at https://www.nebraskalegislature.gov/FloorDocs/106/PDF/Intro/LB1091.pdf; NH H.B. 1642 (2020), available\nat http://gencourt.state.nh.us/bill_status/billText.aspx?sy=2020&id=1 202&txtFormat=pdf&v=current; NY S.B. 7572 (2020), available at https://\nlegislation.nysenate.gov/pdf/bills/2019/S7572; VT H. 929 (2020), available at https://legislature.vermont.gov/Documents/2020/Docs/BILLS/H-\n0929/H-0929%20As%20Introduced.pdf.\n\n30 See sup. note 20.\n\n31. See Tim Cushing, “San Francisco Amends Facial Recognition Ban after Realizing City Employees Could No Longer Use Smartphones,” Techdirt,\nDecember 20, 2019, https://www.techdirt.com/articles/20191219/18253743605/san-francisco-amends-facial-recognition-ban-after-realizing-city-\nemployees-could-no-longer-use-smartphones.shtml. The article describes amendments to San Francisco's ban to permit employees to use the\nbiometric lock feature on city-issued cell phones.\n\n32. Moratoria have been used in surveillance policymaking in the past. For example, in 2013, Virginia placed a two-year moratorium on government use\nof drones. The purpose was to give lawmakers time “to work with law enforcement and other stakeholders to adopt reasonable regulations limiting\nthe use of drones and assuring public participation in the oversight of their use.” See ACLU, “Virginia House of Delegates and Senate Approve\nTwo Year Moratorium on Drones,\" February 6, 2013, https://www.aclu.org/press-releases/virginia-house-delegates-and-senate-approve-two-year-\nmoratorium-drones.",
    "Page_91": "Jameson Spivack & Clare Garvie | A Taxonomy of Legislative Approaches to Face Recognition in the United States | 91\n\n1. Time-bound moratoria\n\nTime-bound moratoria stop virtually all use of face recognition for a predetermined amount of\ntime.*° The purpose of this pause is to give elected officials and the public time to learn about face\nrecognition, reconvening later once the moratorium expires. At this point, legislators can decide if,\nand how, to regulate face recognition.\n\nAt the municipal level, in early 2020, Springfield, Massachusetts, placed a moratorium on face\nrecognition until 2025.* At the state level, a 2020 bill introduced in the Maryland legislature would\nprohibit all public and private use of face recognition for one year.°° The bill does not include any\nother provisions or directions, but rather states the moratorium “shall remain effective for a period\nof one year from the date it is enacted and, at the end of the one-year period, this Act, with no\nfurther action required by the General Assembly, shall be abrogated and of no further force and\neffect.’2°\n\nTime-bound moratoria raise the possibility for public engagement and the future implementation\nof either a permanent ban or strong regulation. These bills prompt discussion within legislative\ncommittees—the members of which are often unfamiliar with face recognition—about the\ntechnology, including its potential harms. There is a risk, however, that if the legislature fails to act\nonce the moratorium period is over, use of face recognition will recommence with no safeguards\nin place.\n\n2. Directive moratoria\n\nDirective moratoria temporarily stop face recognition use while explicitly instructing the legislature\nor other government officials to take additional steps. Often this entails the creation of a task\nforce, working group, or commission organized by either the legislature or attorney general to\nstudy face recognition and recommend policy responses.*”\n\nA bill introduced in Washington state in 2019 proposed a moratorium on government use of\n\nface recognition technology while setting up a task force to study the technology. The task force\nwould be composed of members of historically oversurveilled communities, and would deliver a\nreport to the legislature about potential effects. The bill would also require the attorney general to\nprovide a report certifying the tools in use did not contain accuracy or bias issues, as tested by an\nindependent third party.°°\n\n33. Time-bound moratoria often have carve-out provisions for face recognition use during emergencies or exigent circumstances and in the case of\nmissing children. Some also have carveouts for use in fraud detection by state driver's licensing departments.\n\n34 = Sup. note 23\n\n35 MAS.B. 0857 (2020), available at http://mgaleg.maryland.gov/mgawebsite/Legislation/Details/sb0857. Note: the original bill would prohibit\ngovernment and private use of face recognition for one year. An amendment, discussed at a hearing for the bill, would eliminate the moratorium on\nprivate use.\n\n36 Ibid\n\n37 Provisions creating working groups are often part of non-moratorium regulatory bills, which allow continued use of face recognition until the\nworking group makes further recommendations.\n\n38 WAS.B. 5528 (2019-2020), available at https://app.leg.wa.gov/billsummary?BillNumber=55288lnitiative=false&Year=2019. (Note that this bill is no\nlonger under consideration.)",
    "Page_92": "92 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nDirective moratoria can also pause face recognition use until the legislature passes certain laws.\nIn contrast to the above example, in which decisions about future policy are left to the working\ngroup, this kind of moratorium sets minimum thresholds that future legislation must achieve.\n\nFor example, a bill introduced in Massachusetts in 2019 would place a moratorium on\ngovernment use of biometric surveillance, including face recognition, “[alosent express\nstatutory authorization.” That authorization must provide guidance on who is able to use\nbiometric surveillance systems, their purposes, and prohibited uses; standards for data use\nand management; auditing requirements; and rigorous protections for civil rights and liberties,\nincluding compliance mechanisms.*?\n\nAt the federal level, the Facial Recognition and Biometric Technology Moratorium Act of 2020\nprohibits federal use of certain biometric technologies such as face recognition until Congress\nexplicitly allows their use, with certain limitations. It also conditions federal grant funding to state\nand local agencies on their adoption of moratoria similar to that proposed in the federal bill.“\n\nThese bills encourage jurisdictions to research the full implications of face recognition use and\nengage with members of the public before enacting a more permanent law. Moratoria also limit\nthe risk of reverting to status quo use once the time period is over. However, there is a risk that a\ntask force or commission may not be representative of affected communities; may lack authority;\nor may be inadequately funded, restricting its effectiveness.*!\n\nC. Regulatory Bills\n\nRegulatory bills seek to place restrictions on face recognition’s use, rather than stop it altogether.\nRegulatory bills range along a spectrum from more narrowly focused (regulating only specific\nuses or other elements of face recognition) to broader (regulating more of these elements).\n\n1. Common elements of regulatory bills\nFace recognition bills propose a wide range of measures, including:\n\n* Task force or working group: groups must study face recognition and make policy\nrecommendations.\n\n* Requirements on companies: face recognition vendors must open up their software to\naccuracy and bias testing; commercial users must get consent or provide notice of use, as well\nas allow data access, correction, and removal.\n\n39 MAS.B. 1385 (2019), available at https://malegislature.gov/Bills/191/S1385.\n\n40 See Senators Markey and Merkley, and Reps. Jayapal, Pressley to Introduce Legislation to Ban Government Use of Facial Recognition, Other\nBiometric Technology (June 25, 2020), available at https://www.markey.senate.gov/news/press-releases/senators-markey-and-merkley-and-reps-\njayapal-pressley-to-introduce-legislation-to-ban-government-use-of-facial-recognition-other-biometric-technology.\n\n41 See, e.g., Governor Jay Inslee, Letter To the Honorable President and Members, The Senate of the State of Washington (Mar. 31, 2020), available at\nhttps://crmpublicwebservice.des.wa.gov/bats/attachment/vetomessage/559a6f89-9b73-ea1 1-8168-005056ba278b (vetoing a section of WA S.B.\n620 (regulatory bill) that established a face recognition task force on the grounds that it was not funded in the budget).",
    "Page_93": "Jameson Spivack & Clare Garvie | A Taxonomy of Legislative Approaches to Face Recognition in the United States | 93\n\n* Accountability and transparency reports: implementing agencies must provide details on the\nface recognition tools they use, including how and how often, to elected officials. Some require\nreports before implementation, and many require ongoing reports.*?\n\n* Implementing officer process regulations: officers must receive periodic trainings, conduct\nmeaningful reviews of face recognition search results, and disclose to criminal defendants that\nface recognition was used in identifying them.\n\n* Explicit civil rights and liberties protections: such as prohibiting the use of face recognition\nto surveil people based on characteristics including but not limited to race, immigration status,\nsexual orientation, religion, or political affiliation.\n\n* Data and access restrictions: such as prohibiting the sharing of face recognition data with\nimmigration enforcement authorities, limiting federal access to face recognition systems, and\nprohibiting use on state driver's license databases.\n\n* Targeted bans: prohibiting specific uses, such as live facial recognition, or in conjunction with\nbody-worn cameras or drones. Face recognition use can also be limited by type of crime—for\nexample, only to investigate violent felonies.\n\n* Court order requirements: law enforcement must obtain a court order backed by probable\ncause (or, in some instances, only reasonable suspicion*’) to run face recognition searches.\nSome bills more narrowly apply this requirement to ongoing surveillance or real-time tracking\nonly.*4 This can also apply narrowly to law enforcement seeking face recognition data from\nprivate entities that have collected it, rather than law enforcement searches themselves.\n\n2. Examples of regulatory bills\n\nA narrower bill proposed in Indiana calls for a “surveillance technology impact and use policy,’ but\nincludes no other restrictions.*° In New Jersey, a proposed bill requires the attorney general to\narrange for third-party accuracy and bias testing.*° In 2019, the California legislature passed a law\nprohibiting “a law enforcement agency or law enforcement officer from installing, activating, or\nusing any biometric surveillance system in connection with an officer camera or data collected by\nan officer camera.”4”\n\nAt the other end of the spectrum, broader regulatory bills address multiple elements of face\nrecognition development and use. Though they address a wider range of concerns, this does not\nmean they necessarily address all legitimate areas of concern related to face recognition, or that\nthe proposed rules are substantive or enforceable.\n\n42 Some of these provisions are modeled on the federal Wiretap Act. See 18 U.S.C. § 2519, reports concerning intercepted wire, oral, or electronic\ncommunications, https://www.law.cornell.edu/uscode/text/18/2519.\n\n43 IDH.B. 492 (2020), available at https://legislature.idaho.gov/sessioninfo/2020/legislation/H0492/,\n\n44 See, e.g., sup. note 22.\n\n45 INH.B. 1238 (2020), available at http://iga.in.gov/legislative/2020/bills/house/1238.\n\n46 NJA.B. 989 (2020), available at https://www.njleg.state.nj.us/2020/Bills/A1000/989_I1.PDF.\n\n47 CAAB. 1215 (2019), available at https://leginfo.legislature.ca.gov/faces/billNavClient.xhtm|?bill_id=201920200AB1215.",
    "Page_94": "94 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nFor example, in March 2020, Washington state passed a law that regulates numerous elements\nof face recognition.* The bill includes provisions like these: a pre-implementation accountability\nreport documenting use practices and data management policies for any new face recognition\nsystems; “meaningful human review” when face recognition is used in legal decisions; testing in\noperational conditions; face recognition service APIs made available for independent accuracy\nand bias testing; periodic training for officers; mandatory disclosure to criminal defendants;\nwarrants for ongoing, “real-time” or “near-real-time” use; civil rights and liberties protections; and\nprohibitions against image tampering in face recognition searches.*?\n\nRegulatory bills seek to strike a balance between the benefits and harms of face recognition use.\nFor example, while a separate privacy bill introduced in Washington in 2019 garnered industry\nsupport for its light-touch approach to regulating face recognition, it elicited criticism from privacy\nadvocates for containing loopholes and providing inadequate enforcement mechanisms.°\nNarrowly targeted bills have a greater likelihood of passing through support from well-resourced\nlaw enforcement and company stakeholders, yet often fail to meaningfully protect against the\ntrue scope of possible harms.*' Some advocates are also critical of regulatory bills, particularly\nmore limited ones, for using up available political capital and possibly eliminating the chance of\nstronger regulation in the future.\n\nCONCLUSION\n\nIn the past year, the United States has turned a significant corner in its approach to face\nrecognition. There is now widespread agreement that regulation is necessary, even as lawmakers,\nadvocates, law enforcement, and other stakeholders may disagree on exactly what that looks\nlike.5* The status quo—expansive, unregulated, secret face recognition use—is no longer\nacceptable.\n\n48 See Mariella Moon, “Washington State Approves Stronger Facial Recognition Regulations,” Engadget, March 13, 2020, https://www.engadget\ncom/2020-03-13-washington-facial-recognition-regulations.htm.\n\n49 Sup. note 27\n\n50 See Lucas Ropek, “Why Did Washington State's Privacy Legislation Collapse?,’ Govtech.com, April 19, 2019, https://www.govtech.com/policy/Why-\nDid-Washington-States-Privacy-Legislation-Collapse.html.\n\n51 See, e.g., Ban Facial Recognition (https://www.banfacialrecognition.com), a widely supported petition site calling for a complete ban on police face\nrecognition use.\n\n52 This includes both Republican and Democratic lawmakers, as well as face recognition vendors and law enforcement officials. See, e.g.,\nShirin Ghaffary, “How to Avoid a Dystopian Future of Facial Recognition in Law Enforcement, Vox, December 10, 2019, https://www.vox.com/\nrecode/2019/12/10/20996085/ai-facial-recognition-police-law-enforcement-regulation; see, e.g., Brad Smith, “Facial Recognition: It's Time for\nAction,’ Microsoft on the Issues, December 6, 2018, https://blogs.microsoft.com/on-the-issues/2018/12/06/facial-recognition-its-time-for-action/;\nsee, e.g., Pat Garrett, “Facial Recognition Technology,” Washington County Sheriff, Oregon, June 10, 2020, https://www.co.washington.or.us/sheriff/\nCrimePrevention/facial-recognition-technology.cfm",
    "Page_95": "Jameson Spivack & Clare Garvie | A Taxonomy of Legislative Approaches to Face Recognition in the United States | 95",
    "Page_96": "96 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nBIPA: The Most Important\nBiometric Privacy Law in the US?\n\nWoodrow Hartzog (Northeastern University)\n\nn May 2020, Clearview Al abruptly ended all service contracts with all non-law enforcement\n\nentities based in Illinois.! The reason? It hoped to avoid an injunction and potentially large\n\ndamages under one of the most important privacy laws in America: the Illinois Biometric\nInformation Privacy Act (BIPA).?\n\nEnacted in 2008 in the wake of the bankruptcy of a high-profile fingerprint-scan system,\nlawmakers designed BIPA to provide “safeguards and procedures relating to the retention,\ncollection, disclosure, and destruction of biometric data.” It was the first state law in the US\n\nto specifically regulate biometrics. Remarkably, as the bill was being deliberated by the Illinois\nlegislature, “there were no questions or discussion, and the bill proceeded immediately to a vote\nand unanimously passed in the House.”\n\nBIPA’s substantive rules follow a traditional approach to data protection. Compared to omnibus\nand complex data-protection laws like GDPR, BIPA’s rules are simple. Private entities must get\n\n1 Clearview Al scraped billions of images of people without their permission from social media websites to power their facial recognition app.\nClearview filed legal documents in Illinois stating that “Clearview is cancelling the accounts of every customer who was not either associated with\nlaw enforcement or some other federal, state, or local government department, office, or agency.” See Ryan Mac, Caroline Haskins, and Logan\nMcDonald, “Clearview Al Has Promised to Cancel All Relationships with Private Companies,’ BuzzFeed, May 7, 2020, https://www.buzzfeednews.\ncom/article/ryanmac/clearview-ai-no-facial-recognition-private-companies.\n\n2 740 Ill. Comp. Stat. Ann. 14/15.\n\nCharles N. Insler, How to Ride the Litigation Rollercoaster Driven by the Biometric Information Privacy Act, 43 S. Ill. U. L.J. 819, 820 (2019).\n\n4 Anna L. Metzger, The Litigation Rollercoaster of BIPA: A Comment on the Protection of Individuals from Violations of Biometric Information Privacy,\n50 Loy. U. Chi. L.J. 1051, 1063 (2019).\n\n \n\nwo",
    "Page_97": "Woodrow Hartzog | BIPA: The Most Important Biometric Privacy Law in the US? | 97\n\ninformed consent before collecting or disseminating a person's biometric information.® They\n\nare prohibited from selling, leasing, trading, or otherwise profiting from a person's biometric\ninformation.° Companies must also follow specific retention and destruction guidelines.’ Finally,\nthe statute binds private entities to a standard of care in transmitting, storing, and protecting\nbiometric information that is equal to or more protective than for other confidential and sensitive\ninformation.®\n\nWhile other states such as Texas and Washington have passed standalone biometrics laws,°? BIPA\nis the only biometric privacy law in the United States with a private cause of action. Multiple states\nrequire notice and consent before parties can collect biometric identifiers, require reasonable\nsecurity measures for covered information, restrict the disclosure of biometric identifiers to\nspecific circumstances, and limit companies’ retention of biometric identifiers. But only in Illinois\ncan people who have been aggrieved by companies that violated the rules bring their own action\nagainst the alleged violation instead of waiting for the government to file a complaint or levy a\npenalty.\n\nGiven the limited scope of biometric laws, BIPA’s private cause of action might not seem\nmonumental—yet it is revelatory in how is has distinguished itself from other biometrics laws.\nFor example, Texas and Washington both authorize their state attorneys general to enforce their\nbiometric privacy laws in ways similar to how states enforce their general data-privacy rules.'° In\ncontrast, BIPA's private cause of action has meaningfully shaped the practices of companies who\ndeploy biometrics. It has also forced judges to resolve longstanding issues of injury and standing\nfor privacy violations, among the most vexing issues for all privacy-related claims by plaintiffs in\ncivil courts.\n\nPlaintiffs alleging privacy-related harms from things like data breaches, abusive surveillance,\n\nand unauthorized disclosure have had a notoriously difficult time in court. Some of this is\nattributable to the general erosion of access to the American court system through tort reform.\nPlaintiffs struggle to certify classes for mass litigation, and arbitration clauses are embedded\n\nin the ubiquitous terms-of-use agreements online. But a huge roadblock for plaintiffs is the\nslippery nature of privacy harms.\" Courts have long been skeptical of emotional and reputational\n\n5 740 Ill. Comp. Stat. Ann. 14/15 (\"§15(b) No private entity may collect, capture, purchase, receive through trade, or otherwise obtain a person's or\na customer's biometric identifier or biometric information, unless it [informs the subject what is being collected and receives a written release]\n§15(c) (d) No private entity in possession of a biometric identifier or biometric information may disclose, redisclose, or otherwise disseminate\na person's or a customer's biometric identifier or biometric information unless [the subject of the biometric identifier or biometric information\nconsents or disclosure is required pursuant to a valid warrant or subpoena].”\n\n6 Id. §15(c)\n\n7 Id. § 15(a). (‘A private entity in possession of biometric identifiers or biometric information must develop a written policy, made available to the\npublic, establishing a retention schedule and guidelines for permanently destroying biometric identifiers and biometric information when the initial\npurpose for collecting or obtaining such identifiers or information has been satisfied or within 3 years of the individual's last interaction with the\nprivate entity, whichever occurs first.’).\n\n8 Id. §15(e)\n\n9 Tex. Bus. & Com. Code §503.001; Wash. Rev. Code Ann. §19.375.020; California Consumer Privacy Act (CCPA); N.Y. 2019 Stop Hacks and Improve\nElectronic Data Security (SHIELD) Act (broadening information covered by data breach response law to include biometric information); N.Y. Lab.\nLaw §201-a (prohibiting fingerprinting as a condition of employment); Arkansas Code §4-110-103(7) (amending data breach response law to\ninclude biometric information).\n\n10. For more information on the role of state attorneys general in privacy policymaking, see Danielle Keats Citron, The Privacy Policymaking of State\nAttorneys General, 92 Notre Dame L. Rev. 747, 748 (2016).\n\n11. M.Ryan Calo, The Boundaries of Privacy Harm, 86 IND. L.J. 1131, 1133 (2011); Daniel Solove and Danielle Citron, Risk and Anxiety: A Theory of\nData Breach Harms, 96 Texas Law Review 737 (2018); Ryan Calo, Privacy Harm Exceptionalism, 12 J. TELECOMM. & HIGH TECH. L. 361, 361, 364\n(2014); Paul Ohm, Sensitive Information, 88 S. CAL. L. REV. 1125, 1196 (2015).",
    "Page_98": "98 | Regulating Biometrics: Global Approaches and Urgent Questions\n\ndamages absent a more obvious physical or financial harm.'2 The Federal Trade Commission, the\npremier privacy regulator in the US, creates waves when it even hints at the idea that something\nmore than physical or financial harm or extreme emotional suffering should be considered in\ndetermining whether particular acts are unfair.\" This is to say nothing of the high-stakes debate\nover whether less specific harms such as anxiety and exposure to risk of data abuses, standing\nalone, can constitute an actionable injury in the context of claims of negligence which led to a\ndata breach.'4\n\nBut most discrete and individual privacy encroachments are not catastrophic. The modern\nprivacy predicament is more akin to death by a thousand cuts. Small intrusions and indiscreet\ndisclosures could lead to compromised autonomy, obscurity, and trust in relationships. What's\nmore, it can be difficult to specifically articulate and identify the ways in which data breaches\nmake us more vulnerable. Torts require a clear line of causation from fault to harm. That's usually\nrelatively easy to prove with things like physical injuries from car wrecks, though it is less so with\ndata breaches. Even if it’s clear that a malicious actor has gained access to peoples’ information,\ncriminals don't always straightforwardly use data obtained from a breach to inflict direct financial\nor emotional injury upon the data subject. They often aggregate the information in a pool for\nfurther exploitation or sit on it for years so as not to arouse suspicion. Often people have no idea\nwho wronged them online. American data-privacy law simply isn’t built to respond to this kind of\ndiffuse and incremental harm.\"\n\nBIPA has spurred a key intervention into this morass. Specifically, with BIPA, several judicial\nopinions have affirmed the argument that regardless of whether wrongful acts with biometric\ninformation resulted in chilling effects or financial or emotional injury, the collection and\nprocessing of biometric data without notice and consent is alone a cognizable injury because\nit is an affront to a person's dignity and autonomy. Two cases in particular demonstrate the\nimportance of BIPA.\n\nIn Rosenbach v. Six Flags Entm’t Corp., a mother brought a claim on behalf of her son against\n\nSix Flags amusement park for the company’s failure to give notice or obtain consent when\ncollecting the child's fingerprints for their biometric identification system.'® At issue was whether\nthe plaintiffs alleged sufficient actual or threatened injury to have standing to bring suit. Plaintiffs\ndid not allege financial or extreme emotional harm, but rather a harm resulting solely from the\nprohibited collection and processing of personal biometric data without making the required\n\n12) Id\n\n13. See FT.C. v. Wyndham Worldwide Corp., 10 F. Supp. 3d 602, 623 (D.N.J. 2014), aff'd, 799 F.3d 236 (3d Cir. 2015) (\"The parties contest whether non-\nmonetary injuries are cognizable under Section 5 of the FTC Act....Although the Court is not convinced that non-monetary harm is, as a matter of\nlaw, unsustainable under Section 5 of the FTC Act, the Court need not reach this issue....’)\n\n14 Daniel Solove and Danielle Citron, Risk and Anxiety: A Theory of Data Breach Harms, 96 Texas Law Review 737 (2018).\n\n15 Daniel J. Solove and Danielle Keats Citron, Risk and Anxiety: A Theory of Data-Breach Harms, 96 Tex. L. Rev. 737, 762 (2018) (“Hackers may not\nuse the personal data in the near term to steal bank accounts and take out loans. Instead, they may wait until an illness befalls a family member\nand then use personal data to generate medical bills in a victim’s name. They may use the personal data a year later but only use some individuals’\npersonal information for fraud.’)\n\n16 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 8, 129 N.E.3d 1197, 1200-01 (\"The complaint alleges that this was the first time Rosenbach\nlearned that Alexander's fingerprints were used as part of defendants’ season pass system. Neither Alexander, who was a minor, nor Rosenbach,\nhis mother, were informed in writing or in any other way of the specific purpose and length of term for which his fingerprint had been collected\nNeither of them signed any written release regarding taking of the fingerprint, and neither of them consented in writing ‘to the collection, storage,\nuse sale, lease, dissemination, disclosure, redisclosure, or trade of, or for [defendants] to otherwise profit from, Alexander's thumbprint or associated\nbiometric identifiers or information.”).",
    "Page_99": "Woodrow Hartzog | BIPA: The Most Important Biometric Privacy Law in the US? | 99\n\ndisclosures or obtaining written consent. The Appellate Court of Illinois held that “a plaintiff is not\n‘aggrieved’ within the meaning of the Act and may not pursue either damages or injunctive relief\nunder the Act based solely on a defendant's violation of the statute. Additional injury or adverse\neffect must be alleged.\"'? However, the Supreme Court of Illinois disagreed.\n\nChief Justice Lloyd A. Karmeier, writing the opinion of the court, noted that if the Illinois legislature\nhad wanted to impose an injury requirement beyond disclosure and consent failures, they likely\nwould have done so, as they have in other legislation.'® Using accepted principles of statutory\nconstruction, the court interpreted BIPA’s language that “[a]ny person aggrieved by a violation\n\nof this Act shall have a right of action” according to its commonly understood legal meaning.\nSpecifically, they found that “to be aggrieved simply ‘means having a substantial grievance; a\ndenial of some personal or property right.”'® Justice Karmeier wrote, “A person who suffers actual\ndamages as the result of the violation of his or her rights would meet this definition of course, but\nsustaining such damages is not necessary to qualify as ‘aggrieved.””°\n\nThe court in Rosenbach found that Six Flags violated BIPA’s “right to privacy in and control\n\nover their biometric identifiers and biometric information.’”' BIPA’s disclosure and consent\nrequirements give shape to that right. Thus, if a company violates BIPA, then the data subject is\nlegally “aggrieved” because their right to privacy in and control over their biometric data has been\ncompromised.”\n\nPerhaps the most significant passage in Rosenbach concerned the court's response to the\ndefendant's argument that its BIPA violations were merely “technical” in nature. The court\n\nargued that such a characterization misunderstands not only what the legislature was trying\n\nto accomplish but also the unique nature of how biometrics threaten peoples’ privacy and how\nprocedural rules mitigate that threat. “The Act vests in individuals and customers the right to\ncontrol their biometric information by requiring notice before collection and giving them the power\nto say no by withholding consent.’ Peoples’ unique biometric identifiers, now easily wholesale\ncollected and stored, are not like other kinds of authenticators like passwords and social security\nnumbers because if they are compromised, they cannot be changed. Even beyond identity theft,\nthe court noted that biometrics are particularly concerning because their full risks are not known.\nThe court was direct in its finding:\n\n17 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 15, 129 N.E.3d 1197, 1202 (citing Rosenbach v. Six Flags Entm't Corp., 2017 IL App (2d)\n170317, rev'd, 2019 IL 123186, 129 N.E.3d 1197)\n\n18 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 25, 129 N.E.3d 1197, 1204. (\"Defendants read the Act as evincing an intention by the\nlegislature to limit a plaintiff's right to bring a cause of action to circumstances where he or she has sustained some actual damage, beyond\nviolation of the rights conferred by the statute, as the result of the defendant's conduct. This construction is untenable. When the General Assembly\nhas wanted to impose such a requirement in other situations, it has made that intention clear.’)\n\n19 — Id. (citing Glos v. People, 259 Ill. 332, 340, 102 N.E. 763 (1913)).\n\n20. Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 30, 129 N.E.3d 1197, 1205 (“Rather, ‘[a] person is prejudiced or aggrieved, in the legal sense,\nwhen a legal right is invaded by the act complained of or his pecuniary interest is directly affected by the decree or judgment.”) (citing Glos v. People,\n259 Ill. 332, 340, 102 N.E. 763 (1918)).\n\n21 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 33, 129 N.E.3d 1197, 1206.\n\n22. Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 33, 129 N.E.3d 1197, 1206 (“No additional consequences need be pleaded or proved. The\nviolation, in itself, is sufficient to support the individual's or customer's statutory cause of action.”)\n\n23. Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 34, 129 N.E.3d 1197, 1206.",
    "Page_100": "100 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nWhen a private entity fails to adhere to the statutory procedures, as defendants are\nalleged to have done here, “the right of the individual to maintain [his or] her biometric\nprivacy vanishes into thin air. The precise harm the Illinois legislature sought to prevent\nis then realized.”... This is no mere “technicality.” The injury is real and significant.’*\n\nThe court also highlighted how integral a private cause of action was in implementing the\nlegislature's privacy goals for BIPA. When companies face liability for legal violations without\nburdening plaintiffs to show some additional injury, “those entities have the strongest possible\nincentive to conform to the law and prevent problems before they occur and cannot be undone.”>\nThe court noted that the cost of complying with BIPA is “likely to be insignificant compared to the\nsubstantial and irreversible harm that could result if biometric identifiers and information are not\nproperly safeguarded; and the public welfare, security, and safety will be advanced.”*° According to\nthe court, to force plaintiffs to wait until they could prove some sort of financial or emotional harm\nwould counteract BIPA’s prevention and deterrence goals.\n\nThe other case illustrative of BIPA’s potency, Patel v. Facebook,”’ involves federal standing doctrine\nas required by Article Ill of the US Constitution, a concept linked to injury and harm thresholds.\nStanding doctrine requires that plaintiffs “must have suffered an ‘injury in fact’—an invasion of\n\na legally protected interest which is (a) concrete and particularized; and (b) actual or imminent,\nnot conjectural or hypothetical.’** In a landmark 2016 US Supreme Court case, Spokeo, Inc. v.\nRobins affirmed that an injury-in-fact for information-related complaints like those against data\nbrokers for mishandling, inaccuracies, and indiscretion must be “concrete,” though the court was\nfrustratingly vague about what kinds of harms met that threshold.”?\n\n \n\nPatel v. Facebook involved a complaint that Facebook violated BIPA with its use of facial\nrecognition tools. The Ninth Circuit applied a two-part test to determine “(1) whether the statutory\nprovisions at issue were established to protect [the plaintiffs] concrete interests (as opposed\n\nto purely procedural rights), and if so, (2) whether the specific procedural violations alleged in\n\nthis case actually harm, or present a material risk of harm to, such interests.”°° The Ninth Circuit\n\nanswered yes to both questions.\n\n \n\nIn determining that BIPA protected a concrete interest rather than a purely procedural protection,\nthe Ninth Circuit noted that privacy rights have long served as the basis for legal action in the\ncommon law, constitutional law, and in statues at both the state and federal level. The court noted\nthe significant vulnerabilities created by facial recognition technology:\n\n24\n25\n26\n27\n28\n29\n\n30\n\n \n\nId\n\nRosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 4 37, 129 N.E.3d 1197, 1206.\n\nId\n\nPatel v. Facebook, Inc., 932 F.3d 1264 (9th Cir. 2019), cert. denied, 140 S. Ct. 937, 205 L. Ed. 2d 524 (2020).\n\nLujan v. Defs. of Wildlife, 504 U.S. 555, 561, 112 S.Ct. 2130, 119 L.Ed.2d 351 (1992).\n\nSpokeo, Inc. v. Robins, 136 S. Ct. 1540, 1548-49, 194 L. Ed. 2d 635 (2016), as revised (May 24, 2016). (\"When we have used the adjective ‘concrete;\nwe have meant to convey the usual meaning of the term—‘real’ and not ‘abstract’...Concreteness, therefore, is quite different from particularization.\n‘Concrete’ is not, however, necessarily synonymous with ‘tangible’ Although tangible injuries are perhaps easier to recognize, we have confirmed in\nmany of our previous cases that intangible injuries can nevertheless be concrete.”) The Court went on to muddy the waters in Spokeo even further,\nwriting, “Article IIl standing requires a concrete injury even in the context of a statutory violation. For that reason, [Plaintiff] could not, for example,\nallege a bare procedural violation, divorced from any concrete harm, and satisfy the injury-in-fact requirement of Article III... This does not mean,\nhowever, that the risk of real harm cannot satisfy the requirement of concreteness.” Id at 1549.\n\nPatel v. Facebook, Inc., 932 F.3d 1264, 1270-71 (9th Cir. 2019), cert. denied, 140 S. Ct. 937, 205 L. Ed. 2d 524 (2020) (citing Robins v. Spokeo, Inc.,\n867 F.3d 1108, 1113 (9th Cir. 2017) (Spokeo I!))",
    "Page_101": "Woodrow Hartzog | BIPA: The Most Important Biometric Privacy Law in the US? | 101\n\n[T]he facial-recognition technology at issue here can obtain information that is\n“detailed, encyclopedic, and effortlessly compiled,’ which would be almost impossible\nwithout such technology...Once a face template of an individual is created, Facebook\ncan use it to identify that individual in any of the other hundreds of millions of photos\nuploaded to Facebook each day, as well as determine when the individual was present\nat a specific location. Facebook can also identify the individual's Facebook friends\n\nor acquaintances who are present in the photo...[It] seems likely that a face-mapped\nindividual could be identified from a surveillance photo taken on the streets or in\n\nan office building. Or a biometric face template could be used to unlock the face\nrecognition lock on that individual's cell phone.*!\n\nThe court concluded that “the development of a face template using facial-recognition technology\nwithout consent (as alleged here) invades an individual's private affairs and concrete interests.\nSimilar conduct is actionable at common law.”*? The court cited the language in Rosenbach in\nholding that “the statutory provisions at issue’ in BIPA were established to protect an individual's\n‘concrete interests’ in privacy, not merely procedural rights,’ and that by alleging a BIPA violation\n\nthe “the plaintiffs have alleged a concrete injury-in-fact sufficient to confer Article Ill standing.°°\n\nBIPA has a number of virtues. Thanks to BIPA’s private cause of action, it has become the key\nfor holding companies that use biometric systems accountable.* In the absence of a private\ncause of action, enforcement of biometrics and consumer protection laws is generally left to\nstate attorneys general (AG). While state AGs are certainly key to privacy policymaking in the US,\nthey have limited resources and a host of issues on their plate.°° Even with unlimited bandwidth,\nstate AGs have limited legal ability and political capital to extract the kind of fines necessary to\n\nsufficiently deter companies. The same holds true for the Federal Trade Commission, which is\n\nAmerica’s primary privacy regulator.°°\n\nSo far, only private causes of action seem capable of meaningfully deterring companies\nfrom engaging in practices with biometrics based on business models that inevitably lead to\nunacceptable abuses. Regulators are more predictable than plaintiffs and are vulnerable to\npolitical pressure. Facebook's share price actually rose 2 percent after the FTC announced its\n\nhistoric $5 billion fine for the social media company’s privacy lapses in the Cambridge Analytica\ndebacle.°? Meanwhile, Clearview Al specifically cited BIPA as the reason it is no longer pursuing\nnon-government contracts.*° On top of that, Clearview Al is being sued by the ACLU for violating\n\n \n\n31 Patel v. Facebook, Inc., 932 F.3d 1264, 1273 (9th Cir. 2019), cert. denied, 140 S. Ct. 937, 205 L. Ed. 2d 524 (2020) (citations omitted). BIPA's focus on\nface templates as a creation that grants surveillance and other affordances is properly distinguished from a standard photograph, which does not\nprovide the same affordance of serving as a beacon\n\n32 Id\n\n33 Id. at 1274.\n\n34 Over three hundred class action lawsuits have been brought under BIPA as of June 2019. See Seyfarth Shaw, “Biometric Privacy Class Actions by\nthe Numbers: Analyzing Illinois’ Hottest Class Action Trend,” Seyfarth, June 28, 2019, https://www.workplaceclassaction.com/2019/06/biometric-\nprivacy-class-actions-by-the-numbers-analyzing-illinois-hottest-class-action-trend/.\n\n35 See Danielle Keats Citron, The Privacy Policymaking of State Attorneys General, 92 Notre Dame L. Rev. 747 (2016).\n\n36 See Daniel Solove and Woodrow Hartzog, The FTC and the New Common Law of Privacy, 114 Columbia Law Review 583 (2014); Woodrow Hartzog\nand Daniel Solove, The Scope and Potential of FTC Data Protection, 83 George Washington Law Review 2230 (2015).\n\n37 Charlotte Jee, “Facebook Is Actually Worth More Thanks to News of the FTC's $5 Billion Fine,’ MIT Technology Review, July 15, 2019, https://www.\ntechnologyreview.com/2019/07/15/134196/facebook-is-actually-richer-thanks-to-news-of-the-ftcs-5-billion-fine/\n\n38 Nick Statt, “Clearview Al to Stop Selling Controversial Facial Recognition App to Private Companies,’ Verge, May 7, 2020, https://www.theverge.\ncom/2020/5/7/21251387/clearview-ai-law-enforcement-police-facial-recognition-illinois-privacy-law.",
    "Page_102": "102 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nBIPA by creating faceprints of people without their consent.°? It is no wonder that the private\ncause of action is one of two reasons the United States does not have an omnibus federal\n\ndata privacy law (the other being federal preemption of state privacy frameworks).*° In general,\nbusinesses have opposed private causes of action more than other proposed privacy rules, short\nof an outright ban.‘\n\nEven given BIPA‘s virtues and remarkable effectiveness, it is probably not the best model for\nAmerica’s biometric privacy identity. A private cause of action is necessary, but not sufficient, to\nrespond to the risk of biometrics. BIPA is rooted in a myopic and atomistic “notice and choice”\napproach to privacy.\n\nThere are two major problems with building a biometric privacy framework almost exclusively\naround concepts of transparency and informational self-determination. First, by focusing on\ngiving people control over their data and mandating procedural disclosure obligations, these\nframeworks fail to impose substantive limits on how far companies can encroach into our\n\nlives and how deeply these systems can be entrenched. Procedural transparency and consent\nregimes end up serving as a justification mechanism for all kinds of encroachments without\n\nany clear backstop to how vulnerable we can be made to these systems, so long as we consent.\nFurthermore, BIPA fails to address the issues around privacy in public spaces or in data that\nalready has been exposed to the public. For example, judges considering privacy claims have said\nrepeatedly that “there can be no privacy in that which is already public.”\n\nPrivacy is about more than just informational self-determination. It is about trust, dignity, freedom\nfrom oppression, and laying the preconditions for human flourishing. But those values are not\nnecessarily reflected in the net outcome of billions of individual decisions. Moreover, companies\ncreate structured environments that can heavily influence these discrete choices, with powerful\nincentives to get us to say “yes” any way they can.*°\n\n39 ACLU, American Civil Liberties Union, American Civil Liberties Union of Illinois, Chicago Alliance Against Sexual Exploitation, Sex Workers Outreach\nProject Chicago, Illinois State Public Interest Research Group, Inc., and Mujeres Latinas en Accion v. Clearview Al, Inc., https://www.aclu.org/sites/\ndefault/files/field_document/aclu_v_clearview_complaint_final.pdf.\n\n40 See Makena Kelly, “Congress Is Split over Your Right to Sue Facebook,\" Verge, December 3, 2019, https://www.theverge.com/2019/12/3/20993680/\nfacebook-google-private-right-of-action-sue-data-malpractice-wicker-cantwell; and Emily Birnbaum, “Lawmakers Jump-Start Talks on Privacy Bill,”\nThe Hill, August 7, 2019, https://thehill.com/policy/technology/456459-lawmakers-jump-start-talks-on-privacy-bill; and Ben Kochman, “Senate\nPrivacy Hearing Zeroes in on Right to Sue, Preemption,’ Law360, December 4, 2019 (paywall), https://www.law360.com/articles/1224809/senate-\nprivacy-hearing-zeroes-in-on-right-to-sue-preemption; and Cameron F. Kerry, John B. Morris, Caitlin Chin, and Nicol Turner Lee, “Bridging the Gaps:\nA Path Forward to Federal Privacy Legislation,’ Brookings, June 3, 2020, https://www.brookings.edu/research/bridging-the-gaps-a-path-forward-to-\nfederal-privacy-legislation/.\n\n41 See Issie Lapowsky, “New York's Privacy Bill Is Even Bolder than California's, Wired, June 4, 2019, https://www.wired.com/story/new-york-privacy-\nact-bolder/; DJ Pangburn, “How Big Tech Is Trying to Shape California's Landmark Privacy Law;’ Fast Company, April 25, 2019, https://www.\nfastcompany.com/90338036/how-big-tech-is-trying-to-shape-californias-landmark-privacy-law; John Hendel and Cristiano Lima, “Lawmakers\nWrangle over Consumer Lawsuits as Privacy Talks Drag,” Politico, June 5, 2019, https://www.politico.com/story/2019/06/05/privacy-advocates-\nconsumer-lawsuits-1478824; and “Potentially Expanded Private Right of Action Increases Risk of Class Action Exposure under the California\nConsumer Privacy Act,” Dorsey, May 1, 2019, https://www.dorsey.com/newsresources/publications/client-alerts/2019/04/private-right-of-action-\nincreases-risk.\n\n42 Woodrow Hartzog, The Public Information Fallacy, 98 Boston University Law Review 459 (2019). The FBI alleges it does not need permission to\nconduct surveillance using powerful technologies like cell-site simulators (often called “Stingrays’), so long as they are doing so in public places.\nJudges have refused to punish people for taking “upskirt” photos because the women photographed have no reasonable expectation of privacy “in\npublic,” no matter how fleeting their exposure. Id.\n\n43 Woodrow Hartzog, Privacy’s Blueprint: The Battle to Control the Design of New Technologies (Cambridge, MA: Harvard University Press, 2018).",
    "Page_103": "Woodrow Hartzog | BIPA: The Most Important Biometric Privacy Law in the US? | 103\n\nBIPA is simply not capable of providing individuals with meaningful agency over modern data\npractices.** “Informed consent” is a broken privacy regulatory mechanism.*° It doesn't scale, it\noffloads risk onto the person giving the consent, and it is easily manufactured by companies who\ncontrol what we see and what we can click. Companies deploy malicious user interfaces and a\nblizzard of dense fine print to overwhelm our decision-making process. Consent regimes give\nthe illusion of control while justifying dubious practices that people don't have enough time or\ncognitive resources to understand. Even if people were able to adequately gauge the risks and\nbenefits of consenting to biometric practices, they often don’t have a meaningful choice in front\nof them since they cannot afford to say no and decline a transaction or relationship. While people\nshould be protected regardless of what they consent to, BIPA is largely agnostic to the post-\npermission risks of biometric technologies.\n\nBIPA is far more effective than any other law on the books in protecting our biometric privacy\nwith respect to private companies. However, it does not confront the structural change and\nsubstantive limits necessary for a sustainable future with biometric technologies. BIPA allows\ncompanies to exploit people as their consent is harvested through systems designed to have\nthem hurriedly click “| Agree” and get on with their busy lives. BIPA’s success entrenches an\noverly individualistic and procedural approach to privacy, but has shown lawmakers what is\nindispensable in a biometric privacy framework. It is a guide not just because of what it provides\nbut also because of what it lacks.\n\n44 Woodrow Hartzog, The Case Against Idealising Control, 4 European Data Protection Law Review 423 (2018).\n45 Neil Richards and Woodrow Hartzog, The Pathologies of Digital Consent, 96 Washington University Law Review 1461 (2019); Evan Selinger and\nWoodrow Hartzog, The Inconsentability of Facial Surveillance, 66 Loyola Law Review 101 (2019).",
    "Page_104": "104 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nBottom-Up Biometric Regulation:\nA Community's Response to\nUsing Face Surveillance in\nSchools\n\nStefanie Coyle (NYCLU)\nRashida Richardson (Rutgers University, Al Now Institute, NYU)\n\nrange of school safety issues. Because events like school shootings are both nuanced and\n\npolitically or socially charged, school administrators often rush to embrace technological\ntools without proper consideration or community consultation. The risks, concerns, and\nbureaucratic pitfalls of this approach are most salient in the context of biometric technologies\nused in schools. This case study examines the controversial move by a school district in Lockport,\nNew York, to implement a facial and object-recognition system, and the community-driven\nresponse that sparked a national debate and led to state-wide legislation regulating the use of\nbiometric technologies in schools.\n\n[> ublic schools are increasingly turning to invasive technological solutions to address a wide\n\nFACIAL RECOGNITION SURVEILLANCE IN SCHOOLS\n\nSurveillance technologies are becoming a norm in many public schools.’ School administrators\nare turning to a rapidly growing market of “free”* or subsidized tools that monitor student emails\nfor concerning phrases, measure student bathroom breaks, proctor exams, or provide real-time\n\n1 Circumstances leading to increased adoption of surveillance technologies in schools may vary by country. This chapter focuses on the United\nStates\n\n2 RealNetworks, Inc., “RealNetworks Provides SAFR Facial Recognition Solution for Free to Every K-12 School In the United States and Canada,’ July\n18, 2018, https://www.prnewswire.com/news-releases/realnetworks-provides-safr-facial-recognition-solution-for-free-to-every-k-12-school-in-the-\nus-and-canada-300681977.htm|",
    "Page_105": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 105\n\nalerts of potential crises,? often without proper consideration or community consultation. School\nadministrators have shown significant interest in biometric and other access-control technologies\nfor targeting nuanced school safety issues, with few existing regulations to hold them back. In\n2019, Wired “identified eight public school systems, from rural areas to giant urban districts, that\nhave moved to install facial recognition systems,’ though national use statistics remain unknown.®\n\nBecause these technologies can be enabled as “add-on” features or easily integrated with existing\nsystems used by a school or school district (e.g., closed-circuit television), administrators\n\noften adopt or test them without fully considering the risks they entail.° For example, school\nadministrators may face legal obligations regarding the storage and use of biometric data,\n\nand may not have policies in place to deal with a data breach or sufficient funding available for\nmaintenance of these systems.\n\nBiometric technologies present a veneer of social control or risk mitigation,’ but in reality\n\nthey pose unique social and legal concerns for students, particularly in the K-12 setting.\nThough students have some enhanced data-privacy protections and greater expectations\nregarding government oversight and enforcement,’ they are particularly vulnerable because\n\nthe consequences of privacy and other legal violations may not be immediately felt or obvious.\nMoreover, for decades, critical scholars and educators have criticized these types of reactionary\neducational policies and practices because they are not long-term solutions. Indeed, they tend\nto reproduce, maintain, and naturalize structural inequities that pervade the American education\nsystem and allow policymakers to avoid necessary structural reforms.°\n\nInternationally, some national authorities have opposed facial recognition and other biometric\ntechnologies in schools, finding some uses in schools to be unlawful although not banning the\n\n3 Meghna Chakrabarti and Hilary McQuilkin, “When Schools Use Tech to Monitor Students Online, Class Is Always in Session,” WBUR, October 31,\n2019, https://www.wbur.org/onpoint/2019/10/31/school-surveillance-students-online-privacy-safety.\n\n4 The last substantive guidance on the Family Educational Rights and Privacy Act (FERPA) from the United States Department of Education was\nissued in 2007; it described the application of FERPA to the use of security videos and the transfer of educational records. US Department of\nEducation, “Balancing Student Privacy and School Safety: A Guide to the Family Educational Rights and Privacy Act for Elementary and Secondary\nSchools,’ October 2007, https://www2.ed.gov/policy/gen/guid/fpco/brochures/elsec.htm|\n\n5 Tom Simonite and Gregory Barber, “The Delicate Ethics of Using Facial Recognition in Schools, Wired, October 17, 2019, https://www.wired.com/\nstory/delicate-ethics-facial-recognition-schools/.\n\n6 Bart Simon, “The Return of Panopticism: Supervision, Subjection and the New Surveillance,’ Surveillance & Society 3, no. 1 (September 1, 2002)\n1-20, https://doi.org/10.24908/ss.v3i1.3317. Simon notes that individuals perform compliance around surveillance technologies, which makes\nassessing broader, long-term efficacy or value difficult. See also Clive Norris and Gary Armstrong, The Maximum Surveillance Society: The Rise of\nCCTV (Oxford: Berg, 1999). The authors discuss how public surveillance systems have inherent blind spots that diminish effectiveness or intended\ngoals.\n\n7 Andrew Hope, “Seductions of Risk, Social Control, and Resistance to School Surveillance” in Schools Under Surveillance: Cultures of Control in Public\nEducation, eds. Torin Monahan, R. Torres, and Aaron Kupchik (New Brunswick: Rutgers University Press, 2009), 230-235.\n\n8 See, e.g., Family Educational Rights Privacy Act (FERPA), 20 U.S.C. § 1232g; Protection of Pupil Rights Amendment (PPRA), 20 U.S.C. § 1232h;\nChildren’s Online Privacy Protection Act (COPPA), 15 U.S.C. §§ 6501-6506; Individuals with Disabilities Education Act (IDEA), 20 U.S.C. § 1400 et\nseq; Children’s Internet Protection Act (CIPA), 47 CFR § 54.520; National School Lunch Act, 42 U.S.C. § 1751 (2008). Several states have additional\nstudent privacy laws. FERPA/SHERPA, State Student Privacy Laws (2019), https://ferpasherpa.org/state-laws/ (updated Aug. 6, 2019).\n\n9 See, e.g., Daniel Kiel, “No Caste Here? Toward a Structural Critique of American Education,” Penn State Law Review 119, no. 3 (2015): 611-644,\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=2738704. Kiel argues that colorblind classification methods in education policy help maintain\nracial hierarchies in society and insulate educational institutions from legal, political, and practical interventions. See also Jason P. Nance, “Student\nSurveillance, Racial Inequalities, and Implicit Racial Bias,’ Emory Law Journal 66, no. 4 (2017): 765-837. Nance documents the ways in which\nintensified school surveillance practices and policies disproportionately and negatively affect students of color. See also David Gillborn, “Education\nPolicy as an Act of White Supremacy: Whiteness, Critical Race Theory and Education Reform,’ Journal of Education Policy 20, no. 4 (2005): 485-505,\nhttps://doi.org/10.1080/02680930500132346. Gilborn argues that educational policy in the United Kingdom reinforces and facilitates racial\ninequities.",
    "Page_106": "106 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nuse of the technology in other settings.'° At the same time, several states and localities have\npassed or are considering laws that will ban government use of facial recognition technologies,\nwhich applies to public schools.\" US civil society organizations Fight for the Future and Students\nfor Sensible Drug Policy created a campaign to ban use of facial recognition technology on\ncollege campuses.\"? This campaign successfully forced the University of California Los Angeles\n(UCLA) to reverse its plans to implement facial recognition for campus security,'* and has\ngarnered support from teachers’ unions that are expanding the campaign's call to extend to K-12\nschools.'4\n\nIn 2019, New York became the first state to introduce legislation that explicitly sought to bar\nschool districts from purchasing biometric surveillance technologies, and directed the State\nEducation Commissioner to conduct a study on the use of such technologies in schools and\nissue statewide recommendations.'® This legislation was in response to and in collaboration with\nacommunity-led advocacy effort in Lockport, New York.\n\nLOCKPORT, NEW YORK: A CASE STUDY IN\nCOMMUNITY-DRIVEN PUSHBACK TO FACIAL\nRECOGNITION IN SCHOOLS\n\nIn 2014, New York voters approved the Smart Schools Bond Act (SSBA), which set aside $2\nbillion for school districts to “improve learning and opportunity for students throughout” New\nYork State.'® An inconspicuous provision within the SSBA allowed school districts to utilize the\n\n10 The Administrative Court of Marseille (https://www.laquadrature.net/wp-content/uploads/sites/8/2020/02/1090394890_1901249.pdf) found\nthat the use of facial recognition gates in two French high schools without student consent violated GDPR, despite national government interest\nin establishing a legal framework for public biometric video surveillance. See also European Data Protection Board, “Facial Recognition in Schools\nRenders Sweden's First GDPR Fine,” August 22, 2019, https://edpb.europa.eu/news/national-news/2019/facial-recognition-school-renders-swedens-\nfirst-gdpr-fine_en. The article discusses the issuing of Sweden's first GDPR fine for failure to perform an adequate impact assessment and unlawful\nprocessing of sensitive biometric data to a municipality that used facial recognition technology to monitor student attendance. And see Scottish\nGovernment, “Biometric Technologies in Schools: Draft Guidance for Education Authorities, September 9, 2008, http://www.scotland.gov.uk/\nPublications/2008/09/08135019/0. The document discourages educational authorities from adopting biometric technologies but does not rescind\npreexisting findings that biometric technologies in schools are not illegal, even when introduced without parental consultation.\n\n11 See Kristin Lam, “Portland, the Largest City in Oregon, Plans to Propose First Facial Recognition Ban Affecting Private Companies,’ USA Today,\nDecember 3, 2019, https://www.usatoday.com/story/tech/2019/12/03/facial-recognition-portland-oregon-ban/2601966001/; Tom McKay, “Berkeley\nBecomes Fourth U.S. City to Ban Face Recognition in Unanimous Vote,’ Gizmodo, October 16, 2019, https://gizmodo.com/berkeley-becomes-fourth-\nu-s-city-to-ban-face-recogniti-1839087651; City and County of San Francisco Board of Supervisors, File # 190110, May 31, 2019, https://sfgov.\nlegistar.com/LegislationDetail.aspx?|D=3850006&GUID=12FCSDF6-AAC9-4F 4E-8553-8FOCDOEBD3F6; Christine Fisher, “Oakland Bans City Use of\nFacial Recognition Software,” Engadget, July 17, 2019, https://www.engadget.com/2019/07/17/oakland-california-facial-recognition-ban/; ACLU\nMassachusetts, “Somerville City Council Moves to Ban Government Face Surveillance,” June 24, 2019, https://www.aclum.org/en/news/somerville-\ncity-council-moves-ban-government-face-surveillance.\n\n12 — Stop Facial Recognition on Campus, https://www.banfacialrecognition.com/campus/,\n\n13 Lilah Burke, “Facial Recognition Surveillance on Campus,’ Inside Higher Ed, February 21, 2020, https://www.insidehighered.com/news/2020/02/21/\nucla-drops-plan-use-facial-recognition-security-surveillance-other-colleges-may-be.\n\n14 Boston Teachers Union (@BTU66), “The BTU joins with teachers, students, civil rights and immigrant rights groups across the nation today in\nsupport of a ban on dangerous and inaccurate facial recognition technology in schools and universities,’ Twitter, March 2, 2020, 4:37 p.m., https://\ntwitter.com/BTU66/status/1234593709267865603.\n\n15 Assembly Bill A6787-D (2019-20), https://www.nysenate.gov/legislation/bills/2019/a6787\n\n16 Smart Schools Bond Act (2014), http://www.p12.nysed.gov/mgtserv/smart_schools/home.html.",
    "Page_107": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 107\n\nfunds on “high-tech security” projects, with little guidance. The SSBA is a reimbursement scheme\nthat requires school districts to submit proposals and records of community engagement to the\nSmart Schools Review Board for review and approval.'”\n\nSince 2014, many school districts have applied for and obtained reimbursement for funding to\nacquire student instructional technology, such as laptops, smart boards, and 3D printers, and to\nupgrade aging internet and Wi-Fi systems.\"® As part of the application process, districts must\ncertify that they have engaged stakeholders on the projects— specifically requiring that parents,\nstudents, teachers, and the community be notified of the project. Districts are also required to\nhold a public hearing about the proposals and post the proposal documentation on the district's\nwebsite for at least thirty days.'° Ostensibly, these requirements are designed to ensure that\nschool community members are able to give input about the wisdom of the district's proposed\nuse of state funding.\n\nIn 2016, the Lockport City School District proposed the use of $3,810,833 in SSBA funds for “new\ncameras and wiring...to provide viewing and automated facial and object recognition of live and\nrecorded surveillance video,’ as well as “additional surveillance servers...to provide enhanced\nstorage of recorded video and processing.””° Lockport allegedly purchased the system to prevent\nschool shootings.*! It held its required public hearing on the proposal in the middle of summer\nbreak; unsurprisingly, it did not receive any comments or questions from the community about the\npurchase.” Lockport certified that it had engaged with all required stakeholders and its proposal\nwas approved by the Smart Schools Review Board in November 2017.2\n\nThe first public criticism of the project started in February 2018 when the local newspaper,\n\nthe Lockport Union-Sun & Journal, published a piece on one of two resolutions approved at the\nFebruary 2018 Lockport school board meeting.** The resolution was to allow the use of “a new\nfacial and shape recognition software’ in the school system.*° Lockport resident and parent Jim\nShultz was alarmed by the revelation and wrote an article in his opinion column for the newspaper\nquestioning the need for such a system, underscoring other, better uses for the funds, and\n\n17. The makeup of the Smart Schools Review Board is governed by statute and is comprised of the Commissioner of the New York State Education\nDepartment, the Director of the Office of the Budget, and the Chancellor of the State University of New York system, or their designees. N.Y. Educ.\nLaw § 3641(16)(a)(2)\n\n18 Approved Smart Schools Investment Plans, http://p1232.nysed.gov/mgtserv/smart_schools/ApprovedSSIPs.htm. See, e.g., Adirondack Central\nSchool District's request for upgrades to wireless connectivity and Chromebooks for students (http://p1232.nysed.gov/mgtserv/documents/\nADIRONDACKCSD_ADKInvestmentPlan11.16.pdf).\n\n19 Smart Schools Bond Act Implementation Guidance, p. 19, http://www.p12.nysed.gov/mgtserv/documents/SSBAGuidancerev_6_1_18_Final.pdf.\n\n20 Lockport City School District, Smart Schools Investment Plan (2016-17), http://p1232.nysed.gov/mgtserv/documents/LOCKPORTCITYSD.pdf (last\nmodified October 23, 2017).\n\n21 Lockport City School District, Aegis Security System, May 2019, https://www.smore.com/q13ms.\n\n22 Lockport City School District, August 2016 Regular Board Meeting Minutes, August 17, 2016, https://www.lockportschools.org/site/default.aspx?Pa\ngeType=14&DomainID=1298&PagelD=9632&ModulelnstancelD=11244&View|D=1e008a8a-8e8a-4ca0-9472-a8f4a723a4a7&lsMoreExpandedView=\nTrue\n\n23. Governor Andrew M. Cuomo, “Governor Cuomo Announces Approval of 88 Smart Schools Investment Plans Totaling $75.6 Million,’ November 27,\n2017, https://www.governor.ny.gov/news/governor-cuomo-announces-approval-88-smart-schools-investment-plans-totaling-756-million. Despite\nthe district's certification, David Lowry, the president of the Lockport Education Association, stated that teachers were not consulted in a discussion\nof how to use the funding, as was required. See Tim Fenster, “Trying for More Secure Schools: Lockport District Turning to Facial Recognition\nSoftware,’ Lockport Union-Sun & Journal, March 4, 2018, http://www.lockportjournal.com/news/local_news/trying-for-moresecure-schools-lockport-\ndistrict-turning-to-facial/article_f1cc9cfa-0898-5da0-ac5d-d600df21bed7.html\n\n24 Connor Hoffman, “Lockport Schools Look to Cut Energy Costs,” Lockport Union-Sun & Journal, February 8, 2018, https://www.lockportjournal.com/\nnews/local_news/lockport-schools-look-to-cut-energy-costs/article_6374faf0-7c5b-57d9-be37-a1 542df857a5.html.\n\n25 Ibid",
    "Page_108": "108 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nwarning of the risks to privacy for students and teachers.” Shultz created a petition asking the\nschool district to put the project on hold and to schedule a public hearing to receive input from\nthe community.”’ The petition, signed by over a hundred Lockport residents, raised additional\nquestions about the district's engagement with stakeholders, potential conflicts of interest\nbetween the district and the security consultant that pitched the product, and the effectiveness of\nthe system.\n\nAfter the petition was turned in, the Lockport Journal editorial board called on the district to\npostpone its scheduled vote to award an installation contract for the system.” Despite this call\nto action, the Lockport school board approved the contract.*° Shultz then called for residents\n\nto vote down Lockport’s proposed school budget until the district agreed to stop its facial\nrecognition proposal, but was unsuccessful.*! The Lockport Journal, however, continued to run\npieces on the dangers of facial recognition technology, questioning its accuracy and, in particular,\ndiscrepancies in the systems’ ability to identify people of color.*? Shultz wrote monthly columns\nabout the project, and enlisted local support through Lockport’s Facebook group. He also solicited\nthe help of the New York Civil Liberties Union, which targeted the district and the New York\n\nState Education Department (NYSED) with letters and requests under New York's freedom of\ninformation law.%?\n\nThis advocacy garnered the attention of Monica Wallace, Democrat Assembly member\nrepresenting New York's 143rd Assembly District, which borders Lockport and includes the town\nof Depew.*4 Wallace was aware of the school district’s proposal because the superintendent of\nthe Depew Union Free School District had expressed interest in obtaining the same system.*>\nWallace reached out to advocates in an effort to understand the concerns. As a lawyer and\nparent, she understood the tension between safety and privacy, but worried that the system had\nthe potential to do more harm than good.\n\n26 — Jim Shultz, “Lockport Schools’ Security Plan Warrants Scrutiny,” Lockport Union-Sun & Journal, February 21, 2018, https://www.lockportjournal.com/\nopinion/lockport-schools-security-plan-warrants-scrutiny/article_34f86bd0-849c-5251-8e73-387b90af357b.html.\n\n27 Jim Shultz, “More Questions about School Surveillance Plan,’ Lockport Union-Sun & Journal, March 21, 2018, https://www.lockportjournal.com/\nopinion/more-questions-about-school-surveillance-plan/article_0c5c6948-cded-5fe2-8d9c-c3e1 45ae2ed4. html.\n\n28 ~~ Ibid\n\n29 US&J Editorial Board, “OUR VIEW: Action on School Security Bid Should Be Postponed,’ Lockport Union-Sun & Journal, March 28, 2018, https://www.\nlockportjournal.com/opinion/our-view-action-on-school-security-bid-should-be-postponed/article_464b8f55-733e-554f-9ddb-c066ab3ce169.html.\n\n30 Minutes of the Board of Education of the Lockport City School District, March 28, 2018, https://www.lockportschools.org/site/default.aspx?PageTyp\ne=14&DomainID=1298&PagelD=9632&ModulelnstancelD=118448&View|D=1e008a8a-8e8a-4ca0-9472-a8f4a723a4a7&lsMoreExpandedView=True.\n\n31 Jim Shultz, “Vote ‘No’ on Spy Cameras in Lockport’s Schools,” Lockport Union-Sun & Journal, April 25, 2018, https://www.lockportjournal.com/\nopinion/vote-no-on-spy-cameras-in-lockports-schools/article_6c8cf70a-9551-5974-b396-67bfb5672789.html; Jim Shultz, “Reject False Security:\nVote ‘No’ on Lockport School Budget,’ Lockport Union-Sun & Journal, May 11, 2018, https://www.lockportjournal.com/opinion/reject-false-security-\nvote-no-on-lockport-school-budget/article_672a7a72-4908-588e-becc-Ocddaf4683b0.html.\n\n32 Tim Fenster, “Questions remain on school district security project,” Lockport Union-Sun & Journal, May 13, 2018, https://www.lockportjournal.com/\nnews/local_news/questions-remain-on-school-district-security-project/article_97ce6fd3-490e-5837-834b-217b29474ee5.html.\n\n33. See Connor Hoffman, “Civil Liberties Union Asks State to Halt Lockport Schools Security Project,’ Lockport Union-Sun & Journal, June 18, 2018,\nhttps://www.lockportjournal.com/news/local_news/civil-liberties-union-asks-state-to-halt-lockport-schools-security/article_dbf50305-cd3a-54d9-\n8757-4c874c02c61b.html; Stefanie D. Coyle and John A. Curr Ill to Commissioner MaryEllen Elia, June 18, 2018, https://www.nyclu.org/sites/\ndefault/files/field_documents/june18_2018_nyclu_letter_re_lockport_city_school_district.pdf.\n\n34 Assemblymember Monica P. Wallace, https://assembly.state.ny.us/mem/Monica-P-Wallace/about/.\n\n35 Thomas J. Prohaska, “Lockport Schools Turn to State-of-the-Art Technology to Beef up Security,” Buffalo News, May 20, 2018, https://buffalonews.\ncom/2018/05/20/lockport-schools-turn-to-state-of-the-art-technology-to-beef-up-security/,",
    "Page_109": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 109\n\nIn March 2019, Wallace introduced a bill (A6787) in the New York State Assembly that would\nplace a moratorium on the use or purchase of any “biometric identifying technology” in a school\nsystem.* This broad definition covers not only facial recognition technology, but any technology\nthat uses a fingerprint, handprint, iris, retina, DNA sequence, voice, gait, or facial geometry to\nidentify a person.’” Wallace consulted with advocates on the bill draft to make sure it addressed\nconcerns about the system. The bill requires NYSED to commission a study on the following\nissues: the privacy implications of collecting sensitive biometric information; the risks of false\nidentification for certain subgroups of individuals; whether information from the system might be\nshared with outside individuals, including law enforcement; the length of time information from\nthe system can be retained; the risk of an unauthorized breach; maintenance costs; audits of the\nvendors; and how the technology should be disclosed to the public.** These questions are critical\nfor analyzing the utility, efficacy, and harms of such systems, as is involving the public in decisions\nrelating to the use of surveillance technology in schools.\n\nThe bill requires NYSED to consult with many New York State agencies in preparing the report and\nrequires the Commissioner of Education to hold public hearings seeking feedback from teachers,\nschool administrators, parents, and experts in school safety, data privacy, and civil rights and civil\nliberties.°° In many ways, Wallace's bill mirrors the concerns raised by residents in the community\nand advocates across the state and country. A senate version of the bill was introduced in April\n2019.*° The bill passed the New York Assembly with a bipartisan vote of 128 to 19 on the final day\nof the 2019 legislative session.*' The bill was not considered in the Senate, effectively killing the\nbill for the 2019 legislative session and teeing up a new fight in 2020.\n\nMeanwhile, the community continued its efforts to prevent the use of the technology. Connor\nHoffman, a reporter from the Lockport Journal, attended every school board meeting and\n\nfiled multiple requests for information from the school district and NYSED. Hoffman received\ninformation that had not yet been publicly disclosed about the accuracy rates of Lockport's\nsystem, revealing that Black women are sixteen times as likely as white men to be misidentified\nby the system.” The persistent reporting led to national press coverage, including a feature in the\nNew York Times,*® a New York Times op-ed by Shultz,*# and an MTV News documentary.*° Without\nthe diligence of concerned citizens and the local and national press, Lockport’s acquisition of the\nfacial recognition system and NYSED's failure to regulate this type of technology might have gone\nunnoticed.\n\n36  Sup., note 16.\n\n37 Ibid.\n38 Ibid.\n39 Ibid.\n\n40 Senate Bill S5140-B, https://www.nysenate.gov/legislation/bills/2019/s5140. Introduced by Senator Brian Kavanagh, a senator representing New\nYork's 26th Senate District, which includes Lower Manhattan and parts of Brooklyn.\n\n41 New York State Assembly, A06787 Floor Votes, https://assembly.state.ny.us/leg/?default_fld=&bn=A067878&term=2019&Summary=Y&Actions=Y&-\nText=Y&Committee%26nbspVotes=Y &Floor%26nbspVotes=Y.\n\n42. New York Civil Liberties Union v. New York State Education Department, Index No. 903807-20, Exh. 9, G-2, p. 94, Albany County Supreme Court, June\n18, 2020.\n\n43 Davey Alba, “Facial Recognition Moves into a New Front: Schools,’ New York Times, February 6, 2020, https://www.nytimes.com/2020/02/06/\nbusiness/facial-recognition-schools.htm\n\n44 — Jim Shultz, “Opinion: Spying on Children Won't Keep Them Safe,” New York Times, June 7, 2019, https://www.nytimes.com/2019/06/07/opinion/\nlockport-facial-recognition-schools.html.\n\n45 MTV News (@MTVNEWS), \".@NYCLU says tech companies are using the fear of school shootings to turn students into lab rats for experimental\ntechnology. Meet the 25-year-old reporter @_hoffingtonpost who's exposing the spread of facial recognition in schools,” Twitter, March 12, 2020,\n12:31 p.m., https://twitter.com/MTVNEWS/status/1238140444992774145.",
    "Page_110": "110 | Regulating Biometrics: Global Approaches and Urgent Questions\n\nDespite the pushback, Lockport activated its facial recognition system on January 2, 2020.*°\nParents and students were not notified ahead of the deployment, nor were they given a chance\nto publicly comment on the system.’ It remains unclear why the district pushed ahead with the\nsystem given the concerns of the community.\n\nDespite this setback, there have been promising developments in the community's fight\nagainst this technology. Though the 2020 legislative session was interrupted by the COVID-19\nglobal pandemic, the bill was amended in both houses to increase the amount of time for the\nmoratorium until July 2022 or until the Commissioner of Education explicitly authorizes the use\nof the technology after issuance of the report, whichever occurs later.** The bill has widespread\nsupport from across the state and across the country, even garnering support from the\n\nUnited Federation of Teachers (UFT), the New York City affiliate of the American Federation of\nTeachers.” During the week of July 20, 2020, the bill passed both the Assembly and the Senate,\nand now awaits signature by the Governor to become law.°\n\nIn February 2020, the New York Civil Liberties Union led a town hall in Lockport attended by nearly\nfifty parents and concerned community members about the system. The town hall was headlined\nby Shultz and a recent alumna of the school district.o' For many, it was the first time they had all\nbeen in a room together to discuss the system. Several people asked the school board members\nin attendance why there had not been a community forum sponsored by the district to answer\nquestions and hear concerns. Community members expressed consternation over Lockport’s\nlack of responsiveness, but planned to continue vocalizing their opposition and making their\nvoices heard at school board meetings.\n\nTHE IMPORTANCE OF COMMUNITY-DRIVEN POLICY\nADVOCACY\n\nThe community-driven advocacy response in Lockport demonstrates that persistent and\norganized public scrutiny can illuminate bureaucratic failures, shape necessary reforms, and\nshift narratives. The district's decision to purchase and use a facial recognition system follows a\n\n46 See Troy Licastro, “Lockport City School District Begins Using Facial Recognition System,’ WIVB-TV, January 2, 2020, https://www.wivb.com/\nnews/local-news/niagara-county/lockport/lockport-city-school-district-begins-using-facial-recognition-system/. Lockport deployed its system\nafter receiving permission from NYSED. See also Temitope Akinyemi to Michelle Bradley, https://int.nyt.com/data/documenthelper/6688-nysed-\nlockport/03fc55526445f8ef41 aa/optimized/full.pdf.\n\n47 “Something this big should have been properly told to us.” Christiana Silva, “Facial Recognition Technology Is Taking Over Schools—and Students\nAren't OK with It,” MTV News, March 13, 2020, http://www.mtv.com/news/3159161/facial-recognition-technology-schools-students-respond/.\n\n48  Sup., note 16.\n\n49 This is the same organization that experimented with Clearview Al's technology. See Michael Elsen-Rooney, “Racial Justice Groups Criticize City\nTeachers Union's Use of Controversial Face Recognition Technology,” Daily News, March 27, 2020, https://www.nydailynews.com/new-york/\neducation/ny-uft-facial-recognition-20200327-msxxnS5mmw5dtjmrisjfyaj7xqq-story.html.\n\n50 New York Assembly Bill A6787-D, https://www.nysenate.gov/legislation/bills/2019/a6787.\n\n51 Connor Hoffman, “Spotlight on Facial Recognition: Civil Liberties Union Hosts a ‘Town Hall’ in Lockport,” Lockport Union-Sun & Journal, February 25,\n2020, https://www.lockportjournal.com/news/local_news/spotlight-on-facial-recognition-civil-liberties-union-hosts-a-town/article_32c29556-9f05-\n5934-8c03-29ef5e08212a.html.\n\n52 In addition to concerns over the use of the facial recognition system, community members demanded that a beloved middle-school peer mediator’s\nemployment not be terminated by the school district. Connor Hoffman, “Trying to Have Mr. Cheatham’s Back, Lockport Union-Sun & Journal, January\n23, 2020, https://www.lockportjournal.com/news/trying-to-have-mr-cheatham-s-back/article_a814300a-3e61-1 1ea-b6fa-43627916ccc3.htmI.",
    "Page_111": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 111\n\ncommon yet flawed pattern that government officials rely on to justify the adoption of surveillance\ntechnologies. The school district conflated an abstract or speculative risk to student safety with\nan objective fact of real harm. They installed an unproven and potentially ineffective system\n\nthat will likely undermine students’ civil rights and liberties. Though school safety concerns are\nlegitimate and warrant critical review, the school district's actions demonstrated the inherently\npolitical nature of privileging certain risks and interests over community needs.*? Rather than\nconsult the community to assess actual needs and concerns, the district adopted a technological\nsolution in search of a problem.\n\nThe community-driven advocacy made the flawed logic of this approach apparent. Parents\nshifted the discourse from debating whether the biometric surveillance system was necessary\n\nto focusing on the real harms posed to students if the school district decided to move forward\nagainst community opposition. In particular, Shultz’s early writings on the facial recognition\nsystem emphasized the dangers to student and teacher privacy at a time when the district\ntrivialized the idea that the system could negatively impact student privacy In 2019, however,\nthe superintendent of the school district reluctantly acknowledged that “[p]rivacy matters are a big\ndeal nowadays.”°* This emphasis on privacy is echoed in the current legislation.°°\n\nLockport also failed to acknowledge that deployment of a flawed facial recognition system\n\ncould compound preexisting racial-equity concerns regarding its school safety practices and\npolicies. For instance, the district has struggled to address existing issues of disproportionate\ndiscipline when it comes to students of color, a problem that can be exacerbated by the use of an\ninaccurate and racially biased facial recognition system.”\n\nDuring Lockport's school board elections this year, a new slate of parents, energized by the fight\nagainst facial recognition technology, organized to run for multiple open seats on the board.® This\nyear’s voter turnout was four times higher than the district's five-year average turnout.°? Though\nthe fight in Lockport and New York State continues, this community-driven advocacy effort\ndemonstrates the importance of empowering those directly affected by problematic government\ndecision-making to lead the change they want to see.\n\n53 See Hope, “Seductions of Risk,” in Schools Under Surveillance, for a discussion of how adoption of surveillance technologies can unreasonably\nlimit students’ educational experience. See also Mary Douglas and Aaron Wildavsky, Risk and Culture: An Essay on the Selection of Technological\nand Environmental Dangers (Berkeley: University of California Press, 1983), 29-38. Douglas and Wildavsky describe how labeling risks is a social\nprocess complicated by new technologies that provoke cultural and social reassessments.\n\n54 — Sup., note 31: \"When the privacy issue was raised at that March meeting it was dismissed away as a joke about the likelihood of North Korea\nhacking into student records.”\n\n55 Connor Hoffman, “Lockport School District Cancels Security Contract,” Lockport Union-Sun & Journal, April 11, 2019, https://www.lockportjournal.\ncom/news/local_news/lockport-school-district-cancels-security-contract/article_b5612839-21 1e-53ff-a70b-c1de1f66abd6.html|\n\n56 Sup., note 16. The legislation requires NYSED to consider “the privacy implications of collecting, storing, and/or sharing biometric information of\nstudents, teachers, school personnel and the general public entering a school or school grounds.”\n\n57 During the 2015-2016 school year, Black students made up just 12.3 percent of the student population but represented more than a quarter of the\nstudents receiving out-of-school suspensions. Students of two or more races represented 5.9 percent of the student population in Lockport, but\n15 percent of the students who received out-of-school suspensions. See Civil Rights Data Collection, “Lockport City School District,” 2015, https://\nocrdata.ed.gov/Page?t=d&eid=31 160&syk=88pid=25398&Report=6. See also Paul Hirschfield, “School Surveillance in America: Disparate and\nUnequal,” in Schools Under Surveillance, for a description of the disparate impact of school surveillance.\n\n58 — Jim Shultz, “Time for Change on the Lockport School Board,” Lockport Union-Sun & Journal, May 23, 2020, https://www.lockportjournal.com/\nopinion/jim-shultz-time-for-change-on-the-lockport-school-board/article_142c3905-0612-54d6-8f66-22097b35e5cb.html.\n\n59 Connor Hoffman, “Renee Cheatham wins a seat on the school board,” Lockport Union-Sun & Journal, June 18, 2020, https://www.lockportjournal.\ncom/news/local_news/renee-cheatham-wins-a-seat-on-the-school-board/article_6e3e73ba-b42c-5820-98df-01ccd23d234a.html",
    "Page_112": "AINOW"
}