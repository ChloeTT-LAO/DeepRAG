{
    "Page_1": "MINOW\nREGULATING\nBIOMETRICS\nGlobal Approaches and\nUrgent Questions\nEdited by Amba Kak\nSeptember 2020",
    "Page_2": "REGULATING BIOMETRICS\nEdited by Amba Kak\nSeptember 2020\nInstitute, September 1 2020, https://ainowinstitute.org/regulatingbiometrics.html\nACKNOWLEDGMENTS\nI would like to acknowledge and thank Luke Strathmann for his steadfast editorial support,\nfor her meticulous copyediting. I'm immensely grateful to the authors of the chapters in this\nAlejandro Calcano, Theodora Dryer, Sarah Myers West, Varoon Mathur, and Inioluwa Deborah\nRaji for their detailed feedback and edits; and to Jason Schultz and Kate Crawford for their\nintroductory chapter.\nARTWORK\nThe images used on the cover and throughout this compendium are by Heather Dewey-Hagborg,\nVisiting Assistant Professor of Interactive Media at NYU Abu Dhabi and Artist Fellow at Al Now.\nimages that are detected as \"faces\" or are recognized as her. Starting from primitive curves\nresponse.\nWe see the face reduced to a white circle, laying bare the racial assumptions that underpin facial\ndetection technologies.\nemerge as neighboring facial vectors to the artist's own.\nThe outcome of these experiments is a series of images that give us a window into how we are\nseen by the opaque technologies of artificial intelligence and facial recognition\nLearn more about the project at https://deweyhagborg.com/projects/how-do-you-see-me.\nBYND\nThis work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License",
    "Page_3": "Regulating Biometrics: Global Approaches and Urgent Questions |3\nNOTEFROMTHEEDITOR\nAmid heightened public scrutiny, interest in regulating biometric technologies has grown across\nlocal political contexts. Common across these diverse movements is a growing sense that\nthese technologies are no longer inevitable, accompanied by questions as to whether they are\nnecessary at all. Advocates continue to remind developers, profiteers, and those using and\nregulating these biometric systems that the future course of these technologies must-and\nranging legal regulation in many parts of the world that could alter the future course of these\ntechnologies. Addressing this moment of possibility, this compendium presents eight case\nstudies from academics, advocates, and policy experts offering a variety of perspectives and\nnational contexts. These expert contributors illuminate existing attempts to regulate biometric\nsystems, and reflect on the promise, and the limits, of the law.\nThe compendium begins with an introduction and a summary chapter that identifies key themes\nhighlight the critical research needed to inform ongoing national policy and advocacy efforts to\nregulate biometric recognition technologies.\nMINOW",
    "Page_4": "4 | Regulating Biometrics: Global Approaches and Urgent Questions\nCONTENTS\nChapter 0.\nIntroduction\n6\nAmba Kak \nChapter 1.\nThe State of Play and Open Questions for the Future\n16\nTimeline of Legal Developments\n42\nAmba Kak\nChapter 2.\nAustralian Identity-Matching Services Bill\n44\nJake Goldenfein and Monique Mann\nChapter 3.\nThe Economy (and Regulatory Practice) That Biometrics Inspires:\nA Study of the Aadhaar Project\n52\nNayantara Ranganathan\nChapter 4.\nA First Attempt at Regulating Biometric Data in the European Union\n62\nEls Kindt\nChapter 5.\nReflecting on the International Committee of the Red Cross's Biometric Policy:\nMinimizing Centralized Databases\n70\nBen Hayes and Massimo Marelli\nChapter 6.\nPolicing Uses of Live Facial Recognition in the United Kingdom\n78\nPeter Fussey and Daragh Murray\nChapter 7. A Taxonomy of Legislative Approaches to Face\nRecognition in the United States\n86\nJameson Spivack and Clare Garvie\nChapter 8.\nBIPA: The Most Important Biometric Privacy Law in the US?\n96\nWoodrow Hartzog\nChapter 9.\nBottom-Up Biometric Regulation: A Community's Response to\nUsing Face Surveillance in Schools\n104\nStefanie Coyle and Rashida Richardson",
    "Page_5": "Regulating Biometrics: Global Approaches and Urgent Questions | 5",
    "Page_6": "6 |  Regulating Biometrics: Global Approaches and Urgent Questions\nIntroduction\nAmbaKak\nIthough the terminology varies, we use the phrase biometric recognition technologies\nto describe systems that \"fix\"2 official identities to bodily, physiological, or behavioral\n traits,β providing new ways for individuals to identify themselves, and also to be identified\n or tracked. While fingerprints have the longest history as a marker of identity and continue to be\nretina are proliferating, with significant research exploring their potential large-scale application.\nEmerging areas of interest in this field include using behavioral biometrics like gait (i.e., how\na person walks), keyboard keystroke patterns, and multimodal combinations of biometrics to\nidentify and potentially make inferences about individuals.4\nBeyond identifying people, these systems increasingly claim to be able to infer demographic\ncharacteristics, emotional states, and personality traits from bodily data. (This practice is\nsometimes referred to as \"soft biometrics\"5 in technical literature.) In other words, there has\nbeen a change in questioning that historian Jane Caplan has summarized as a shift from \"What\nperson is that?\" to \"What type of person is that?\"6 Scholars have pointed to the fact that many\nof these systems that claim to detect interior characteristics from physical information are built\nThe terms biometric recognition, identification, and processing are sometimes used interchangeably; other times, they are given more precise and\ndistinct definitions. We use the umbrella terms biometric systems, biometric technologies, or biometric recognition (which has broad cachet in \npolicy discourse) to cover the range of automated technologies that use biometric identifiers to identify, verify, or confirm a person's official identity\nWe also highlight open questions about whether systems that produce other kinds of inferences from bodily data (beyond offcial identity) should \nbe included. This compendium does not analyze the regulation of DNA identifiers. While DNA is recognized as biometric information because of its\nability to uniquely identify individuals, it is generally regulated under separate genetic privacy laws rather than biometric privacy laws, and its use in\nthe criminal justice system has also been regulated under specific rules.\nSee Aaron K. Martin and Edgar A. Whitley, “Fixing identity? Biometrics and the Tensions of Material Practices, Media, Culture & Society 35, no. 1\n(2013): 52-60, https://doi.org/10.1177/0163443712464558.\nSee Kelly A. Gates, Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance (New York: New York University Press, 2011),\nRiad I. Hammoud, Besma R. Abidi, Mongi A. Abidi, Face Biometrics for Personal Identification: Mult-Sensory Multi-Modal Systems (Berlin: Springer,\n2007). See also sections on gait recognition and multimodal biometrics in Global Biometric Authentication and Identification Market: Focus on\nModality (Face, Eye,Fingerprint, Palm,and Vein),Motility, Application, and Technology Trends Analysis and Forecast: 20182023, MarketResearch.\ncom, March 2019, https://www.marketresearch.com/BIS-Research-v4011/Global-Biometric-Authentication-ldentification-Focus-12342594/\nSoft biometrics are defined as ancillary characteristics that provide some information, but not enough to identify a person.See Abdelgader\nAbdelwhab and Serestina Viriri,\"A Survey on Soft Biometrics for Human Identification,\" in Machine Learning and Biometrics, ed. Jucheng Yang\net al. (London: Intech0pen, 2018), https://doi.org/10.5772/intechopen.76021. See also U. Park and A. K. Jain, “Face Matching and Retrieval\nUsing Soft Biometrics,\" IEEE Transactions on Information Forensics and Security 5, no. 3 (September 2010): 406-415, https://doi.org/10.1109/\nTIFS.2010.2049842. And see A. Dantcheva, \"What Else Does Your Biometric Data Reveal? A Survey on Soft Biometrics,\" IEEE Transactions on\nJane Caplan, \"This or That Particular Person': Protocols of Ildentification in Nineteenth-Century Europe, in Documenting Individual Identity: The\nDevelopment of State Practices in the Modern World, ed. Jane Caplan and John Torpey (Princeton: Princeton University Press, 2001). Cf. Jake \nGoldenfein, Facial Recognition Is Only the Beginning, Public Books, January 27, 2020, https://www.publicbooks.org/facial-recognition-is-only-the-\nbeginning/#fn-33473-10.",
    "Page_7": "Amba Kak| Introduction| 7\non debunked and racist scientific and cultural assumptions about who looks like what \"type\" of\neducation.8\nThe rapid expansion of the biometrics industry coincides with advancing technical methods\ntechnologies being produced today are designed to have higher resolution, the ability to work\nfrom greater distances, and night-vision sensors that create the conditions for live facial\nrecognition into a tool that could enable persistent remote surveillance.1\nMeanwhile, the ubiquity of face photographs and voice recordings tagged with people's names\non the internet has greatly decreased the financial and technical resources required to create the\ndatabases that underpin face and voice recognition systems. Clearview Al provides an example.\nfor the three billion labeled face images (matched to names) it scraped from the web without\nAl is not unique. In July 2020, the German digital rights blog Netzpolitik uncovered a Polish\nhundred million images scraped from the web.13 The magnitude of these companies' systems,\nalong with their relative obscurity, demonstrates the way the market for biometric recognition\nsystems consists of a number of nontransparent vendors that sell their systems globally without\nany oversight or scrutiny.14\nIn June 2020, the civil society collective Coalition for Critical Technology called for publishers to stop allpublication of computational research\nclaiming to identify or predict \"criminality\" using biometric data. See Coalition for Critical Technology, \"Abolish the #TechToPrisonPipeline, Medium,\nJune 23, 2020, https://medium.com/@CoalitionForCriticalTechnology/abolish-the-techtoprisonpipeline-9b5b14366b16. See also Lisa Feldman\nBarrett, Ralph Adochs, and Stacy Marsella, \"Emotional Expressions Reconsidered: Challenges to Inferring Emotion from Human Facial Movements,\"\nWhitney,\" Tracking the Affective State of Unseen Persons,\" Proceedings of the National Academy of Sciences, February 5, 2019, https://www.\npnas.org/content/pnas/early/2019/02/26/1812250116.full,pdf; Ruben van de Ven, \"Choose How You Feel; You Have Seven Options, Institute of\nNetwork Cultures, January 25, 2017, https://networkcultures.org/longform/2017/01/25/choose-how-you-feel-you-have-seven-options/; and Lauren\nRhue, \"Racial Influence on Automated Perceptions of Emotions\" Race, Al, and Emotions, November 9, 2018, https://papers.ssrn.com/sol3/papers.\ncfm?abstract_id=3281765.\nSee Jayne Williamson-Lee,\"Amazon's A.l. Emotion-Recognition Software Confuses Expressions for Feelings,\" OneZero, Medium, October 28, 2019\nHegarty.\"A Leader Doesn't Sound Lesbian!: The Impact of Sexual Orientation Vocal Cues on Heterosexual Persons' First Impression and Hiring\nDecision,\"Psychology of Women Quarterly 44,no.2(June 2020):234-55,https://doi.org/10.1177/0361684319891168\nSee Jay Stanley,\"The Dawn of Robot Surveillance: Al, Video Analytics, and Privacy\" ACLU, June 17, 2019, https://www.aclu.org/sites/default/fles/\nfield_document/061819-robot_surveillance.pdf. See also Kelly Gates, “Policing as Digital Platform,\" Surveillance & Society 17, no. 1/2 (2019),htps:/\nojs.library.queensu.ca/index.php/surveilance-and-society/article/view/12940\n10\nSee Vivian Hung, Steven Babin, and Jacqueline Coberly, \"A Market Survey on Body Worn Camera Technologies, National Institute of Justice, Johns\nHopkins University Applied Physics Laboratory, November 2016, https://www.ncjrs.gov/pdffles1/nij/grants/250381.pdf.\n Andreas Nautsch et al.,\"The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps towards a Common \nUnderstanding,\" Proc. Interspeech, 2019, https://arxiv.org/abs/1907.03458\n12\nKashmir Hil, The Secretive Company That Might End Privacy as We Know It, New York Times, January 18, 2020, https://www.nytimes.\ncom/2020/01/18/technology/clearview-privacy-facial-recognition.html.\n13\nSee Daniel Laufer and Sebastian Mainek,\"A Polish Company Is Abolishing Our Anonymity,\" NetzPolitik, July 10, 2020, https://netzpolitik.org/2020/\npimeyes-face-search-company-is-abolishing-our-anonymity/.\n14\nhttps://onezero.medium.com/from-realplayer-to-toshiba-tech-companies-cash-in-on-the-facial-recognition-gold-rush-b40ab3e8f1e2",
    "Page_8": "8 |Regulating Biometrics: Global Approaches and Urgent Questions\ndatabases into biometric recognition systems capable of identifying individuals at a large scale.\nCreating such a system requires a combination of human and computational labor, as well as\na formidable technical, financial, and political infrastructure. Labeling and tagging biometric\ndata in order to make it searchable and to prepare it to feed into machine learning systems\nrequires significant, on-demand human labor power. There is no reliable way to create these\nsystems without such labeled data. At present, much of this data labeling work, often contingent\nand calibrate computer models that are designed and optimized to predict \"matches\" within a\ndatabase, which in turn confirm or reveal identity.16\neffective, necessary, and beneficial. Their core claim is that a strong connection exists between\nbodily traits and identity, and that biometric identifiers can be uniquely attributed to a particular\nbiometric systems, as is the corollary belief that these digital technologies have lower chances of\nThese claims of accuracy and efficiency are often taken as a given, and transposed onto broader\nsocietal and economic values like security, safety, and more efficient service delivery.19 while\nof applications across the world,2° other bodily markers like face, voice, and iris or retina are\n15See Mary L. Gray and Siddharth Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (New York: Houghton Mifflin\n2019).\n16 Neural network architectures like ResNet are widely available for training on individual datasets so developers can more quickly and efficiently build\ntheir own models. See Connor Shorten,\"lntroduction to ResNets,\" Towards Data Science, Medium, January 24, 2019, https://towardsdatascience.\ncom/introduction-to-resnets-c0a830a288a4\n17\nChris Burt,\"Global Biometrics Revenues to Approach $43B by 2025: Market Research Briefs, BiometricUpdate.com, November 28, 2019, https://\n18 Sup. 3. Humans have always identified one another in part based on the way we look or sound. Kelly Gates explains how face recognition has\nproliferated in part because it digitized and automated an already existing documentary regime of face verification, where passport photos were\nroutinely affixed to all manner of government identification documents.\n19\n On the \"securitization of identity\" see generally Nikolas Rose, Powers of Freedom: Reframing Political Thought (Cambridge: Cambridge University\nPress, 1999).\n21\n See Tom Wilson and Madhumita Murgia, \"Uganda Confirms Use of Huawei Facial Recognition Cameras,\" Financial Times, August 20, 2019, https://\nwww.ft.com/content/e20580de-c35f-11e9-a8e9-296ca66511c9; see also Robert Muggah and Pedro Augusto Pereira,\"Brazil's Risky Bet on Tech\nto Fight Crime, InSight Crime, February 19, 2020, https://www.insightcrime.org/news/analysis/brazil-risky-tech-fight-crime/; Vidushi Marda, \"View\nFrom Protests to Chai, Facial Recognition Is Creeping Up on Us\" Economic Times, January 7, 2020, https://carnegieindia.org/2020/01/07/view-\nfrom-protests-to-chai-facial-recognition-is-creeping-up-on-us-pub-80708; and Clare Garvie, Alvaro Bedoya, and Jonathan Frankle, \"The Perpetual\nLine-Up: Unregulated Police Face Recognition in America, Georgetown Law, Center on Privacy & Technology, October 18, 2016, https://www.\nperpetuallineup.org/; Jonathan Hillman and Maesea McCalpin,\"Watching Huawei's'Safe Cities\",\" Center for Strategic & International Studies,\nNovember 4,2019,https://www.csis.org/analysis/watching-huaweis-safe-cities.\n22\n Jennifer Valentino-DeVries,\"How the Police Use Facial Recognition, and Where It Falls Short, New York Times, January 12, 2020, https://www\nnytimes.com/2020/01/12/technology/facial-recognition-police.html",
    "Page_9": "Amba Kak | Introduction | 9\nfor known criminals, and to surveil protests.23 Beyond face-based systems, a recent investigation\nrevealed that dozens of prisons across the US were creating voice-print databases of inmates and\napplying voice recognition to their phone communication to detect when particular voice prints\nappear, track call recipients of interest, and even to identify external people who were contacting\n people in prison most often.24 Meanwhile, amid an environment of heightened xenophobia\ncontrol technology.25 The rationale of security is by no means restricted to law and immigration\nenforcement. It has driven the use of these tools as access control technologies for workplaces\nbehavior to determine entry permissions.26\ntrajectory to the rapid growth of closed-circuit television (CCTV) use through the 2oo0s, despite\nno clear evidence that it was effective in controlling crime. Security systems are often installed as\na reaction to severe crimes, but without evidence that they would have prevented that crime in the\noutrage or rising concerns over crime.\"27\nToday, governments across the world are the largest customer of the global biometrics\nindustry, sustaining and shaping its growth. The development of tools for this wide range of\ngovernment functions is typically outsourced to private firms that develop, market, and maintain\nbiometric identifiers as a routine part of service delivery, with the active support of international\ndevelopment institutions and donor agencies. Biometric IDs are promoted as a means to prevent\n 23\"As Global Protests Continue, Facial Recognition Technology Must Be Banned,\" Amnesty International, June 11, 2020, https://www.amnesty.org/en/\nlatest/news/2020/06/usa-facial-recognition-ban/; Dave Gershgorn, \"Facial Recognition Is Law Enforcement's Newest Weapon Against Protesters,\"\nOneZero, Medium, June 3, 2020, https://onezero.medium.com/facial-recognition-is-law-enforcements-newest-weapon-against-protestors-\nc7a9760e46eb; Blake Schmidt, \"Hong Kong Police Have Al Facial Recognition Tech-Are They Using It against Protesters?\" October 22, 2019,\nhttps://www.bloomberg.com/news/articles/2019-10-22/hong-kong-police-already-have-ai-tech-that-can-recognize-faces; Alexandra Ulmer and Zeba\nSiddiqui, “India's Use of Facial Recognition Tech during Protests Causes Stir\" Reuters, February 17, 2020, https://www.reuters.com/article/us-india-\ncitizenship-protests-technology/indias-use-of-facial-recognition-tech-during-protests-causes-stir-idUSKBN20BOZQ; Jameson Spivack,\"Maryland's\nFace Recognition System Is One of the Most Invasive in the Nation\" Baltimore Sun, March 9, 2020, https://www.baltimoresun.com/opinion/op-ed/\nbs-ed-op-0310-face-recognition-20200309-hg6jkfav2fdz3ccs55bvqjtnmu-story.html\n24\n George Joseph and Debbie Nathan,“Prisons across the US Are Quietly Building Databases of Incarcerated People's Voice Prints\" Intercept, January\n30, 2019, https://theintercept.com/2019/01/30/prison-voice-prints-databases-securus/\n25\n Mark Latonero and Paula Kift,\"On Digital Passages and Borders: Refugees and the New Infrastructure for Movement and Control, Social Media +\nSociety 4, no. 1 (March 2018): 1-11, https://journals.sagepub.com/doi/full/10.1177/2056305118764432.\nepdf/10.1111/j.1467-8322.2009.00654.x; see generally BiometricUpdate.com, Access Control,https://www.biometricupdate.com/biometric-news/\naccess-control-biometric-articles\nClive Norris, Mike McCahill, and David Wood, \"The Growth of CCTV: A Global Perspective on the International Diffusion of Video Surveillance in\nPublicly Accessible Space, Surveillance & Society 2, no. 2/3 (2004), https://doi.org/10.24908/ss.v2i2/3.3369\n28\n The development of tools for government functions is typically outsourced to private frms that develop, market, and maintain these systems. See,\ne.g., \"Global $52Bn Biometric Authentication & Identification Market, 2023: Focus on Modality, Motility, Application and Technology\" Business Wire,\nApril 10, 2019,https://www.businesswire.com/news/home/20190410005486/en/Global-52Bn-Biometric-Authentication-ldentification-Market-2023",
    "Page_10": "10 |Regulating Biometrics: Global Approaches and Urgent Questions\nservice delivery fraud. Many of the ID systems are being rolled out in Global South countries-like\nin India, the Philippines, Kenya, and Brazil-and are not sector-specific, but are instead \"general-\npurpose\" IDs that construct a digital, biometric identity for each resident.29\nOutside of government, biometric recognition systems have been normalized as part of everyday\nexperiences, largely driven by the goal of preventing fraud. Biometric locks are now a staple\nfeature of many smartphones and laptops, and biometric profiles of customers offer a way to\npromoted as a novel and promising consumer advertising technology,30 where individuals can\nfor loyalty programs seamlessly.31\nThe last few years mark a critical juncture, perhaps even a turning point, in the trajectory\nof continued biometric expansion. Civil-society advocates have challenged the foundational\nhighlighting the tangible harms caused by their use. Mounting research demonstrates that these\nsystems perform poorly when used in real-life contexts,32 even when the system meets narrow\nassessment standards that the industry relies on to back claims of accuracy.33 Even systems\nthat boast high accuracy rates have unevenly distributed errors. They perform less well on certain\nminorities, young and old people, members of the disabled community, and manual laborers.35\nBeyond accuracy, research and civil society are also challenging the dominant discourses\n29\n See Frank Hersey,\"2019: A Critical Year for Biometrics and Digital ID in the Global South,\" BiometricUpdate.com, December 23, 2019, https://www.\nbiometricupdate.com/201912/2019-a-critical-year-for-biometrics-and-digital-id-in-the-global-south; and, for an analysis of several national biometric\nID projects, see Alice Munyua and Udbhav Tiwari,\"What Could an 'Open' ID System Look Like?: Recommendations and Guardrails for National\nBiometric ID Projects\" Open Policy & Advocacy, January 20, 2020, https://blog.mozilla.org/netpolicy/2020/01/22/what-could-an-open-id-system-\nlook-like-recommendations-and-guardrail-for-national-biometric-id-projects/\n30\nSee Joseph Turow, The Aisles Have Eyes: How Retailers Track Your Shopping, Strip Your Privacy, and Define Your Power (New Haven: Yale University\nPress, 2016). See also Robert Lee Angell and James R. Kraemer, \"Using Biometric Data for a Customer to Improve Upsale Ad Cross-Sale of Items.\nUS Patent US9031858B2, filed September 26, 2007, and issued May 12, 2015, https://patents.go0gle.com/patent/US9031858B2/en.\n Justin Lee,\"Touche Launches Biometrics-Based Loyalty and Payment Platform, BiometricUpdate.com, January 18, 2017, https://www.\nbiometricupdate.com/201701/touche-launches-biometrics-based-loyalty-and-payment-platform; Esther Fung, \"Shopping Centers Exploring Facial\nRecognition in Brave New World of Retail,\" Wall Street Journal, July 2, 2019, https://www.wsj.com/articles/shopping-centers-exploring-facial-\nrecognition-in-brave-new-world-of-retail-11562068802.\nuk/wp-content/uploads/2018/05/Face-Off-final-digital-1.pdf.\" The Metropolitan Police has the worst record, with less than 2% accuracy of its\nautomated facial recognition 'matches' and over 98% of matches wrongly identifying innocent members of the public, the authors write. See also\nNIST, \"NIST Study Evaluates Effects of Race, Age, Sex on Face Recognition Software, December 19, 2019, https:/www.nist.gov/news-events/\nnews/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software. For error rates relating to biometric capture in India's Aadhaar\nhttps://medium.com/karana/a-critical-examination-of-the-state-of-aadhaar-2018-report-by-idinsight-ef751e24d6c5.\n33\nInioluwa Deborah Raji and Genevieve Fried,\"About Face: A Survey of Facial Recognition Evaluation,\" Meta-Evaluation workshop at AAAl Conference\non Artificial Intelligence, 2020.\n 34  Joy Buolamwini and Timnit Gebru,\"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, Proceedings\n\"Characterizing the Variability in Face Recognition Accuracy Relative to Race,” Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (2019), https://arxiv.org/abs/1904.07325; Cynthia M. Cook et al.,\"Demographic Effects in Facial Recognition and Their\nDependence on Image Acquisition: An Evaluation of Eleven Commercial Systems\" IEEE Transactions on Biometrics, Behavior, and Identity Science \n1, no. 1 (Jan. 2019): 32-41, https://ieeexplore.ieee.org/document/8636231; Inioluwa Deborah Raji and Joy Buolamwini,\"Actionable Auditing\nInvestigating the Impact of Publicly Naming Biased Performance Results of Commercial Al Products,\" Proceedings of the Conf. on Artificial\nIntelligence, Ethics, and Society (2019), https://www.aies-conference.com/2019/wp-content/uploads/2019/01/AIES-19_paper_223.pdf; Morgan\nKlaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker, \"How Computers See Gender: An Evaluation of Gender Classification in Commercial Facia\nAnalysis Services,\" Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (November 2019): 1-33, https://doi.org/10.1145/3359246\n 35 Ursula Rao, “Biometric Bodies, or How to Make Electronic Fingerprinting Work in India, Body & Society 24, no. 3 (September 2018): 68-94, https://\ndoi.org/10.1177/1357034×18780983.",
    "Page_11": "Amba Kak| Introduction|  11\nof security, safety, and efficiency that have driven marketing and demand for these systems\nAdvocates are increasingly asking for whom such systems provide safety and security. The\nclaim that biometric surveillance “makes communities safer\" is heavily marketed but loosely\ndifficult to obtain, but even so, there is strong evidence that these systems are being deployed\nin ways that harm historically marginalized people and communities. For example, in the US,\nsuspects, including cases where facial recognition is used as primary evidence to determine\n guilt.36 This harm is compounded by the systematic denial of basic due process rights during trial,\nwere used.37 Even outside of law enforcement, there is no transparency at all when it comes\nto privately created “watch list\" databases, which are likely being shared and institutionalized\nthrough their use at large-scale events, retail stores, and housing complexes. At a recent Taylor\n Swift concert, all attendees were subject to facial recognition without their knowledge or consent,\nblacklisted unfairly by these systems.38\nAs new applications of these technologies are created, so are new forms of pushback. Real-\nsurveillance can potentially produce chilling effects on the democratic exercise of rights to\nfacial recognition and other property technologies (\"PropTech\") to control access to residential\nbuildings, arguing that they provide landlords with greater unaccountable control, and the ability to\nharass and surveil tenants.43 Meanwhile, coalitions between digital-rights organizations and social\nwelfare and accountability activists have challenged biometric ID schemes for social service\n 36Kashmir Hill,\"Wrongfully Accused by an Algorithm, New York Times, June 24, 2020 https://www.nytimes.com/2020/06/24/technology/facial-\nrecognition-arrest.html; Rashida Richardson, Jason M. Schultz, and Vincent M. Southerland, \"Litigating Algorithms 2019 US Report: New Challenges\nto Government Use of Algorithmic Decision Systems,\" Al Now Institute, September 2019, https://ainowinstitute.org/litigatingalgorithms-2019-us.\npdf; Bob Van Voris, \"Apple Face-Recognition Blamed by N.Y. Teen for False Arrest,\" Bloomberg, April 22, 2019, https://www.bloomberg.com/\nnews/articles/2019-04-22/apple-face-recognition-blamed-by-new-york-teen-for-false-arrest; Jeremy C. Fox,\"Brown University Student Mistakenly\nIdentified as Sri Lanka Bombing Suspect,\" Boston Globe, April 28, 2019, https://www.bostonglobe.com/metro/2019/04/28/brown-student-mistaken-\nidentified-sri-lanka-bombings-suspect/0hP2YwyYi4qrCEdxKZCpZM/story.html\n37\nFor an analysis of criminal due process in the context of facial recognition, see Emma Lux,\"Facing the Future: Facial Recognition Technology Under\nthe Confrontation Clause\" American Criminal Law Review 57, no. 0 (Winter 2020), https:/www.law.georgetown.edu/american-criminal-law-review/\nsample-page/facing-the-future-facial-recognition-technology-under-the-confrontation-clause/.\n38\n See Jay Stanley,\" The Problem with Using Face Recognition on Fans at a Taylor Swift Concert, ACLU, December 14, 2018, https://www.aclu.org/\nblog/privacy-technology/surveillance-technologies/problem-using-face-recognition-fans-taylor-swift; and Parmy Olson, “Facial Recognition's Next\nBig Play: The Sports Stadium, Wall Street Journal, August 1, 2020, htps://www.wsji.com/articles/facial-recognitions-next-big-play-the-sports-\nstadium-11596290400.\n39\n Blake Schmidt, \"Hong Kong Police Already Have Al Tech that Can Recognize Faces\" Bloomberg, October 22, 2019, https://www.bloomberg.com/\nnews/articles/2019-10-22/hong-kong-police-already-have-ai-tech-that-can-recognize-faces.\n40\nAlexandra Ulmer and Zeba Siddiqui, “lndia's Use of Facial Recognition Tech During Protests Causes Stir\" Reuters, February 17, 2020, https:/www\nreuters.com/article/us-india-citizenship-protests-technology/indias-use-of-facial-recognition-tech-during-protests-causes-stir-idUSKBN20BOZQ.\n41\n\"Protesters Demand to Discontinue Facial Recognition Technology\" CBS Detroit, June 16, 2020, https://www.newsbreak.com/michigan/detroit/\nnews/0PLepn7b/protesters-demand-to-discontinue-facial-recognition-technology.\n42\nSpivack,\"Maryland's Face Recognition System Is One of the Most Invasive in the Nation.\"\n43\nTranae Moran, Fabian Rogers, and Mona Patel,\"Tenants Against Facial Recognition,\" Al Now 2019 Symposium, October 2, 2019, https:/\nCrisis Capitalism Comes to Real Estate, Boston Review, May 7, 2020, http:/bostonreview.net/class-inequality-science-nature/erin-mcelroy-",
    "Page_12": "12 |  Regulating Biometrics: Global Approaches and Urgent Questions\nto technical or operational failures in these systems.44 Advocacy campaigns continue to question\nthe use of facial recognition at airports, as well as the reuse of driver's licenses and other civilian\nbiometric databases for immigration enforcement and private investigation purposes.45\nWhile public advocacy is increasing in many parts of the world, and each campaign has its unique\npushback is the insistence that these technologies are not inevitable. Questioning technological\nthese systems that the future course of these technologies must and will be subject to greater\ndemocratic control.\nCalls for regulation include demands to introduce new laws (e.g., like data-protection\nframeworks); to reform and update existing laws (e.g., laws that currently only regulate\nrollout of large-scale biometric ID projects without such laws in place.46 Parliamentarians and\ngovernment officials in the UK47 and a government-appointed advisory group in Scotland have\nacknowledged the need for a broad regulatory framework for biometric use, alongside the need to\nupdate existing laws that only apply to fingerprint and DNA biometrics.48 The clearest pushback\non the idea that these technologies are inevitable has come in the form of advocacy championing\n44 See Jamaican Supreme Court Decision, Julian Robinson v. Attorney General of Jamaica [2019] JMFC Full 04, https://supremecourt.gov.jm/sites/\ndefault/files/judgments/Robinson%2C%20Julian%20v%20Attorney%20General%20of%20Jamaica.pdf;Indian Supreme Court decision, K. S.\nPuttaswamy v. Union of India, Supreme Court of India, Writ Petition (Civil) No. 494 of 2012, https://indiankanoon.org/doc/127517806/; see also\n#WhyID,\"An Open Letter to the Leaders of International Development Banks, the United Nations, International Aid Organisations, Funding Agencies,\nand National Governments,\" Access Now, https://www.accessnow.org/whyid-letter/\n45\n See Project South,\"Georgia Department of Driver's Services Colludes with Immigration and Customs Enforcement and Law Enforcement Agencies\n2020, https://projectsouth.org/wp-content/uploads/2020/03/GA-DDS-ICE-Fact-Sheet-.pdf; Joseph Cox,\"DMVs Are Selling Your Data to Private\nInvestigators, Vice, September 6, 2019, https://www.vice.com/en_us/article/43kxzq/dmvs-selling-data-private-investigators-making-millions-of-\ndollars; Drew Harwell and Erin Cox, “ICE Has Run Facial-Recognition Searches on Millions of Maryland Drivers, Washington Post, February 26,\n2020 https://www.washingtonpost.com/technology/2020/02/26/ice-has-run-facial-recognition-searches-milions-maryland-drivers/; Sushovan\nSircar,\"Selling Vehicle Owners' Data as Public Good', Govt Earns Rs 65 Cr\" Quint, July 10, 2015, https://www.thequint.com/news/india/ministry-of-\ntransport-and-highways-rs-65-crore-driving-license-vehicle-registration-bulk-data-sale;\"Opposition to Face Recognition Software in Airports Due to\nand-privacy-concerns.\n46\ncom/indepth/opinion/kenya-huduma-data-commodification-government-tyranny-190806134307370.html; Vrinda Bhandari,\"Why Amend the\nAadhaar Act without First Passing a Data Protection Bill?, The Wire, January 4, 2019, htps:/thewire.in/law/aadhaar-act-amendment-data-\nprotection.\n47\n\"The Future of Biometrics,\" UK Parliament Post, February 6, 2019, https://www.parliament.uk/documents/post/Future%20of%20Biometrics_\nThrillers Like James Bond, Says Cressida Dick, Telegraph, June 3, 2019, https://www.telegraph.co.uk/news/2019/06/03/public-expect-police-using\nfacial-recognition-technology-seeing/\n48S\n Scottish Government, “Independent Advisory Group on the Use of Biometric Data in Scotland, March 2018, https://www.gov.scot/binaries/\ncontent/documents/govscot/publications/independent-report/2018/03/report-independent-advisory-group-use-biometric-data-scotland/\ndocuments/00533063-pdf/00533063-pdf/govscot%3Adocument/00533063.pdf\nSee generally Melina Sebastian,\"Normalizing Resistance: Saying No to Facial Recognition Technology\" Feminist Media Studies 20, no. 4 (May 2020):\norg.uk/campaigns/stop-facial-recognition/; Urvashi Aneja and Angelina Chamauh,\"We Need to Ban Facial Recognition Altogether, Not Just\nRegulate Its Use, Tandem Research India, January 19, 2020, https:/tandemresearch.org/publications/we-need-to-ban-facial-recognition-altogether-\nnot-just-regulate-its-use.",
    "Page_13": "Amba Kak 丨 Introduction 丨 13\nbiometric ID project, a dissenting opinion from one of the judges also made clear that it's not too\nlate to turn back, ordering that \"all such data be destroyed.\"50 \nAmazon have released calculated public statements in support of facial recognition regulation.5\nMore recently, IBM, Microsoft, Amazon, and others committed to pause their use of these\ntechnologies, citing disproportionate harms to people of color amid widespread antiracist Black\nAmerican policing as we know it, must go.\"53\nAmid heightened public scrutiny, interest in regulating biometric technologies has grown\nsignificantly. The degree of openness to legislating technology varies, and for some countries\nthe next few years do seem poised to produce wide ranging regulation and with that, offer the\n50\nJustice Chandrachur (dissenting opinion) in K. S. Puttaswamy v. Union of India, Supreme Court of India, Writ Petition (Civil) No. 494 of 2012,\nhttps:/indiankanoon.org/doc/127517806/; see also Ashok Kini, \"Jamaican SC Quotes Justice Chandrachud's Dissent to Strike Down Aadhaar-Like\nProgramme, The Wire, April 13, 2019, https://thewire.in/law/jamaica-supreme-court-aadhaar-justice-chandrachud.\n51\nOften these companies publicly champion the need for some \"regulation\" but simultaneously lobby against moratoria and bans.\nKate Kaye,“M, Microsoft, and Amazon's Face Recognition Bans Don't Go Far Enough, Fast Comany, June 13, 2020, htps://ww.fastcompany\n52\n53\nMalkiaDevich-Cyril \"Defund Facial Recognigion, Atlantic, July 5, 2020,https://www.theatlantic.com/technology/archive/2020/07/defund-facial\nrecognition/613771/",
    "Page_14": "14 | Regulating Biometrics: Global Approaches and Urgent Questions\nMonique Mann (Deakin University) track the institutional and political maneuvers that resulted in\nProject: Nayantara Ranganathan (lawyer and independent researcher, India) explains how law and\ninfluenced by the logics and cultures of the project it sought to regulate.\nA First Attempt at Regulating Biometric Data in the European Union: Els Kindt (KU Leuven)\nprovides a detailed account of the European Union's General Data Protection Regulation (GDPR)\nnational laws, she warns of potential loopholes and highlights key areas for reform.\nReflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing\nin the context of humanitarian assistance, with a focus on minimizing the creation of databases\nand risks to vulnerable populations.\nPolicing Uses of Live Facial Recognition in the United Kingdom: Peter Fussey (University of Essex)\nand regulatory tools fell short, with broader lessons for the regulation of LFR in the UK and elsewhere.\nBIPA: The Most Important Biometric Privacy Law in the Us? Woodrow Hartzog (Northeastern\nAct (BlPA) and, more broadly, of the private right of action model. He questions the inevitable\nlimits of a law that is centered on notice and consent\nBottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in\nSchools: Stefanie Coyle (NYCLU) and Rashida Richardson (Rutgers University, A/ Now Institute, NYU)\nand object recognition system. They highlight the community-driven response that incited a national\ndebate and led to statewide legislation regulating the use of biometric technologies in schools",
    "Page_15": "Amba Kak 丨 Introduction 丨 15",
    "Page_16": "16 | Regulating Biometrics: Global Approaches and Urgent Questions\nThe State of Play and\nOpen Questions for\ntheFuture\nAmbaKak\nsystems. We draw insights primarily from the essays in this compendium to surface\nanalysis of the current state of play, we pose open questions about where regulation needs\nrevision, or reimagination. We explore the rapidly evolving policy conversation around new\nconcerns about these technologies.\nRegulation of biometric systems has largely been through data-protection laws. Biometric data\nis typically designated as an especially sensitive category of personal data and is regulated\nthrough a series of restrictions on the collection, retention, and disclosure of such data.1 The 2016\nare currently over 140 countries with national data-protection laws that cover private- and public-\nsector use of data.2 The United States lacks a comprehensive federal data privacy regulation\nsimilar to the GDPR, but state laws like the 2018 Illinois Biometric Information Privacy Act (BlPA)\nThis compendium does not analyze the regulation of DNA identifiers. While DNA is recognized as biometric information because of its ability to\nuniquely identify individuals, it is generallyregulated under separate genetic privacy laws rather than biometric privacy laws, and its use in the \ncriminal justice system has also been regulated under specific rules.\nGraham Greenleaf and Bertil Cottier,\"2020 Ends a Decade of 62 New Data Privacy Laws\"Privacy Laws & Business International Report 163, no. 24\n26 (January 29, 2020), https://ssrn.com/abstract=3572611. (According to this research, the count was at 142 at the end of 2019.)",
    "Page_17": "Amba Kak | The State of Play and Open Questions for the Future |17\nfollow a similar data-protection approach to regulating biometric data.3 Key elements of this\napproach are also included in laws that establish and govern biometric ID systems like India's\n2016 Aadhaar Act,4 Australia's 2019 Identity Services Matching Bill, and Kenya's 2019 Huduma\nNamba bill.6 Section 1 (\"The Data-Protection Lens\") examines these approaches to regulating\nWhile data-protection laws have made fundamental shifts in the way companies and government\napproach the collection, retention, and use of personal data, there are clear limitations on their\nability to address the full spectrum of potential harms produced by new forms of data-driven\nconceptions of harm fails to meaningfully address questions of discrimination and algorithmic\non society, in which imperfect but established methods of accountability, contestation, and\ndemocratic decision-making are undercut by the introduction of opaque automated technology.8\nIn contrast, there has been a flurry of legislation, mostly in the United States, that bans the use\nof these systems in particular sectors, across certain uses, or for lengths of time until there is\na more participatory and deliberative process of decision-making in place. Sector-specific rules\nhave also emerged, like those that address the harms of biometric systems in criminal justice\nconcerns and legal approaches.\nIllinois Biometric Privacy Act, 740 Ill. Comp. Stat. Ann. 14/15. Texas and Washington have passed similar biometric privacy laws (see Tex. Bus.\nNew York Biometric Privacy Act NY SB 1203 in New York are also explicitly modeled after BIPA. For other examples of a data privacy approach\nto biometric data, see California Consumer Privacy Act of 2018 (CCPA) [1798.100 - 1798.199]; N.Y. 2019 Stop Hacks and Improve Electronic Data\n2020, as this compendium was going into print,the National Biometric Privacy Act was introduced by Senators Bernie Sanders and Jeff Merkley\nalong similar lines to BIPA. See The National Law Review, \"National Biometric Information Privacy Act, Proposed by Sens. Jeff Merkley and Bernie\nSanders\", The National Law Review, August 5, 2020 https://www.natlawreview.com/article/national-biometric-information-privacy-act-proposed\nsens-jeff-merkley-and-bernie.\n4\nMinistry of Law and Justice (Legislative Department), Aadhaar (Targeted Delivery of Financial and Other Subsidies, Benefits and Services) Act, 2016,\n5\nParliament of Australia, Identity-Matching Services Bill 2019 (Cth),* https://www.aph.gov.au/Parliamentary_Business/Bills_Legislation/Bills_Search_\nResults/Result?bld=r6387.\nSee generally Martin Tisne,\"The Data Delusion: Protecting Individual Data Isn't Enough When the Harm Is Collective, Stanford Cyber Policy Center,\nn.d., https://fsi-live.s3.us-west-1.amazonaws.com/s3fs-public/the_data_delusion_formatted-v3.pdf; Linnet Taylor, Luciano Floridi, and Bart van\nder Sloot, eds., Group Privacy: New Challenges of Data Technologies (Cham: Springer, 2016), https://www.springer.com/gp/book/9783319466064\nBrent Mittelstadt, “From Individual to Group Privacy in Big Data Analytics\" Philosophy & Technology 30, no. 4 (February 2017): 475-494, https://ink\nspringer.com/article/10.1007/s13347-017-0253-7.\nSee generally Amba Kak and Rashida Richardson,\"Artificial Intelligence Policies Must Focus on Impact and Accountability\" CIGI Online, May 1, 2020,\nhttps://www.cigionline.org/articles/artificial-intelligence-policies-must-focus-impact-and-accountability.",
    "Page_18": "18 I  Regulating Biometrics: Global Approaches and Urgent Questions\nThe following is a summary of the questions that this compendium raises, pointing to research,\nadvocacy efforts:\n1. The Data-Protection Lens\n·How should regulation define \"biometric data\"?\nbiometric surveillance infrastructure by government?\n· Is meaningful notice and consent possible in the context of biometric systems? What are\nthe limitations of a consent-based approach and what supplements or alternatives might be\nrequired?\n2. Beyond Privacy: Accuracy, Discrimination, Human Review, and Due Process\n· How should regulatory frameworks address concerns about accuracy and non-\ndiscrimination in biometric systems?\n· To what extent should regulation rely on standards of performance and accuracy set by\ntechnical standards-setting bodies?\n· Does requiring “meaningful human review\" of biometric recognition systems ensure\noversight and accountability?\n· Should regulatory frameworks create a risk-based classification between \"identification\" and\n\"verification\" uses of biometric recognition?\n· What are the potential risks of a permissive regulatory approach to verification?\nrecognition?\n· Should law enforcement have access to these systems to begin with?\ncharacteristics covered under existing biometric regulation?\n· Should such systems be permitted at all, given their contested scientific foundations\nand mounting evidence of harm?\n3. Emerging Regulatory Tools and Enforcement Mechanisms\n· What different types of \"bans\" and moratoria have been passed in the US over the past few\nyears?\ndeliberative processes are robust?\n· How will bans and moratoria on government use impact the private development and\nproduction of biometric systems?\n· What regulatory tools can be used to create public transparency around the development,\npurchase, and use of biometric recognition tools?\n· What role can community-led advocacy play in shaping the priorities and impact of\nregulation?",
    "Page_19": "Amba Kak | The State of Play and Open Questions for the Future | 19\nSECTION 1.THEDATA-PROTECTIONLENS\nHow should regulation define \"biometric data\"?\nUnder the dominant data-protection approach to regulating biometric systems, meeting\nas the object of regulation.\nIn laws that establish and regulate biometric ID systems, the definition of biometric data\nbiometrics collected under these projects.\nIn defining biometric data and systems, the law not only reflects but also entrenches certain\nexample, the GDPR states that biometric data is bodily, physiological, and behavioral data that\n'allow or confirm the unique identification of that natural person,\"9 while the Illinois BlPA provides\nan exhaustive list of identifiers that count as biometric data and requires that they are “used to\nidentify an individual.\"1o These foundational beliefs about the ability of biometric data to uniquely\nidentify an individual are not stable and are today highly contested. Research has demonstrated\nvulnerabilities as people age, and the inaccuracies that creep in when these systems are used\nto identify people of color, young and old people, manual laborers, those who speak English\nwith a non-native accent, and many other demographic and phenotypic subgroups.11 Biometric\nequivalence to real identity as given.\nIn data-protection laws, fulfilling the definition of \"biometric data\" or \"biometric information\" is the\nprocessing, storage, and use) at which these protections are activated. When part of a broader\npersonal data-protection law like the GDPR, such definitions usually work to distinguish biometric\ndata from other kinds of personal data in order to offer special or stricter levels of protection. In\nlaws like BlPA, which is solely focused on biometric data, the definition determines the scope of\nthe legislation as a whole. Laws that establish government biometric ID projects, on the other\nto these categories of data collected at will.\nArticle 4(14), GDPR.\nSection 10, BIPA.\n See footnotes 34 and 35 of this compendium's Introduction.\nand iris patterns, toe impression, voice waves, blood typing, photograph, or such other biological attributes of an individual obtained by way of\nbiometrics.\n13\n The Aadhaar Act, 2016 defines biometric information as follows: \"biometric information' means photograph, fingerprint, iris scan, or such other\nbiological attributes of an individual as may be specified by regulations.",
    "Page_20": "20 | Regulating Biometrics: Global Approaches and Urgent Questions\nAs legislation moves beyond traditional data privacy and security concerns to questions of\naccountability around whether or how to use these systems, and who is liable if these systems\nfail, some recent bills shift the focus from \"data\" to \"systems.\" For example, recent US legislation\nthat restricts the use of these technologies does not define biometric data at all, and instead\nfocuses on \"face recognition systems\" or \"services,\"14 or face/biometric \"surveillance systems\"\nas the object of regulation.15 The definitions of these terms emphasize the eventual uses or\nintentions that drive the application of such systems in social contexts (such as surveillance)\nidentification, verification, or tracking)\nThe legal definition of biometric data is usually restricted to data that has been\ntechnically processed for use in an algorithmic system by specifying a particular\ndigital representation (e.g., \"template\" or \"print\"). The definition often explicitly excludes\nphotographs and voice recordings and creates a loophole around foundational stages\nwhen data is collected, processed, and stored.\nThe definition of biometric data has generally been restricted to mean a technically defined digital\nrepresentation of bodily traits that have already been processed for machine or algorithmic\nanalysis. This is suggested by semi-technical terms like “templates,\" “geometry” “prints,16 or, in the\nvoice recording. Modern machine learning systems do not need \"all\" of the data, but instead rely\nexpressly excluded from the definition of biometric data in the BIPA18 and the GDPR.19\n14   Recent US legislation uses terms like \"facial recognition systems\" as in the City of Boston Ordinance Banning Face Surveillance Technology, https://\n6280, is defined as “technology that analyzes facial features and is used by a state or local government agency for the identification, verification, or\npersistent tracking.\" See SB 6280 (2019-20), https://app.leg.wa.gov/billsummary?BillNumber=6280&Year=2019&Ilnitiative=false\n15  The phrase “face surveillance systems\" appears in S.4084 (Facial Recognition and Biometric Technology Moratorium Act introduced in June 2020,\ngovernment-use-of-facial-recognition-other-biometric-technology. The term “biometric surveillance systems\" is used in the California A.B. 1215\nthat bans face recognition on body-worn cameras; the bil refers to\"any computer software or application that performs facial recognition or other\nbiometric surveillance.\n16\n See, for example,the way biometric data is defined in BIPA and other state biometric privacy laws, which specify “facial geometry, \"voice prints, and\nfingerprints.\n17\n18\n Several defendants sued under BIPA have unsuccessfully argued before the courts that the specific exclusion of photographs means that\ninformation derived from photographs should also be excluded. In Rivera v. Google, Inc. (238 F. Supp. 3d 1088, 1095 (N.D. ll. 2017), Google argued\nthat its facial templates were derived from photographs, and therefore excluded from BIPA's definition of biometric information, but the court held \nthat templates were stillbiometric identifiers, since BIPA does not qualify the definition of biometric identifiers based on how they were derived. See\nMathew T. Hays, \"Technology Defendants Continue to Test Whether the Illinois BIPA Law Can Cope with Modern Facial Recognition Technology\"\nFirewall, December 6, 2019, htps://www.thefirewall-blog.com/2019/12/technology-defendants-continue-to-test-whether-the-illinois-bipa-law-can-\ncope-with-modern-facial-recognition-technology/.\n19   Recital 51 of the GDPR notes that \"the processing of photographs should not systematically be considered to be processing of special categories\nof personal data as they are covered by the definition of biometric data only when processed through a specific technical means allowing the\nunique identification or authentication of a natural person.",
    "Page_21": "Amba Kak |  The State of Play and Open Questions for the Future |  21\nThis narrow technical definition of biometric data creates a set of troubling loopholes. In her\nof so called “raw\" biometric data adversely limits the impact of the GDPR. She argues that\nheightened protections, like explicit consent, are foregone in the initial stage of data collection\nand storage (such as when a photo is uploaded to a social media site) and that use of such data\nsuch data has been collected.\ncommercial and government surveillance systems are developed and deployed today. The\nto create face-name databases. These databases are the foundation of sophisticated and covert\nsurveillance tools created by private firms, who often do so in secret and proceed with almost no\nThe definition of biometric data offered in the California Consumer Protection Act (CCPA) of 2018\nRather than the current representation of the data, CCPA's definition focuses on the ability to\nextract anidentifiertemplate that can be algorithmicallyprocessed in order to determine whether\nit falls within the scope of the law.22\nWhy have data-protection laws had limited effectiveness in\ncurbing the expansion of biometric surveillance infrastructure by\ngovernment?\nscrutiny of the link between the means and the ends, the broad rationale of security and\nefficiency in service delivery has usually served to enable rather than restrict the use of\nbiometric systems.\n 20  See Daniel Laufer and Sebastian Mainek, \"A Polish Company Is Abolishing Our Anonymity,\" NetzPolitik, July 10, 2020, https://netzpolitik.org/2020\npimeyes-face-search-company-is-abolishing-our-anonymity/; and Louise Matsakis,\"Scraping the Web Is a Powerful Tool. Clearview Al Abused It\nWired,January 25,2020,https://www.wired.com/story/clearview-ai-scraping-web/\n21\nJeremy Kirk, \"Hey Alexa. Is This My Voice or a Recording?\" BankinfoSecurity, July 6, 2020, https://www.bankinfosecurity.com/hey-alexa-this-my-\nvoice-or-recording-a-14562; George Joseph and Debbie Nathan, \"Prisons across the U.S. Are Quietly Building Databases of Incarcerated People's\n 22 See Section 3(e) of the CCPA of 2018: “Biometric information includes, but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm,\nvein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted.\"\n(emphasis mine)",
    "Page_22": "22 |Regulating Biometrics: Global Approaches and Urgent Questions\nand reflected in a number of data-protection laws across the world.23 They require that any\ninfringement of privacy or data-protection rights be necessary and strike the appropriate balance\nbetween the means used and the intended objective. The proportionality principle is also central\ncompeting right or public interest.24\nsimilar data-protection laws, the \"data minimization\" provision in Article 5 requires that entities\nlimit personal data collection to that which is \"adequate, relevant and limited to what is necessary\nData Protection Law Enforcement Directive (DP LED) requires a higher standard of whether that\nbiometric data collection is \"strictly necessary.\"28\nTaken seriously, these provisions question whether the collection of biometric data is necessary\nrecognition in schools on the grounds that its use for attendance was a disproportionate means\nto achieve this goal when far less intrusive means exist.30 The French Data Protection Authority\nFrance.31\nIn Ben Hayes and Massimo Marelli's chapter, they explain how the International Committee of the\nRed Cross (ICRC) applied data-protection proportionality principles to the use of biometrics for aid\ndistribution to people in need of humanitarian assistance. While the ICRC eventually determined\nuse to a \"token-based system\" (i.e., a card on which people's biometric data is securely stored)\nThe ICRC decided not to collect, retain, or further process people's biometric data, and therefore\nnot to establish a biometric database. If people want to withdraw or delete their biometric data\nthey can either return the card or destroy it themselves.\n23  See Privacy International, \"Towards International Principles on Communications Surveillance, November 20, 2012, https://privacyinternational.org/\nblog/1360/towards-international-principles-communications-surveillance. The article refers to a meeting of experts in Brussels in October 2012. See\nalso European Data Protection Supervisor,\"Necessity & Proportionality,\" n.d., https://edps.europa.eu/data-protection/our-work/subjects/necessity-\nInternational Data Privacy Law 1, no. 4 (November 2011): 239-248, https://doi.org/10.1093/idpl/ipr015\n24\n See generally Alec Stone Sweet and Jud Mathews, “Proportionality Balancing and Global Constitutionalism,\" Columbia Journal of Transnational Law\n47, no. 72 (2008-09): 112.\n25\n See Article 5(c), GDPR on \"data minimization,\" and Article 9, GDPR on processing of special categories of personal data\n 26  See Article 5(b), GDPR on \"purpose limitation.\n See Article 5(e), GDPR on \"storage limitation.\n28 See Article 10, DP LED on processing of \"sensitive categories\" of personal data.\n See Els Kindt,\"Biometric Applications and the Data Protection Legislation: The Legal Review and the Proportionality Test,\" Datenschutz und\nYue Liu, \"The Principle of Proportionality in Biometrics: Case Studies from Norway\" Computer Law & Security Review 25, no. 3 (December 2009):\n237-250\n30\n Sofia Edvardsen,\"How to Interpret Sweden's First GDPR Fine on Facial Recognition in School,\" IAPP, August 27, 2019, https:/iapp.org/news/a/how\nto-interpret-swedens-first-gdpr-fine-on-facial-recognition-in-school/\n 31 EDRi,\"Ban Biometric Mass Surveillance, May 13, 2020, https://edri.org/wp-content/uploads/2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf",
    "Page_23": "Amba Kak | The State of Play and Open Questions for the Future | 23\nUnfortunately, the application of these principles to challenge the creation of biometric systems\nand databases is rare, especially during the key initial or \"pilot\" stages before these systems are\nbuilt and used.32 More often than not, inquiries into \"necessity\" are structured to enable rather\nnotoriously broad but powerful rationale of \"efficiency\" or “law and order\" and \"national security\"\nserve to grant most government uses of biometrics a free pass without any evidence-based\nscrutiny of the relationship between means and ends.33 As noted in the European Digital Rights\nattributed to the European Union's inadequately resourced and politically disempowered National\nData Protection Authorities. On the other hand, in countries that still lack data-protection laws\nrights-infringing means to achieve that goal.34\nimmigrationbiometric databases.Driver's licenseface databases are akeysitefor this\nkind of \"function creep\" and require urgent policy intervention.\nThe \"purpose limitation\" principle restricts the use of data for purposes beyond what it was\noriginally collected for; a specified purpose must not be used for another \"incompatible\" purpose\nYet pervasive \"security\" imperatives often blur the boundaries between criminal, welfare,\nand immigration processes and, consequently, obfuscate what is perceived and understood\nas a \"compatible\" purpose. Under the US federal Secure Communities program (S-COMM),\nstates submit fingerprints of arrestees to criminal as well as immigration databases, allowing\nImmigration and Customs Enforcement (ICE) to access this information.35 ICE has also requested\nface recognition searches of driver's license databases in multiple states in the Us.36 In Australia,\nthe Home Affairs department has been centralizing state driver's license face databases to use\n 32  See EDRi,\"“Evidence on Biometrics and Fundamental Rights\" July 2020 (submitted to the European Commission consultation and on file with the\nauthor) for a list of projects, including multiple case studies from Europe that were not properly assessed due to the claim that they were in the\n'experimental\" or \"pilot\" stage.\n 33 In the European context, see Fundamental Rights Agency (FRA), “Facial Recognition Technology: Fundamental Rights Considerations in the Context\nof Law Enforcement, 2020, https://fra.europa.eu/sites/default/fles/fra_uploads/fra-2019-facial-recognition-technology-focus-paper-1_-en.pdf. The\nauthors note than “laln objective of general interest-such as crime prevention or public security-is not, in itself, sufficient to justify an interference\nwith fundamental rights, meaning that the Law Enforcement Directive's data protections must apply.\n349\n See Mariyan Kamil, \"The Aadhaar Judgment and the Constitution - ll: On Proportionality\" Indian Constitutional Law and Philosophy, September 30,\n2018, https://indconlawphil.wordpress.com/2018/09/30/the-aadhar-judgment-and-the-constitution-i-on-proportionality-guest-post/.\ndetention. See Jennifer Lynch,\"From Fingerprints to DNA: Biometric Data Collection in U.S. Immigrant Communities and Beyond,\" American\nImmigration Council, May 23, 2012, https://www.americanimmigrationcouncil.org/research/fingerprints-dna-biometric-data-collection-us-\nimmigrant-communities-and-beyond; see also ACLU, \"Secure Communities ('S-Comm)\" n.d., https://www.aclu.org/other/secure-communities-s-\ncomm.\nhttps://medium.com/center-on-privacy-technology/ice-searches-of-state-drivers-license-databases-4891a97d3e19\n 37  See Jake Goldenfein and Monique Mann, \"Australian Identity-Matching Services Bill' in this compendium.",
    "Page_24": "24 | Regulating Biometrics: Global Approaches and Urgent Questions\nsecurity purposes in limited circumstances,38 and the National Crime Bureau has publicly stated\nand remove the purpose limitations on data use.\nThe failure of proportionality safeguards is also borne out in the context of centralized\ntypically been implemented as technocratic exercises driven by executive agencies, often with\nthe glaring absence of law. Even as advocacy efforts focus on demanding legal frameworks to\nensure legislative and public scrutiny, legislation often comes too little, too late. For one, many\nscrutiny altogether.40 In other cases, weak procedural safeguards are proposed, but the broader\ncentralization of power in a few agencies remains unchallenged.\nIn Jake Goldenfein and Monique Mann's chapter, they argue that the Australian Identity Services\nconclude that \"a true proportionality analysis\" might have questioned whether a centralized facial\no uosuedxa buinuuoo algeua peun sem u puoado si buwe su, Aean ui nng 'pne\nsurveillance systems.\nwith existential threats, like the potential of being invalidated by the highest courts, data-privacy\nrules have repeatedly been held up as an adequate safeguard against the concerns raised, leading\ndecade after biometric data collection began. This massive delay is even more concerning given\nthe absence of a data-privacy law that applied to government agencies. In her contribution to this\ncompendium, Nayantara Ranganathan challenges foundational assumptions about the role of the\n38Vrinda Bhandari and Renuka Sane, \"A Critique of the Aadhaar Legal Framework\" National Law School of India Review 31, no. 4 (2019):1-23.\n39\n Aman Sharma,\"Cannot Share Aadhaar Biometric Data for Crime Investigations\" Economic Times, June 22, 2018, https://economictimes.indiatimes\ncom/news/politics-and-nation/cannot-share-aadhaar-biometric-data-for-crime-investigations-uidai/articleshow/64700379.cms\n See Nayantara Ranganathan's chapter in this compendium, \"The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the\nAadhaar Project, in which she describes the truncated and legally dubious passage of the Aadhaar as a “\"money bill\" See also ADC, \"ADC Files an\nAction of Unconstitutionality before GCBA after the Introduction of Face Recognition System, November 6, 2019, https://adc.org.ar/en/2019/11/06/\nadc-files-an-action-of-unconstitutionality-before-gcba-after-the-introduction-of-face-recognition-system/\n41\n See commentary on the Kenyan data-protection law by Rasna Warah,\"Data Protection in the Age of Huduma Namba: Who Will Benefit?\" Elephant\nPraavita, \"Can the Aadhaar Act and a Data Protection Act Coexist?\" The Wire, July 30, 2018, https:/thewire.in/law/can-the-aadhaar-act-and-a-data\nprotection-act-coexist",
    "Page_25": "Amba Kak | The State of Play and Open Questions for the Future | 25\nlaw in relation to these projects, characterizing regulation as a legitimizing force that reflects the\ninterests of the powerful actors that drive these systems. She argues that Aadhaar's regulation\nfunctioned to \"consolidate the developments of the first seven years of the project, and also\npresented a revisionist history of the actual goals of the project, obscuring the stakes for private\ninterests...[M]any of the problems with Aadhaar should not be understood as failures of law or\nregulation, but products of law and regulation.\nIs meaningful notice and consent possible in the context of biometric\nwhat supplements or alternatives might be required?\nbeen a cornerstone of biometric regulation, yet the well-documented limitations of this\nrequirements as a core component of meaningful notice.\nWhile notice and consent has traditionally been the cornerstone of data-protection and privacy\napproaches globally, its limitations have been laid bare in recent years, leading to skepticism\nBen Hayes and Massimo Marelli explain why the Red Cross removed consent as a legal \"ground\n of processing\"43 in emergency humanitarian contexts where, the authors argue, consent can never\nbe assumed to be “freely given.\"\nAt the same time, the individual's right to refuse or revoke permission for the collection or use of\ntheir data has been an important tool in challenging biometric systems like live facial recognition\nin public spaces that are designed to evade such active permission. As described in Woodrow\nHartzog's chapter, under BlPA, the failure to obtain consent from individuals before using their\nbiometric data has led to several successful lawsuits against some of the largest tech companies\nin the world and is the basis for the lawsuit recently launched against Clearview Al.44\n42  For a rejection of the idea of privacy as \"control,\" see generally Ruth Gavison,\"Privacy and the Limits of Law, Yale Law Journal 89, no. 3 (January\n Grounds of processing is a legal term of art popularized by the GDPR. Itrefers to a number of legal justifications, of which at least one must be\nmet in order to \"process\" (i.e., collect, store, use, etc.) personal data. Grounds of processing include consent, performance of a contract, legitimate\nnterests of a business, and so or\n44  Woodrow Hartzog, \"BIPA: The Most Important Biometric Privacy Law in the US?\" in this compendium.",
    "Page_26": "26 |  Regulating Biometrics: Global Approaches and Urgent Questions\nthat hold irrespective of whether consent is obtained. By contrast, US state laws like BlPA focus\nAs Hartzog concludes, BlPA has done \"very little to bring about the kind of structural change and\nsubstantive limits necessary.\" For one, he explains how most of us are simply \"not capable of\nmeaningfully exercising our agency over modern data practices\" and argues that BIPA provides\nlittle protection from the \"post-permission risks\" of biometric technologies.46 This underscores the\nrobust notice and consent regime.\nEmerging regulatory approaches for algorithmic or Al systems include a broader understanding\nof notice that goes beyond simply informing the individual that algorithmic tools are being\nused. These newer approaches also take into account how these systems work, the context in\nwhich these systems are used, and what criteria are informing algorithmic decisions. This broad\nscope will be especially valuable in regulating biometric systems that serve purposes beyond\nidentification and verification. The Illinois Artificial Intelligence Video Interview Act is an example\nof a notice provision tailored to the specific context of job interviews; it requires that all job\napplicants be informed when Al systems used to assess their performance as a candidate are\ninformation about \"how the artificial intelligence works and what general types of characteristics\nit uses to evaluate applicants.\"47 whether such explanations are possible, and whether they can\nin the context of a job interview, have yet to be seen.\nSee Article 5 GDPR including principles of data minimization, collection limitation, purpose limitation, storage limitation principles.\n45\n46Ibid.\n 47 See Section 5, \"Artificial Intelligence Video Interview Act,\" http:/www.ilga.gov/legislation/publicacts/fultext.asp?Name=101-0260",
    "Page_27": "Amba Kak | The State of Play and Open Questions for the Future | 27\nSECTION 2. BEYOND PRIVACY: ACCURACY\nDISCRIMINATION, HUMAN REVIEW,AND DUE\nPROCESS\nHow should regulatory frameworks address concerns about accuracy\nand non-discrimination in biometric systems?\nTo what extent should regulation rely on standards of performance\nand accuracy set by technical standards-setting bodies?\nWhile accuracy and discrimination concerns are at the forefront of public debate\nHowever, recent legislation and advocacy efforts in the US have mandated accuracy and\nnondiscrimination audits for facial recognition systems, going as far as to require such\naudits as a condition for lifting a moratorium on use.\nWhile technical standards (e.g., NiST's Face Recognition Vendor Test) are evolving to\naccount for bias and inaccuracy, they generally underperform in \"real-life\" contexts and are\nlimited in their ability to address the broader discriminatory impact of these systems as\nthey are applied in practice. If such standards are positioned as the sole check on facial\nrecognition systems, they could function to obfuscate, rather than mitigate, harm.\nAccuracy and \"error rates\" metrics are a staple of the mainstream conversations around\nbiometrics and are used as a tool in the machine learning field to compare systems and assess\nprogress. Accuracy claims have been a simple way for those developing, marketing, and\napplying these systems to \"prove\" effectiveness, and to demonstrate that automation offers an\nsystems that boast high accuracy rates according to such narrow metrics have been shown\nto perform less well when accuracy rates are stratified across demographics like age, race,\ngender, and disability.48 \"Errors\" in these systems are not evenly distributed, and reflect historical\n 48   Joy Buolamwini and Timnit Gebru, \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,\" Proceedings\nCharacterizing the Variability in Face Recognition Accuracy Relative to Race, Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (2019), https://arxiv.org/abs/1904.07325: Cynthia M. Cook et al.,\"Demographic Effects in Facial Recognition and Their\nDependence on Image Acquisition: An Evaluation of Eleven Commercial Systems\" IEEE Transactions on Biometrics, Behavior, and Identity Science\n1, no. 1 (Jan. 2019): 32-41, https:/ieexplore.ieee.org/document/8636231; Inioluwa Deborah Raji and Joy Buolamwini,\"Actionable Auditing\nInvestigating the Impact of Publicly Naming Biased Performance Results of Commercial Al Products\" Proceedings of the Conf. on Artificial\nIntelligence, Ethics, and Society (2019), https://www.aies-conference.com/2019/wp-content/uploads/2019/01/AlES-19_paper_223.pdf; Morgan\nKlaus Scheuerman, Jacob M. Paul, and Jed R. Brubaker,\"How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial",
    "Page_28": "28 | Regulating Biometrics: Global Approaches and Urgent Questions\nhave called for auditing on accuracy across specific demographic and phenotypic subgroups,\naccompanied by measures that can close performance gaps where they arise.49\nTo accomplish such audits, many are turning to technical standard-setting bodies that set\nRegulators and lawmakers have also begun to take notice, calling for audits by technical\nstandards-setting bodies that set benchmarks for accuracy, performance, and safety. In March\n2020, the UK Equality and Human Rights Commission called to suspend the use of facial\nethnicity, gender, age, or disability status. If independent testing reveals \"material unfair\nperformance differences\" companies are required to rectify the issues within ninety days. Another\nproposed federal bill (S.2878: the Facial Recognition Technology Warrant Act of 2019) requires\nfederal law enforcement agencies to work with NIST50 to establish testing systems to ensure\nWhile these standards are a step in the right direction, it would be premature to rely on them to\nassess performance, and they do not adequately capture the broader discriminatory impacts\nthese systems might have when they are used. First, researchers and advocacy organizations\nhave found that many of the systems that \"pass\" current benchmark evaluations continue\ndocument and communicate the histories and limits of benchmarking datasets, and thus no way\nto determine their applicability to a particular system or suitability for a given context.\nMoreover, creating a solely technical threshold to judge discriminatory impact can distort the\ngroups. For example, facial recognition systems are deployed disproportionately in minority\ncommunities, so even the most accurate systems will be discriminatory. They also “run the risk of\nproviding 'checkbox certification, allowing vendors and companies to assert that their technology\nis safe and fair without accounting for how it will be used, or its fitness for a given context.\"52\n49\n See Raji and Buolamwini, \"Actionable Auditing,\" and Buolamwini et al., Gender Shades, MIT Media Lab, http://gendershades.org.\n 50  NIST is a non-regulatory federal agency within the US Department of Commerce. Its mission is to promote US innovation and industrial\ncompetitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve quality of\nlife. Auditing protocols like the NIST 2019 Face Recognition Vendor Test (part three) evaluate whether the algorithm performs differently across\ndifferent demographicsin the dataset.\n51 See Inioluwa Deborah Raji and Genevieve Fried,\"About Face: A Survey of Facial Recognition Evaluation, Meta-Evaluation workshop at\nAAAI Conference on Artificial Inteligence (forthcoming, 2020); Pete Fussey and Daragh Murray, \"lndependent Report on the London \nMetropolitan Police Service's Trial of Live Facial Recognition Technology,\" The Human Rights, Big Data and Technology Project, July 2019,\nhttps://48ba3m4eh2bf2sksp43rq8kk-wpengine.netdna-ssl.com/wp-content/uploads/2019/07/London-Met-Police-Trial-of-Facial-Recognition-Tech-\nReport.pdf\n52\n Written Testimony of Meredith Whittaker, US House of Representatives Committee on Oversight and Reform, “Facial Recognition Technology (Part\nIl: Ensuring Commercial Transparency & Accuracy, January 15, 2020, https://oversight.house.gov/sites/democrats.oversight.house.gov/fles\ndocuments/WRITTEN%20testimony%20-%20MW%20oversight.pdf/",
    "Page_29": "Amba Kak | The State of Play and Open Questions for the Future |  29\nsystems ensure oversight and accountability?\nRecent legislation includes provisions that mandate \"meaningful human intervention'\nin the results of biometric systems. However, a large body of research suggests that\nthe people who review the results of biometric systems overwhelmingly overestimate\ncredibility, and often respond inaccurately and with bias.\n22 of the GDPR, for example, includes a restriction on \"solely automated decisions,\" and requires\nhuman intervention when automated systems impact “legal or similarly significant\" decisions\nrecognition law similarly includes provisions for \"meaningful human review\" and periodic officer\ntraining as conditions for the use of biometric technology. Human review is defined in terms of\n\"review or oversight by one or more individuals...who have the authority to alter the decision under\nreview.\"54\nHowever, a large body of research demonstrates that human intervention in these systems does\nnot address major concerns about transparency or control. Individuals who review results are\noften unable to accurately evaluate the quality or fairness of the outputs, and often respond to\npredictions in biased and inaccurate ways.55 The ACLU has pointed to the imprecisely defined\nnotion of meaningful human review as \"deeply flawed\" given its vague definition. They maintain\nsystems in sensitive social domains like welfare and criminal justice.56\nfacial recognition \"matches\" often defer to the algorithm's output, despite the known inaccuracy\n\"humans overwhelmingly overestimated the credibility of the system.\"57 The Indian government\nestablished a system of \"manual overrides\"to address the issue of biometric errors that lead to\n 53 Jennifer Lee,\"We Need a Face Surveillance Moratorium, Not Weak Regulations: Concerns about SB 6280\" ACLU, March 31, 2020, htps://www.aclu-\nwa.org/story/we-need-face-surveillance-moratorium-not-weak-regulations-concerns-about-sb-6280\n54\nSection 2(7),Washington Senate Bl 6280, htt:/lawflsext.leg.wago/biennium/2019-20/Pdf/Bils/Senate%20Passed%20Legislature/680-S.\nPL.pdf?q=20200331083729.\nwww.benzevgreen.com/wp-content/uploads/2019/02/19-fat.pdf; Ben Green and Yiling Chen,\"The Principles and Limits of Algorithm-in-the-Loop\nRisk Assessment in Action, Minnesota Law Review 103, no. 303 (2018), https://dx.doi.org/10.2139/ssrn.3016088; Berkeley J. Dietvorst, Joseph P.\nSimmons, and Cade Massey, \"Algorithm Aversion:People Erroneously Avoid Algorithms after Seeing Them Err\" Journal of Experimental Psychology\n144, no. 1 (February 2015): 114-126, https://psycnet.apa.org/fulltext/2014-48748-001.html; Amirhossein Kiani et al.,\"Impact of a Deep Learning\nAssistant on the Histopathologic Classification of Liver Cancer,\" npj Digital Medicine 3, no. 23 (2020), https://doi.org/10.1038/s41746-020-0232-8\n56\nLee,\"We Need a Face Surveillance Moratorium.",
    "Page_30": "30 |Regulating Biometrics: Global Approaches and Urgent Questions\nlegal norms did not always govern the behavior of those operating the biometric systems on\nthe ground. Those managing these systems often failed to exercise this option and refused\novercoming technological failure.\"59\nfor human oversight. This would include an assessment of the gaps in knowledge, biases,\nor inefficiencies that limit accountability and prevent human operators from assessing or\nanticipating problems with these systems.\nShould regulatory frameworks create a risk-based classification\nbetween \"identification\" and “verification\" uses of biometric\nrecognition?\nWhat are the potential risks of a permissive regulatory approach to\nverification?\nless risky use compared to identification (1:n) in terms of accuracy, data security\nvulnerabilities, and the capacity for meaningful consent.\nHowever, any broad-brush permissive approach to verification in the law should be\navoided. Even if participation in a verification system is with knowledge, these systems\nspaces or services\nare who they claim to be through a one-to-one match that queries biometric information (e.g.,\nor 1:n, is a more technically involved process that compares the biometric information of an\n58\nstatic/5b7cc54eec4eb7d25f7af2be/t/5bbd2874c8302561862f03d4/1539123330295/State+of+Aadhaar+Report_2017-18.pdf.\n59\nSee Bidisha Chaudhuri, “Paradoxes of Intermediation in Aadhaar: Human Making of a Digital Infrastructure, Journal of South Asian Studies 42\n(2019):572-587, https://doi.org/10.1080/00856401.2019.1598671.\n 60 See Stan Z. Li and Anil K. Jain, Handbook of Face Recognition (New York: Springer, 2005), 1-15",
    "Page_31": "Amba Kak | The State of Play and Open Questions for the Future | 31\nunknown person against a database of many people's biometric data. An algorithm determines\nif the person is represented in the database and who they might be. Some identification systems\nRecent official policy documents62 as well as data-protection authorities in the EU63 suggest\nthat verification is an inherently less risky use of biometrics in terms of accuracy, data security\ncommon example used to demonstrate these claims, and San Francisco recently amended its\nissued cell phones.64 By contrast, some of the most controversial reported cases of facial\nrecognition largely pertain to identification (1:n) systems like live facial recognition (LFR), which\nhas a record of high error rates.\nidentification, people could be unaware of being identified, which increases the error rates.65 Any\ngeneral assumption that verification systems involve the active and targeted participation of the\nindividual, however, rests on shaky foundations. While these systems might have higher accuracy\nrates than identification systems, they are still predictive and not immune to the same kinds of\nerrors and biases across lines of race, gender, and other demographic traits. More importantly,\neven if participation is done with volition and knowledge, these systems might not afford\nindividuals real choice when they act as gatekeepers to access to essential spaces and services\nwell as in the use of biometric systems in humanitarian contexts.67\n61Ibid.\n See Luana Pascu,\"New EU Al Strategy Puts Remote Biometric Identification in 'High-Risk' Category\" BiometricUpdate.com, February 19, 2020,\nhttps://www.biometricupdate.com/202002/new-eu-ai-strategy-puts-remote-biometric-identification-in-high-risk-category; Paul de Hert and Koen\nInstitute for Law, Technology, and Society, April 2013,https://m.coe.int/progress-report-on-the-application-of-theprinciples-of-convention-\n108/1680744d81; see also, e.g., Article 29-Data Protection Working Party,\"Working Document on Biometrics (WP 80)\" 2003, https://ec.europa.\neu/justice/article-29/documentation/opinion-recommendation/fles/2003/wp80_en.pdf; “Opinion 02/2012 on Facial Recognition in Online and\nMobile Services (WP192)\" March 23, 2012, https://www.pdpjournals.com/docs/87997.pdf; and \"Opinion 3/2012 on Developments in Biometric\nTechnologies (WP193)\" April 2012, https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2012/wp193_en.pdf; and\nchallenges.\nSee French data-protection authority CNIL, Communication central storage fingerprint, 2007; and cf. Els Kindt, Privacy and Data Protection Issues of \nBiometric Applications: A Comparative Legal Analysis (Dordrecht: Springer, 2013), 540\n64\n Tim Cushing,\"San Francisco Amends Facial Recognition Ban after Realizing City Employees Could No Longer Use Smartphones, Techdirt,\nDecember 20, 2019, https://www.techdirt.com/articles/20191219/18253743605/san-francisco-amends-facial-recognition-ban-after-realizing-city-\nemployees-could-no-longer-use-smartphones.shtml.\n65\n See Li and Jain, Handbook of Face Recognition, 1-15.\n66\n Abdi Latif Dahir and Carlos Mureithi,\"Kenya's High Court Delays National Biometric ID Program,\" New York Times, January 31, 2020, https://www.\nnytimes.com/2020/01/31/world/africa/kenya-biometric-ID-registry.html; see also reportage on the farcical nature of \"consent camps\" for Aadhaar\ndiscussed during the People's Tribunal on Aadhaar-related Issues, February 28, 2020, https://threadreaderapp.com/thread/1233608762604154880.\nhtml.\n67\nPetra Molnar, \"The Contested Technologies That Manage Migration, CIGI Online, December 14, 2018, https://www.cigionline.org/articles/contested-\ntechnologies-manage-migration",
    "Page_32": "32 |Regulating Biometrics: Global Approaches and Urgent Questions\nProponents of permissive approaches to verification typically argue that these systems\ninvolve local data storage, which minimizes the data security risks that come with centralized\ndatabases. However, access control at borders, airports, and buildings often centralize biometric\nauthentication systems for access to services, and many of these systems maintain centralized\nstorage and authentication records.68 Risks associated with biometric use are certainly\nWhat kinds of due process safeguards are required for law\nenforcement use of biometric recognition?\nShould law enforcement have access to these systems to begin with?\nfor ongoing surveillance, restricting the use of facial recognition to serious crimes, and\nthem.\nWhile facial recognition has received special regulatory attention, these tools should be\nunderstood as part of a broader set of algorithmic police surveillance tools, including\ndrone surveillance, license plate recognition, and predictive policing\nconviction), and the circumstances under which it should be deleted from such databases (for\nexample, if a person is never convicted or if a conviction is overturned). The increasing shift to\nuse of face and voice identifiers has exacerbated some of these existing concerns and created\nnew ones. Indeed, law enforcement use of facial recognition has been the subject of intense\npublic and regulatory scrutiny recently. These systems have misidentified people and been\ndisproportionately used to target communities of color. Moreover, the vast majority of cases\ninvolving face recognition searches are not disclosed, depriving defendants of the ability to\nchallenge evidence that could determine their fate in criminal trials.70 \n 68   For an enumeration of concerns with centralized or centrallylinked biometric ID infrastructures, see Access Now, #WhyiD campaign, 2019, https://\n69  See Robyn Caplan et al,\"Data & Civil Rights: Biometric Technologies in Policing,\" Data & Society, October 27, 2015, https://datasociety.net/library/\ndata-civil-rights-biometric-technologies-in-policing/; Brandon L Garrett,\"DNA and Due Process,\" Fordham Law Review 78, no. 6 (2010): 2919-2960\nElizabeth N. Jones, \"Spit and Acquit': Legal and Practical Ramifications of the DA's DNA Gathering Program, Orange County Lawyer Magazine 51, no.\n9 (September 2009), http://papers.ssrn.com/sol3/papers.cfm?abstractid=1809997\n70\n See section on Florida case involving FACES facial recognition system used in the case against Wilie E. Lynch in Rashida Richardson, Jason M.\nSchultz, and Vincent M. Southerland,\" Litigating Algorithms 2019 US Report: New Challenges to Government Use of Algorithmic Decision Systems,",
    "Page_33": "Amba Kak | The State of Play and Open Questions for the Future |  33\nOutside of complete bans, there are multiple proposals that seek to regulate different aspects\nregulatory approaches in the US that focus on limiting the use of facial recognition. Some\ncourt order to run facial recognition searches,71 as well as one that would require that defendants\nhave access to source code and other information necessary to exercise their due process rights\nwhen algorithms are used to analyze evidence in their case.72\nlicense plate recognition, and predictive policing.73 These systems raise similar challenges for\nestablished principles around procedural fairness, such as notice, hearing, the disclosure of\nevidence, establishing reasons for decisions, and the ability to challenge these decisions\npublic and regulatory scrutiny. Advocacy demands range from requiring a specific\nauthorizing law to calls to ban law enforcement use of LFR altogether\nLive facial recognition systems in public spaces are particularly controversial. Typically, cameras\nare deployed at a fixed location and the list of people who are identified is communicated to law\nenforcement officers on the ground.74 Despite LFR's implications for privacy, criminal due process,\nand freedom of speech or expression, these tools have largely been rolled out without undergoing\npublic and parliamentary scrutiny.\nand discuss how the London Metropolitan Police successfully argued before the High Court\nthat LFR was part of their inherent powers, and thus did not need new legislation to explicitly\nauthorize its use.'6 The case is on appeal, but one of the factors that contributed to the Court's\ndecision was the notion that LFR was not \"invasive\" technology and therefore did not require\nmunicipal government pushed through a resolution with truncated processes that authorized\nthe use of these systems with minimal safeguards. Advocacy organizations have challenged the\nconstitutionality of this ordinance.78\n71\nSee, e.g., the proposed Facial Recognition Technology Warrant Act Of 2019, https://www.coons.senate.gov/imo/media/doc/FRTWA%200ne-\nPager%20FinalFinal.pdf.\n72\n\"HR. 4368: Justice in Forensic Algorthms Act of 2019\" htp://www.congress.gov/bil/116th-congress/house-bil/4368/text\n 73 See Jay Stanley,\" The Dawn of Robot Surveillance: Al, Video Analytics, and Privacy\" ACLU, 2019, https://www.aclu.org/report/dawn-robot-\nsurveillance\n74\n LFR refers to facial recognition that is \"always on, identifying people in real time as they move through public and private space.\n75  Metropolitan Police UK, \"Live Facial Recognition, n.d,htps://www.met.police.uk/advice/advice-and-information/facial-recognition/live-facial-\nrecognition/\n76\nR(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, 4 September 2019, para. 78.(\"AFR Locate\" is South Wales\nPolice's nomenclature for LFR.)\n77\nDave Gershorn,\"The U.S. Fears Live Facial Recognition. In Buenos Aires, It's a Fact of Life, OneZero, Medium, March 4, 2020, https://onezero\nmedium.com/the-u-s-fears-live-facial-recognition-in-buenos-aires-its-a-fact-of-life-52019eff454d\n78 ADC, \"ADC Files an Action of Unconstitutionality before GCBA.",
    "Page_34": "34 |  Regulating Biometrics: Global Approaches and Urgent Questions\nsafeguards.\nAre systems that process bodily data for purposes beyond establishing\nindividual identity, like making inferences around emotional state,\npersonality traits, or demographic characteristics, covered under\nexisting biometric regulation?\nShould such systems be permitted at all, given their contested\nscientific foundations and mounting evidence of harm?\nSince many emotion recognition and personality prediction systems rely on face and\nvoice data that could be used to identify an individual (even if that is not its current\npurpose), these systems could fulfill the definitional threshold of data-protection laws like\nthe GDPR. Many recent moratorium bills in the US include systems that infer \"emotion,\nassociations, activities, or the location of an individual.\"\nHowever, many organizations are calling to ban these systems altogether given\ndiscredited scientific foundations and mounting evidence of harm.\nIt is unclear whether existing biometric regulation will apply to systems where the primary\nprocessing under the GDPR should be interpreted to include \"detection of appearance, inferred\nbehavior, predicted emotions or other personal characteristics.\"80\n79   Some technical literature uses the term \"soft biometrics\" to define the process of \"categorizing information about bodily traits where a person may\nnot be identified in the process.\" See U. Park and A. K. Jain, “Face Matching and Retrieval Using Soft Biometrics,\" IEEE Transactions on Information\nForensics and Security 5, no. 3 (September 2010): 406-415, https://doi.org/10.1109/TIFS.2010.2049842; and see A. Dantcheva, \"What Else\nDoes Your Biometric Data Reveal? A Survey on Soft Biometrics\" IEEE Transactions on Information Forensics and Security 11, no. 3 (March 2016):\n441 -467, https://doi.org/10.1109/TIFS.2015.2480381\n 80  Sarah Chander, \"Recommendations for a Fundamental Rights-Based Artificial Intelligence Regulation, EDRi, June 4, 2020, https://edri.org/wp\ncontent/uploads/2020/06/Al_EDRiRecommendations.pdf",
    "Page_35": "Amba Kak | The State of Play and Open Questions for the Future | 35\nMany recent moratorium bills in the US include systems that use facial data for broader\ninferences, such as inferring \"emotion, associations, activities, or the location of an individual.\"81\nThe 2019 moratorium bills introduced in New York*2 and Washington*3 include any automated\nprocess by which characteristics of a person's face are analyzed to determine \"the person's\nsentiment, state of mind, or other propensities including, but not limited to, the person's level\n of dangerousness.\" In specific contexts, these systems will require additional norms around\nexplainability or transparency about how inferences are made, such as in the Illinois Al\nVideoconferencing Act 2019, which regulates the use of these tools in hiring.\n81\nE.g., Bill S.1385/H.1538. See ACLU Massachusetts,\"Face Surveillance Moratorium,\" n.d.,https://www.aclum.org/en/legislation/face-surveillance-\nmoratorium. See also Ed Markey,\"Senators Markey and Merkley, and Reps. Jayapal, Pressley to Introduce Legislation to Ban Government Use \nof Facial Recognition, Other Biometric Technology\" June 25, 2020, https://www.markey.senate.gov/news/press-releases/senators-markey-and-\nmerkley-and-reps-jayapal-pressley-to-introduce-legislation-to-ban-government-use-of-facial-recognition-other-biometric-technology; and Cory\npress/booker-introduces-bil-banning-facial-recognition-technology-in-public-housing.\n82\nBill A6787D, New York https://www.nysenate.gov/legislation/bills/2019/a6787.\n83 Bill HB 2856, Washington, https://app.leg.wa.gov/billsummary?BilINumber=2856&Year=2019&lInitiative=false,",
    "Page_36": "36 | Regulating Biometrics: Global Approaches and Urgent Questions\nSECTION3.EMERGING REGULATORYTOOLSAND\nENFORCEMENTMECHANISMS\nWhat are the different types of \"bans\" and moratoria that have been\npassed in the US over the last few years?\nHow can moratoria conditions be strengthened to ensure that eventual\nlegislative or deliberative processes are robust?\nHow will bans and moratoria on government use impact the private\ndevelopment and production of biometric systems?\nuse of facial recognition in the US, and some states have also proposed similar bills. Many\ncome out in favor of regulation, they have consistently pushed back against bans, often\nfavoring much less stringent approaches.\nThe term \"moratorium\" is shorthand for a range of regulatory interventions with varying\nconditions for when the restrictions would be lifted-from straightforward time-bound\ngoals for drafting and authorizing legislation to the establishment of deliberative,\nconsultative processes. Some moratorium bills prescribe specific conditions to ensure\nprocess.\nMany cities and states in the US have recently introduced legislation that bans government use of\nfacial recognition, with a primary focus on law enforcement use.84 These legislative interventions\nhave played an outsize role in shaping the regulatory landscape by introducing a complete\nprohibition as a regulatory option against which other, less strict interventions will be compared.\nAs Jameson Spivack and Clare Garvie point out in their chapter, advocates have been critical\ndemands for bans in the future.85\n84  In their contribution to this compendium, Jameson Spivack and Clare Garvie track this legislative activity, noting that “lals of July 2020, the following\nmunicipalities had banned face recognition: Alameda, California; Berkeley, California; Boston, Massachusetts; Brookline, Massachusetts; Cambridge,\nMassachusetts; Easthampton, Massachusetts; Northampton, Massachusetts; Oakland, California; San Francisco, California; and Somerville\nMassachusetts. A number of states proposed bans on face recognition during the 2019-2020 legislative session: Nebraska, New Hampshire, New\nYork,andVermont.\n85  See Spivack and Garvie, \"A Taxonomy of Legislative Approaches to Face Recognition in the United States, in this compendium",
    "Page_37": "Amba Kak | The State of Play and Open Questions for the Future | 37\nSome of the largest technology companies that develop and sell these systems to law\ncriticisms about their involvement in pushing through a law that was considered weak by many\n organizations, and that effectively undercut a potential ban on government use.86\nMoratoria and bans are often used interchangeably, yet Spivack and Garvie argue that this\nrecommences without any further legislative intervention. On the other hand, directive moratoria\nban the use of facial recognition until a law is passed and/or a statutory body (e.g, a task force or\ncommittee) is formed to submit recommendations for what to include in the law.\nMoratoria can work to fast-track a deliberative or legislative process where one might not\notherwise have been possible. While this is welcome, it is also eventually susceptible to the\nvested public and private interests that will push for weak or no legislation. There is a risk that a\ntask force created by these laws \"may not be representative of affected communities; may lack\nauthority; or may be inadequately funded.\"87 Some moratoria do more to prevent weak regulation\nthan others. A 2019 Massachusetts law sets minimum requirements for what future legislation\nliberties. Similarly, the recently passed Washington State law specifies that the legislative task\nof communities historically impacted by surveillance technologies including, but not limited\nreligious minorities, protest and activist groups, and other vulnerable communities.\"88\nmedium.com/a-microsoft-employee-literally-wrote-washingtons-facial-recognition-legislation-aab950396927; and see Lee, \"We Need a Face\nSurveillanceMoratorium.\n87\nSee Spivack and Garvie, \"A Taxonomy of Legislative Approaches\"; see also Rashida Richardson, ed.,\"Confronting Black Boxes: A Shadow Report ot\nthe New York City Automated Decision System Task Force, Al Now Institute, December 2019, https://ainowinstitute.org/ads-shadowreport-2019\nhtml.\n88\n Section 10, Washington Senate Bill 6280, http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20Legislature/6280-S.\nPL.pdf?q=20200331083729",
    "Page_38": "38 |Regulating Biometrics: Global Approaches and Urgent Questions\nWhat regulatory tools can be used to create public transparency\naround the development, purchase, and use of biometric recognition\ntools?\ncomment periods, and publicly accessible registries of vendors and uses.\nto reveal such \"proprietary information.\" Once these tools are built, the purchase and subsequent\ninvestigations have made clear, there are hundreds of globally distributed vendors selling\nbiometric recognition technology without people's knowledge or explicit consent. It was only when\nthe Clearview Al story broke that lawsuits were filed under llinois BIPA, prompting quick action\nface images off the web.89\nas early in the process as possible. Many of these policies target government use to ensure\nthat there is public notice and consultation before these tools are acquired and implemented.\nFor example, in June 2020, after years of civil society advocacy, and in the context of sustained\nwould require the New York Police Department (NYPD) to issue a surveillance impact and use\nassessment would include information about capabilities, processes and guidelines, and any\nsafeguards and security measures in place. In the EU, advocacy organizations like Access\nused in the public sector, in conjunction with a mandatory human rights or algorithmic impact\nassessment.91\nAdvocates have also demanded that regulation should ensure that external researchers and\nauditors have access to algorithmic systems in order to understand their workings, as well as the\ndesign choices and incentives that informed their development and commercialization, and to\nengage the public and impacted communities in the process. Meaningful access includes making\n89  Even these faced the barrier of establishing legal standing because it was difficult to confirm that an llinois resident was in fact part of Clearview's\ndataset due to the lack of publicly available information. See ACLU v. Clearview Al, https://www.aclu.org/cases/aclu-v-clearview-ai\n90\nThe surveillance impact and use policy would first be released in draft form for review by the public. See STOP Spying, POST Act, signed July 7,\n2020, https://www.stopspying.org/post-act\n91\nAccess Now, \"Access Now's Submission to the Consultation on the White Paper on Artificial Intelligence-a European Approach to Excellence ano\nTrust, May 2020, https://www.accessnow.org/cms/assets/uploads/2020/05/EU-white-paper-consultation_AccessNow_May2020.pdf",
    "Page_39": "Amba Kak | The State of Play and Open Questions for the Future | 39\nchecks and balances. The GDPR currently has provisions for data-protection impact assessments\n(DPlA) and \"privacy by design\" assessments that kick in when there is any \"large-scale\" processing\nof biometric data and also in cases of surveillance in publicly accessible spaces. In theory, these\noffer a robust assessment of the rights implications of the use of these systems, including\nchapter, DPlAs have been challenging to implement in practice, with wide variations across\ndifferent member countries of the EU. Moreover, the predominant focus on data-protection\nconcerns can leave out inquiries about accuracy or discriminatory impact. Recent proposals\nprocess.92\nWhile transparency and accountability measures have gained momentum, procurement contracts\nintellectual property claims. When challenged, governments have denied any knowledge or ability\nto explain and remedy the problems created by these systems. Recent advocacy by civil society\norganizations and certain city governments in Europe focuses on including standard contractual\nclauses in these contracts that include waivers to trade secrecy, non-disclosure agreements, or\nother confidentiality clauses, as well as terms that ensure the process of procurement involves\nopen bidding and public notice.94\nWhat role can community-led advocacy play in shaping the priorities\nand impact of regulation?\nevidence of harm, and shaping the rights and protections that policy interventions\neventually offer.\nAdvocacy and mobilization against the use of biometric systems have taken many forms. While\ntraditional digital rights or privacy groups remain active, over the past few years, directly impacted\nexperiences of harm.\n92  See Al Now's detailed AlA framework that public agencies can draw from when implementing AlAs: Dillon Reisman et al.,\"Algorithmic Impact\nAssessments: A Practical Framework for Public Agency Accountability\" Al Now Institute, April2018, https://ainowinstitute.org/aiareport2018.pdf\nThe Canadian government's Algorithmic Impact Assessment tool is also a useful template for regulatory agencies; see Government of Canada,\nAlA, 2019, https://canada-ca.github.io/digital-playbook-guide-numerique/views-vues/automated-decision-automatise/en/algorithmic-impact-\nand increase reflexivity at every stage of ADS procurement or development.\n93\nSee Houston Federation of Teachers v. Houston Independent School District and Ark. Dep't of Human Servs. v. Ledgerwood cases in Richardson,\nSchultz, and Southerland, \"Litigating Algorithms 2019 US Report.'\n94\nAl Now Institute, City of Amsterdam, City of Helsinki, Mozilla, and Nesta, \"Using Procurement Instruments to Ensure Trustworthy Al\" June 15, 2020\nhttps://foundation.mozilla.org/en/blog/using-procurement-instruments-ensure-trustworthy-ai/",
    "Page_40": "40 |Regulating Biometrics: Global Approaches and Urgent Questions\nIn India, a coalition of privacy groups and grassroots welfare activists formed to publicly protest\nby the government, the coalition surfaced specific examples of exclusion due to the technical\nand bureaucratic failures of the system. In their chapter, Stefanie Coyle and Rashida Richardson\nrecount the community-driven advocacy in Lockport, New York, where a group of parents\nschools. Eventually, Coyle and Richardson note, “Iplarents shifted the discourse from debating\nYork State Senate introduced a moratorium bill that \"mirrors the concerns raised by residents in\nthe community and advocates across the state and country.\nLarge-scale biometric projects are often promoted in terms of lofty claims about security,\naccuracy, and efficiency. Community advocacy, particularly on the part of those directly impacted\nby these systems, has been critical in surfacing key questions like: Efficiency for whom? (In)\nguarantee the just use of these technologies without centering the experiences of those affected\nby their use. Recent attempts demonstrate how community interventions can be structured, for\nNew York City ADS Task Force \"Shadow Report\" prepared by a civil society coalition with detailed\nthese systems are used, but also to the kinds of rights and protections that policy interventions\neventually offer\n95\nSee Rethink Aadhaar, https:/rethinkaadhaar.in/\n96\n See Ada Lovelace Institute, \"Citizen's Biometric Council https://www.adalovelaceinstitute.org/our-work/identities-liberties/citizens-biometrics-\ncouncil/\n97\nInstitute, 2019, https://ainowinstitute.org/ads-shadowreport-2019.html",
    "Page_41": "Amba Kak | The State of Play and Open Questions for the Future | 41",
    "Page_42": "42|Regulating Biometrics: Global Approaches and Urgent Questions\nTIMELINEOFLEGALDEVELOPMENTS\nspecific chapters where they are discussed are noted below.\nOctober 2008\nApril 2018\nUnited States\nEuropean Union\nIllinois Biometric Information Privacy Act (BIPA)\nGeneral DataProtection Regulation(provisionson\nenacted (See Chapter 8)\nbiometric data) enacted (See Chapter 4)\nData Protection Law Enforcement Directive enacted \nMarch 2016\n(See Chapter 4)\nIndia\nAadhaar Act enacted (See Chapter 3)\nSeptember 2018\nIndia\nIndian Supreme Court restricts private use of\nAadhaar Biometric ID system (See Chapter 3)\n2008\n2016\n2017\n2018\n2019\nApril 2019\nAugust 2019\nJamaica\nInternational Committee of Red Cross\nJamaican Supreme Court rules biometric ID system\nICRC assembly adopts Biometrics Policy (See\nunconstitutional (See Chapter 1)\nChapter 5)\nMay 2019\nSeptember 2019\nUnited States\nSan Francisco ban on government use of facial\nUnited States\nrecognition technology passed (See Chapter 7)\nCalifornia Body Camera Accountability Act\n(A.B 1215) (moratorium on existing use of face\nJune 2019\nrecognition on body-worn cameras till 2023) passed \n(See Chapter 7)\nUnited States\nSomerville, MA ban on government use of facial \nUnited Kingdom\nrecognition technology (See Chapter 7)\nUK High Court finds Live Facial Recognition\npermissible, rules out need for new authorizing\nJuly2019\nlegislation (See Chapter 6)\nUnited States\nUnited States\nOakland, CA ban on government use of facial\nJustice in Forensic Algorithms Act of 2019 (HR\nrecognition technology (See Chapter 7)\n4368) introduced (See Chapter 1)\nAustralia\nIdentity Service Matching Bill introduced (See\nChapter 2)\nKenya\nHuduma Bill (legal authorization for NMIMS project)\n introduced (See Chapter 1)",
    "Page_43": "Amba Kak 丨 The State of Play and Open Questions for the Future |43\nOctober 2019\nNovember 2019\nUnited States\nUnited States\nNo Biometric Barriers to Housing Act (S 2689)\nThe Facial Recognition Technology Warrant Act of\nintroduced (See Chapter 1)\n2019 (S 2878) introduced (See Chapter 1)\nAustralia\nIdentity Service Matching Bill rejected by Australian\nDecember 2019\nParliament (See Chapter 2)\nUnited States\nUnited States\nNorthampton, MA ban on government use of facial\nBerkeley, CA ban on government use of facial\nrecognition technology passed (See Chapter 7)\nrecognition technology passed (See Chapter 7)\nUnited States\nArgentina\nAlameda, CA ban on government use of facial \nConstitutional challenge to Buenos Aires Live Facial\nrecognition technology passed (See Chapter 7)\nRecognition project (See Chapter 1)\nUnited States\nBrookline, MA ban on government use of facial\nrecognition technology passed (See Chapter 7)\n2019\n2020\nJanuary 2020\nJune2020\nUnited States\nUnited States\nCambridge, MA ban on police use of facial\nFacial Recognition & Biometric Technologies\nrecognition technology passed (See Chapter 7)\n Moratorium Bill S 4084 introduced (See Chapter 7)\nKenya\n United States\nKenyan High Court suspends NMIMS biometric ID\nNew York Public Oversight of Surveillance\nproject (See Chapter 1)\nTechnology (P0ST) Act (Int 0487-2018) passed (See\n Chapter 1)\nUnited States\nCalifornia Consumer Privacy Act (provisions on \nbiometric data) enacted (See Chapter 1)\nJuly 2020\nUnited States\nFebruary 2020\nNew York Senate Bill S5140B (regulating biometric\ntechnologies in school) passed (See Chapter 9)\nUnited States\nSpringfield, MA moratorium on government use of\nfacial recognition technology passed (See Chapter 7)\nAugust 2020\n United States\nMarch 2020\nNational Biometric Privacy Act (S -—-) introduced\n(See Chapter 1)\nUnited States\nWashington SB 6280 (regulates government use\nof facial recognition technology) passed (See\nChapter 7)",
    "Page_44": "44 |  Regulating Biometrics: Global Approaches and Urgent Questions\nAustralian Identity-Matching\nServices Bill\nJake Goldenfein (Melbourne Law School)\nMonique Mann (Deakin University)\nAustralian federal government, have culminated in a new biometric identity-information system.\nFederal authorities have argued that facial recognition technology is useful for law enforcement\nand preventing identity fraud, but to achieve those benefits, they have combined civil and criminal,\nas well as state and federal, identity systems into a powerful intelligence apparatus controlled by\na single government department: the Australian Department of Home Affairs.\nHome Affairs was created in 2017 through a merger of the Department of Immigration and\nBorder Protection and the Australian Border Protection Service. As a result of the merger, Home\nAffairs assumed multiple policing and intelligence competencies from the Attorney General's\nDepartment (AGD), including those related to national security, immigration, organized crime,\ncybersecurity, and public safety policing. Home Affairs also took over control and operation of the\nnational identity-matching services, which included the one-to-one facial recognition verification\nsystem known as the \"Face Verification Service\" (FVS).\nto prevent identity fraud by ensuring an individual presenting to an agency is who they claim to be.",
    "Page_45": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 45\nThe Australian government has been developing the institutional, technical, and legal architecture\nMatching Services Bill.3 The original bill was rejected, however, for a lack of privacy protection and\noversight, and is presently being redrafted. The new bill will likely increase parliamentary oversight\nof the system and the amount of necessary reporting, but will not challenge the fundamental\ninstitutional changes that are already underway, such as the aggregation of civil and criminal\nsystems, or increased control of state-level civic data within a federal intelligence system.\nAlthough governments have always had the function of identifying their citizens,4 they have not\nagencies. Indeed, the intermingling of civil and criminal identity systems has been the concern of\nhuman rights jurisprudence for some time.° Biometrics are of particular concern to the linkage\ncapacities. By advancing a centralized identity matching system, Australia is pushing beyond the\nlimits of legitimate state function.\nBIOMETRICSDEVELOPMENTINAUSTRALIA\nAustralia has collected biometric information, including images for facial recognition, since at\nthe border and through civic licensing agencies.6 States have also used biometric systems for\ndevelopment of a national biometric interoperability framework, which was launched in 2012.10\nPlans for a further national facial biometric matching \"Capability\" to enable cross-jurisdictional\nsharing of identity information, the precursor to the identity matching system operated by Home\nAffairs, were announced in 2014.1\nSee, e.g., Australian Government, Department of Home Affairs,\"Agreement to a National Identity Security Strategy\" April 2007, https://www\n3\nIdentity-Matching Services Bill 2019 (Cth). The note (Cth) indicates that this is a commonwealth or federal bill. The Identity-Matching Services Bill\nwas first introduced in February 2018, but did not progress through parliament and lapsed in April 2019. It was reintroduced in July 2019.\nSee, e.g., Markus Dirk Dubber, The Police Power: Patriarchy and the Foundations of American Government (New York: Columbia University PresS\n2005).\n5\nSee, e.g., Jake Goldenfein, Monitoring Laws: Profiling and Identity in the World State (Cambridge: Cambridge University Press, 2019).\n：\nParliament of Australia, \"CrmTrac Overview 2009\" direct download, PDF), hts:/www.aph.gov.au/DocumentStore.ashx?id=dd60984f-33e2-4836-\n/\n85a4-690052ca7914.\nAustralian Government, Department of Home Affairs,\"An Agreement to a National Identity Security Strategy\" April 2007, https://www.homeaffairs.\ngov.au/criminal-justice/files/inter-gov-agreement-national-identity-security-strategy.pdf\nAustralian Government, Department of Home Affairs,\"A National Biometric Interoperability Framework for Government in Australia, n.d., https:/\n10\n11\nLaw, Crime and Community Safety Council, Communique, COAG Meeting, Canberra, October 3, 2014, https://parlinfo.aph.gov.au/parlinfo/search/\ndisplay/display.w3p;query=ld:%22media/pressrel/3523//9%22",
    "Page_46": "46 |  Regulating Biometrics: Global Approaches and Urgent Questions\nThe one-to-one face verification system (FVS) that Home Affairs took over from the Attorney-\nfrustrated by state privacy laws, which prohibit providing federal agencies direct access to their\ndatabases.14 The result has been limited and complex arrangements for cross-jurisdictional\npolicing and intelligence remit\nCENTRALIZATIONOFIDENTITYDATABASES\nIn 2017, the Australian states agreed multilaterally to enable federal access to their identity data\nunder the auspices of the IGA. Some states made explicit the value they saw in the system,\nwould contribute to enhanced security at the Commonwealth Games.16 Other states were more\nreluctant, raising the alarm about possible contravention of state-level human rights protections\nand suggesting that there were inadequate protections for civil liberties.17\nNonetheless, the IGA established the framework for a data-sharing regime, gave immunity from\nfacial identification service (FiS) to complement the FVS. Such systems are the primary facial\nrecognition tool used in policing in Australia. The system allows for law enforcement, national\nsecurity, and related entities at state and federal level to run queries through the technical\ninfrastructure of a host agency: originally the AGD, and then the Department of Home Affairs.\nImportantly, while the IGA introduced a technical architecture for information sharing, it left\ncontrol over identity databases with the states.18\n12  See Allie Coyne,\"Australia's New Facial Verification System Goes Live,\" IT News, November 16, 2016, https://www.itnews.com.au/news/\npercent of the population (https://www.passports.gov.au/2019-passport-facts)\n13\n Monique Mann and Marcus Smith,\"Automated Facial Recognition Technology: Recent Developments and Strengthening Oversight,\" UNsW Law\nJournal 40, no. 1 (2017): 121-145.\n14S\n See, e.g., the Parliament of the Commonwealth of Australia,\"lIdentity Matching Services Bill 2019, Explanatory Memorandum, describing Clause 19\nof the Bill. An exception is the NsW Roads and Maritime Services, which provides access to the Australian Security Inteligence Organisation (ASIO)\nand the Australian Federal Police (AFP) for the purposes of investigating terrorism offenses.\n15 Council of Australian Governments,\"lIntergovernmental Agreement on Identity Matching Services, October 5, 2017, https://www.coag.gov.au/sites/\ndefault/fles/agreements/iga-identity-matching-services.pdf.\n16\n Mark Ryan,\"Queensland Leads Nation to Strengthen Security Measures,\" Queensland Government, The Queensland Cabinet and Ministerial\nDirectory, March 7, 2018, http://statements.qld.gov.au/Statement/2018/3/7/queensland-leads-nation-to-strengthen-security-measures,\nhttps://www.smh.com.au/politics/federal/biometrically-opposed-victoria-queries-peter-dutton-over-facial-recognition-scheme-20180502-p4zcvs.\nhtml\n Note that the IGA architecture replicates, and was perhaps inspired by, the FBI's Next Generation Identity system, launched in 2014. See FBl, Next\nGenerationIdentification (NGl),https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi",
    "Page_47": "Jake Goldenfein & Monique Mann | Australian Identity-Matching Services Bill | 47\nA few months later, the government introduced the Identity-Matching Services Bill, which\nostensibly legislated for the IGA. In reality, however, the bill went significantly further, shifting the\nsystem from one that facilitated information sharing into one that enabled the aggregation and\ncentralization of identity information in the Department of Home Affairs\nThis increased centralization is in no way integral to satisfying the objectives of the system, at\ngoal as preventing fraud and identity theft (described as an enabler of organized crime and\nterrorism), but not to build an intelligence apparatus.19 Despite the limited technical capacity\nintelligence agencies.\nthe differences between the IGA and the bill. Beyond addressing identity fraud, we suggest these\nchanges reveal the true underlying political rationalities and motivations for establishing this\nnational facial recognition system as a radical shift in identity data governance arrangements.\nLEGALCONCENTRATION OFPOWER\nThe Identity-Matching Services Bill sought to establish Home Affairs as the \"hub\" through\nwhich government identity-verification and law enforcement suspect-identification requests are\nprocessed, establishing Home Affairs as the central point of information processing across the\nsystem described in the bill and the 2017 IGA.\nThe IGA outlined two technical architectures: 1) The National Driver License Facial Recognition\nAustralia\nIn the IGA, the FRS was described as a federated database system, in which state-level data\nwould be partitioned, and state agencies could control the conditions of access. Databases\nbut subsequently Home Affairs) could not view, modify, or update information in partitioned\nfederated databases containing state-level information. However, the bill only prescribed that\nHome Affairs could not modify or update that data; in other words, it could still view it.20 In fact,\n19\n The Australian Government IDMatch home page, for example, promotes “ldentity Matching Services that help verify and protect your identity\"\n(https:/www.idmatch.gov.au). See also the Parliament of the Commonwealth of Australia, \"Identity Matching Services Bill 2019, Explanatory\nMemorandum,\" https://parlinfo.aph.gov.au/parlnfo/download/legislation/ems/r6387_ems_f8e7bb62-e2bd-420b-8597-8881422b4b8f/upload.\npdf/713695.pdf;fleType=application%2Fpdf\n20   Sup. 11. See IGA clause 6.16.",
    "Page_48": "48 | Regulating Biometrics: Global Approaches and Urgent Questions\nthe legislation clarified that Home Affairs could collect, effectively without limit, information\nflowing through the systems for satisfaction of its \"community safety\" purposes, which include\nwith identity verification. The bill effectively vested control over the databases of driver's license\nimages squarely within Home Affairs, and enabled unrestrained collection of information.\nused for \"relaying electronic communications between bodies and persons for the purposes of\nrequesting and providing identity-matching services.\" Rather than simply routing information\nfrom place to place, however, the bill enabled Home Affairs to collect data flowing through the\nfor the sake of operating that database,21 as well as for its identity and community protection\nactivities.22 The bill thus enhanced the legal capacity of Home Affairs from an infrastructure\nHome Affairs. For instance, the bill enabled the Minister for Home Affairs to expand the powers\nunder the regime without parliamentary oversight. Furthermore, the identity information that\ninformation held by agencies that is about or associated with the identity document.\nIt is difficult to identify a single rationale that may have motivated the changes between the\nIGA and the Identity-Matching Services Bill. New technological affordances associated with\nespecially considering international trends. The institutional culture and political power of the\nDepartment of Home Affairs may also have made centralization and the use of civil documents\nin intelligence investigation more feasible. Indeed, its participation in forms of intelligence work\nand political policing connects it to a policing tradition that has always involved information\naggregation, not necessarily in line with traditional liberal political limits.24 That expansion of\npolitical and technological power is also consistent with Home Affairs' broad portfolio.\nAustralia lacks enforceable human rights protections at the federal level (though some states\nUnder the Australian Constitution,25 crime control and criminal justice are a competency of\nthe states, not the federal government. Policing agencies are historically restricted to identity\n21 Sup. 3. See S 17 (2).\nlegislation because it is split over two provisions. However, it has been interpreted to mean collction is permitted for the broader range of purposes\n(Bills Digest)\n23  In the Bill, S 5; in the IGA, clause 3.1.\n 24  See, e.g., Bernard Porter, The Origins of the Vigilante State: The London Metropolitan Police Special Branch before the First World War (London\nWeidenfeld and Nicolson, 1987).\n25   Commonwealth of Australia Constitution Act 1900 (Cth)",
    "Page_49": "Jake Goldenfein & Monique Mann |  Australian Identity-Matching Services Bill | 49\nfor one government purpose (e.g., licensing drivers), and using it for another (e.g., policing or other\npunitive applications)\nPJCIS REJECTSTHE BILL\nUltimately, the Identity-Matching Services Bill did not pass parliamentary scrutiny and was\nrejected by the Parliamentary Joint Committee on Intelligence and Security (PJCiS). But the\nspecific issues that led to its rejection are unlikely to halt the system's development. In fact\ncentralization, subject to privacy and accountability \"tweaking.\nWhen the bill reached the PJClS, it was rejected largely due to concerns that it would grant too\nmuch executive authority to the Department of Home Affairs, meaning that the Minister for Home\nprivacy advocates around the possibility of a real-time, facial recognition-powered CcTV mass\nthe lack of a dedicated biometric oversight body (both of which exist in the United Kingdom)\nto the issues it purported to solve, or suficiently privacy-protective. But those concerns were\ndata centralization or the aggregation of civil and criminal identity databases. Instead, there\nwas general approval that this type of data sharing would occur subject to a binding legislative\nexemptions to privacy laws.28\n26\n Jake Goldenfein,\"Police Photography and Privacy: Identity, Stigma, and Reasonable Expectation, University of New South Wales Law Journal 36, n0.\n1 (2013): 256-279.\n 27  See Parliament of Australia,\"'Review of Identity-Matching Services Bill 2019 and the Australian Passports Amendment (Identity-Matching Services)\nBill 2019,\" n.d., https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/Intelligence_and_Security/ldentity-Matching2019\n 28  See the “Parliamentary Joint Committee on Intelligence and Security\" (https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/\nIntelligence_and_Security), the \"Advisory Report on the Identity-Matching Services Bill 2019 and the Australian Passport Amendment (Identity\nMatching2019/Report), and \"A Workable Identity-Matching Regime\" (https://www.aph.gov.au/Parliamentary_Business/Committees/Joint/\nInteligence_and_Securty/dentity-Matching2019/Report/section?id=commitees%2Freportint%2F024343%2F27805)pecifically the PCS\nargued that the Identity-Matching Billis designed to \"permit alevels of government and the private sector unprecedented access to Australian\ncitizens' private biometric information in the form of a facial image\" and that \"given the significance of these measures, the Committee considers\nit preferable that privacy oversight and safeguards are established and set out in this enabling legislation rather than only being provided in\nsupplementary agreements or arrangements.",
    "Page_50": "50 |  Regulating Biometrics: Global Approaches and Urgent Questions\nThe PJCiS accordingly recommended redrafting the bill to make its function and purpose\nreject the bill, the use of facial recognition technology, or the new data governance arrangements\nthat would power the system.\nFUTILITY OFAUSTRALIANREGULATORY OVERSIGHT\nThe Identity-Matching Services Bill is presently being redrafted, with the new text yet to be\nreleased. Nonetheless, the states continue to upload identity images to the system in anticipation\nof the law passing and the system developing along similar lines. One reason political review\nhas failed to meaningfully challenge the general structure of the identity matching and facial\n\"privacy versus security\" framing. International human rights law requires that state surveillance\nbe “reasonable\" and \"proportionate” and this language clearly influenced the PCJIS\nsurveillance intervention must be directly related to, and the least restrictive measure for, the\n\"necessary\" purpose pursued. A true proportionality analysis might question whether such\nidentity fraud. In reality, however, this framing is operationalized in ways that enable continuing\nexpansion of surveillance systems, especially in nations like Australia, where it is not backed up by\nactionable protections\nWhen privacy is pitched against security, the benefits of centralization and surveillance\ncivil liberties are we willing to curtail or limit in exchange? Blanket data sharing for policing and\nintelligence agencies is thus readily accepted and normalized as a necessary response to crime\nand insecurity, subject to privacy balancing intended to curtail its most abusive and authoritarian\ndimensions.30 That framing fails to address the reality that the system fundamentally eliminates\nthe need for the largest policing and intelligence apparatus in the country to justify its access\nto personal data that was previously distributed to the states. This goes beyond agencies using\nbiometrics for their democratically constituted civic purposes (e.g., driver's licenses), and beyond\nthe stated intention of the bill (e.g., detecting identity fraud). By pushing this bill forward, Home\nare necessary for its introduction.\n 29 It should be noted that there are oversight bodies responsible for Commonwealth law enforcement agencies under the Law Enforcement Integrity\nCommissioner Act 2006 (Cth) that established the Commonwealth Integrity Commissioner and the Australian Commission for Law Enforcement\nIntegrity, which hasjurisdiction over allCommonwealth law enforcement agencies (including those responsible for the facial biometrics matching\nsystem).\n See, for example, Monique Mann et al.,\"The limits of (Digital) Constitutionalism? Exploring the Privacy-Security(Im)balance in Australia,\"\nInternational Communication Gazette 80, no. 4 (2018), 369-384",
    "Page_51": "institutional momentum also makes resisting significant data governance rearrangements\ndifficult. One recent positive development, however, has been the Australian Human Rights\n Commissioner calling for a moratorium on the use of facial recognition technology as part of\nit is uncertain what impact this will have on the design, development, and eventual deployment\n of facial recognition technology in Australia, especially considering the extent to which the\ninfrastructure is already in place.\nby secretly selling surveillance services to government, while using their own privately operated\ninfrastructure. When governments procure those services, they bypass whatever regulatory or\nthe case, the purposes expressed to justify new facial recognition implementations for the sake\n of those democratic processes appear not to tell the full story. It remains imperative to identify\nand address the institutional realignments and data governance reconfigurations connected to\n 31  Australian Human Rights Commission,\"Human Rights and Technology Discussion Paper\" December 2019, https://www.humanrights.gov.au/our\nwork/rights-and-freedoms/publications/human-rights-and-technology-discussion-paper-2019.\n32\n\"Australian police agencies initially denied they were using the service. The denial held untila list of Clearview Al's customers was stolen and\ndisseminated, revealing users from the Australian Federal Police as well as the state police in Queensland, Victoria and South Australia.\" See Jake\nGoldenfein,\"Australian Police Are Using the Clearview Al Facial Recognition Technology with No Accountability\" The Conversation, March 4, 2020,\nhttps://theconversation.com/australian-police-are-using-the-clearview-ai-facial-recognition-system-with-no-accountability-132667",
    "Page_52": "52 |Regulating Biometrics: Global Approaches and Urgent Questions\nThe Economy (and Regulatory\nPractice) That Biometrics\nInspires: A Study of the\nAadhaar Project\nNayantara Ranganathan (lawyer and independent researcher, India)\n he Government of India launched the Aadhaar biometric identity project in 2009 with\nthe aim of providing identification for all residents.1 The project called for a centralized\ndatabase that would store biometric information (fingerprints, iris scans, and photographs)\nfor every individual resident in India, indexed alongside their demographic information and a\nunique twelve-digit \"Aadhaar\" number. India's now-dissolved Planning Commission formed the\nUnique Identification Authority of India (UIDAl) to plan the project, as well as implement and\nperform regulatory functions.2 The scale and ambitions of the project are matched only by the\nlong and rich history of resistance to it. Economists, technologists, people's movements, and\nconcerned citizens have questioned the amplified surveillance dangers, indignities from exclusion\ndue to failures in biometric identification systems, and lack of institutional accountability.3 The\nproject proceeded without any legal framework to govern it for seven years after its inception\n(government use of data in India is still not governed by any dedicated law).\nnotification_28_jan_2009.pdf (last accessed on July 15, 2020). UIDAl was set up under the chairmanship of one of the foremost industry leaders of\nthe Indian IT sector, Nandan Nilekani.\nbid.\n3\nFor some critiques by technologists, see, for example, Rethink Aadhaar, https://rethinkaadhaar.in; and the Medium site Kaarana, https://medium.\ncom/karana. For a compilation of dissenting notes by various authors, see Reetika Khera, ed., Dissent on Aadhaar: Big Data Meets Big Brother\n(Hyderabad: Orient Blackswan, 2019).",
    "Page_53": "Responding to the glaring lack of accountability raised by public advocacy and litigation, the\nIndian government passed the Aadhaar (Targeted Delivery of Financial and Other Subsidies\nbiometric information in the service of the data economy. This essay explores the continuing\nlegal and regulatory complicity in constructing data as a resource for value extraction, and how\nregulatory practice mimics the logics and cultures of the technologies it seeks to regulate\nMAKING DATA MARKET-READY\nThe law goes to great lengths to sustain the idea of biometric data as signifying truth, supporting\nand maintaining an infrastructure that is foundational for the data economy.\nThe Truth about Biometrics\nEarly planning documents of the Aadhaar project refer to biometrics as a fundamental identity,\nwhile older forms of identification based on demographic information are considered \"surrogates\nof identity.\"6 Yet biometric information is also a class of media, offering representations of bodily\nattributes captured at a particular moment in time under specific material conditions, and of\nno greater epistemic caliber. However, when coupled with the moral timbre of truth, biometric\ninformation can perform the important function of instituting people as data points within\ndatabases. This allows datafication of flows like cash exchanges or road traffic to be easily\nmapped onto signifiers of \"real\" people within databases, making these newly captured and latent\ndataflows more meaningful and profitable\nIn the Aadhaar project, high-resolution photographs of people's irises and fingerprints were\ncollected at the time of enrollment into the database, along with standard photographs of faces.\nAn equivalence between media artifacts captured about a person and their true identity might\n+\nHereinafter calld the Aadhaar Act, or simply Aadhaar. For the text of the act, see the Ministry of Law and Justice, \"The Aadhaar (Targeted Delivery\n of Financial and Other Subsidies, Benefits and Services) Act, 2016, March 26, 2016, https:/uidai.gov.in/images/targeted_delivery_of_financial_and_\nother_subsidies_benefits_and_services_13072016.pdf\ncaravanmagazine.in/vantage/aadhaar-violates-democratic-process-constitutional-rights. See also Software Freedom Law Center (SFLC),\"How\nParliament Debated the Aardhaar Bill, 2016, March 19, 2016, https://sflc.in/how-parliament-debated-aadhaar-bil-2016.\nUIDAl, \"Role of Biometric Technology in Aadhaar Enrollment,\" January 21, 2012, http://www.dematerialisedid.com/PDFs/role_of_biometric_\ntechnology-in-aadhaar_jan21_2012.pdf. See also UIDAl,\"Basic Knowledge of UIDAl and Aadhaar, Module 1\" March 16, 2015, https://uidai.gov.in/\nimages/training/module_1_basic_knowledge_of_uidai_and_aadhaar_16032015.pdf\nprocess.html",
    "Page_54": "54 |  Regulating Biometrics: Global Approaches and Urgent Questions\nbe common in popular parlance, but with Aadhaar, such an equivalence was crystallized in law.8\nrepresentational media to their revelatory quality is a tactical one that several actors in the data\nsignifier, offering itself as part of an \"identity layer\" that may then be used as a foundation for the\ndatafication of realms like finance, taxation, healthcare, and education.10\nGrooming UniquenessasTruth\npue uoo siue dn doid ot payom aaey uoeinbau pue mel ina tu'adsns si aldoad inoge yinn\nattach market value to it, through the legally defined processes of \"deduplication, the mandatory\ndata.\nof thedatabase.\nUpdating biometrics information and technology: While uniqueness is architected through\ndeduplication, fidelity of the media at the time of enrollment to the biological attributes of\ninformation as a fundamental identity and an authentication key, the law uses minor fixes while\nstill equating biometric information with biological attributes. The law gives UlDAl powers to\nErasure of the fact of mediation surfaces in the definition of \"biometric information\" Notice that in the definition of \"biometric information\" in Section\n8\n2(g) of the Act, there is a slippage or equivalence between media (e.g., a photograph) and the subject (e.g., a fingerprint). In other words, there is a\nslippage and equivalence between biological attributes and their representation that is captured in the machines. Section 2(g) states that \"biometric\ninformation means photograph, fingerprint, iris scan, or such other biological attributes of an individual as may be specified by regulations.\" This\nsuch other biological attribute of an individual as may be specified by regulation\" A definition not making this erasure might read as follows: \"core\nbiometric information' means fingerprint, iris scan, or such other representations of biological attributes of an individual as may be specified by\nregulation.\nUsha Ramanathan, \"Aadhaar-From Welfare to Proft, in Dissent on Aadhaar: Big Data Meets Big Brother, ed. Reetika Khera (Hyderabad: Orient\nBlackswan, 2019),178.\n10\n See \"Basic Knowledge of UIDAl and Aadhaar, Module 1,\" https:/uidai.gov.in/images/training/module_1_basic_knowledge_of_uidai_and.\naadhaar_16032015.pdf\n11F\nFor a discussion of the issues surrounding use of biometrics as a stand-in for truth about people,e.g., with creating a \"self-referential system\" that\nfo a u u! enpiu aui uouapi pue Auapi, yus suusin as pne pue enb auoew uiee jo saial au yrm pauauosun s!\nNetworked Governance, Socio-Legal Review 11, no. 2 (2015): 22-40, http://docs.manupatra.in/newsline/articles/Upload/D47CF36C-C409-45BF-\n8AE6-659D7B6281FB.pdf; on the harms of treating bodies as data, see Anja Kovacs, \"When Our Bodies Become Data, Where Does That Leave Us?\"\nDeep Dives, May 28, 2020, https://deepdives.in/when-our-bodies-become-data-where-does-that-leave-us-906674f6a969.\n12\nUIDAl,\"Features of Aadhaar\" https:/uidai.gov.in/my-aadhaar/about-your-aadhaar/features-of-aadhaar.html.\n u ao osiad aes au jo suaua au s suie sd sa n pne o pu uo",
    "Page_55": "\"to ensure continued accuracy\" and not, say, to correct the inevitable deterioration of fidelity of\nbiometric information.'4 Ihe law even anticipates, supports, and relies on ever-better biometrics\ntechnologies, bridging any imagined distance between a thing and its representation.15\nLending truth to demographic data: Biometrics' reputation of truth, and its resultant market\ndata does not benefit from the same heightened data-security protections,17 is unverified, and is\nunaudited.\nKEEPINGDATAMARKET-READY\nstart-up,\"18 and is emblematic of the close cooperation between private actors and UiDAl. As a\nresult of these close ties, regulation of the Aadhaar project enacts itself as cybernetic feedback\ninfrastructural building block of the data economy alive\nAadhaar asaBuildingBlock\nFrom the outset, the UiDAl envisioned Aadhaar as an identity\"platform\": an infrastructure that\nof identification for the marginalized, and to enable efficient and targeted welfare delivery.20\n14  See Section 6 of the Aadhaar Act (htps://uidai.gov.in/images/targeted_delivery_of_financial_and_other_subsidies_benefits_and_services_13072016\npdf). The Act places the responsibility of such updates with the Aadhaar number holder\n15\n The Aadhaar Act anticipates and privileges the proliferation of biometrics technologies by including an expansive definition of biometrics; see\nSection 2(g) of the Act. Additionally, the Act also reserves the power of UiDAl to promote research and development for advancement in biometrics\nand related areas; see Section 23(2)(q) of the Act.\n16  The Aadhaar number is used for various purposes,including bank verification, despite the low quality of data and its unverified and unaudited\nnature.\n17\n On the authentication of the Aadhaar number, see Section 8 of the Aadhaar Act; on the restriction on sharing information, see Section 29; or\nbiometric information deemed to be sensitive personal information, see Section 30. Conversely, where concerns around the Aadhaar project have\narisen, they are allayed by a false sense of safety provided for the biometric information, while the associated demographic information fills any\ngaps created by the withdrawal of biometrics.\n18  Viral Shah,\"Like Narendra Modi, Nandan Nilekani Too Understands the Transformative Power of Technology\" India Today, September 16, 2017,\nhttps://www.indiatoday.in/magazine/news-makers/story/20170925-pm-narendra-modi-nandan-nilekani-adhaar-ekyc-gst-artificial-intelligen\nce-1044702-2017-09-16.\n The Biometrics Standard Committee set up by the UIDAl in its report as far back as December 2009 stated that the UIDAl would “create a\nplatform to first collect identity details of residents, and subsequently perform identity authentication services that can be used by government\nand commercial service providers.\" See UIDAl, \"Biometrics Design Standards for UID Applications,\" December 2009, htps://archive.org/details/\nBiometricsStandardsCommitteeReport/mode/2up. See also Ramanathan, \"Aadhaar-From Welfare to Profit,\" 177\n 20  See Krishnadas Rajagopal,\"Centre's Aadhaar Affidavit in Supreme Court: Welfare of Masses Trumps Privacy of Elite,\" The Hindu, June 9, 2017,\nhttps://www.thehindu.com/news/national/centres-aadhaar-affidavit-in-supreme-court-welfare-of-masses-trumps-privacy-of-elite/article18951798.\nece; and see the Preamble of The Aadhaar Act.",
    "Page_56": "56 | Regulating Biometrics: Global Approaches and Urgent Questions\ndatabases.21\nonly drastically reduced the costs of performing door-to-door verification required of banking and\ntelecom service providers, but also held the promise of entirely new use cases for businesses.24\nThese APls are part of \"India Stack\" a growing set of APls built by a group of self-styled\nvolunteers called India Software Products Roundtable (iSPIRT), or Product Nation.25 iSPIRT\ndesigns and builds these APls for use by government entities and businesses alike, in the process\necosystem.\nInstitutions\nWith a strong need for identity verification, the finance sector was the first to fully embrace\nThese products allowed banks to use the Aadhaar number to make remittances?' or to authorize\nfirms to query the Aadhaar database to verify and onboard customers.29 With these Aadhaar\nintegrations into legacy banking services in place, NPCl launched a payments system that\nintroduced interoperability between different payments and settlements systems through the\n 21  Consider, for example, the orientation of the UIDAl Security Policy and Framework for UIDAl Authentication, which provided mandatory and\nrecommendatory security considerations to Authentication User Agencies (AUA), Authentication Service Agencies (ASA), Devices, etc. This\ndocument, speaking directly to security and privacy concerns, which are traditionally welcome as areas of regulation, primarily deals with network\nsecurity concerns like distributed denial of service (DDos) attacks that protect the conditions that are critical for the authentication infrastructure\nto run seamlessly. While no doubt this is required, the policy is entirely unconcerned with simpler and more commonplace risks that have prover\nto affect individuals to disastrous effect. For example, people who were not used to treating erstwhile ID cards as private information continued\nto share their Aadhaar numbers and related information with no hesitation, sometimes compromising their economic security. The mandate to\nauoo saiouae oenai  jo ue pu buos pou si bug sanno ueuai paunoin eoos ons pun sys nos isuieee pen\nthemselveswith.\n 22 \"An APl is a set of definitions and protocols for building and integrating application software..APls let a product or service communicate with other\nproducts and services without having to know how they're implemented.\" See \"What Is an APi?\" RedHat, https://www.redhat.com/en/topics/api/\nwhat-are-application-programming-interfaces\n23\n Aadhaar Authentication APl: returns “yes/no\" responses to queries seeking authentication of biometric or demographic data. Aadhaar electronic\n25\nSee iSPIRT, https://ispirt.in/.\n26\nE.g., the National Payments Corporation of India (NPCl); and the central bank and finance regulator, Reserve Bank of India\n27\nSee Aadhaar Enabled Payment System.\n 29  See Aadhaar-based Biometric Authentication and electronic Know-Your-Customer norms.",
    "Page_57": "firms could now build payment-related products and users could easily make payments through\ntheir smartphones. As a cohesive suite of technology \"platforms\" these products and switches31\nenabled the creation, capture, and monetization of data flows in finance.32\nand financial tools from the start,3 and conflicts of interest were notable. People associated with\nmonetization. Venture capitalists associated with the cashless layer went on to back these very\nstartups.34\nfinancial inclusion.35 As a testament to its value, Nandan Nilekani notes that securing a loan has\nnow become as simple as having \"a richer digital footprint.\"36\nHowever, these narratives recast complex sociopolitical issues like lack of access to banking\nas individual journeys of competition for artificially scarce resources, to be won by participating\nand winning in the data economy. These interventions are far from actually addressing issues\nof financial inclusion.37 while the financial sector led the efforts to monetize Aadhaar, many\nother industries continue to follow suit (e.g, with \"technology stacks\" for healthcare, lending.\ntelemedicine, and agriculture).38\n 30 See \"United Payments Interface, February 2015, https://archive.vn/xZEW0#selection-3321.29-3327.29.\n A switch handles authentication and communication between issuing and acquiring banks.\n32\n A landscaping study of companies built on top of India Stack recorded at least 150 startups doing background verification, digital lending, and\ndigital wallets as far back as 2018. Bharat Inclusion Fund,\"Startups building on IndiaStack: A Landscaping Study, Medium, August 23, 2018, https://\nmedium.com/bharatinclusion/startups-building-on-indiastack-a-landscaping-study-a77344b51d19\nUIDAl provided blueprints for how its architecture may be used for financial-sector commercial products to the Reserve Bank of India\nreporttaskforceaadhaarpaymentinfra. Indeed, RBl leadership in charge of developing standards for payments and settlements included industry\nplayers behind Aadhaar. See Anuj Srivas,\"“Exclusive: How the RBl Forced National Payments Body to Hire Government Favourite as CEO\" The Wire,\nFebruary 14, 2018, https://thewire.in/business/rbi-npci-digital-india. The committee set up for \"deepening digital payments\" was helmed by the first\nchairperson of the UIDAl, Nandan Nilekani. See Aria Thaker, \"Behind RBl's Digital Payments Panel, a Controversial Firm's Shadow, Conflict of Interest\nAllegations Scrollin January 10 9,https://scrollin/article9882/behind-rbis-digital-payments-panel-a-controversial-firmsshadconflit-of-\ninterest-allegations.\n34\n Aria Thaker,\" The New Oil: Aadhaar's Mixing of Public Risk and Private Profit,\" Caravan, April 30, 2018, https://caravanmagazine.in/reportage/\naadhaar-mixing-public-risk-private-profit\n 35 See Suprita Anupam, \"Nandan Nilekani on Creating the Architecture for India's Digital Future, Inc42, April 24, 2019, https://inc42.com/features/\nnandan-nilekani-on-aadhaar-digital-india-kyc-gst-upi-payments-fastag/. See also ProductNation/iSPIRT, \"Nandan Nilekani: Identity, Payments,\nData Empowerment 2019, SlideShare, December 9, 2019, https://www.slideshare.net/ProductNation/nandan-nilekani-identity-payments-data-\nempowerment-2019; and ITU News, \"Aadhaar: India's Route to Digital Financial Inclusion, June 26, 2017, https://news.itu.int/aadhaar-indias-route\nFinancial-Inclusion.pdf.\n36\n See ProductNation/iSPIRT,\"Nandan Nilekani: Identity, Payments, Data Empowerment 2019.\"\n37\nFor example, according to economist and author M. S. Sriram, issues of identity, deduplication, and authentication were not the most significant\nbarriers to financial inclusion. See Sriram, \"Moving Beyond Aadhaar: Identity for Inclusion, Economic & Political Weekly 49, no. 28 (July 12, 2014),\nhttps://www.epw.in/journal/2014/28/special-articles/identity-inclusion.html (paywall).\n38\nNHS-Strategy-and-Approach-Document-for-consultation.pdf; and Seema Singh and Arundhati Ramanathan, \"The Elite VC-Founder Club Riding\nAarogya Setu to Telemed Domination, The Ken, May 18, 2020, https://the-ken.com/story/the-elite-vc-founder-club-riding-aarogya-setu-to-telemed-\ndomination/. On loans for MSME, see Arundhati Ramanathan,\"Sahay, India's Fintech Disruption Sequel\" The Ken, May 8, 2020, https://the-ken.com/\nstory/sahay-indias-fintech-disruption-sequel/",
    "Page_58": "58 |  Regulating Biometrics: Global Approaches and Urgent Questions\nAs private-sector use of Aadhaar took off, many harms materialized?9 and several entities\nsector uses of Aadhaar in 2018,41 and dealt an existential blow to entire sectors42 built with its\ngain.\nopposition to private use, the Ministry of Law and Justice introduced an ordinance amending\n\"offline verification\" and \"alternative virtual identity.\"43 This allowed Aadhaar number holders to\naccommodations, businesses were still disgruntled, as the ease and low costs of verification\nwere nevertheless affected.45\nIn response, the Central Government issued a note to allow private entities to use Aadhaar-based\nverification facilities upon the fulfillment of certain conditions, and at the discretion of UiDAl and\nYour Customer (ekYC) authentication.48\n39\n E.g., through the profling of blue-collar workers, or fraudulent uses of data. See Usha Ramanathan, \"The Future Is Here: A Private Company Claims\nIt Can Use Aadhaar to Profile People, Scroll.in, March 16, 2016, https://scrollin/article/805201/the-future-is-here-a-private-company-claims-to-have\naccess-to-your-aadhaar-data; and \"UIDAI Suspends Airtel, Airtel Payments Bank's e-KYC License over Aadhaar Misuse,\" Economic Times, December\n16, 2017, https://economictimes.indiatimes.com/news/politics-and-nation/uidai-suspends-airtel-airtel-payments-banks-e-kyc-licence-over-aadhaar-\nmisuse/articleshow/62096832.cms.\n40\nFor example, activists challenged the linking of Aadhaar to bank accounts and mobile numbers. See Laxmi Prasanna, \"New Petition in Apex Court\nChallenges Linking Aadhaar with Bank Acount and Phones\" Times of India, October 19, 2017, https://timesofindia.indiatimes.com/india/new-\npetition-in-apex-court-challenges-linking-aadhaar-with-bank-account-and-phones/articleshow/61145283.cms.See also AnooBhuyan,\"Aadhaal\nIsn't Just about Privacy. There Are 30 Challenges the Govt Is Facing in Supreme Court,\" The Wire, January 18, 2018, https:/thewire.in/government/\naadhaar-privacy-government-supreme-court\n41\nJustice K. S. Puttaswamy and Another v. Union of India and Others, Writ Petition (Civil) No. 494 of 2012, https://main.sci.gov.in/\nsupremecourt/2012/35071/35071_2012_Judgement_26-Sep-2018.pdf.\nKomal Gupta, \"Aadhaar Verdict Puts Fintech Firms in a Spot, Livemint, September 28, 2018, https://www.livemint.com/Politics/\ngIGcFQMgHR146zXfPkGqjO/Aadhaar-verdict-puts-fintech-firms-in-a-spot.html.\n43\nAnita Baid, \"RBl Amends KYC Master Directions: Aadhaar to Be Officially Valid Document Now, Moneylife, May 31, 2019, https://www.moneylife.in/\narticle/rbi-amends-kyc-master-directions-aadhaar-to-be-officially-valid-document-now/57317.html.\n44\nQR_Code_15032019.pdf. Note that the supposed security features of a digital identity linked to biometrics is undone when the artifact of proof of\nidentity becomes part of an .xml file.\n Pratik Bhakta,\"lndia's Fintech Companies Struggle for an Alternative to Aadhaar\" Economic Times, December 21, 2018, https://economictimes\nindiatimes.com/small-biz/startups/features/indias-fintech-companies-struggle-for-an-alternative-to-adhaar/articleshow/67186586.cms.\nPratik Bhakta,\"Soon, Non-Banking Companies May Verify via eKYC, Economic Times, May 17, 2019, https:/economictimes.indiatimes.com/\nindustry/banking/finance/banking/soon-non-banking-companies-may-verify-via-ekyc/articleshow/69366383.cms.\n47\n'Banks Can Use Aadhaar for KYC with Customer's Consent: RBI\" May 29, 2019, https://economictimes.indiatimes.com/industry/banking/finance/\nbanking/banks-can-use-aadhaar-for-kyc-with-customers-consent-rbi/articleshow/69568435.cms.\nThis was based on a creative interpretation of the opinion of the attorney general. For example, authentication functions were allwed for purposes\nof welfare delivery. UIDAl applied this as if products using Aadhaar-enabled Payments System (AePS) could access it, since AePS might be used in\nthe course of welfare delivery.",
    "Page_59": "REMAKING REGULATIONIN TECHNOLOGY'SIMAGE\nsame values, managerial styles, procedural cadence, interests, and language of communication\nas the applications of technologies it seeks to regulate.\nRegulation as Public Relations and Marketing\nFor the first seven years of its existence, Aadhaar had little oversight and was shaped by UIDAl,\nthe conditions for use of the biometric data by private companies, to the artificial exclusion of\nyears of the project, but also presented a revisionist history of the actual goals of the project,\nobscuring the stakes for private interests.50 For this and other reasons, many of the problems\nwith Aadhaar should not be understood as failures of law or regulation, but as products of law and\nregulation\nWhile law and regulation were meant to address the risks of Aadhaar, the instruments uncritically\nsem peum ,oualoa, pue ,uoeouu,\"uoisniou! lenoueuy, aall uobe snonuabussip padope\nrighteously proclaimed by UiDAl as public buy-in for the project owed some credit to incentives\nprovided to enrollment agencies,51 as well as expertise drawn from \"multiple areas of marketing,\nchannels, branding and positioning.\"52\nRegulation as Technology Product\nThe private sector's direction and influence in the development and adoption of technology\nitself a product, feature, and layer.\n49 Law and regulation of the finance sector, for example, creates an artificial distinction between civil and political rights (often framed in the narrow\nlanguage of privacy) and economic imperatives (generalied benefits for the country). See also Nandan Nilekani, \"Data to the People: India's\nInclusive Internet, Foreign Affairs, September/October 2018, https:/uidai.gov.in/images/news/Data-to-the-people-Nandan-Nilekani-foreign-affairs,\npdf.\n50\n51\n Anand Venkatanarayanan, \"How Trustworthy Are the Entries in the Aadhaar Database?\" MediaNama, September 28, 2017, https://www.medianama\ncom/2017/09/223-how-safe-is-the-aadhaar-database/\n 52 UIDAl, \"Aadhaar Awareness and Communications Strategy Advisory Council Order,\" February 17, 2010, https://archive.org/details/\nUIDAIMediaAwarenessAdvisoryCouncil/page/n1/mode/2up. See also conflicts of interest within initiatives like ID4D. Transnational interests like the\nWorld Bank's ID4D initiative, pushing digital identification in the language of rights to developing countries, even as its composition reveals shocking\nconflicts of interests, including investors in fintech and related data economy businesses, venture capitalists as well as Nandan Nilekani himself\nSee also Anandita Thakur and Karan Saini, \"Selling Aadhaar: What the UIDA's Advertisements Don't Tell You,\" The Wire, August 23, 2018, https://\nthewire.in/rights/aadhaar-advertisements-identity-citizenship-rights. \"If the advertisements espoused by the UIDAl were to be believed, the prospect\nof biometric failures and internet connectivity issues do not even figure into the day-to-day business of the coercive practice of making Aadhaar an\nunsubstitutable instrument of citizen life in India.",
    "Page_60": "60 | Regulating Biometrics: Global Approaches and Urgent Questions\nStack took it upon itself to \"innovate\" around encoding data-protection safeguards (e.g., through\n\"consent\" and \"transparency\") within the technology ecosystem and to solve for data protection.\nThis maneuver simultaneously tries to foreclose demands for a data-protection law (which India\ndoes not have) and, more importantly, distracts from broader questions about whether such\ndatafication is at all necessary and who benefits from it, making the present trajectory seem\ninevitable.\nConsent: Arguably one of the biggest issues with Aadhaar has been its coercive nature and\nand Data Empowerment and Protection Architecture (DEPA).5 The former is an entity legally\ninstituted by the Reserve Bank of India, which is tasked with consolidating, organizing, and\nretrieving data about a customer's different types of financial arrangements, including mutual\nfunds and insurance schemes. The latter aims to provide \"a modern privacy data sharing\nframework\" and introduces convenience into the process of sharing personal data in exchange\nfor finance, healthcare, and other services by building an interface for the purpose. The contents\nof this layer effectively make consent a bureaucratic formality and logistical complication to be\nfor private firms.\nall necessary, or what the subsequent terms of use of this data might be, ultimately cornering\nof transparency. One of the main sources of proactive disclosure about Aadhaar is the UIDAl\ndashboard,56 where monthly data about enrollments, updates, and authentication are maintained.\nThe transparency-related artifacts use aesthetic devices like dashboards, data visualizations, and\ninformation.\n 53  Anuj Srivas, \"Aadhaar Moves Forward as Ministries Navigate SC Order and Public Backlash, The Wire, September 20, 2016, https:/thewire.in\ngovernment/aadhaar-supreme-court-compliance.\nJayadevan PK, \"Consent, the Final Layer in Indias Ambitious Data Regime, Falling in Place” Factor Daily, September 5, 2017, https:/factordaily.com/\nconsent-architecture-indiastack/.\n 55 Se IndiaStack, About Data Empowerment and Protection Architecture (DEPA), htps:/www.indiastack.org/depa/.\n 56 See UIDAl, Aadhaar Dashboard, https://uidai.gov.in/aadhaar_dashboard/\n 57   See Gus Hosein and Edgar Whitley, \"Identity and Development: Questioning Aadhaar's Digital Credentials\" in Dissent on Aadhaar.",
    "Page_61": "Nayantara Ranganathan | The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project | 61\nRegulationas Optimization\nof the data economy. Within the broader vision for technology-enabled governance, agencies are\nout a perfect system.\"58\nBesides aligning regulatory priorities with the workflows and cultures of technology firms, there is\na push for regulatory practice to adopt the same logics (prediction, optimization) as technology\ninterventions, e.g., for managing loan defaults.\"59\nCONCLUSION\nThe data economy relies on instituting individuals as data points within databases. Law and\nbe possible: architecting biometric information as truth, and facilitating its use, integration, and\nquestions of enrichment.\nInstead of treating biometric information simply as data to be guarded, law and regulation should\nmotion, as well as regulation's own malleability in the face of these forces.\n58\nMinistry of Finance, \"Report of the Technology Advisory Group for Unique Projects,\" January 31, 2011, https://www.finmin.nic.in/sites/default/fles/\nTAGUP_Report.pdf\n 59  See ProductNation/iSPIRT, \"Nandan Nilekani: Identity, Payments, Data Empowerment 2019.",
    "Page_62": "62 |Regulating Biometrics: Global Approaches and Urgent Questions\nA First Attempt at Regulating\nBiometric Data in the\nEuropean Union\nEls Kindt (KU Leuven)\nINTRODUCTION\nn 2004, the European Union (\"Union\") enacted legislation that obligated Member States (\"MS\")\nto store facial images and fingerprints in citizens' passports and travel documents.1 Around\nthe same time, the Union set up large-scale databases containing the biometric data of\nasylum and visa seekers and an information system for protecting the Schengen Area.2 It wasn't\n crowd control, access control in the workplace, and monitoring in schools. While acknowledging\nthat the use of biometric technology has many potential benefits, the Council of Europe warned\ncontains information about health and race, has the ability to identify people, can make it easier to\nlink records, and is irrevocable.3\nEU Regulation No 2252/2004, December 13, 2004.\nConsider, for example, Eurodac, the Visa Information System (VIS), and the Schengen Information System (currently SIS Il), which all emerged after\n2000. Biometric data remains central to the Union's information systems, including the European Travel Information and Authorisation System\n(ETIAS), the Entry/Exit System (EES) (Regulation No 2017/2226), and the European Criminal Records Information System for Third-Country\nNationals (ECRIS-TCN), as is clear from the recent interoperability framework (Regulation No 2019/817 and Regulation No2019/818).\nSee Council of Europe,Progress Report on the Application of the Principles of Convention 108 to the Collection and Processing of Biometric Data\n(Strasbourg: 2005), and the updated Progress Report of 2013, T-PD(2013)06, https://rm.coe.int/progress-report-on-the-application-of-the-principles-\nof-convention-108/1680744d81. The Council of Europe adopted the European Convention on Human Rights in 1950, and the Convention No.108\non data protection in 1981, as revised in 2018 (Convention No. 108+). The Council of Europe consists of forty-seven Member States and is distinct\nfrom the European Union.",
    "Page_63": "Despite the risks, the general data-protection framework and most national legislation did not\ncontain specific provisions on biometric data use and processing,4 and guidance remained\nlimited while these technologies were being developed.5 To address these gaps, some national\nsupervisory data protection authorities (SAs) developed frameworks for biometric use.6 As part of\nthese frameworks, SAs have focused on the sensitive nature of the data, the risks of maintaining\nbiometrics was proportionate to the legitimate aim sought to be achieved (i.e., the \"proportionality\nIt was against this backdrop that the Union introduced the General Data Protection Regulation\n2016/679 (GDPR) in 2016. The regulation is directly applicable in Member States and includes\nprovisions for both public and private biometric data processing. The Union also introduced\nof crime by law enforcement authorities (LEAs)\nTHEEU'S REGULATORYAPPROACH TOBIOMETRIC\nDATAPROCESSING\nBoth the GDPR and DP LED provide, for the first time, a definition of biometric data: \"personal data\nresulting from specific technical processing relating to the physical, physiological or behavioural\ncharacteristics of a natural person, which allow or confirm the unique identification of that natural\nSee Directive 95/46 and Framework Decision 2008/977/JHA.\"Processing\" is understood very broadly, and is defined as \"any operation or set of\noperations ... whether or not by automated means, such as the collection, recording, organization, structuring, storage, adaptation or alteration,\nretrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure\nor destruction\" (Article 4(2), GDPR). Only a few Member States introduced specific legal provisions, e.g., France, Article 25 and 27 Act No. 78-17.\nSee e.g., the Article 29 WP, Working Document on Biometrics 2003 (WP 80), Opinion 2/2012 on facial recognition in online and mobile services\nC\n(WP192), and Opinion 3/2012 on developments in biometric technologies (WP193).\nThis is done by advising, adopting opinions, and issuing guidelines, authorizations, restrictions, and bans. See, e.g., for France, Claire Gayel, \"The\nPrinciple of Proportionality Applied to Biometrics in France: Review of Ten Years of CNIL's Deliberations,\" Computer Law & Security Review 32, no. 3\n(June 2016), 450-461, https://doi.org/10.1016/j.clsr.2016.01.013.\nThis can happen, for example, when an agency uses the data for something other than its original purpose (e.g., for law enforcement purposes)\nSee also CNIL, \"Communication de la CNIL relative a la mise en oeuvre de dispositifs de reconnaissance par empreinte digitale avec stockage dans\nune base de données, Communication central storage fingerprint, December 28, 2007, https://www.cnil.fr/sites/default/fles/typo/document/\nCommunication-biometrie.pdf\nThe proportionality principle is an important principle in data-protection legislation. It requires that the processing is lawful and the data adequate\nand relevant and not excessive for the purpose specified. When interfering with human rights, the proportionality principle requires in addition\na three-step test: that there is accessible and suffciently certain law allowing the interference (\"rule of law\"); a legitimate aim; and necessity in a\ndemocratic society. For assessing the latter, one needs to determine whether (1) the interference answers a \"pressing social need.\" (2) the argued\nreasons for deploying the interference are relevant and sufficient, and last but not least (3) whether all of this, in particular the interference (in our\ncase the use of biometric technology), is in proportion with the legitimate aim pursued. As there remained confusion about the need for double\nreview and because there was also lack of clarity about the three-pronged approach, this resulted in divergent and unpredictable outcomes when\napplying the proportionality principles and in broad \"margins of appreciation.\" See also E. Kindt, Privacy and Data Protection Issues of Biometric\nApplications: A Comparative Legal Analysis (Dordrecht: Springer, 2013), 403 et seq. and 621 et seq.",
    "Page_64": "64 | Regulating Biometrics: Global Approaches and Urgent Questions\nperson, such as facial images or dactyloscopic [fingerprint] data.\"9 A particularly noteworthy\nstored and retained in databases (e.g., of facial images captured on CcTV, voice recordings,\nor fingerprints),11 or when published on a website or social network. The GDPR accounts also\nmention that the \"processing of photographs should not systematically be considered to be\nconsidered biometric data as long as it has not been specifically technically processed in order to\ncontribute to the identification of the individual.13\nWhile the GDPR states that \"processing of biometric data for the purposes of uniquely identifying'\nis prohibited,14 there are many exceptions to this prohibition, including when the data \"are\nrespect the essence of the right to data protection and provide for suitable and specific measures\nto safeguard the fundamental rights and the interests of the data subject.\"15 Because the\nexceptions remain vague (e.g., \"substantial public interest\") and are numerous,16 the GDPR still\ngive explicit consent.17 Finally, the GDPR specifies that Member States may maintain or introduce\nfurther conditions or limitations.18\nArticle 4(14) GDPR and Article 3(13) Directive 2016/680. The technical process is likely to be understood as a biometric technical processing.\nThe original definition in the EU Commission's GDPR proposal of January 25, 2012 COM(2012) 11 final and in the European Parliament's position\nin its first reading of April 13, 2014 was broader: \"biometric data means any [personal data] relating to the physical, physiological, or behavioural\ncharacteristics of an individual which allow their unique identification, such as facial images, or dactyloscopic data\" (Article 4(11). Experts view this\ndefinition as narrow and contrary to a general understanding of biometric data. For examples, see the ISO/IEC 2382-37 Information Technology-\nVocabulary-Part 37: Biometrics (2017), where \"biometric data\" (3.3.6) includes both biometric samples (analog or digital representations of\nbiometric characteristics), hence the initial or \"raw\" data, and the technically processed data thereof. See also EES, where biometric data is\ndefined as including images: \"biometric data' means fingerprint data and facial image\" (Regulation 2017/2226, article 3.1 (18)). On the biometric\nterminology and possible confusion, see also Catherine Jasserand, \"Legal Nature of Biometric Data: From 'Generic' Personal Data to Sensitive Data,\n Added by the Council of the Union, composed of the heads of the Member States and governments. See Council doc. 15395/14, December 19,\n2014, https://www.statewatch.org/media/documents/news/2014/dec/eu-council-dp-reg-15395-14.pdf. This modification was requested and added\nto the initially proposed definition and finally adopted. On the origin of this modification, see E. J. Kindt,\"Having Yes, Using No? About the New Legal\nRegime for Biometric Data, Computer Law & Security Review 34, no. 3 (June 2018):523-538, https://doi.org/10.1016/j.clsr.2017.11.004. This article\nalso contains a graphic showing what counts as biometric data and not, and which legal provisions apply.\nFor example,the collction of facial images by governments to issue identity documents, and their storage in databases.\n12\nRec. 51 GDPR. See also EDPB, Guidelines 3/2019 on Processing of Personal Data through Video Devices, on Video Surveillance, January 29, 2020,S\n 74 (\"EDPB Guidelines 3/2019 on video devices\").\n13\nIbid.\n Article 9.1 GDPR. The general prohibition was an amendment requested by the European parliament (EP) to the original proposal of the EU\nCommission. The processing of biometric data was hereby hence added to the list of special categories of data. EP first reading, T7-0212/2014,\nMarch 12, 2014. This followed the 2012 suggestions of the Council of Europe's Consultative Committee working on the modernization of\nConvention No. 108. Compare with Article 6.1 of Convention 108+ of the Council of Europe. The words “for purposes of uniquely identifying\" were\nadded later during the trilogue in 2016. See Council position, 05419/1/2016, April 8, 2016.\n15\narticle-9-processing-of-special-categories-of-personal-data-GDPR.htm\n16\nThe exception \"personal data which are manifestly made public by the data subject\" is also much debated. See also, e.g., EDPB Guidelines 3/2019\non video devices, S 70.\n17\n18 Article 9.4 GDPR. For example, the Netherlands adopted a law allowing biometric data processing if necessary for \"authentication or security\npurposes.\" Article 29 Dutch GDPR implementation Act of May 16, 2018. The Dutch SA however seems to apply this in a strict manner: see the\ndecision of the Dutch SA (Autoriteit Persoonsgegevens), December 4, 2019, imposing a fine of 725,000 euros for unlawful fingerprinting of\nemployees for access control (appeal pending), available at https://autoriteitpersoonsgegevens.nl/sites/default/files/atoms/fles/onderzoek\nvingerafdrukken_personeel.pdf (in Dutch).",
    "Page_65": "Under the DP LED, LEAs do not face a prohibition and may process biometric data to uniquely\nidentify people where strictly necessary, subject to appropriate safeguards, and only in three\nsituations: if authorized by law, to protect vital interests, or where the processing relates to data\nonly if there is \"specific technical processing\" LEAs may collect data (e.g., facial images or voice\nIn cases where new technologies lead to processing that is \"likely to result in a high risk\" or in\ncase of large-scale processing of special categories of personal data, the GDPR and DP LED\nfor systematic monitoring of a publicly accessible area on a large scale.2° DPlAs mandate entities\nto conduct a comprehensive assessment of the risks of processing, as well as of the necessity\ninterferes with fundamental human rights and freedoms, including the right to privacy and the\nright to personal data protection, the fundamental rights framework shall be applied as well.23\nThe following sections outline the key learnings from these regulatory attempts, discuss their\nASSESSMENTANDEFFECTSOFTHEREGULATORY\nCHOICES\nImpact of Definitional Choices\nSince the GDPR and DP LED definitions of biometric data require \"specific technical processing\" the\n19S\n See Article 10 Directive 2016/680. Note that in the two last situations, the need for an authorizing law doesn't seem to be required. For \"data\nmanifestly made public by the data subject, this is meant to cover social media.\n20\nArticle 35 GDPR and Article 27 Directive 2016/680. This DPIA requirement was part of the original EU Commission's GDPR proposal of January 25,\n2012 COM(2012) 11 final.\n 21 While the DPIA requirement adds important responsibility (and liability) for assessing the risks, necessity, and proportionality of biometric systems,\npost-GDPR experience already shows that such assessment is in general very difficult to conduct in practice. For the French SA's guidance, see\nCNIL, The Open Source PIA Software Helps to Carry out Data Protection Impact Assessment\" and its updates, June 25, 2019, https://www.cnil.fr/fr\nnode/23992\n22\nSee, e.g., CNIL (French SA),\"Delibération no. 2019-001\" January 10, 2019, https://www.cnil.fr/sites/default/files/atoms/files/deliberation-2019-\n001-10-01-2019-reglement-type-controle-dacces-biometrique.pdf. The document discusses the processing of employee biometric data for access\ncontrol to premises, devices, and apps at work, which requires such DPIA, Article 11.\n 23 See Kindt, Privacy and Data Protection Issues, 570 et seq; see also supra note 8 on the proportionality principle. The relevant fundamental rights\nof the European Convention on Human Rights and of the EU Charter that could be affected by biometric technology include, besides the right to\nprivacy and data protection, the right to freedom of expression and of free movement, non-discrimination, and the right to assembly. In relation to\nLFR and LEAs, see FRA,\"Facial Recognition Technology: Fundamental Rights Considerations in the Context of Law Enforcement, November 27,\n2019, https://fra.europa.eu/en/publication/2019/facial\nntal-rights-considerations-context-law.See alsoPeteFussey\nand Daragh Murray, \"Independent Report on the London Metropolitan Police Service's Trial of Live Facial Recognition Technology\" The Human\nRights, Big Data and Technology Project, July 2019. National traditions interpreting these fundamental rights must also be taken into account,\nadding complexity to the matter.",
    "Page_66": "66 |  Regulating Biometrics: Global Approaches and Urgent Questions\nthe need for law as required under Article 9.2 GDPR. It is the use of the data, rather than its sensitive\nnature or its ability to enable identification, that determines when data becomes biometric.24\nBecause of the way the definition was written, the risks of biometric data collection are not\nand the DP LED, this implies that companies and government can collect large databases of\nenforcement purposes.26\nFinally, the definition is not in line with the European Court of Human Rights case law, which has\nin databases interferes with the right to respect for private life.27 Such interference was confirmed\nfor facial images in Gaughran v. The United Kingdom, where the Court took facial recognition and\nfacial mapping techniques into account, and “found that the retention of the applicant's DNA profile\nfingerprints and photograph amounted to an interference with his private life.\"28\nfor identification purposes or could be used by automated processes, and regulation should also\nrestrict the storage of this data in databases.29 An alternative definition of biometric data could be:\n\"all personal data (a) relating directly or indirectly to unique or distinctive biological or behavioural\ncharacteristics of human beings and (b) used or fit for use by automated means (c) for purposes of\nidentification, identity verification, or verification of a claim of living natural persons.\"30\nLack of Clarity around Biometric \"Prohibition\" and Sweeping\nExceptions\nbiometrics comparisons (i.e., verification), and one-to-many (1:n) comparisons (i.e., identification).31\n 24  The definition of biometric data does not include so-called \"soft\" biometrics, such as emotions, since they usually do not allow for identification or\nidentity verification.\n 25Article 23 GDPR allows Union or MS law to restrict the rights of data subjects, including the right to information, e.g. to protect public security. See\nalso Article 13.3 Directive 2016/680.\n 26For example, LEAs could use FR technology combined with social media profls; or see the online dating investigation tool offered by Socialcatfish.\ncom, which has commercialized social media and dating profile data.\n27\nECtHR, S. and Marper 2008, S 86; ECtHR, M.K. vFrance 2013,S 26; see also Cons. const.(France) no. 2012-652, March 22, 2012(Loi protection de\nI'identite), S 6.\n 28 ECtHR, Gaughran v. The United Kingdom 2020, S70.\n29\n This should come first and in addition to a prohibition to use for identification purposes, except for precise limited exceptions determined by law\n30\n See also Kindt, 2013, Privacy and Data Protection Issues 144 et seq. and 851 et seq.\nSee and compare the wording of the prohibition with the definition of article 4(14) GDPR, which refers to the two functionalities (\"which allow or\nconfirm the unique identification\"): article 9 GDPR forbids only \"biometric data for the purpose of uniquely identifying, leaving it uncertain if this\nprohibition also includes processing for purposes of confirming identification (verification).",
    "Page_67": "I n   i    e a  a I  s\nMeanwhile, the Council of Europe and SAs have stated that biometric verification contains less risk\nthan biometric identification because no database is needed.32\nOn the other hand, one-to-many comparisons (i.e., identification) introduce additional risks,\naddress the relative risks of each functionality, discouraging or banning those that pose real\nprotections\nFinally, the broad exceptions and overall vagueness of the law leaves the door open for\nspecifically risky uses of biometric data like live facial recognition (LFR). The GDPR exceptions\npublic interest\" based on law. Because of the way this and other exceptions are worded, it remains\nLFR (e.g., at large stadium events).34 The GDPR and DP LED alone will not resolve these questions,\nand additional specific EU and national laws are needed.35\nCONCLUSION\nThe GDPR and DP LED approaches to defining biometric data exclude the collection of so-called\n\"raw\" data like facial images, yet protection is most important at the initial stage of the creation\nhuman rights case law and its own approach to data \"processing\" which is that data protection\nshould start at the collection stage. A comprehensive legal framework should also aim to restrict\n32\n(Conclusion). One shall hence keep in mind that it is precisely the use of databases against a general public in public places (or in places accessible\nto the public, such as shops) and the identification functionality that pose the most risk, e.g., of surveillance or of unwanted identification. Fo\nexample, verification could use local storage and strict safeguards that offer increased security for people trying to access phones or bank\naccounts, e.g., by local comparison of a facial image locally stored in a protected template form under the individual's control, e.g., on a smartphone,\nfor controlling access to a payment application. Verification and identification have also been rightly distinguished by data protection authorities\nsuch as the French SA: see CNIL, \"Communication central storage fingerprint,\" December 28, 2007, 5-6.\n33\n Such methods exist, in particular template-protection methods, permitting pseudonymous, revocable, and unlinkable biometric identifiers. See also\nCoE, Progress Report 2013 (supra note 3), 30-31 and Kindt, Privacy and Data Protection Issues, 801-807. Because of the data-protection-by-design\nobligation, such methods are very important.\n 34  See Danish SA, \"Tiladelse til behandling af biometriske data ved brug af automatisk ansigtsgenkendelse ved indgange pa Brondby Stadion, May 24\n2019, https://www.datatilsynet.dk/tilsyn-og-afgoerelser/tilladelser/2019/maj/tilladelse-til-behandling-af-biometriske-data-ved-brug-af-automatisk-\nansigtsgenkendelse-ved-indgange-paa-broendby-stadion/\n 35  Any interference with fundamental rights and freedoms requires a law that shall be sufficiently precise and certain (foreseeability) and accessible,\nin order to exclude arbitrariness. This is especially important for technology because \"the technology available for use is continually becoming\nalso be tempted to deploy LFR for controlling movement restrictions. Because of the risks posed for fundamental rights, the EU Commission\nrecently launched a debate about possibly additional legislation for remote biometric identification: see EU Commission, White Paper on Artificial\nIntelligence: a European Approach to Excellence and Trust, February 19, 2020, https://ec.europa.eu/info/publications/white-paper-artificial\nintelligence-european-approach-excellence-and-trust-en",
    "Page_68": "68 | Regulating Biometrics: Global Approaches and Urgent Questions\nany biometric data storage in databases, and should offer clear guidance as to any undesirable\nverification solutions, under precise conditions. More precise laws around police collection and\nnecessity and proportionality tests as they apply to law enforcement use.\nenhanced consideration of the fundamental rights' three-steps test, policymakers should adopt\nproportionate for substantial public interests described in law. This is crucial, especially if LFR\nassembly, which should not be left to case-by-case assessment.\ncollection and use, this chapter has aimed to highlight the challenges posed by uncritically\nadopting the text of the GDPR and DP LED. For any future legislation, it will be important to\nrecognize the risks and functionalities of biometric data systems, starting from the collection",
    "Page_69": "Els Kindt |  A First Attempt at Regulating Biometric Data in the European Union | 69",
    "Page_70": "70 |Regulating Biometrics: Global Approaches and Urgent Questions\nReflecting on the International\nCommittee of the Red Cross's\nBiometric Policy: Minimizing\nCentralized Databases\nBen Hayes (AWO agency, Consultant legal advisor to the ICRC)\nMassimo Marelli (Head of the ICRC Data Protection Office)\n he International Committee of the Red Cross (ICRC) works with some of the most\nvulnerable people in the world, providing humanitarian assistance to populations affected\nby armed conflict and other situations of violence.1 Like many other humanitarian\nAs part of its digital transformation agenda, the ICRC developed a Biometrics Policy (\"the Policy\")\nthat both facilitates the responsible use of biometrics and addresses data-protection challenges.\nICRC adopted the Policy in August 2019,2 which recognizes the legitimacy and value of using\nbiometrics to support its programmatic and operational objectives while also ruling out the\ncreation of any central, biometric databases in the short term. This article discusses some of the\nfactors brought to bear on the decision-making process we went through as an institution.3\nInternational Committee of the Red Cross, \"The ICRC's Mandate and Mission,\" https:/www.icrc.org/en/mandate-and-mission.\npolicy.\nThis article builds on Ben Hayes and Massimo Marelli,\"Facilitating innovation, ensuring protection: the ICRC Biometrics Policy,\" ICRC, Humanitarian\nLaw & Policy, October 18, 2019, https://blogs.icrc.org/law-and-policy/2019/10/18/innovation-protection-icrc-biometrics-policy.",
    "Page_71": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 71\nBIOMETRICSIN THEHUMANITARIAN SECTOR\nThe ICRC works in more than ninety countries and is part of a global humanitarian network\neducation, employment, and assistance to detained persons, and also helps restore family\nlinks by reuniting separated persons and finding missing persons. To address the logistical\nwhen providing services or assistance. The primary justification for this use is that recipients of\nto be identifiable.\ncontinuity of healthcare and some forms of humanitarian assistance clearly need people to be\nidentifiable (e.g., for provision of travel documents or financial services). For example, the United\nNations Refugee Agency (UNHCR) has a clear mandate to identify refugees and asylum seekers,\nand to provide them with identity documents6 (though it has been heavily criticized for deploying\nprovide people with an identity or supporting documentation. They have primarily developed and\nimplemented biometric ID systems because of the perceived efficacy and accountability gains\nsuch systems provide.8\nhumanitarian organizations to check or verify an individual's identity, these cannot be\nunequivocally associated with a single individual in the way that a biometric ID can. Biometric\ndatabases can also be used to prevent the same individual from registering in an aid program\nmore than once, which is attractive for humanitarian organizations that are concerned about\nICRC, \"The International Red Cross and Red Crescent Movement,\" https://www.icrc.org/en/who-we-are/movement.\n5\nSee, for example, \"Head to Head: Biometrics and Aid\", The New Humanitarian, July 17, 2019, https://www.thenewhumanitarian.org/\nopinion/2019/07/17/head-head-biometrics-and-aid; and Katja Lindskov Jacobsen, Kristin Bergtora Sandvik, and Sean Martin McDonald,\n\"Humanitarian Experimentation, IRC Humanitarian Law& Policy, November 28, 2017, https:/bogs.icrc.org/la-and-policy/20171/28/\nhumanitarian-experimentation/.\nUnited Nations High Commissioner for Refugees,\"Note on the Mandate of the High Commissioner for Refugees and His Office, Refworld, October\n2013, https://www.refworld.org/docid/5268c9474.html. Note: The ICRC also issues emergency travel documents, albeit very few by comparison.\n/\nSee for example Chris Burt,\"UNHCR Reaches 7.2M Biometric Records but Critics Express Concern, Biometric Update, June 24, 2019, https://www\nbiometricupdate.com/201906/unhcr-reaches-7-2m-biometric-records-but-critics-express-concern.\n8\nThe Engine Room and Oxfam, \"Biometrics in the Humanitarian Sector,\" March 2018: https://www.theengineroom.org/wp-content/uploads/2018/03/\nEngine-Room-Oxfam-Biometrics-Review.pdf.\n9\nLaura Gordon, \"Risk and Humanitarian Cash Transfer Programming: Background Note for the High Level Panel on Humanitarian Cash Transfers,\"\nOverseas Development Institute, May 2015, https://www.odi.org/sites/odiorg.uk/files/odi-assets/publications-opinion-files/9727.pdf.\n10\nGuidelines-for-ID4D-Diagnostics.pdf. Cash and other forms of direct financial disbursement are widely viewed as providing beneficiaries of \nhumanitarian programs with more dignity and autonomy than food parcels and other disbursed goods, but donors are concerned that these\nasne pe pn  aldaosns ao ae snod",
    "Page_72": "72 | Regulating Biometrics: Global Approaches and Urgent Questions\nand cash recipients, biometric data could offer a simple and straightforward way to meet multiple\noperational needs and legal obligations.11\npossible, and to ensure that scarce humanitarian services and assistance are provided to\nintended recipients. There is also implicit pressure to use biometrics from donors, which\nincreasingly demand \"end-to-end auditability\" (allowing the tracking of humanitarian funds from\ndonor to recipient) and make funding contingent on anti-fraud and accountability processes. All\nbeneficiary registration and aid distribution. And why not, if everyone else is doing it?\nRISKSAND CONCERNS\nConcerns about the use of biometrics in the humanitarian sector are well known, but are often\noverlooked.12 Biometric data are unique, immutable, and create a permanently identifiable record\nfor individuals in vulnerable humanitarian contexts who may not want to be identifiable forever.\nThe creation of a permanent biometric record underpins concern that this record could increase\nthe risk of harm to the persons concerned in the event it was subsequently accessed by or\nprovided to the regime or non-State actor they had fled.\nBiometrics constitute particularly sensitive data13 due to the potential for reuse or misuse, as\nwell as \"function creep,\" i.e., the possibility that biometrics may be used in a new way, separate\nfrom the original purpose and without the understanding or consent of the affected individuals.\n11\n These assumptions also dovetail with the UN's Sustainable Development Agenda, which mandates the provision of legal identity to alland targets\nincreased financial inclusion, tacitly encouraging States and the financial sector to predicate both on a biometric identity. See, for example,\nSustainable Development Goal (SDG) target 16.9: \"By 2030, provide legal identity for all including birth registration: Promote just, peaceful and\ninclusive societies.\" Financial inclusion is a target for eight of the seventeen SDGs. United Nations, Department of Economic and Social Affairs\nSustainable Development, The 17 Goals, https://sdgs.un.org/goals.\n12\n See, for example, Gus Hosein and Carly Nyst,“Aiding Surveillance, Privacy International, October 2013, https://privacyinternational.org/report/841/\naiding-surveillance. See also Katja Lindskov Jacobsen, \"On Humanitarian Refugee Biometrics and New Forms of Intervention, Journal of\n13  The General Data Protection Regulation (EU) 2016/679 (GDPR), for example, introduces a general prohibition against the processing of biometric\ndata unless,inter alia, the data subject has given their \"explicit consent\" (something which is problematic in a humanitarian context, as discussed\nfurther below); the processing is subject to a specific law or legal agreement; the processing is necessary to protect the vital interests of data\nadequate measures to protect the interests and safeguard the fundamental rights of the data subject (Article 9). The recently adopted \"Modernised\nCoE Convention 108+\" on data protection broadly adopts the same approach to biometric data as the GDPR by classifying them as \"sensitive data\nalso imposes restrictions on the processing of biometric data\n14 Affected populations have expressed serious concerns about the use of biometrics and potential access to the data by non-humanitarian\norganizations. See, for example, Aziz El Yaakoubi and Lisa Barrington,\"Yemen's Houthis and WFP Dispute Aid Control as Milions Starve\"\nReuters, June 4, 2019, https://www.reuters.com/article/us-yemen-security-wfp/yemens-houthis-and-wfp-dispute-aid-control-as-millions-starve\nidUSKCN1T51YO; \"Rohingya Refugees Protest, Strike Against Smart ID Cards Issued in Bangladesh Camps,\" Radio Free Asia, October 26, 2018,\nSeek Shelter in Rwanda, Voice of Africa News, March 8, 2018, https://www.voanews.com/africa/over-2500-burundi-refugees-congo-seek-shelter-\nrwanda.",
    "Page_73": "not want, understand, or consent to. Humanitarian databases may, for example, be integrated\nor government partners. Technology may also advance to allow biometric profiles to be used\nto ascertain additional information about the data subject-for example regarding their health,\nethnicity, or genetic makeup.\nStates have shown increasing interest in biometrics to monitor the movement of populations and\nuse of biometric ID systems to identify terrorist suspects, mandating all UN Member States to\n'develop and implement systems to collect biometric data, which could include fingerprints\nhave already come under pressure from States to disclose biometric data for non-humanitarian\ndata.16\nBiometric data use was a central theme at the 33rd International Conference of the Red Cross\n\"urges States and the Movement to cooperate to ensure that personal data is not requested or\nused for purposes incompatible with the humanitarian nature of the work of the Movement.\"19\nRATIONALIZING BIOMETRICSAT THEICRC\nPrior to the adoption of its biometrics policy, the ICRC was already employing biometrics in limited\nuse cases, for example in forensics and the restoration of family links, and by putting fingerprints\non the travel documents it issues (but not into any database). In addition to using DNA profiling\n15\n UN Security Council Resolution 2396, adopted December 21, 2017 under Chapter VIl of the UN Charter on “Foreign Terrorist Fighters.\" As the UN\nSpecial Rapporteur for the Protection and Promotion of Human Rights While Countering Terrorism has stated, the biometrics mandate provided by\nthe Security Council is \"deeply concerning\" because the Resolution does not contain any explicit reference to constitutional or legislative protections\nfor privacy or data protection. See Fionnuala Ni Aolain, “The UN Security Council, Global Watch Lists, Biometrics, and the Threat to the Rule of Law,\nJust Security,January 17, 2018, https://www.justsecurity.org/51075/security-council-global-watch-lists-biometrics/\n16\n Massimo Marelli, \"Hacking Humanitarians: Moving towards a Humanitarian Cybersecurity Strategy,\" ICRC, Humanitarian Law & Policy, January 16,\n17\nInternational Federation of Red Cross and Red Crescent Societies, 33rd International Conference, 2019, https://rcrcconference.org/about/33rd-\ninternational-conference/\n Reuniting families separated by conflict and disaster is a core activity of the International Red Cross and Red Crescent Movement globally. See\n\"Restoring Family Links While Respecting Privacy, including as it Relates to Personal Data Protection\" ( 33IC/19/R4), 33rd International Conference\n of the Red Cross and Red Crescent, December 9-12, 2019, https://rcrcconference.org/app/uploads/2019/12/33IC-R4-RFL-CLEAN_ADOPTED_\nen.pdf.\n19 Ibid., Article 11",
    "Page_74": "74 | Regulating Biometrics: Global Approaches and Urgent Questions\nto help identify human remains to determine the fate of the missing, the ICRC is exploring facial\nhumanitarian emergencies.20 \nThis is part of a broader ICRC strategy to transform and adapt its humanitarian response\nby seizing the opportunities that new technologies offer its operations and beneficiaries.\nManaging the attendant risks is central to this digital transformation agenda.21 Early in 2018,\nfollowing significant interest in expanding biometric data use, the ICRC Directorate requested an\nassessment of the operational, ethical, and reputational risks involved, as well as an institution-\nwide policy that would facilitate both innovation and data protection.\nICRC developed the policy over an eighteen-month period that included extensive research,\nanalysis, consultation, and reflection. ICRC reviewed all scenarios in which the ICRC processed or\nobjectives cannot be realized without using biometrics. Examples include using DNA to determine\nthe fate or whereabouts of the missing, or using facial recognition to match missing and\nmandate with respect to persons separated or missing in humanitarian emergencies.\nOther cases are much more challenging: for example, when the potential use case involves\nbiometrics for beneficiary management and aid distribution, where requiring the identification of\nindividuals may not be viewed as an integral part of an ICRC mandate-based activity. Because the\nau au noum panasp (uaa sy buoi puee aa uo pie pue uaa Aewud si asodnd\nfor biometrics, the ICRC determined that the \"legitimate interest\" of using a biometric identity-\nmanagement system did not outweigh the potential concerns over rights and freedoms. This\nbalancing test is typical of data-protection laws (e.g., as in GDPR), whenever a data controller\nrelies on their own interests as a basis for processing.25\n 20  See \"Rewards and Risks in Humanitarian Al: An Example, ICRC, Inspired, September 6, 2019, htps:/blogs.icrc.org/inspired/2019/09/06/\nhumanitarian-artificial-intelligence/\n21\n In addition to \"doing no harm, ICRC maintains principles of impartiality, neutrality, and independence. The protection of personal data that could\nbe misused or whose disclosure could put its beneficiaries at risk is an integral means of ensuring these principles are upheld. See ICRc,\"The\nlist/4046-the_fundamental_principles_of_the_international_red_cross_and_red_crescent_movement.pdf\n 22 See ICRC, \"Rules on Personal Data Protection,\" (\"ICRC Rules\"), https://www.icrc.org/en/publication/4261-icrc-rules-on-personal-data-protection. The\nrules were adopted by the Directorate of the ICRC on February 24, 2015 (updated on November 10, 2015), and updated and adopted by the ICRC\nAssembly on December 19, 2019.\n23\nICRC Rules, Article 1\n25 ICRC Rules, Article 1; GDPR, Article 6",
    "Page_75": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 75\nAfter careful consideration, ICRC concluded that it was possible to leverage the efficiency and\neffectiveness gains of biometric authentication, as well as end-to-end accountability in its aid\ndistributions, while also minimizing the risks to its beneficiaries. This balance rests on using\nbased system. In practice, this means that beneficiaries could be issued a card on which their\nbiometric data is securely stored, but that the ICRC will not collect, retain, or further process their\nbiometric data (and therefore not establish a biometric database)\naid reaches those individuals for whom it has been earmarked, but no other use will be possible\nIf the beneficiary wants to withdraw or delete their biometric data, they may return or destroy the\nthe biometric data of beneficiaries, the ICRC will not face such pressure because it will not have\nthe data.\nKEYFEATURESOFTHEPOLICY\nAdopted by the ICRC Assembly in August 2019, the ICRC Biometrics Policy sets forth staff and\nprogram roles and responsibilities,26 the legitimate basis for processing biometric data by the\nICRC,2/ the specific purposes and use cases for which the use of biometrics is authorized,28 and\nthe types of biometric data that may be processed by the ICRC.29 Specifically, it allows the ICRC\nto:\n· include the fingerprints of the holder on travel documents issued by the ICRC to persons\nwho have no valid identity papers, enabling them to return to their country of origin or\nhabitual residence or to go to a country which is willing to receive them;\n· use biometric identification systems to restrict access to strictly confidential information\nand/or mission-critical resources such as servers and control rooms in ICRC premises;\n· use fingerprints, facial scans, and DNA to identify human remains recovered from disaster\nor conflict zones or in connection with other situations of violence;\nmissing persons;\n· use biometric data to ascertain the identity or fate of specific individuals in the course of\ninvestigations related to the abduction of, or attacks upon, ICRC staff members;\non a case-by-case basis, where it has been determined that it is in the best interest of the\npersons concerned, collect biological reference samples for the purposes of DNA profiling\n· use biometrics to provide beneficiaries with a token-based verification credential such as a\ncard that can be used to verify their receipt of those services, where the token is held solely\nby the Data Subject.\n26\nICRC Biometrics Policy, Article 4.\n27\nIbid., Article 5\nIbid., Article 6.\n29Ibid.,Article 7",
    "Page_76": "76 | Regulating Biometrics: Global Approaches and Urgent Questions\nThere are additional caveats:\nrequire a high level of security and where profiling is limited to staff authorized to access\nthem).\npersons are actually related is required under national law or policy.\nThe Policy also expressly rules out the creation of biometric databases with respect to the\nauthorized use cases. Finally, where ICRC programs or delegations wish to process biometric\n\"consent,\" which humanitarian organizations have traditionally sought from the people who use\nhave been quite robust. In others, however, people have routinely signed \"consent forms\"\nor provided a thumbprint in lieu of a signature (e.g., for those unable to write; as part of its\nbiometrics review, the ICRC is also putting an end to this practice). \"Informed consent\" in data\n\"freely given, specific, informed indication of his or her wishes by which a Data Subject signals\nagreement to the Processing of Personal Data relating to him or her.\"31\nWhile the ICRC is firmly committed to transparency, it does not believe that consent provides a\ncannot be regarded as valid if the individual has no real choice: for example, where the provision\nunlikely to be \"freely given. In addition, power imbalances may imply no real \"choice, and\nbiometrics are concerned, it is extremely difficult to ensure that consent is genuinely \"informed,\nsince affected populations may not be able to fully comprehend the technology, information\nflows, risks, or benefits that underpin biometric data processing.\nThe Biometrics Policy requires that the ICRC explain the basis and purpose of data processing\nprocessing.32 The ICRC also seeks to ensure that beneficiaries have the opportunity to ask\nquestions and object if they wish, particularly where data may be shared with third parties.33 If\npeople do not want to provide their biometric or other personal data, or share their data with\n30\nIbid., Articles 10 and 11.\n31\nICRC Rules, Definitions: \"Consent.\n32\nICRC Biometrics Policy, Article 18. This is in line with the ICRC Rules on Personal Data Protectior\n33Ibid., Article 18.4",
    "Page_77": "Ben Hayes & Massimo Marelli | Reflecting on the International Committee of the Red Cross's Biometric Policy: Minimizing Centralized Databases | 77\nFinally, under no circumstances will the ICRC share biometric data with third parties, including\nauthorities, that may use them for non-humanitarian purposes.36 Even where exclusively\nhumanitarian grounds for sharing biometric data can be identified, strict conditions must be\nsatisfied before ICRC will transfer any data.37\nThe ICRC will review the Biometrics Policy at least every three years,38 including the decision\nnot to establish biometric databases for the purposes of identity management. ICRC will review\ndevelopments around the availability, security, cost, effectiveness, and impact of biometric\ntechnology, and may amend the Policy to widen the scope for using biometrics, or to introduce\nnewsafeguards.\nLESSONSLEARNED\nDuring its deliberations, the ICRC considered the option of not adopting a biometrics policy\nand leaving decisions about how and when to use these data to programs, operations, and\ndelegations in the field. This option was rejected as \"high risk on the basis that it could undermine,\ninter alia, the rights of the ICRC's beneficiaries, the 'do no harm' principle, and ICRC's reputation.\nWhile the internal organizational debates have been challenging, the Policy has provided much\nneeded clarity and operating procedures for staff who were struggling to balance the perceived\nbenefits and risks of specific uses.\nICRC consulted internal staff and external stakeholders in order to answer questions around\nCase-by-case assessment of the existing and possible use cases was fundamental in shaping\nthe ICRC Biometrics Policy. However, ICRC faced many challenges because it was already using\nbiometrics, and the new Policy could have led to changes in practice or prohibitions against\nAs biometric data use-case law and data-protection enforcement actions continue to expand, the\n34 Ibid., Articles 19 and 20.\n35\nIbid., Article 6.1\n36 Ibid., Article 14\n37\nIbid., Article 15\n38 Ibid., Article 21",
    "Page_78": "78 |Regulating Biometrics: Global Approaches and Urgent Questions\nPolicing Uses of Live Facial\nRecognition in the United\nKingdom\nPeter Fussey (University of Essex)\nDaragh Murray (University of Essex)\nBACKGROUNDTOTHEUSEOF FACIALRECOGNITION\nIN THE UK\nondon has a long history of trialing advanced surveillance technology. Police agencies first\ninstalled closed-circuit television (CCTV) cameras in the city in 1953, and until recently\ncity deployed one of the world's first automatic license plate recognition (ALPR) systems in the\nmid-1990s, and has since introduced crowd-modeling video analytics to survey its mass transit\nsystems.? London was also one of the first cities in the world to trial facial recognition (FR) in\nthe east of the city during the late 1990s, although technological limitations at the time led to its\nabandonment.3\nPete Fussey, \"Beyond Liberty, Beyond Security: The Politics of Public Surveillance, British Politics 3, no. 1 (April 2008): 120-135. See also Jess\nYoung, \"A History of CCRV Surveillance in Britain, SWNS, January 22, 2018, https://stories.swns.com/news/history-cctv-surveillance-britain-93449/\nLondon remains the most CCTV-heavy city outside of China.\nPete Fussey,\"Observing Potentiality in the Global City: Surveillance and Counterterrorism in London, International Criminal Justice Review 17, no.3\n(September 1, 2007): 171-192.\nPete Fussey,\"Eastern Promise? East London Transformations and the State of Surveillance, Information Polity, 17, no. 1 (January 2012): 21-34",
    "Page_79": "With rapid advancements in FR technology, the Metropolitan Police Service (MPS) conducted\na series of ten live facial recognition (LFR) test deployments between 2016 and 2019, moving\nto operational deployments in early 2020.4 South Wales Police have also been using LFR since\nby installing temporary cameras at a fixed geographic location6 for a fixed time period.7 Police\ngenerally mount the cameras on an LFR van with a control center used to monitor the live LFR\nfeeds and to communicate with officers on the ground. LFR cameras scan the faces of all\nprofiles against a watch list containing persons of interest. To date, police have only deployed\nLFR technology in this standalone manner, and have not, for example, integrated it into existing\ninfrastructure, such as CCTV networks.8\nPolice use of LFR has resulted in significant controversy, with a number of human rights and civil\nhave initiated advocacy campaigns calling for either a moratorium on the use of LFR,9 or an\nthe MPS invited the authors to provide an independent academic report on the last six LFR\ntest deployments.12 We conducted ethnographic observations from beginning to end of each\n of other planning meetings. We also held interviews with key stakeholders and analyzed large\nquantities of MPS internal documents. In this piece, we draw on this research to explore three\nfor LFR; 2) the inability and failure of existing institutions and laws to meaningfully restrict this\ntechnology; and 3) the operational considerations unique to LFR. Our focus is on working toward\nLFR deployments. However, this consideration only comes into play if an appropriate legal basis\nexists.\nHaving moved out of the \"test\" phase, the MPS now has authority to deploy LFR on the basis of operational inteligence. For further information\nsee Metropolitan Police,\"Live Facial Recognition, n.d., https://www.met.police.uk/advice/advice-and-information/facial-recognition/live-facial-\nrecognition/\n5\nFor more information, see South Wales Police,\"Facial Recognition Helps South Wales Police Become Smarter, Creating a Safer and Connected\nCommunity\" n.d., http://afr.south-wales.police.uk. The list of deployments is available at htps://afr.south-wales.police.uk/wp-content/\nuploads/2020/04/All-Deployments.pdf.\n>\nThis is typically a number of hours; to date, no deployments have lasted longer than a day.\n8\n9\nSee, for example, Carly Kind,\"Biometrics and Facial Recognition Technology-Where Next?\" Ada Lovelace Institute, July 2, 2019, https://www\nadalovelaceinstitute.org/biometrics-and-facial-recognition-technology-where-next/.\n See, for example, Liberty, Resist Facial Recognition, https://www.libertyhumanrights.org.uk/campaign/resist-facial-recognition/; and Big Brother\nWatch, Stop Facial Recognition, https://bigbrotherwatch.org.uk/campaigns/stop-facial-recognition/\n11 The complaint was brought by Ed Bridges, who believes he was subject to facial recognition processing at a peaceful anti-arms trade protest, and\nwhile Christmas shopping. See Liberty,\"Liberty Client Takes on Police in Ground-Breaking Facial Recognition Challenge-Hearing Opens Today, May\ntoday/.\n12\n Peter Fussey and Daragh Murray, \"lndependent Report on the London Metropolitan Police Service's Trial of Live Facial Recognition Technology,\nUniversity of Essex Human Rights Centre, section 2,1.1, July 9, 2019, http://repository.essex.ac.uk/24946/",
    "Page_80": "80 |  Regulating Biometrics: Global Approaches and Urgent Questions\nto biometric processing) is directly engaged. Additional, discrete right-to-privacy issues are\ntheir democratic rights due to a fear of the consequences that may follow.15 This may harm a\nnumber of rights, including the right to freedom of expression, the right to freedom of assembly\nand association, and the right to freedom of religion.16\nRights' ), requires that any interference with a right be \"in accordance with the law.\" As such, any\nmeasure interfering with human rights protections must have a legal basis, and that legal basis\nis the foreseeability of the law.18 If a measure fails to satisfy the \"in accordance with the law\"\nrequirement, it is unlawful in and of itself.\nTHE COMMON LAWASALEGALBASIS FORLIVE\nFACIAL RECOGNITION\nlife and property, preserving order, preventing the commission of offenses, and bringing\noffenders to justice.19 Although no legislation exists that explicitly authorizes police use of LFR,\nthe government has claimed that these common-law powers provide sufficient implicit legal\nauthorization to satisfy the \"in accordance with the law\" test.\nIn Bridges v. South Wales Police, the UK High Court agreed with the Government,20 indicating that\nappeal, and this finding is a key point of contention.\n13\nR(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 59\n14\nFor further discussion on indirect discrimination, see D.H. and Others v. the Czech Republic, Judgment ECtHR, App. No. 57325/00, November 13,\n2007,para.184.\n15\nSpiral of Silence Effects in the Wake of NSA Internet Monitoring\" Journalism & Mass Communication Quarterly 93, no. 2 (2016): 296-311; for\na general discussion, see Daragh Murray and Pete Fussey, \"Bulk Surveillance in the Digital Age: Rethinking the Human Rights Law Approach to\nBulk Monitoring of Communications Data, Israel Law Review 52, no. 1 (March 2019): 31-60. For a discussion of the chilling effect as it applies to\njournalists, see Centro Europa 7 S.R.L. and Di Stefano v. Italy, Judgment, European Court of Human Rights, App. No. 38433/09, June 7, 2012, para\n129.\n16  For a more in-depth discussion of potential human rights harms, see Fussey and Murray,\"lndependent Report, section 2,1.2, htp://repository.essex\nac.uk/24946/\n17\nSee, e.g., Shimovolos v. Russia, Judgment, ECtHR, App. No. 30194/09, June 21, 2011, para. 67\n18\n Catt v. the United Kingdom, Judgment, ECtHR, App. No. 43514/15, January 24, 2019, para. 94.\n19\n See, for example, Metropolitan Police,\"Live Facial Recognition, (LFR) MPS Legal Mandate, p. 5, July 23, 2018, https://www.statewatch.org/media/\ndocuments/news/2018/dec/uk-live-facial-recognition-lfr-mps-legal-mandate.pdf.\nR(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 78:“For these reasons, we consider\nthe police's common law powers to be amply sufficient' in relation to the use of AFR Locate. The police do not need new express statutory powers\nfor this purpose.\" (\"AFR Locate\" is South Wales Police's nomenclature for LFR.)\n 21 R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 78.",
    "Page_81": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 81\ncircumstances in which a particular measure may be deployed, such that those circumstances\nare foreseeable, thereby protecting against arbitrary rights interference. Relying on the common\nfor LFR, and that new statutory powers were not required, was the classification of LFR as a\nnonintrusive means of obtaining information,23 and as \"no more intrusive than the use of CCTV in\nthe streets.\"24 This is clearly contentious: it appears inconsistent with common understandings of\nthe UK. It also appears inconsistent with the High Court's own finding that-as a form of biometric\nfield of vision.25\nprovide the legal basis for the use of LFR is likely to be incompatible with the UK's obligations\nwill provide further guidance in this regard. However, irrespective of the outcomes of these cases,\nestablishing an explicit legal and regulatory basis for the use of LFR would provide much needed\nclarity, both for the public and for the police\nOTHER LAWS, LEGISLATIONS, AND AGENCIES THAT\nAPPLYTO LFR\nPolice documentation and political debate have consistently referred to the oversight roles of the\nmultiple data-protection and surveillance-related authorities in the UK.26 These include the UK's\ndata-protection authority, the Information Commissioner's Office (Ico); the Surveillance Camera\nCommissioner; the Biometrics Commissioner; and the Investigatory Powers Commissioner's\nOffice. While these agencies have contributed to the debate, each body is narrowly relevant\nto a specific aspect of LFR and, critically, they do not have explicit authorization to limit LFR\ndeployments. Indeed, while many of these regulatory bodies are heralded as a safeguard to\npromote appropriate use, their mandates do not provide meaningful oversight. This is explained in\nthe following table\nprotection against arbitrariness are equally applicable in this regard. This conclusion is supported by relevant case law. See, for example, S and\n23\nR(Bridges) v. CCSWP and SSHD,[2019] EWHC 2341 (Admin), Case No. CO/4085/2018, September 4, 2019, para. 74.\n24\n25\nThis finding distinguishes LFR as more invasive than CCTV. See R(Bridges) v. CCSWP and SSHD, [2019] EWHC 2341 (Admin), Case No\nCO/4085/2018, September 4, 2019, paras. 59, 62\n 26 As noted above, these considerations are irrelevant i the \"in accordance with the law\" requirement is not satisfied.",
    "Page_82": "82 |Regulating Biometrics: Global Approaches and Urgent Questions\nAuthority\nRole\nApplication to LFR\nThe Information\nOversees issues relating to data\nAlthough important, data protection\n Commissioner's\nprotection in the UK, particularly\nlaw cannot adequately address the\nOffice (ICO)\nthe Data Protection Act 2018\nbroad range of potential human\nand the General Data Protection\nrights harms brought about by police\nRegulation.27 In 2017, they\nLFR deployments. It does not, for\npublished \"In the Picture: A Data\nexample, fully address issues relating\nProtection Code of Practice\nto whether the use of LFR is necessary\nfor Surveillance Cameras\nor proportionate. As such, the impact\nand Personal Information,\"\nof the ICO on the overall LFR debate is\nwhich provided best practices\nrelatively limited.\nfor automated recognition\ntechnologies.28\nThe Surveillance\nEstablished by the Protection of\nWhile they are primarily focused\nCamera\nFreedoms Act 2012 to oversee\non CCTV systems, and LFR is\nCommissioner\nthe use of closed-circuit television\nimplemented through standalone video\nsystems (CCTV).29\nsystems, they have published guidance\non police use of LFR.30\nThe Biometrics\nEstablished by the Protection of\nThe Biometrics Commissioner's role is\nCommissioner\nFreedoms Act 2012 to oversee\nrestricted in statute to fingerprints and\nretention and use of biometric\nDNA data, and so does not extend to\ninformation.31\nLFR. The Commissioner has published\nseveral statements questioning the\nuse of LFR and has said that \"we need\nproper governance of new biometric\ntechnologies such as LFR through\nlegislation.\"32\nThe Investigatory\nEstablished under the\nAs currently deployed, the principal\nPowers\nInvestigatory Powers Act 2016.\nuses of LFR by police in the UK are not\nCommissioner's\nhas authority to oversee covert\nclassified as covert. This may change\nOffice\npolice deployments.33\ngoing forward.\n27\n See, further, the Information Commissioner's Office (ICo), https:/ico.org.uk/about-the-ico/.\n28\nICO, “ln the Picture: A Date Protection Code of Practice for Surveillance Cameras and Personal Information, Version 1.2, June 9, 2017, https:/ico.\norg.uk/media/1542/cctv-code-of-practice.pdf.\n29\nProtection of Freedoms Act ref. See, further, the Surveillance Camera Commissioner, https://www.gov.uk/government/organisations/surveillance\ncamera-commissioner/about.\n30\nSee, for example, the Surveillance Camera Commissioner's Code of Practice, June 2013, https://www.gov.uk/government/publications/surveillance-\ncamera-code-of-practice; and “The Police Use of Automated Facial Recognition Technology with Surveillance Camera Systems\" Section 33\nProtection of Freedoms Act 2012, March 2019, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/\nfile/786392/AFR_police_guidance_of_PoFA_V1_March_2019.pdf.\n31\nProtection of Freedoms Act ref. See, further, Office of the Biometrics Commissioner, https://www.gov.uk/government/organisations/biometrics-\ncommissioner/about.\n32\nSee, for example, GOV.UK, \"Automated Facial Recognition, September 10, 2019, https://www.gov.uk/government/news/automated-facial-\nrecognition, https://www.gov.uk/government/news/biometrics-commissioner-on-the-police-use-of-live-facial-recognition.\n33\nInvestigatory Powers Act 2016, https://www.legislation.gov.uk/ukpga/2016/25/contents/enacted.",
    "Page_83": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 83\nAs it stands, police use of LFR in the UK is not subject to adequate oversight or meaningful\nregulation. This must urgently be addressed\nISSUES ARISINGIN THE CONTEXT OF POLICELIVE\nFACIALRECOGNITION DEPLOYMENTS\nThis section examines a number of issues arising in the context of LFR deployments, including\nwatch lists, the \"presumption to intervene\" and associated deficits in effective human oversight\nthe uncertainty associated with LFR deployments, contesting police claims of utility, and highlight\nproblems arising from the absence of appropriate regulation\nOperational Considerations\nof the total number of individuals passing through an LFR camera's field of vision during a\ngiven deployment. These numbers are widely cited by supporters of LFR, yet they offer only a\nof this approach was adopted by the MPS in a recently published evaluation of their LFR trial\ndeployments,35 leading to widely publicized claims that the technology was \"70% effective.\"36\nsupport the claim of 70 percent effectiveness) and live data (camera performance when there is\nno certainty about whether suspects will walk past the cameras)\nAnother shortcoming of this methodology is the way it de-emphasizes the impact of LFR on\nthe individual rights-based interferences brought by LFR. Other measures of LFR performance\nhuman might decide the LFR system is wrong, regardless of the veracity of the computational\ndecision.\n34  See comments by Baroness Wiliams of Trafford regarding \"a one in 4,500 chance of triggering a false alert,\" House of Lords, January 27, 2020,\nhttps://www.theyworkforyou.com/lords/?id=2020-01-27a.1300.2&p=12902.\n 35 National Physical Laboratory and Metropolitan Police Service, \"Metropolitan Police Service Live Facial Recognition Trials,\" February 2020, https://\nwww.met.police.uk/SysSiteAssets/media/downloads/central/advice/met/facial-recognition/met-evaluation-report.pdf.\n 36 Vikram Dodd,\"Met Police to Begin Using Live Facial Recognition Cameras in London, Guardian, January 24, 2020, https://www.theguardian.com/\ntechnology/2020/jan/24/met-police-begin-using-live-facial-recognition-cameras.\n37\nBethan Davies, Martin Innes, and Andrew Dawson, An Evaluation of South Wales Police's Use of Automated Facial Recognition (Cardiff: Universities\nPolice Science Institute, Crime and Security Research Institute, Cardiff University, 2018)",
    "Page_84": "84 |  Regulating Biometrics: Global Approaches and Urgent Questions\nWe designed our independent academic review of the MPS system to address the above\na. When an LFR system matches someone to the watch list, how often is it verifiably correct?39\nb. To what extent do human adjudicators consider LFR matches to be credible? To understand if\ncheck of the suspect.\nFor (a), our research found that out of forty-two computer-generated LFR matches, eight were\nverifiably correct (19.05 percent). For (b), human adjudicators judged twenty-six out of forty-two\nmatches were sufficiently credible to apprehend the matched individual (61.91 percent), meaning\nthat humans overwhelmingly overestimated the credibility of the system. Four of these matched\nsystem.\nTwo conclusions can be drawn from this. First, there is a \"presumption to intervene\" on behalf of\nhuman operators assessing the credibility of LFR matches. Second, this tendency of deference to\nmajority ofcases.\nThese conclusions hold relevance for considerations over the form of human adjudication taking\nplace around LFR systems. Policy emphasizes the importance of \"the human in the loop\" as\na safeguard against algorithmic-induced harms. That human adjudication takes place is not\nin question, however. The issue at stake is the form it takes, and the degree of critical human\nA final question is whether LFR is discriminatory. UK police forces have made repeated claims\na complex issue and covers both the capability of the technology in identifying faces from a\nconclusions. For example, the US National Institute of Standards and Technology (NiST)\nclaims made in the technical evaluation of the MPS LFR scheme that \"differences in FR algorithm\nnumber of matches themselves are not statistically significant.\n38\nSup.35\n39 In other words, could it be definitively concluded that the individual identified by LFR matched the individual on the watch list, such as by means of a\nsubsequent identity check?\n40\n See, for example, public statements by Metropolitan Police Commissioner Dame Cressida Dick, RUSl Annual Security Lecture, London, February 24,\n41\ndoi.org/10.6028/NISTIR.8280\n 42   National Physical Laboratory and Metropolitan Police Service,\"Metropolitan Police Service Live Facial Recognition Trials,\" p.4.",
    "Page_85": "Peter Fussey & Daragh Murray | Policing Uses of Live Facial Recognition in the United Kingdom | 85\nAccording to the MPS statistics, twenty-eight people were engaged by a police officer after being\nmatched by LFR systems across their ten test deployments. We contend that it is impossible to\nConcerns over this issue have been most recently articulated in March 2020 in calls by Great\nBritain's Equality and Human Rights Commission to suspend the use of facial recognition in\nEngland and Wales until its impact has been independently scrutinized.43\nCONCLUSION\nlaw\" test established by human rights law. Second, although a number of UK regulatory bodies\nengage in this area, there is no dedicated body with authority to limit or effectively oversee LFR\nhuman oversight, and discriminatory outcomes. These highlight the practical consequences and\n43\nEquality and Human Rights Commission,\"Facial Recognition Technology and Predictive Policing Algorithms Out-pacing the Law,\" March 12, 2020,\nology-and-predictive-policing-algorithms-out-pacing-law",
    "Page_86": "86 |Regulating Biometrics: Global Approaches and Urgent Questions\nA Taxonomy of Legislative\nApproaches to Face Recognition\nin the United States\nJameson Spivack (Georgetown Center on Privacy and Technology)\nClare Garvie (Georgetown Center on Privacy and Technology)\nINTRODUCTION:POLICEFACERECOGNITIONIN THE\nUNITEDSTATES\nn December 25, 2015, Florida resident Willie Allen Lynch was arrested for selling fifty\ndollars' worth of crack cocaine to two undercover Jacksonville sheriffs three months\nearlier. The only thing tying Mr. Lynch to the crime was a face recognition search\ncomparing photographs the officers had taken of the drug sale to the county's mugshot database.\nThe search returned five possible matches-Mr. Lynch and four other suspects. Mr. Lynch and\nhis defense attorney were given no information about the use of face recognition: its accuracy,\npotential biases, or even a list of the other possible suspects. Despite this, Mr. Lynch, who\nmaintains his innocence, was sentenced to eight years in prison.\nSee Lynch v. State, 260 So. 3d 1166 (Fla. Dist. Ct. App. 2018). For an overview of how face recognition was used in the case, see Lynch v. State, No.\nSC2019-0298 (Fla. Sup. Ct. 2019), Amici Curiae Brief in Support of Petitioner, available at htps://www.aclu.org/sites/default/files/field_document/\nflorida_face_recognition_amici_brief.pdf. The lower court's decision was affrmed on appeal, and the State Supreme Court determined it did not have\n(   ) a      0",
    "Page_87": "Jameson Spivack & Clare Garvie I A Taxonomy of Legislative Approaches to Face Recognition in the United States | 87\nPolice use of face recognition is pervasive, affects most Americans, and, until very recently,\nhas persisted under a widespread lack of transparency, oversight, and rules governing its use.2\nPolice departments across the United States have deployed face recognition technology in\nagencies across the country had access to a face recognition system.5 Because thirty-one states\nallow police searches of DMV databases, more than half of all American adults can be identified\nhave also used Clearview Al's face recognition service, which has amassed a database of an\nadditional three billion images scraped from Facebook, Instagram, Twitter, Venmo, YouTube, and\nelsewhere.\nface recognition by the FBl.8 It made recommendations to increase transparency, enhance privacy\nand many other reports have highlighted unique risks posed by police face recognition use\n· Face recognition poses a threat to privacy. Under the Fourth Amendment of the US\nConstitution, the right to privacy extends beyond the home, protecting \"reasonable\nexpectations of privacy\" in some public settings and activities.9 Face recognition gives police\nthe power to conduct identity-based surveillance and the ability to scan and identify groups\nof people in secret, as well as to track someone's whereabouts through a network of security\ncameras. Without a warrant, this power may violate the Fourth Amendment, interpreted in the\nSupreme Court's 2018 decision in Carpenter v. United States as including a right to privacy\nin our movements across time and space.10 The enrollment of most American adults into\nlaw enforcement access to personal data, to which the American public did not consent.11\n2\nFor an overview of the state of face recognition and laws governing its use, see Clare Garvie, Alvaro M. Bedoya, and Jonathan Frankle,\"The \nPerpetual Line-Up: Unregulated Face Recognition in America\" Georgetown Law Center on Privacy & Technology, (October 18, 2016): 25, 35, https://\nwww.perpetuallineup.org/report.\nSee Pinellas County Sherif's Offce, Florida's Facial Recognition Network (Mar. 26, 2014), available at https://drive.google.com/file/d/0B-\nGeofeedia, Baltimore County Police Department and Geofeedia Partner to Protect the Public During Freddie Gray Riots (obtained by ACLU Northern\nSee supra note 2, at 25. This is a conservative estimate-the actual number is likely much higher. Prior to being terminated by the Attorney General's\nOffice, allaw enforcement agencies in the country were able to request searches of the Vermont driver's license face recognition system. See\nimmediate-end-dmv-facial-recognition-program,\nSee Statement of Clare Garvie, Senior Associate, Center on Privacy & Technology at Georgetown Law before the U.S. House of Representatives\nCommittee on Oversight and Reform (May 22, 2019), 5, available at https://docs.house.gov/meetings/G0/G000/20190522/109521/HHRG-116-\nGO00-Wstate-GarvieC-20190522.pdf.\nSee Kashmir Hill,\"The Secretive Company That Might End Privacy as We Know It, New York Times, January 18, 2020, https://www.nytimes.\ncom/2020/01/18/technology/clearview-privacy-facial-recognition.html. Former Center technologist Jonathan Frankle cautioned against just\nsuch a tool in 2016. See Jonathan Frankle, \"How Russia's New Facial Recognition App Could End Anonymity,\" Atlantic, May 23, 2016, https://www.\ntheatlantic.com/technology/archive/2016/05/find-face/483962/\n8\nGovernment Accountability Office (GAO), “Face Recognition Technology: FBI Should Better Ensure Privacy and Accuracy,\" May 2016, https://www\ngao.gov/assets/680/677098.pdf.\n9\nU.S. Const. Amend. IV; Katz v. United States, 389 U.S. 347 (1967)\n10\nCarpenter v. United States, 138 S. Ct. 2206, 2217 (2018).\n11 See supra note 6, at 5-7.",
    "Page_88": "88 | Regulating Biometrics: Global Approaches and Urgent Questions\nthe US Constitution protects the right to free speech, assembly, and association.12 As law\nand lead to self-censorship and inhibition\"-chilling our ability to participate in constitutionally\nprotected activities.13\n· Searches may lead to misidentifications. While the algorithms behind face recognition have\nimproved significantly since 2001, misidentification is still a major issue. Low-quality images,\nincrease the odds that the wrong person will be investigated, arrested, and charged with a\ncrime they did not commit.14\n· Face recognition may have a disparate impact on communities of color. Communities\nsurveillance.15 In San Diego, for example, police have used face recognition technology and\nlicense-plate readers up to two and a half times more on people of color than expected by\npopulation statistics.16 The technology performs less accurately on people of color, meaning\nthe risks of the face recognition police use, and the mistakes it may make, will not be\ndistributed equally.\n. The failure to disclose a face recognition search may deprive a defendant of due process.\nThe risks of misidentification and bias are not mitigated by a fair, transparent court process.\ninnocence. Per the constitutional right to due process and the Supreme Court's decision in\nand indeed the vast majority of cases involving a face recognition search, this information is\nnot disclosed.18\nIn response to growing concern over the risks that the use of unregulated police face recognition\nposes to our civil rights and liberties, legislators have begun introducing-and passing-face\nrecognition bans, moratoria, and regulatory bills.19\n12\nU.S. Const. Amend. I\n13\nInternational Justice and Public Safety Network (Nlets), “Privacy Impact Assessment Report for the Utilization of Facial Recognition Technologies to\nIdentify Subjects in the Field, June 30, 2011, 2, https://www.eff.org/files/2013/11/07/09_-_facial_recognition_pia_report_final_v2_2.pdf\n14\n For a discussion of how face recognition is used in practice and its associated risks, see Clare Garvie,\"Garbage In, Garbage Out: Face Recognition\non Flawed Data, Georgetown Law Center on Privacy & Technology, May 16, 2019, https://www.flawedfacedata.com\n15\n See supra note 2 at 56 (describing disproportionately high arrest rates of black Americans); see Grother, Ngan, & Hanoaka, Face Recognition Vendor\nTest (FRVT) Part 3: Demographic Effects, Nat'l Institute of Standards and Technology (NIST) (Dec. 2019), https://nvlpubs.nist.gov/nistpubs/ir/2019/\nNIST.IR.8280.pdf (\"We found empirical evidence for the existence of demographic differentials in the majority of contemporary face recognition\nalgorithms that we evaluated.\")\n16S\n See, e.g., Automated Regional Justice Information System, San Diego's Privacy Policy Development: Efforts & Lessons Learned, 11, available at\nhttps://drive.google.com/file/d/1ZR2jiLcBMUKnHTRk1ZC248NbFUqNRww/view?us p=sharing (indicating that black Americans were 1.5-2.5 times\nmore likely to be the targets of police use of licence-plate readers and face recognition technology).\nBrady v. Maryland, 373 U.S. 83, 87 (1963) (holding that the suppression of evidence that is material to the guilt or innocence of the accused violates\nhis due process rights under the Fourteenth Amendment).\n Most law enforcement agencies consider face recognition searches to produce “investigative leads\" only, not probable cause to make an arrest. But\nin practice, face recognition matches are often not independently corroborated through additional investigative steps before an arrest is made. See\nsup. note 14",
    "Page_89": "PROPOSEDANDENACTEDLEGISLATION\nGenerally, there have been three legislative approaches to regulating face recognition in the United\nStates: complete bans, moratoria, and regulatory bills. Moratoria can be further broken down into\nand directive moratoria, which \"pause\" face recognition use and require legislative action-such\nas a task force or express statutory authorization-to supersede the moratoria. Most of these\nbills have covered all government use of face recognition, with particular attention given to limits\nplaced on police use. This section focuses on police use as well.\nType of legislation\nWhat it does\nExamples\nBan\n Complete shutdown of all\nEnacted: San Francisco, CA;20\nface recognition use\nCambridge, MA21\nProposed: Nebraska22\nMoratorium: time-bound \nFace recognition use paused\nEnacted: Springfield, MA23\nfor a set amount of time\nProposed: Maryland24\nMoratorium: directive\nFace recognition use paused,\nProposed: Massachusetts25\nrequires legislative action to\nsupersede\nRegulatory bill\nRegulates specific elements\nEnacted:\nof face recognition, along\nCalifornia: prohibited in conjunction\na spectrum from narrowly\nwith police body-worn cameras26\nfocused to broader\n(narrower)\nWashington: regulates numerous\nelements27 (broader)\n See Kate Conger, Richard Fausset, and Serge F. Kovaleski,\"San Francisco Bans Facial Recognition Technology\" New York Times, May 14, 2019,\nhttps://www.nytimes.com/2019/05/14/us/facial-recognition-ban-san-francisco.html\nSee Jackson Cote,\"Cambridge Bans Facial Recognition Technology, Becoming Fourth Community in Massachusetts to Do So, MassLive,\nFebruary 27, 2020, https://www.masslive.com/news/2020/01/cambridge-bans-facial-recognition-technology-becoming-fourth-community-in-\nmassachusetts-to-do-so.html.\n22\nSee LB1091, \"Adopt the Face Surveillance Privacy Act, Nebraska Unicameral Legislature, available at https://www.nebraskalegislature.gov/bills/\nview_bill.php?DocumentID=41387.\n23\n See Jackson Cote,\"Springfield City Council Passes Facial Recognition Moratorium, MassLive, February 25, 2020, https://www.masslive.com/\nspringfield/2020/02/springfield-city-council-passes-facial-recognition-moratorium.html.\n24\nMD S.B.857 (2020), available at http://mgaleg.maryland.gov/2020RS/bills/sb/sb0857F.pdf.\n25\nMA S.B. 1385 (2019), available at https://malegislature.gov/Bills/191/S1385.\n26\nCA A.B. 1215 (2019) (prohibited only until Jan. 1, 2023), available at https://leginfo.legislature.ca.gov/faces/bilTextClient.xhtml?bill\nid=201920200AB1215.\nWA Engrossed. Subst. S.B. 6280 (2020), available at http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20\nLegislature/6280-S.PL.pdf.",
    "Page_90": "90 | Regulating Biometrics: Global Approaches and Urgent Questions\nA. Bans\nThe strongest legislative response is to ban the use and acquisition of the technology completely\ncities in California and Massachusetts. As of July 2020, the following municipalities had banned\nMassachusetts; Cambridge, Massachusetts; Easthampton, Massachusetts; Northampton,\nMassachusetts; Oakland, California; San Francisco, California; and Somerville, Massachusetts.28\nA number of states proposed bans on face recognition during the 2019-2020 legislative session:\nNebraska, New Hampshire, New York, and Vermont.29\nof face recognition technology. They represent what is possible with a transparent, democratic\nprocess, and the power of proactive localities. In the words of the San Francisco city supervisor\nprecisely because they are headquartered here.\"30 It is unclear at this point, however, whether face\nrecognition bans will take hold at the local, state, or federal level. Some jurisdictions may also\nfind the bans to be unintentionally overbroad, restricting uses of the technology deemed to be\nnecessary or uncontroversial.31\nB. Moratoria\nAnother strong measure that a legislature can take is to place a moratorium on the technology,32\nwhich has two forms: time-bound and directive.\n28  See Peter Hegarty, “East Bay City Becomes Latest to Ban Use of Facial Recognition Technology\" East Bay Times, December 18, 2019, https://www\neastbaytimes.com/2019/12/18/east-bay-city-becomes-latest-to-ban-use-of-facial-recognition-technology; see Tom McKay,\"Berkeley Becomes \nFourth U.S. City to Ban Face Recognition in Unanimous Vote, Gizmodo, October 16, 2019, https://gizmodo.com/berkeley-becomes-fourth-u-s-city-\nto-ban-face-recogniti-1839087651; see Nik DeCosta-Klipa,\"Boston City Council Unanimously Passes Ban on Facial Recognition Technology\" Boston.\ncom, June 24, 2020, https://www.boston.com/news/local-news/2020/06/24/boston-face-recognition-technology-ban; see ACLU of Massachusetts,\nsurveillance; see sup. note 20; see Michael Connors, “Easthampton Bans Facial Recognition Technology\" Daily Hampshire Gazette, July 3, 2020\nhttps://www.gazettenet.com/Easthampton-City-Council-passes-ordinance-banning-facial-recognition-survaillance-technology-35048140; see\nJackson Cote,\"Northampton Bans Facial Recognition Technology, Becoming Third Community in Massachusetts to Do So, MassLive, February 27,\n2020, https://www.masslive.com/news/2019/12/northampton-bans-facial-recognition-technology-becoming-third-community-in-massachusetts-\nto-do-so.html; see CBS SF,\"Oakland Officials Take Steps Towards Banning City Use of Facial Recognition Tech, July 16, 2019, https://sanfrancisco,\ncbslocal.com/2019/07/16/oakland-officials-take-step-towards-banning-city-use-of-facial-recognition-tech; see sup. note 20; see Alex Newman,\n\"Somerville Bans Facial Recognition Technology\" Patch, June 28, 2019, https://patch.com/massachusetts/somerville/somerville-bans-facial-\nrecognition-technology.\n NE L.B. 1091 (2020), available at https://www.nebraskalegislature.gov/FloorDocs/106/PDF/Intro/LB1091.pdf; NH H.B. 1642 (2020), available\nat http://gencourt.state.nh.us/bill_status/bilText.aspx?sy=2020&id=1202&txtFormat=pdf&v=current; NY S.B. 7572 (2020), available at https://\nlegislation.nysenate.gov/pdf/bills/2019/S7572: VT H. 929 (2020), available at htps://legislature.vermont.gov/Documents/2020/Docs/BILLS/H-\n0929/H-0929%20As%20Introduced.pdf.\n See sup. note 20.\n31\n See Tim Cushing,\"San Francisco Amends Facial Recognition Ban after Realizing City Employees Could No Longer Use Smartphones,\" Techdirt,\nbiometric lock feature on city-issued cell phones.\n32\n Moratoria have been used in surveillance policymaking in the past. For example, in 2013, Virginia placed a two-year moratorium on government use\nthe use of drones and assuring public participation in the oversight of their use.\" See ACLU, \"Virginia House of Delegates and Senate Approve\nTwo Year Moratorium on Drones,\" February 6, 2013, htps://www.aclu.org/press-releases/virginia-house-delegates-and-senate-approve-two-year\nmoratorium-drones",
    "Page_91": "Jameson Spivack & Clare Garvie I  A Taxonomy of Legislative Approaches to Face Recognition in the United States I 91\n1.Time-bound moratoria\nTime-bound moratoria stop virtually all use of face recognition for a predetermined amount of\ntime.33 The purpose of this pause is to give elected officials and the public time to learn about face\nand how, to regulate face recognition.\nAt the municipal level, in early 2020, Springfield, Massachusetts, placed a moratorium on face\nother provisions or directions, but rather states the moratorium \"shall remain effective for a period\nfurther action required by the General Assembly, shall be abrogated and of no further force and\neffect.\"36\n of either a permanent ban or strong regulation. These bills prompt discussion within legislative\ncommittees-the members of which are often unfamiliar with face recognition-about the\ntechnology, including its potential harms. There is a risk, however, that if the legislature fails to act\nonce the moratorium period is over, use of face recognition will recommence with no safeguards\nin place.\n2. Directive moratoria\nDirective moratoria temporarily stop face recognition use while explicitly instructing the legislature\nor other government officials to take additional steps. Often this entails the creation of a task\nstudy face recognition and recommend policy responses.37\nA bill introduced in Washington state in 2019 proposed a moratorium on government use of\n          n   o  \nwould be composed of members of historically oversurveilled communities, and would deliver a\nprovide a report certifying the tools in use did not contain accuracy or bias issues, as tested by an\nindependent third party.38\n33 Time-bound moratoria often have carve-out provisions for face recognition use during emergencies or exigent circumstances and in the case of\nmissing children. Some also have carveouts for use in fraud detection by state driver's licensing departments.\n34\n Sup. note 23.\n35\n MA S.B. 0857 (2020), available at http://mgaleg.maryland.gov/mgawebsite/Legislation/Details/sb0857. Note: the original bill would prohibit \ngovernment and private use of face recognition for one year. An amendment, discussed at a hearing for the bill would eliminate the moratorium on\nprivateuse\nIbid.\n37\n Provisions creating working groups are often part of non-moratorium regulatory bills, which allow continued use of face recognition until the\nworking group makes further recommendations\n 38 WA S.B. 5528 (2019-2020), available at htps://app.leg.wa.gov/billsummary?BilINumber=5528&Initiative=false&Year=2019. (Note that this bill is no\nlonger under consideration.)",
    "Page_92": "92 |  Regulating Biometrics: Global Approaches and Urgent Questions\nIn contrast to the above example, in which decisions about future policy are left to the working\ngroup, this kind of moratorium sets minimum thresholds that future legislation must achieve.\ngovernment use of biometric surveillance, including face recognition, \"[a]bsent express\nbiometric surveillance systems, their purposes, and prohibited uses; standards for data use\nand management; auditing requirements; and rigorous protections for civil rights and liberties,\nincluding compliance mechanisms.39\nAt the federal level, the Facial Recognition and Biometric Technology Moratorium Act of 2020\nprohibits federal use of certain biometric technologies such as face recognition until Congress\nand local agencies on their adoption of moratoria similar to that proposed in the federal bill.40\nThese bills encourage jurisdictions to research the full implications of face recognition use and\n       a  n   s  \ntask force or commission may not be representative of affected communities; may lack authority;\nor may be inadequately funded, restricting its effectiveness.41\nC.RegulatoryBills\nRegulatory bills seek to place restrictions on face recognition's use, rather than stop it altogether\nuses or other elements of face recognition) to broader (regulating more of these elements).\n1. Common elements of regulatory bills\nFace recognition bills propose a wide range of measures, including:\n· Task force or working group: groups must study face recognition and make policy\nrecommendations.\n· Requirements on companies: face recognition vendors must open up their software to\nas allow data access, correction, and removal.\n39\n MA S.B. 1385 (2019), available at https://malegislature.gov/Bills/191/S1385.\n40 See Senators Markey and Merkley, and Reps. Jayapal, Pressley to Introduce Legislation to Ban Government Use of Facial Recognition, Other\nBiometric Technology (June 25, 2020), available at https://www.markey.senate.gov/news/press-releases/senators-markey-and-merkley-and-reps-\n41\nSee,e.g., Governor Jay Inslee, Letter To the Honorable President and Members, The Senate of the State of Washington (Mar. 31, 2020),available at\nhttps://crmpublicwebservice.des.wa.gov/bats/attachment/vetomessage/559a6f89-9b73-ea11-8168-005056ba278b (vetoing a section of WA S.B\n620 (regulatory bill that established a face recognition task force on the grounds that it was not funded in the budget)",
    "Page_93": "Jameson Spivack & Clare Garvie I  A Taxonomy of Legislative Approaches to Face Recognition in the United States I  93\n· Accountability and transparency reports: implementing agencies must provide details on the\nface recognition tools they use, including how and how often, to elected officials. Some require\nface recognition was used in identifying them.\nE\nExplicit civil rights and liberties protections: such as prohibiting the use of face recognition\nto surveil people based on characteristics including but not limited to race, immigration status,\nsexual orientation, religion, or political affiliation\nData and access restrictions: such as prohibiting the sharing of face recognition data with\nprohibiting use on state driver's license databases.\n· Targeted bans: prohibiting specific uses, such as live facial recognition, or in conjunction with\nbody-worn cameras or drones. Face recognition use can also be limited by type of crime-for\nexample, only to investigate violent felonies\ncause (or, in some instances, only reasonable suspicion43) to run face recognition searches.\nprivate entities that have collected it, rather than law enforcement searches themselves\n2. Examples of regulatory bills\nincludes no other restrictions.45 In New Jersey, a proposed billrequires the attorney general to\nusing any biometric surveillance system in connection with an officer camera or data collected by\nan officer camera.\"47\nAt the other end of the spectrum, broader regulatory bills address multiple elements of face\nthe proposed rules are substantive or enforceable\n42Some of these provisions are modeled on the federal Wiretap Act. See 18 U.S.C. S 2519, reports concerning intercepted wire, oral, or electronic\ncommunications, https://www.law.cornell.edu/uscode/text/18/2519.\n43  ID H.B. 492 (2020), available at https:/legislature.idaho.gov/sessioninfo/2020/legislation/H0492/\n44\nSee, e.g., sup. note 22\n45\n IN H.B. 1238 (2020), available at http://iga.in.gov/legislative/2020/bills/house/1238.\n46 NJ A.B. 989 (2020), available at https://www.njleg.state.nj.us/2020/Bills/A1000/989_I1.PDF.\n47CA A.B. 1215 (2019), available at https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=201920200AB1215",
    "Page_94": "94 |  Regulating Biometrics: Global Approaches and Urgent Questions\nof face recognition.48 The bill includes provisions like these: a pre-implementation accountability\noperational conditions; face recognition service APls made available for independent accuracy\nsupport for its light-touch approach to regulating face recognition, it elicited criticism from privacy\nadvocates for containing loopholes and providing inadequate enforcement mechanisms.50\nNarrowly targeted bills have a greater likelihood of passing through support from well-resourced\nlaw enforcement and company stakeholders, yet often fail to meaningfully protect against the\ntrue scope of possible harms.51 Some advocates are also critical of regulatory bills, particularly\nstronger regulation in the future.\nCONCLUSION\nIn the past year, the United States has turned a significant corner in its approach to face\nadvocates, law enforcement, and other stakeholders may disagree on exactly what that looks\nlike.52 The status quo-expansive, unregulated, secret face recognition use-is no longer\nacceptable.\n48\ncom/2020-03-13-washington-facial-recognition-regulations.html.\nSup. note 27.\n50\n See Lucas Ropek, \"Why Did Washington State's Privacy Legislation Collapse?,\" Govtech.com, April 19, 2019, https://www.govtech.com/policy/Why-\nDid-Washington-States-Privacy-Legislation-Collapse.html.\nrecognitionuse\n52  This includes both Republican and Democratic lawmakers, as well as face recognition vendors and law enforcement officials. See, e.g.\nAction, Microsoft on the Issues, December 6, 2018, https://blogs.microsoft.com/on-the-issues/2018/12/06/facial-recognition-its-time-for-action/;\nCrimePrevention/facial-recognition-technology.cfm.",
    "Page_95": "Jameson Spivack & Clare Garvie I A Taxonomy of Legislative Approaches to Face Recognition in the United States | 95",
    "Page_96": "96 | Regulating Biometrics: Global Approaches and Urgent Questions\nBIPA: The Most Important\nBiometric Privacy Law in the US?\nWoodrow Hartzog (Northeastern University)\nae ienuaod pue uooundui ue pone ot padou ll iuosea ul t'sioul ui paseq saa\ndamages under one of the most important privacy laws in America: the Illinois Biometric\nInformation Privacy Act (BIPA).2\nEnacted in 2008 in the wake of the bankruptcy of a high-profile fingerprint-scan system,\nlawmakers designed BIPA to provide \"safeguards and procedures relating to the retention,\ncollection, disclosure, and destruction of biometric data.\"3 It was the first state law in the US\nto specifically regulate biometrics. Remarkably, as the bill was being deliberated by the Illinois\nlegislature, \"there were no questions or discussion, and the bill proceeded immediately to a vote\nand unanimously passed in the House.\"4\nBIPA's substantive rules follow a traditional approach to data protection. Compared to omnibus\nand complex data-protection laws like GDPR, BIPA's rules are simple. Private entities must get\nClearview filed legal documents in llinois stating that \"Clearview is cancelling the accounts of every customer who was not either associated with\nlaw enforcement or some other federal, state, or local government department, office, or agency\" See Ryan Mac, Caroline Haskins, and Logan\nMcDonald,\"Clearview Al Has Promised to Cancel All Relationships with Private Companies,\" BuzzFeed, May 7, 2020, https://www.buzzfeednews.\ncom/article/ryanmac/clearview-ai-no-facial-recognition-private-companies.\n740 Ill. Comp. Stat. Ann. 14/15.\n3\nCharles N. Insler, How to Ride the Litigation Rollercoaster Driven by the Biometric Information Privacy Act, 43 S. Ill. U. L.J. 819, 820 (2019).\nAnna L. Metzger, The Litigation Rollercoaster of BIPA: A Comment on the Protection of Individuals from Violations of Biometric Information Privacy\n50 Loy. U. Chi. L.J. 1051, 1063 (2019).",
    "Page_97": "are prohibited from selling, leasing, trading, or otherwise profiting from a person's biometric\nthe statute binds private entities to a standard of care in transmitting, storing, and protecting\nbiometric information that is equal to or more protective than for other confidential and sensitive\ninformation.8\nWhile other states such as Texas and Washington have passed standalone biometrics laws,9 BlPA\nrequire notice and consent before parties can collect biometric identifiers, require reasonable\nsecurity measures for covered information, restrict the disclosure of biometric identifiers to\nspecific circumstances, and limit companies' retention of biometric identifiers. But only in lllinois\ncan people who have been aggrieved by companies that violated the rules bring their own action\npenalty.\nGiven the limited scope of biometric laws, BlPA's private cause of action might not seem\n For example, Texas and Washington both authorize their state attorneys general to enforce their\nbiometric privacy laws in ways similar to how states enforce their general data-privacy rules.1° In\nfor privacy violations, among the most vexing issues for all privacy-related claims by plaintiffs in\nCivilcourts\nPlaintiffs alleging privacy-related harms from things like data breaches, abusive surveillance,\nand unauthorized disclosure have had a notoriously difficult time in court. Some of this is\nPlaintiffs struggle to certify classes for mass litigation, and arbitration clauses are embedded\nin the ubiquitous terms-of-use agreements online. But a huge roadblock for plaintiffs is the\nslippery nature of privacy harms.11 Courts have long been skeptical of emotional and reputational\n740 Ill Comp. Stat. Ann. 14/15 (\"S15(b) No private entity may collect, capture, purchase, receive through trade, or otherwise obtain a person's or\na customer's biometric identifier or biometric information, unless it [informs the subject what is being collected and receives a written release].\nS15(c) (d) No private entity in possession of a biometric identifier or biometric information may disclose, redisclose, or otherwise disseminate\na person's or a customer's biometric identifier or biometric information unless [the subject of the biometric identifier or biometric information\nconsents or disclosure is required pursuant to a valid warrant or subpoena]l'\nId. S 15(c).\nId. S 15(a). (\"A private entity in possession of biometric identifiers or biometric information must develop a written policy, made available to the\npublic, establishing a retention schedule and guidelines for permanently destroying biometric identifiers and biometric information when the initial\npurpose for collecting or obtaining such identifiers or information has been satisfied or within 3 years of the individual's last interaction with the\nprivate entity, whichever occurs first.\").\nId. S 15(e).\n9\nTex. Bus. & Com. Code S503.001; Wash. Rev. Code Ann. S19.375.020; California Consumer Privacy Act (CCPA); N.Y. 2019 Stop Hacks and Improve\nElectronic Data Security (SHIELD) Act (broadening information covered by data breach response law to include biometric information); N.Y. Lab.\n mel asuodsai yoeana eep buuawe) ()ol-ol-ts apo sesueyia (suawsoidwa jo uouoo e se unudiabuy buioid) e-los me\ninclude biometric information).\n10\nFor more information on the role of state attorneys general in privacy policymaking, see Danielle Keats Citron, The Privacy Policymaking of State\nAttorneys General, 92 Notre Dame L. Rev. 747, 748 (2016)\n11\nM. Ryan Calo, The Boundaries of Privacy Harm, 86 IND. L.J. 1131, 1133 (2011); Daniel Solove and Danielle Citron, Risk and Anxiety: A Theory of\nData Breach Harms, 96 Texas Law Review 737 (2018); Ryan Calo, Privacy Harm Exceptionalism, 12 J. TELECOMM. & HIGH TECH. L. 361, 361, 364\n(2014); Paul Ohm, Sensitive Information, 88 S. CAL. L. REV. 1125, 1196 (2015).",
    "Page_98": "98 | Regulating Biometrics: Global Approaches and Urgent Questions\npremier privacy regulator in the US, creates waves when it even hints at the idea that something\nmore than physical or financial harm or extreme emotional suffering should be considered in\ndetermining whether particular acts are unfair.13 This is to say nothing of the high-stakes debate\n over whether less specific harms such as anxiety and exposure to risk of data abuses, standing\nalone, can constitute an actionable injury in the context of claims of negligence which led to a\ndata breach.14\nprivacy predicament is more akin to death by a thousand cuts. Small intrusions and indiscreet\ndisclosures could lead to compromised autonomy, obscurity, and trust in relationships. What's\nmore, it can be difficult to specifically articulate and identify the ways in which data breaches\nmake us more vulnerable. Torts require a clear line of causation from fault to harm. That's usually\nrelatively easy to prove with things like physical injuries from car wrecks, though it is less so with\ndata breaches. Even if it's clear that a malicious actor has gained access to peoples' information,\ncriminals don't always straightforwardly use data obtained from a breach to inflict direct financial\n or emotional injury upon the data subject. They often aggregate the information in a pool for\nfurther exploitation or sit on it for years so as not to arouse suspicion. Often people have no idea\nwho wronged them online. American data-privacy law simply isn't built to respond to this kind of\ndiffuse and incremental harm.15\nBIPA has spurred a key intervention into this morass. Specifically, with BlPA, several judicial\nopinions have affirmed the argument that regardless of whether wrongful acts with biometric\ninformation resulted in chilling effects or financial or emotional injury, the collection and\nit is an affront to a person's dignity and autonomy. Two cases in particular demonstrate the\nimportance of BIPA.\nIn Rosenbach v. Six Flags Entm't Corp., a mother brought a claim on behalf of her son against\nthe plaintiffs alleged sufficient actual or threatened injury to have standing to bring suit. Plaintiffs\n  s ns     u  a     \nprohibited collection and processing of personal biometric data without making the required\n12Id.\n13 See F.T.C. v. Wyndham Worldwide Corp., 10 F. Supp. 3d 602, 623 (D.N.J. 2014), affd, 799 F.3d 236 (3d Cir. 2015) (\"The parties contest whether non\nmonetary injuries are cognizable under Section 5 of the FTC Act...Although the Court is not convinced that non-monetary harm is, as a matter of\nlaw, unsustainable under Section 5 of the FTC Act, the Court need not reach this issue...).\n14  Daniel Solove and Danielle Citron, Risk and Anxiety: A Theory of Data Breach Harms, 96 Texas Law Review 737 (2018).\n15  Daniel J. Solove and Danielle Keats Citron, Risk and Anxiety: A Theory of Data-Breach Harms, 96 Tex. L. Rev. 737, 762 (2018) (\"Hackers may not\nuse the personal data in the near term to steal bank accounts and take out loans. Instead, they may wait until an illness befalls a family member\nand then use personal data to generate medical bils in a victim's name. They may use the personal data a year later but only use some individuals\npersonal information for fraud.\").\n16 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 18, 129 N.E.3d 1197, 1200-01 (*The complaint alleges that this was the first time Rosenbach\nhis mother, were informed in writing or in any other way of the specific purpose and length of term for which his fingerprint had been collected.\nNeither of them signed any written release regarding taking of the fingerprint, and neither of them consented in writing 'to the collection, storage\nuse sale, lease, dissemination, disclosure, redisclosure, or trade of, or for [defendants] to otherwise profit from, Alexander's thumbprint or associated\nbiometric identifiers or information.\").",
    "Page_99": "Woodrow Hartzog I BIPA: The Most Important Biometric Privacy Law in the US? | 99\ndisclosures or obtaining written consent. The Appellate Court of illinois held that \"a plaintiff is not\n n  sp  nsn u  e  e o n  m e\nunder the Act based solely on a defendant's violation of the statute. Additional injury or adverse\neffect must be alleged.\"17 However, the Supreme Court of Illinois disagreed.\nhad wanted to impose an injury requirement beyond disclosure and consent failures, they likely\nwould have done so, as they have in other legislation.18 Using accepted principles of statutory\nof this Act shall have a right of action\" according to its commonly understood legal meaning.\nSpecifically, they found that \"to be aggrieved simply 'means having a substantial grievance; a\nopaabee, se senb o Aessaoau ou s! sabewp yons busuiisns \nThe court in Rosenbach found that Six Flags violated BIPA's \"right to privacy in and control\nover their biometric identifiers and biometric information.\"21 BlPA's disclosure and consent\nrequirements give shape to that right. Thus, if a company violates BlPA, then the data subject is\ncompromised.22\ndefendant's argument that its BlPA violations were merely \"technical\" in nature. The court\nto accomplish but also the unique nature of how biometrics threaten peoples' privacy and how\ncontrol their biometric information by requiring notice before collection and giving them the power\nto say no by withholding consent.\"23 Peoples' unique biometric identifiers, now easily wholesale\nnumbers because if they are compromised, they cannot be changed. Even beyond identity theft,\nthe court noted that biometrics are particularly concerning because their full risks are not known\nThe court was direct in its finding:\n17 Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 15, 129 N.E.3d 1197, 1202 (citing Rosenbach v. Six Flags Entm't Corp.,2017 IL App (2d)\n170317, reV'd, 2019 IL 123186, 129 N.E.3d 1197).\n18  Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 125, 129 N.E.3d 1197, 1204. (\"Defendants read the Act as evincing an intention by the\nlegislature to limit a plaintiff's right to bring a cause of action to circumstances where he or she has sustained some actual damage, beyond\nviolation of the rights conferred by the statute, as the result of the defendant's conduct. This construction is untenable. When the General Assembly\nhas wanted to impose such a requirement in other situations,it has made that intention clear:\").\n19 Id. (citing Glos v. People, 259 Ill. 332, 340, 102 N.E. 763 (1913)\n20F\n Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 30, 129 N.E.3d 1197, 1205 (\"Rather, [al person is prejudiced or aggrieved, in the legal sense,\nwhen a legal right is invaded by the act complained of or his pecuniary interest is directly affected by the decree or judgment.\") (citing Glos v. People\n259 IIl. 32, 340, 102 N.E. 763 (1913)).\n21F\nviolation, in itself, is sufficient to support the individual's or customer's statutory cause of action)\n23Rosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 34, 129 N.E.3d 1197, 1206",
    "Page_100": "100 |  Regulating Biometrics: Global Approaches and Urgent Questions\nWhen a private entity fails to adhere to the statutory procedures, as defendants are\nalleged to have done here, \"the right of the individual to maintain [his or] her biometric\nprivacy vanishes into thin air. The precise harm the lllinois legislature sought to prevent\nlegislature's privacy goals for BlPA. When companies face liability for legal violations without\nburdening plaintiffs to show some additional injury, \"those entities have the strongest possible\nincentive to conform to the law and prevent problems before they occur and cannot be undone.\"25\nThe court noted that the cost of complying with BlPA is \"likely to be insignificant compared to the\nsubstantial and irreversible harm that could result if biometric identifiers and information are not\nthe court, to force plaintiffs to wait until they could prove some sort of financial or emotional harm\nwould counteract BlPA's prevention and deterrence goals.\nThe other case illustrative of BIPA's potency, Patel v. Facebook,27 involves federal standing doctrine\nas required by Article Ill of the US Constitution, a concept linked to injury and harm thresholds\nStanding doctrine requires that plaintiffs “must have suffered an 'injury in fact'-an invasion of\na legally protected interest which is (a) concrete and particularized; and (b) actual or imminent,\nnot conjectural or hypothetical.:\"28 In a landmark 2016 US Supreme Court case, Spokeo, Inc. v.\nRobins affirmed that an injury-in-fact for information-related complaints like those against data\nbrokers for mishandling, inaccuracies, and indiscretion must be \"concrete, though the court was\nPate/ v. Facebook involved a complaint that Facebook violated BlPA with its use of facial\nprovisions at issue were established to protect [the plaintiff's] concrete interests (as opposed\nthis case actually harm, or present a material risk of harm to, such interests.\"3o The Ninth Circuit\nanswered yes to both questions.\nIn determining that BlPA protected a concrete interest rather than a purely procedural protection,\nthe Ninth Circuit noted that privacy rights have long served as the basis for legal action in the\nthe significant vulnerabilities created by facial recognition technology:\nRosenbach v. Six Flags Entm't Corp., 2019 IL 123186, 1 37, 129 N.E.3d 1197, 1206\n26\nId.\nPatel v. Facebook, Inc., 932 F.3d 1264 (9th Cir. 2019), cert. denied, 140 S. Ct. 937, 205 L. Ed. 2d 524 (2020).\n28\n 29 Spokeo, Inc. v. Robins, 136 S. Ct. 1540, 1548-49, 194 L. Ed. 2d 635 (2016), as revised (May 24, 2016). (\"When we have used the adjective 'concrete,\nConcrete\" is not, however, necessarily synonymous with 'tangible: Although tangible injuries are perhaps easier to recognize, we have confirmed in\nmany of our previous cases that intangible injuries can nevertheless be concrete.\") The Court went on to muddy the waters in Spokeo even further\nallege a bare procedural violation, divorced from any concrete harm, and satisfy the injury-in-fact requirement of Article l.This does not mean,\nhowever, that the risk of real harm cannot satisfy the requirement of concreteness\" Id at 1549.\n30F\nPatel v. Facebook,Inc., 932 F.3d 1264, 1270-71 (9th Cir. 2019), cert. denied, 140 S. Ct. 937,205 L. Ed. 2d 524 (2020) (citing Robins v. Spokeo, Inc.\n867 F.3d 1108, 1113 (9th Cir. 2017) (Spoke0 I)",
    "Page_101": "Woodrow Hartzog I BIPA: The Most Important Biometric Privacy Law in the US? | 101\n[T]he facial-recognition technology at issue here can obtain information that is\n\"detailed, encyclopedic, and effortlessly compiled,\" which would be almost impossible\ncan use it to identify that individual in any of the other hundreds of millions of photos\nuploaded to Facebook each day, as well as determine when the individual was present\nindividual could be identified from a surveillance photo taken on the streets or in\nrecognition lock on that individual's cell phone.31\nThe court concluded that \"the development of a face template using facial-recognition technology\nwithout consent (as alleged here) invades an individual's private affairs and concrete interests\nSimilar conduct is actionable at common law.\"32 The court cited the language in Rosenbach in\nholding that \"the statutory provisions at issue' in BlPA were established to protect an individual's\nthe “\"the plaintiffs have alleged a concrete injury-in-fact sufficient to confer Article Ill standing.3\nBIPA has a number of virtues. Thanks to BIPA's private cause of action, it has become the key\nfor holding companies that use biometric systems accountable.34 In the absence of a private\n cause of action, enforcement of biometrics and consumer protection laws is generally left to\nstate attorneys general (AG). While state AGs are certainly key to privacy policymaking in the US,\nstate AGs have limited legal ability and political capital to extract the kind of fines necessary to\nsufficiently deter companies. The same holds true for the Federal Trade Commission, which is\nAmerica's primary privacy regulator.36\nfrom engaging in practices with biometrics based on business models that inevitably lead to\npolitical pressure. Facebook's share price actually rose 2 percent after the FTC announced its\nhistoric S5 billion fine for the social media company's privacy lapses in the Cambridge Analytica\ndebacle.37 Meanwhile, Clearview Al specifically cited BiPA as the reason it is no longer pursuing\n non-government contracts.38 On top of that, Clearview Al is being sued by the ACLU for violating\n31F\nPatel v. Facebook, Inc., 932 F.3d 1264, 1273 (9th Cir. 2019), cert. denied, 140 S. Ct. 937, 205 L. Ed. 2d 524 (2020) (citations omitted). BIPA's focus on\nface templates as a creation that grants surveillance and other affordances is properly distinguished from a standard photograph, which does not\n provide the same affordance of serving as a beacon.\n32\nId.\n33\nId. at 1274.\n 34  Over three hundred class action lawsuits have been brought under BIPA as of June 2019. See Seyfarth Shaw, “Biometric Privacy Class Actions by\nthe Numbers: Analyzing Ilinois' Hottest Class Action Trend,\" Seyfarth, June 28, 2019, https://www.workplaceclassaction.com/2019/06/biometric-\nprivacy-class-actions-by-the-numbers-analyzing-ilinois-hottest-class-action-trend/\n35\n See Danielle Keats Citron, The Privacy Policymaking of State Attorneys General, 92 Notre Dame L. Rev. 747 (2016).\n See Daniel Solove and Woodrow Hartzog, The FTC and the New Common Law of Privacy, 114 Columbia Law Review 583 (2014); Woodrow Hartzog\nand Daniel Solove, The Scope and Potential of FTC Data Protection, 83 George Washington Law Review 2230 (2015).\nCharlotte Jee, “Facebook Is Actually Worth More Thanks to News of the FTC's $5 Billion Fine, MIT Technology Review, July 15, 2019, https://www.\ntechnologyreview.com/2019/07/15/134196/facebook-is-actually-richer-thanks-to-news-of-the-ftcs-5-billion-fine/\n38\nNick Statt, \"Clearview Al to Stop Selling Controversial Facial Recognition App to Private Companies, Verge, May 7, 2020, https://www.theverge\ncom/2020/5/7/21251387/clearview-ai-law-enforcement-police-facial-recognition-illinois-privacy-law",
    "Page_102": "102 |  Regulating Biometrics: Global Approaches and Urgent Questions\nBIPA by creating faceprints of people without their consent.39 It is no wonder that the private\ncause of action is one of two reasons the United States does not have an omnibus federal\nof an outright ban.41\nrespond to the risk of biometrics. BlPA is rooted in a myopic and atomistic \"notice and choice\napproach to privacy.\naround concepts of transparency and informational self-determination. First, by focusing on\ngiving people control over their data and mandating procedural disclosure obligations, these\nframeworks fail to impose substantive limits on how far companies can encroach into our\nlives and how deeply these systems can be entrenched. Procedural transparency and consent\nregimes end up serving as a justification mechanism for all kinds of encroachments without\nany clear backstop to how vulnerable we can be made to these systems, so long as we consent.\nalready has been exposed to the public. For example, judges considering privacy claims have said\nPrivacy is about more than just informational self-determination. It is about trust, dignity, freedom\nfrom oppression, and laying the preconditions for human flourishing. But those values are not\nnecessarily reflected in the net outcome of billions of individual decisions. Moreover, companies\ncreate structured environments that can heavily influence these discrete choices, with powerful\n8t'ueo Kaut <em Kue ,sas, Kes ot sn jab ot sanuaous\n39\n ACLU, American Civil Liberties Union, American Civil Liberties Union of llinois, Chicago Alliance Against Sexual Exploitation, Sex Workers Outreach\nProject Chicago, Ilinois State Public Interest Research Group, Inc., and Mujeres Latinas en Accion v. Clearview Al, Inc., https://www.aclu.org/sites/\ndefault/fles/field_document/aclu_v_clearview_complaint_final.pdf.\n40\nSee Makena Kelly,\"Congress Is Split over Your Right to Sue Facebook, Verge, December 3, 2019, https://www.theverge.com/2019/12/3/20993680/\nfacebook-google-private-right-of-action-sue-data-malpractice-wicker-cantwell; and Emily Birnbaum, \"Lawmakers Jump-Start Talks on Privacy Bill'\"\nThe Hill, August 7, 2019, https://thehill.com/policy/technology/456459-lawmakers-jump-start-talks-on-privacy-bill; and Ben Kochman, \"Senate\nprivacy-hearing-zeroes-in-on-right-to-sue-preemption; and Cameron F. Kerry, John B. Morris, Caitlin Chin, and Nicol Turner Lee,\"Bridging the Gaps:\nA Path Forward to Federal Privacy Legislation, Brookings, June 3, 2020, https://www.brookings.edu/research/bridging-the-gaps-a-path-forward-to-\nfederal-privacy-legislation/\nSeeIssie Laowsky, NYor's Privacy Bils Even Bolder thanCalifornias, Wired, June 4, 209, https:/wwwired.com/story/ne-ork-privay\nact-bolder/; DJ Pangburn, \"How Big Tech Is Trying to Shape California's Landmark Privacy Law, Fast Company, April 25, 2019, https://www.\nconsumer-lawsuits-1478824; and “Potentialy Expanded Private Right of Action Increases Risk of Class Action Exposure under the California\nConsumer Privacy Act, Dorsey, May 1, 2019, https://www.dorsey.com/newsresources/publications/client-alerts/2019/04/private-right-of-action\nincreases-risk\n42\nWoodrow Hartzog, The Public Information Fallacy, 98 Boston University Law Review 459 (2019). The FBl alleges it does not need permission to\nconduct surveillance using powerful technologies like cell-site simulators (often called \"Stingrays\"), so long as they are doing so in public places.\nJudges have refused to punish people for taking \"upskirt\" photos because the women photographed have no reasonable expectation of privacy \"in\npublic,no matter howfleeting their exposure. Id\n43Woodrow Hartzog, Privacy's Blueprint: The Batle to Control the Design of New Technologies (Cambridge, MA: Harvard University Press, 2018)",
    "Page_103": "Woodrow Hartzog I BIPA: The Most Important Biometric Privacy Law in the US? | 103\nBIPA is simply not capable of providing individuals with meaningful agency over modern data\ncontrol what we see and what we can click. Companies deploy malicious user interfaces and a\nblizzard of dense fine print to overwhelm our decision-making process. Consent regimes give\nthe illusion of control while justifying dubious practices that people don't have enough time or\nbenefits of consenting to biometric practices, they often don't have a meaningful choice in front\nof them since they cannot afford to say no and decline a transaction or relationship. While people\npermission risks of biometric technologies.\nBIPA is far more effective than any other law on the books in protecting our biometric privacy\nsubstantive limits necessary for a sustainable future with biometric technologies. BlPA allows\ncompanies to exploit people as their consent is harvested through systems designed to have\nthem hurriedly click \"I Agree\" and get on with their busy lives. BlPA's success entrenches an\nbut also because of what it lacks.\n44\nWoodrow Hartzog, The Case Against Idealising Control, 4 European Data Protection Law Review 423 (2018)\n45\n Neil Richards and Woodrow Hartzog, The Pathologies of Digital Consent, 96 Washington University Law Review 1461 (2019); Evan Selinger and\nWoodrow Hartzog, The Inconsentability of Facial Surveillance, 66 Loyola Law Review 101 (2019).",
    "Page_104": "104 |Regulating Biometrics: Global Approaches and Urgent Questions\nBottom-Up Biometric Regulation:\nA Community's Response to\nUsing Face Surveillance in\nSchools\nStefanie Coyle (NYCLU)\nRashida Richardson (Rutgers University, Al Now Institute, NYU)\nublic schools are increasingly turning to invasive technological solutions to address a wide\nrange of school safety issues. Because events like school shootings are both nuanced and\npolitically or socially charged, school administrators often rush to embrace technological\nused in schools. This case study examines the controversial move by a school district in Lockport,\nresponse that sparked a national debate and led to state-wide legislation regulating the use of\nbiometric technologies in schools.\nFACIAL RECOGNITION SURVEILLANCEIN SCHOOLS\nSurveillance technologies are becoming a norm in many public schools.1 School administrators\nare turning to a rapidly growing market of \"free\"2 or subsidized tools that monitor student emails\nfor concerning phrases, measure student bathroom breaks, proctor exams, or provide real-time \nCircumstances leading to increased adoption of surveillance technologies in schools may vary by country. This chapter focuses on the United\nStates.\nRealNetworks,Inc., \"RealNetworks Provides SAFR Facial Recognition Solution for Free to Every K-12 School In the United States and Canada\" July\n18, 2018, https://www.prnewswire.com/news-releases/realnetworks-provides-safr-facial-recognition-solution-for-free-to-every-k-12-school-in-the-\nus-and-canada-300681977.html",
    "Page_105": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 105\nalerts of potential crises,β often without proper consideration or community consultation. School\nadministrators have shown significant interest in biometric and other access-control technologies\nfor targeting nuanced school safety issues, with few existing regulations to hold them back.4 In\n2019, Wired \"identified eight public school systems, from rural areas to giant urban districts, that\nhave moved to install facial recognition systems,” though national use statistics remain unknown.5\nBecause these technologies can be enabled as \"add-on\" features or easily integrated with existing\nsystems used by a school or school district (e.g., closed-circuit television), administrators\nand may not have policies in place to deal with a data breach or sufficient funding available for\nmaintenance of these systems.\nBiometric technologies present a veneer of social control or risk mitigation,7 but in reality\nthey pose unique social and legal concerns for students, particularly in the K-12 setting\nThough students have some enhanced data-privacy protections and greater expectations\nregarding government oversight and enforcement,8 they are particularly vulnerable because\nthe consequences of privacy and other legal violations may not be immediately felt or obvious.\nMoreover, for decades, critical scholars and educators have criticized these types of reactionary\neducational policies and practices because they are not long-term solutions. Indeed, they tend\nsystem and allow policymakers to avoid necessary structural reforms.9\ntechnologies in schools, finding some uses in schools to be unlawful although not banning the\n3\nMeghna Chakrabarti and Hilary McQuilkin, \"When Schools Use Tech to Monitor Students Online, Class Is Always in Session,\" WBUR, October 31,\nThe last substantive guidance on the Family Educational Rights and Privacy Act (FERPA) from the United States Department of Education was\nissued in 2007; it described the application of FERPA to the use of security videos and the transfer of educational records. US Department of\nEducation, “Balancing Student Privacy and School Safety: A Guide to the Family Educational Rights and Privacy Act for Elementary and Secondary\nSchools,\" October 2007, https://www2.ed.gov/policy/gen/guid/fpco/brochures/elsec.html\nstory/delicate-ethics-facial-recognition-schools/\n1-20, https://doi.org/10.24908/ss.v3i1.3317. Simon notes that individuals perform compliance around surveillance technologies, which makes\nassessing broader, long-term efficacy or value difficult. See also Clive Norris and Gary Armstrong, The Maximum Surveillance Society: The Rise of\nCCTV (Oxford: Berg, 1999). The authors discuss how public surveillance systems have inherent blind spots that diminish effectiveness or intended\ngoals.\nAndrew Hope, \"Seductions of Risk, Social Control, and Resistance to School Surveillance, in Schools Under Surveillance: Cultures of Control in Public\nEducation, eds. Torin Monahan, R. Torres, and Aaron Kupchik (New Brunswick: Rutgers University Press, 2009), 230-235.\n8\nSee, e.g., Family Educational Rights Privacy Act (FERPA), 20 U.S.C. S 1232g; Protection of Pupil Rights Amendment (PPRA), 20 U.S.C. S 1232h;\nChildren's Online Privacy Protection Act (COPPA), 15 U.S.C. SS 6501-6506; Individuals with Disabilities Education Act (IDEA), 20 U.S.C. S 1400 et\nseq; Children's Internet Protection Act (CIPA), 47 CFR S 54.520; National School Lunch Act, 42 U.S.C. S 1751 (2008). Several states have additional\nstudent privacy laws. FERPA/SHERPA, State Student Privacy Laws (2019), https:/ferpasherpa.org/state-laws/ (updated Aug. 6, 2019).\nSee, e.g., Daniel Kiel, \"No Caste Here? Toward a Structural Critique of American Education, Penn State Law Review 119, no. 3 (2015): 611-644,\nracial hierarchies in society and insulate educational institutions from legal, political, and practical interventions. See also Jason P. Nance,\"Student \nSurveillance, Racial Inequalities, and Implicit Racial Bias,\" Emory Law Journal 66, no. 4 (2017): 765-837. Nance documents the ways in which\nintensified school surveillance practices and policies disproportionately and negatively affect students of color. See also David Gillborn, “Education\nPolicy as an Act of White Supremacy: Whiteness, Critical Race Theory and Education Reform, Journal of Education Policy 20, no. 4 (2005): 485-505,\nhttps://doi.org/10.1080/02680930500132346. Gilborn argues that educational policy in the United Kingdom reinforces and facilitates racial\ninequities.",
    "Page_106": "106 |  Regulating Biometrics: Global Approaches and Urgent Questions\nuse of the technology in other settings.1° At the same time, several states and localities have\nwhich applies to public schools.11 US civil society organizations Fight for the Future and Students\nfor Sensible Drug Policy created a campaign to ban use of facial recognition technology on\n college campuses.12 This campaign successfully forced the University of California Los Angeles\n(UCLA) to reverse its plans to implement facial recognition for campus security,13 and has\ngarnered support from teachers' unions that are expanding the campaign's call to extend to K-12\nschools.14\nEducation Commissioner to conduct a study on the use of such technologies in schools and\nissue statewide recommendations.15 This legislation was in response to and in collaboration with\na community-led advocacy effort in Lockport, New York.\nLOCKPORT, NEW YORK: A CASE STUDYIN\nCOMMUNITY-DRIVENPUSHBACKTOFACIAL\nRECOGNITIONINSCHOOLS\nIn 2014, New York voters approved the Smart Schools Bond Act (SSBA), which set aside $2\nbillion for school districts to “improve learning and opportunity for students throughout\" New\n10 The Administrative Court of Marseille (https://www.laquadrature.net/wp-content/uploads/sites/8/2020/02/1090394890_1901249.pdf) found\nthat the use of facial recognition gates in two French high schools without student consent violated GDPR, despite national government interest\nin establishing a legal framework for public biometric video surveilance. See also European Data Protection Board, “Facial Recognition in Schools\nRenders Sweden's First GDPR Fine, August 22, 2019, https://edpb.europa.eu/news/national-news/2019/facial-recognition-school-renders-swedens-\nfirst-gdpr-fine_en. The article discusses the issuing of Sweden's first GDPR fine for failure to perform an adequate impact assessment and unlawful\nprocessing of sensitive biometric data to a municipality that used facial recognition technology to monitor student attendance. And see Scottish\nGovernment, \"Biometric Technologies in Schools: Draft Guidance for Education Authorities, September 9, 2008, http://www.scotland.gov.uk/\nPublications/2008/09/08135019/0. The document discourages educational authorities from adopting biometric technologies but does not rescind\npreexisting findings that biometric technologies in schools are not illegal, even when introduced without parental consultation.\n11\nSee Kristin Lam,\"Portland, the Largest City in Oregon, Plans to Propose First Facial Recognition Ban Affecting Private Companies,\" USA Today\nDecember 3, 2019, https://www.usatoday.com/story/tech/2019/12/03/facial-recognition-portland-oregon-ban/2601966001/; Tom McKay,\"Berkeley\nBecomes Fourth U.S. City to Ban Face Recognition in Unanimous Vote, Gizmodo, October 16, 2019, https://gizmodo.com/berkeley-becomes-fourth-\nu-s-city-to-ban-face-recogniti-1839087651; City and County of San Francisco Board of Supervisors, File # 190110, May 31, 2019, https://sfgov.\nlegistar.com/LegislationDetail.aspx?ID=3850006&GUID=12FC5DF6-AAC9-4F4E-8553-8FOCD0EBD3F6; Christine Fisher, \"Oakland Bans City Use of\nFacial Recognition Software,\" Engadget, July 17, 2019, https://www.engadget.com/2019/07/17/oakland-california-facial-recognition-ban/; ACLU\nMassachusetts,\"Somerville City Council Moves to Ban Government Face Surveillance, June 24, 2019, https://www.aclum.org/en/news/somervile-\nStop Facial Recognition on Campus, https://www.banfacialrecognition.com/campus/\n13\nLilah Burke,\"Facial Recognition Surveillance on Campus,\" Inside Higher Ed, February 21, 2020, https://www.insidehighered.com/news/2020/02/21/\nucla-drops-plan-use-facial-recognition-security-surveillance-other-colleges-may-be.\n14\n Boston Teachers Union (@BTU66), \"The BTU joins with teachers, students, civil rights and immigrant rights groups across the nation today in \nsupport of a ban on dangerous and inaccurate facial recognition technology in schools and universities, Twitter, March 2, 2020, 4:37 p.m., https://\ntwitter.com/BTU66/status/1234593709267865603.\n15\n16 Smart Schools Bond Act (2014), http://www.p12.nysed.gov/mgtserv/smart_schools/home.html.",
    "Page_107": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 107\nthat requires school districts to submit proposals and records of community engagement to the\nSmart Schools Review Board for review and approval. 17\nSince 2014, many school districts have applied for and obtained reimbursement for funding to\nacquire student instructional technology, such as laptops, smart boards, and 3D printers, and to\nhold a public hearing about the proposals and post the proposal documentation on the district's\nschool community members are able to give input about the wisdom of the district's proposed\nuse of state funding.\nIn 2016, the Lockport City School District proposed the use of $3,810,833 in SSBA funds for \"new\ncameras and wiring...to provide viewing and automated facial and object recognition of live and\nschool shootings.21 It held its required public hearing on the proposal in the middle of summer\nbreak; unsurprisingly, it did not receive any comments or questions from the community about the\npurchase.22 Lockport certified that it had engaged with all required stakeholders and its proposal \nThe first public criticism of the project started in February 2018 when the local newspaper,\nthe Lockport Union-Sun & Journal, published a piece on one of two resolutions approved at the\nFebruary 2018 Lockport school board meeting.24 The resolution was to allow the use of \"a new\nfacial and shape recognition software\" in the school system.25 Lockport resident and parent Jim\n17\nThe makeup of the Smart Schools Review Board is governed by statute and is comprised of the Commissioner of the New York State Education\nDepartment,the Director of the Offce of the Budget, and the Chancellor of the State University of New York system, or their designees. N.Y. Educ.\nLaw S 3641(16)(a)(2)\n18A\n Approved Smart Schools Investment Plans, http://p1232.nysed.gov/mgtserv/smart_schools/ApprovedSSIPs.htm. See, e.g., Adirondack Central\nSchool District's request for upgrades to wireless connectivity and Chromebooks for students (http:/p1232.nysed.gov/mgtserv/documents/\nADIRONDACKCSD_ADKInvestmentPlan11.16.pdf).\n19\n Smart Schools Bond Act Implementation Guidance, p. 19, http://www.p12.nysed.gov/mgtserv/documents/SSBAGuidancerev_6_1_18_Final.pdf\n20\n Lockport City School District, Smart Schools Investment Plan (2016-17), http:/p1232.nysed.gov/mgtserv/documents/LOCKPORTCITYSD.pdf (last \nmodified October 23, 2017)\n21\n Lockport City School District, Aegis Security System, May 2019, https://www.smore.com/q13ms.\n22\n Lockport City School District, August 2016 Regular Board Meeting Minutes, August 17, 2016, https://www.lockportschools.org/site/default.aspx?Pa\ngeType=14&DomainID=1298&PagelD=9632&ModulelnstancelD=11244&ViewID=1e008a8a-8e8a-4ca0-9472-a8f4a723a4a7&lsMoreExpandedView=\nTrue\n23\n Governor Andrew M. Cuomo, \"Governor Cuomo Announces Approval of 88 Smart Schools Investment Plans Totaling S75.6 Milion,\" November 27,\nthe district's certification, David Lowry, the president of the Lockport Education Association, stated that teachers were not consulted in a discussion\nof how to use the funding, as was required. See Tim Fenster, “Trying for More Secure Schools: Lockport District Turning to Facial Recognition\nSoftware, Lockport Union-Sun & Journal, March 4, 2018, htp://www.lockportjournal.com/news/local_news/trying-for-moresecure-schools-lockport-\n24\nConnor Hoffman,\"Lockport Schools Look to Cut Energy Costs,\" Lockport Union-Sun & Journal, February 8, 2018, https://www.lockportjournal.com/\nnews/local_news/lockport-schools-look-to-cut-energy-costs/article_6374faf0-7c5b-57d9-bc37-a1542df857a5.html\n25Ibid",
    "Page_108": "108 |Regulating Biometrics: Global Approaches and Urgent Questions\nwarning of the risks to privacy for students and teachers.26 Shultz created a petition asking the\nthe community.27 The petition, signed by over a hundred Lockport residents, raised additional\nbetween the district and the security consultant that pitched the product, and the effectiveness of\nthe system.28\nAfter the petition was turned in, the Lockport Journal editorial board called on the district to\nto vote down Lockport's proposed school budget until the district agreed to stop its facial\ndiscrepancies in the systems' ability to identify people of color.32 Shultz wrote monthly columns\nabout the project, and enlisted local support through Lockport's Facebook group. He also solicited\nthe help of the New York Civil Liberties Union, which targeted the district and the New York\ninformation law.33\nThis advocacy garnered the attention of Monica Wallace, Democrat Assembly member\nrepresenting New York's 143rd Assembly District, which borders Lockport and includes the town\n of Depew.34 Wallace was aware of the school district's proposal because the superintendent of\nWallace reached out to advocates in an effort to understand the concerns. As a lawyer and\nparent, she understood the tension between safety and privacy, but worried that the system had\nthe potential to do more harm than good.\n26\nJim Shultz,\"Lockport Schools' Security Plan Warrants Scrutiny,\" Lockport Union-Sun & Journal, February 21, 2018, https://www.lockportjournal.com/\nopinion/lockport-schools-security-plan-warrants-scrutiny/article_34f86bd0-849c-5251-8e73-387b90af357b.html.\n27\nJim Shultz,\" More Questions about School Surveillance Plan, Lockport Union-Sun & Journal, March 21, 2018, https://www.lockportjournal.com/\nopinion/more-questions-about-school-surveillance-plan/article_0c5c6948-cded-5fe2-8d9c-c3e145ae2ed4.html\n28\nIbid.\n29\nUS&J Editorial Board,\"OUR VIEW: Action on School Security Bid Should Be Postponed,\" Lockport Union-Sun & Journal, March 28, 2018, https://www\nlockportjournal.com/opinion/our-view-action-on-school-security-bid-should-be-postponed/article_464b8f55-733e-554f-9ddb-c066ab3ce169.html\nMinutes of the Board of Education of the Lockport City School District, March 28, 2018, https://www.lockportschools.org/site/default.aspx?PageTyp\ne=14&DomainID=1298&PagelD=9632&ModulelnstanceID=11844&ViewID=1e008a8a-8e8a-4ca0-9472-a8f4a723a4a7&lsMoreExpandedView=True.\n31\nJim Shultz,\"Vote No' on Spy Cameras in Lockport's Schools, Lockport Union-Sun & Journal, April 25, 2018, https://www.lockportjournal.com/\nopinion/vote-no-on-spy-cameras-in-lockports-schools/article_6c8cf70a-9551-5974-b396-67bfb5672789.html; Jim Shultz,\"Reject False Security\nvote-no-on-lockport-school-budget/article_672a7a72-4908-588e-bccc-0cddaf4683b0.html.\n32\nTim Fenster, \"Questions remain on school district security project,\" Lockport Union-Sun & Journal, May 13, 2018, https://www.lockportjournal.com/\nnews/local_news/questions-remain-on-school-district-security-project/article_97ce6fd3-490e-5837-834b-217b29474ee5.html.\n33\nSee Connor Hoffman,\"Civil Liberties Union Asks State to Halt Lockport Schools Security Project,\" Lockport Union-Sun & Journal, June 18, 2018,\nhttps://www.lockportjournal.com/news/local_news/civiliberties-union-asks-state-to-halt-lockport-schools-security/article_dbf50305-cd3a-54d9-\ndefault/fles/field_documents/june18_2018_nyclu_letter_re_lockport_city_school_district.pdf.\n34\n Assemblymember Monica P. Wallace, https://assembly.state.ny.us/mem/Monica-P-Wallace/about/\n 35 Thomas J. Prohaska,\"Lockport Schools Turn to State-of-the-Art Technology to Beef up Security\" Buffalo News, May 20, 2018, https://buffalonews\ncom/2018/05/20/lockport-schools-turn-to-state-of-the-art-technology-to-beef-up-security/.",
    "Page_109": "Stefanie Coyle & Rashida Richardson | Bottom-Up Biometric Regulation: A Community's Response to Using Face Surveillance in Schools | 109\nIn March 2019, Wallace introduced a bill (A6787) in the New York State Assembly that would\nplace a moratorium on the use or purchase of any \"biometric identifying technology\" in a school\nidentify a person.37 Wallace consulted with advocates on the bill draft to make sure it addressed\nconcerns about the system. The bill requires NYSED to commission a study on the following\nidentification for certain subgroups of individuals; whether information from the system might be\nrelating to the use of surveillance technology in schools\nschool administrators, parents, and experts in school safety, data privacy, and civil rights and civil\nliberties.39 In many ways, Wallace's bill mirrors the concerns raised by residents in the community\nand advocates across the state and country. A senate version of the bill was introduced in April\n2019.40 The bill passed the New York Assembly with a bipartisan vote of 128 to 19 on the final day\nof the 2019 legislative session.41 The bill was not considered in the Senate, effectively killing the\nbill for the 2019 legislative session and teeing up a new fight in 2020.\nMeanwhile, the community continued its efforts to prevent the use of the technology. Connor\nHoffman, a reporter from the Lockport Journal, attended every school board meeting and\nfiled multiple requests for information from the school district and NYSED. Hoffman received\ninformation that had not yet been publicly disclosed about the accuracy rates of Lockport's\nsystem, revealing that Black women are sixteen times as likely as white men to be misidentified\nby the system.42 The persistent reporting led to national press coverage, including a feature in the\nNew York Times,43 a New York Times op-ed by Shultz,44 and an MTV News documentary.45 Without\nfacial recognition system and NYSED's failure to regulate this type of technology might have gone\nunnoticed.\n36Sup., note 16.\n37\nIbid.\n38\np!ql\n39\nIbid.\n40\n Senate Bill S5140-B, https://www.nysenate.gov/legislation/bill/2019/s5140. Introduced by Senator Brian Kavanagh, a senator representing New\nNew York State Assembly, A06787 Floor Votes, https://assembly.state.ny.us/leg/?default _fld=&bn=A06787&term=2019&Summary=Y&Actions=Y&\nText=Y&Committee%26nbspVotes=Y&Floor%26nbspVotes=Y\n42\nNew York Civil Liberties Union v. New York State Education Department, Index No. 903807-20, Exh. 9, G-2, p. 94, Albany County Supreme Court, June\n18, 2020.\n43\nDavey Alba, \"Facial Recognition Moves into a New Front: Schools, New York Times, February 6, 2020, https://www.nytimes.com/2020/02/06/\nbusiness/facial-recognition-schools.html.\n44\nJim Shultz,\"Opinion: Spying on Children Won't Keep Them Safe, New York Times, June 7, 2019, https://www.nytimes.com/2019/06/07/opinion/\nlockport-facial-recognition-schools.html.\n 45  MTV News (@MTVNEWS),\"@NYCLU says tech companies are using the fear of school shootings to turn students into lab rats for experimental\ntechnology. Meet the 25-year-old reporter @_hoffngtonpost who's exposing the spread of facial recognition in schools,\" Twitter, March 12, 2020,\n12:31 p.m., https://twitter.com/MTVNEWS/status/1238140444992774145",
    "Page_110": "110 |  Regulating Biometrics: Global Approaches and Urgent Questions\nDespite the pushback, Lockport activated its facial recognition system on January 2, 2020.46\nParents and students were not notified ahead of the deployment, nor were they given a chance\nsystem given the concerns of the community.\nagainst this technology. Though the 2020 legislative session was interrupted by the COVID-19\nglobal pandemic, the bill was amended in both houses to increase the amount of time for the\nmoratorium until July 2022 or until the Commissioner of Education explicitly authorizes the use\nof the technology after issuance of the report, whichever occurs later.48 The bill has widespread\nUnited Federation of Teachers (UFT), the New York City affiliate of the American Federation of\nTeachers.49 During the week of July 20, 2020, the bill passed both the Assembly and the Senate,\nand now awaits signature by the Governor to become law.50\nIn February 2020, the New York Civil Liberties Union led a town hall in Lockport attended by nearly\nfifty parents and concerned community members about the system. The town hall was headlined\nby Shultz and a recent alumna of the school district.51 For many, it was the first time they had all\nin attendance why there had not been a community forum sponsored by the district to answer\nquestions and hear concerns. Community members expressed consternation over Lockport's\nvoices heard at school board meetings.52\nTHEIMPORTANCEOFCOMMUNITY-DRIVENPOLICY\nADVOCACY\norganized public scrutiny can illuminate bureaucratic failures, shape necessary reforms, and\nshift narratives. The district's decision to purchase and use a facial recognition system follows a\n46  See Troy Licastro,\"Lockport City School District Begins Using Facial Recognition System, WIVB-TV, January 2, 2020, https://www.wivb.com/\nnews/local-news/niagara-county/lockport/lockport-city-school-district-begins-using-facial-recognition-system/. Lockport deployed its system\nafter receiving permission from NYSED. See also Temitope Akinyemi to Michelle Bradley, https:/int.nyt.com/data/documenthelper/6688-nysed-\nlockport/03fc55526445f8ef41aa/optimized/full.pdf\n47\n\"Something this big should have been properly told to us.:\" Christianna Silva, “Facial Recognition Technology Is Taking Over Schools-and Students\nAren't OK with It, MTV News, March 13, 2020, http://www.mty.com/news/3159161/facial-recognition-technology-schools-students-respond/\n48\nSup., note 16.\n49\n This is the same organization that experimented with Clearview Al's technology. See Michael Elsen-Rooney,\"Racial Justice Groups Criticize City\nTeachers Union's Use of Controversial Face Recognition Technology\" Daily News, March 27, 2020, https:/www.nydailynews.com/new-york/\neducation/ny-uft-facial-recognition-20200327-msxxn5mmw5dtjimrisjfyaj7xqq-story.html\n50\nNew York Assembly Bill A6787-D, https://www.nysenate.gov/legislation/bills/2019/a6787,\n51\n Connor Hoffman, \"Spotlight on Facial Recognition: Civil Liberties Union Hosts a Town Hall in Lockport\" Lockport Union-Sun & Journal, February 25\n2020, https://www.lockportjournal.com/news/local_news/spotlight-on-facial-recognition-civil-iberties-union-hosts-a-town/article_32c29556-9f05-\n5934-8c03-29ef5e08212a.htm\n52\n In addition to concerns over the use of the facial recognition system, community members demanded that a beloved middle-school peer mediator's\nemployment not be terminated by the school district. Connor Hoffman, \"Trying to Have Mr. Cheatham's Back, Lockport Union-Sun & Journal, January\n23, 2020, https://www.lockportjournal.com/news/trying-to-have-mr-cheatham-s-back/article_a814300a-3e61-11ea-b6fa-43627916ccc3.html",
    "Page_111": "common yet flawed pattern that government officials rely on to justify the adoption of surveillance\ntechnologies. The school district conflated an abstract or speculative risk to student safety with\nlegitimate and warrant critical review, the school district's actions demonstrated the inherently\npolitical nature of privileging certain risks and interests over community needs.53 Rather than\nconsult the community to assess actual needs and concerns, the district adopted a technological\nsolution in search of a problem.\nto focusing on the real harms posed to students if the school district decided to move forward\ntrivialized the idea that the system could negatively impact student privacy.54 In 2019, however\nthe superintendent of the school district reluctantly acknowledged that \"lpJrivacy matters are a big\ndeal nowadays.\"55 This emphasis on privacy is echoed in the current legislation.56\nLockport also failed to acknowledge that deployment of a flawed facial recognition system\npolicies. For instance, the district has struggled to address existing issues of disproportionate\nyear's voter turnout was four times higher than the district's five-year average turnout.59 Though\nthe fight in Lockport and New York State continues, this community-driven advocacy effort\n 53 See Hope,\"Seductions of Risk,\" in Schools Under Surveilance, for a discussion of how adoption of surveillance technologies can unreasonably\nlimit students' educational experience. See also Mary Douglas and Aaron Wildavsky, Risk and Culture: An Essay on the Selection of Technological\nand Environmental Dangers (Berkeley: University of California Press, 1983), 29-38. Douglas and Wildavsky describe how labeling risks is a social\nprocess complicated by new technologies that provoke cultural and social reassessments.\n54S\n Sup., note 31: \"When the privacy issue was raised at that March meeting it was dismissed away as a joke about the likelihood of North Korea\nhacking into student records.\n55\n Connor Hoffman, Lockport School District Cancels Security Contract, Lockport Union-Sun & Journal, April 11, 2019, https://www.lockportjournal.\ncom/news/local_news/lockport-school-district-cancels-security-contract/article_b5612839-211e-53ff-a70b-c1de1f66abd6.html.\n56\n Sup., note 16. The legislation requires NYSED to consider \"the privacy implications of collecting, storing, and/or sharing biometric information of\nstudents, teachers, school personnel and the general public entering a school or school grounds.\"\n 57 During the 2015-2016 school year, Black students made up just 12.3 percent of the student population but represented more than a quarter of the\nstudents receiving out-of-school suspensions. Students of two or more races represented 5.9 percent of the student population in Lockport, but\n15 percent of the students who received out-of-school suspensions. See Civil Rights Data Collection, \"Lockport City School District, 2015, https://\nocrdata.ed.gov/Page?t=d&eid=31160&syk=8&pid=2539&Report=6. See also Paul Hirschfield, \"School Surveillance in America: Disparate and\nUnequal, in Schools Under Surveillance, for a description of the disparate impact of school surveillance.\n 58 Jim Shultz, \"Time for Change on the Lockport School Board,\" Lockport Union-Sun & Journal, May 23, 2020, https://www.lockportjournal.com/\nopinion/jim-shultz-time-for-change-on-the-lockport-school-board/article_142c3905-0612-54d6-8f66-22097b35e5cb.html.\n59\ncom/news/local_news/renee-cheatham-wins-a-seat-on-the-school-board/article_6e3e73ba-b42c-5820-98df-01ccd23d234a.html.",
    "Page_112": "MINOW"
}