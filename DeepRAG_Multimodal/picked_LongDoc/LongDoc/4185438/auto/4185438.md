# Modern Group Theory  

Corran Webster May 3, 2004  

# Contents  

# 1 Introduction 1  

1.1 Symmetry . 1   
1.2 Review: Sets 6   
1.3 Review: Functions 10   
1.4 Permutations 11   
1.4.1 Cycles . 14   
1.4.2 Parity 17   
1.4.3 Permutation Matrices 21   
1.5 Modulo Arithmetic 24   
1.6 Addendum: Technical Details 25   
1.6.1 Symmetry . 25   
1.6.2 Parity . 26  

# 2 Groups 29  

# 2.1 Binary Operations 29  

2.2 Groups . 32   
2.3 Working With Abstract Groups 39   
2.4 Cayley Tables . 41   
2.5 Generators 45   
2.6 Excursion: Introduction to Categories 51   
2.7 Direct Products . 52   
2.8 Subgroups 56   
2.9 Homomorphisms 62  

# 3 The Structure of Groups 71  

3.1 The Subgroup Lattice 71   
3.2 Extension: Lattices . 76   
3.3 The Centre and Centralizers . 81   
3.4 Cosets 86   
3.5 Classifying Groups of Small Order 90   
3.6 Excursion: Equivalence Relations 93   
3.7 Conjugacy Classes . 96   
3.8 Normal Subgroups . 101   
3.9 Groups of Small Order, Part II 110  

3.10 Extension: Cayley Graphs . 112  

# 4 Constructing Groups 117  

4.1 Quotient Groups 117   
4.2 Automorphism Groups . 124   
4.3 Extension: Category Theory . 129   
4.4 Semidirect Products 137  

# Chapter 1  

# Introduction  

Algebraic structures of various types occur naturally in many different areas of mathematics. The most straightforward examples arise in arithmetic, but there are numerous other examples which are not as obvious. In this chapter we start our study of group theory by looking at a number of concrete situations where an algebraic structure arises naturally. We will see that all these algebraic structures share common features, and these common features will lead us to the definition of a group in Chapter 2.  

# 1.1 Symmetry  

You are familiar, at least in an informal way, with the idea of symmetry from Euclidean geometry and calculus. For example, the letter “A” has reflective symmetry in its vertical axis, “E” has reflective symmetry in its horizontal axis, “N” has rotational symmetry of $\pi$ radians about its centre, “H” has all three types of symmetry, and the letter “F” has none of these symmetries.  

Symmetry is also important in understanding real world phenomena. As some examples:  

The symmetries of molecules can affect possible chemical reactions. For example, many proteins and amino acids (the basic building blocks of life) have “left-handed” and “right-handed” versions which are reflections of one-another. Life on earth uses the “left-handed” versions almost exclusively.   
• Crystals have very strong symmetries, largely determined by the symmetries of the atoms or molecules of which the crystal is built.   
• Most animals and plants have some sort of symmetry in their body-shapes, although they are never perfectly symmetrical. Most animals have bilateral symmetry, while plants often have five-fold or six-fold rotational symmetry.  

In art and design, symmetrical patterns are often found to be more pleasing to the eye than asymmetrical patterns, or simply more practical. Waves in fluids, and the vibrations of a drumhead or string are often symmetrical, or built out of symmetric components. These symmetries are usually inherent in the underlying equations that we use to model such systems, and understanding the symmetry can be crucial in finding solutions to these equations.  

But what, precisely, do we mean by symmetry?  

# Definition 1.1  

Let $\Omega$ be a subset of $\mathbb{R}^{n}$ . $A$ symmetry of $\Omega$ is a function $T:\mathbb{R}^{n}\longrightarrow\mathbb{R}^{n}$ such that  

(i) $\{T(x):x\in\Omega\}=\Omega$ , and   
(ii) $T$ preserves distances between points: if $d(x,y)$ is the distance between the points $x$ and $y$ , then $d(T(x),T(y))=d(x,y)$ .  

We denote the set of all symmetries of $\Omega$ by $\mathrm{Sym}(\Omega)$ . Every set $\Omega$ has at least one symmetry, the identity symmetry $I(x)=x$ .  

Functions which preserve distance are called isometries, so every symmetry is an isometry.  

# Proposition 1.1  

Let $\Omega$ be a subset of $\mathbb{R}^{n}$ , and let $S$ and $T$ be symmetries of $\Omega$ . Then  

(i) $T$ is one-to-one and onto.   
(ii) the inverse function $T^{-1}$ is a symmetry of $\Omega$   
(iii) the composition $T\circ S$ is a symmetry of $\Omega$   
(iv) the compositions $T\circ T^{-1}$ and $T^{-1}\circ T$ always equal the identity symmetry $I$ .  

# Proof:  

(i) This follows immediately from the technical result that every isometry from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ is one-to-one and onto (this is proved in Lemma 1.11 at the end of this chapter).  

(ii) Since $T$ is one-to-one and onto, it has an inverse function $T^{-1}$ . We observe that $T^{-1}(\Omega)=T^{-1}(T(\Omega))=\Omega$ , and also that $d(T^{-1}(x),T^{-1}(y))=$ $d(T(T^{-1}(x)),T(T^{-1}(y)))=d(x,y)$ . Hence $T^{-1}$ is a symmetry of $\Omega$ .  

Parts (iii) and (iv) are left as a simple exercise.  

Notation: Many algebra texts write $S T$ for $T\circ S$ , because $S$ is applied first, then $T$ . In these notes, however, we will remain consistent with the traditional function composition order, but you must keep this clear in your head to avoid confusion. Another commonly used convention in algebra is to apply functions on the right, so $T(x)$ is written as $x T'$ , so that $S(T(x))$ ) would be written as $x T S$ .  

We will usually write the composed symmetry $T\circ S$ as simply $T S$ . Remember that because function composition works from right to left, $T S$ means that the symmetry $S$ is applied first, followed by the symmetry $T$ .  

May 3, 2004  

![](images/db4159f995ae3249fd38f9362526564a14e78c11e48798489d1f04a8c8cdc975.jpg)  
Figure 1.1: The set $\Omega$ of Example 1.1  

You should also recall that composition of functions is associative (see Proposition 1.2) so composition of symmetries is always associative. In other words if $S$ , $T$ and $U\in\mathrm{Sym}(\Omega)$ , then $S(T U)=(S T)U$ . However, composition of functions is not usually commutative, so without additional evidence, we cannot conclude that $S T=T S$ .  

# Example 1.1  

Let $\Omega\subseteq\mathbb{R}^{2}$ be the H-shaped set illustrated in Figure 1.1. Then $\Omega$ has percisely the following symmetries:  

$$
\begin{array}{r l r l}{I(x,y)=(x,y)}&{}&&{{}{\mathrm{(Identity)}}}\\ {H(x,y)=(x,-y)}&{}&{{}{\mathrm{(Refection~in~the~}}x{\mathrm{-axis)}}}\\ {V(x,y)=(-x,y)}&{}&{{}{\mathrm{(Refection~in~the~}}y{\mathrm{-axis)}}}\\ {R(x,y)=(-x,-y)}&{}&{{}{\mathrm{(Rotation~by~\pi~radians~about~the~origin)}}}\end{array}
$$  

We can confirm by direct calculation that $I^{-1}=I$ , $H^{-1}=H$ , $V^{-1}=V$ and $R^{-1}=R$ . In other words, each of these transformations is its own inverse. These symmetries compose in the following ways:  

$$
\begin{array}{r l r l r}{H\circ H=I}&{{}}&{H\circ V=R}&{{}\quad H\circ R=V}\\ {V\circ H=R}&{{}}&{V\circ V\ =I}&{{}\quad V\circ R=H}\\ {R\circ H=V}&{{}}&{R\circ V=H}&{{}\quad R\circ R=I}\end{array}
$$  

In fact we can summarize this using a “multiplication table”:  

<html><body><table><tr><td></td><td>I</td><td>H V</td><td>R</td></tr><tr><td>I</td><td>I</td><td>H V</td><td>R</td></tr><tr><td>H</td><td>H</td><td>I</td><td>R V</td></tr><tr><td>V</td><td>V</td><td>R I</td><td>H</td></tr><tr><td>R</td><td>R</td><td>V H</td><td>I</td></tr></table></body></html>  

This sort of “multiplication table” is called a Cayley table for the operation.  

![](images/11b2cc9b5c9da054522df2f3310044ad31467c72babbad2081ed33ac28df3ce2.jpg)  
Figure 1.2: The equilateral triangle of Example 1.2  

The composition of symmetries in this example is commutative. You can verify this by simply checking every possible product. For example, from the Cayley table we have $H R=V$ , and $R H=V$ , so $H R=R H$ . 3  

There is nothing really special about the set $\Omega$ in the previous example, other than the fact that composition is commutative for this set. As the following example shows, we should not expect composition of symmetries to be commutative in every case.  

# Example 1.2  

Let $\Omega\subseteq\mathbb{R}^{2}$ be an equilateral triangle with veritces $(1,0)$ , $(-1/2,{\sqrt{3}}/2)$ and $(-1/2,-{\sqrt{3}}/2)$ . Then $\Omega$ has the following symmetries:  

$I$ Identity   
$R_{1}$ Rotation by $2\pi/3$ radians clockwise   
$R_{2}$ Rotation by $2\pi/3$ radians anticlockwise   
H0 Reflection in the $x$ -axis   
$H_{1}$ Reflection in the line through $(0,0)$ and $(-1/2,{\sqrt{3}}/2)$   
$H_{2}$ Reflection in the line through $(0,0)$ and $(-1/2,-{\sqrt{3}}/2)$  

The precise formulas for these symmetries are an exercise. A little thought tells us that $I^{-1}=I$ , $R_{1}^{-1}=R_{2}$ , $R_{2}^{-1}~=~R_{1}$ , $H_{1}^{-1}=H_{1}$ , $H_{2}^{-1}\ =\ H_{2}$ , and $H_{3}^{-1}=H_{3}$ . The Cayley table for these symmetries is:  

<html><body><table><tr><td></td><td>I</td><td>R1</td><td>R2 Ho</td><td>H1</td><td>H2</td></tr><tr><td>I</td><td></td><td>R1</td><td>R2</td><td>Ho H1</td><td>H2</td></tr><tr><td>R1</td><td>R1</td><td>R2</td><td>I</td><td>H1 H2</td><td>Ho</td></tr><tr><td>R2</td><td>R2</td><td>I</td><td>R1</td><td>H2 Ho</td><td>H1</td></tr><tr><td>Ho</td><td>Ho</td><td>H2</td><td>H1</td><td>I R2</td><td>R1</td></tr><tr><td>H1</td><td>H1</td><td>Ho</td><td>H2</td><td>R1 I</td><td>R2</td></tr><tr><td>H2</td><td>H2</td><td>H1</td><td>Ho</td><td>R2 R1</td><td>I</td></tr></table></body></html>  

This operation is associative, but it is clearly not commutative: $H_{0}\circ H_{1}=$ $R_{1}$ , but $H_{1}\circ H_{0}=R_{2}$ , for example. $\diamondsuit$  

Some sets have infinite collections of symmetries, but even in these cases we can still understand how composition works.  

# Example 1.3  

Let $\Omega=\{(x,y)\in\mathbb{R}^{2}:x^{2}+y^{2}=1\}$ be the unit circle. Then $\Omega$ has infinitely many symmetries, which fall into two classes:  

$R_{\theta}$ Rotation by $\theta$ radians clockwise, $0\leq\theta<2\pi$ $H_{\varphi}$ Reflection in the line which makes an angle $\varphi$ to the $x$ -axis at the origin, $0\leq\varphi<\pi$ .  

The identity is $R_{0}$ , rotation by 0 radians. We can also check that the inverse of $R_{\theta}$ is $R_{2\pi-\theta}$ for $0<\theta<2\pi$ , and the inverse of $H_{\varphi}$ is $H_{\varphi}$ .  

Because the set of symmetries is infinite, we can’t write down a Cayley table, but we can list how the generic symmetries compose:  

$$
\begin{array}{r l r l}{{}}&{{}\ensuremath{R_{\theta}}\circ{}\ensuremath{R_{\omega}}=\ensuremath{R_{\theta+\omega}}}&{}&{{}\ensuremath{R_{\theta}}\circ{}\ensuremath{H_{\varphi}}=\ensuremath{H_{\varphi-\theta/2}}}\\ {{}}&{{}\ensuremath{H_{\varphi}}\circ{}\ensuremath{R_{\theta}}=\ensuremath{H_{\varphi+\theta/2}}}&{}&{{}\ensuremath{H_{\varphi}}\circ{}\ensuremath{H_{\psi}}=\ensuremath{R_{2\psi-2\varphi}}}\end{array}
$$  

where all angles are reduced to lie in the appropriate ranges. The easiest way to verify this table is to note that ${{H}_{\varphi}}={{H}_{0}}\circ{{R}_{2\varphi}}={{R}_{-2\varphi}}\circ{{H}_{0}}$ , which greatly simplifies calculations involving $H_{\varphi}$ . $\diamondsuit$  

All the examples so far have used rotational and reflective symmetries, but some sets also have translational symmetry.  

# Example 1.4  

Let $\Omega\{(x,y)\in\mathbb{R}^{2}:y=0\}$ be the $x$ -axis in $\mathbb{R}^{2}$ . Then $\Omega$ has symmetries of the form  

$$
T_{c}(x,y)=(x+c,y),
$$  

ie. right translation by $c$ , for any $c\in\mathbb{R}$ , and  

$$
S_{c}(x,y)=(-x+c,y),
$$  

ie. reflection about $0$ , followed by right translation by $c$ , for any $c\in\mathbb{R}$ (which is equal to reflection about the point $c/2$ ).  

The identity symmetry is $T_{0}$ , the inverse symmetry of $T_{c}$ is $T_{-c}$ , and the inverse symmetry of $S_{c}$ is $S_{c}$ . The symmetries of this set compose by the rules  

$$
\begin{array}{c c}{{T_{a}\circ T_{b}=T_{a+b}}}&{{\quad T_{a}\circ S_{b}=S_{a+b}}}\\ {{S_{a}\circ T_{b}=S_{a-b}}}&{{\quad S_{a}\circ S_{b}=T_{a-b}}}\end{array}
$$  

# Exercises  

1.1.1. Find the set of symmetries for each capital letter of the alphabet (assume uniform, sans serif letter shapes).  

May 3, 2004  

1.1.2. Prove Proposition 1.1 (iii-iv).  

1.1.3. Write down formulas for each of the symmetries in Example 1.2.  

Hint 1: the point $(x,y)\in\mathbb{R}^{2}$ rotated clockwise by an angle $\theta$ about the origin is $(x\cos\theta+y\sin\theta,-x\sin\theta+y\cos\theta)$ ).  

Hint 2: from the Cayley table, we have $H_{1}=R_{1}\circ H_{0}$ and $H_{2}=R_{2}\circ H_{0}$ , and it is easy to find the formula of a composition of functions.  

1.1.4. Let $\Omega\subseteq\mathbb{R}^{2}$ be a square, centred at the origin, with side length 1. Find all 8 symmetries of $\Omega$ , and write down the formula for each. Find the inverses of each symmetry. Write out the Cayley table for the symmetries of a square.  

1.1.5. ( $*$ ) Let $\Omega\subseteq\mathbb{R}^{3}$ be a regular tetrahedron centred at the origin. Show that $\Omega$ has 24 symmetries.  

1.1.6. ( $*$ ) Let $\Omega=\mathbb{Z}^{2}\subseteq\mathbb{R}^{2}$ be the integer lattice in the plane, ie.  

$$
\mathbb{Z}^{2}=\{(m,n)\in\mathbb{R}^{2}:m,n\in\mathbb{Z}\}.
$$  

Classify the symmetries of $\ensuremath{\mathbb{Z}}^{2}$ . Find the inverses of each symmetry. As in Example 1.3, find the product of typical symmetries.  

# 1.2 Review: Sets  

Group theory does not require a great deal of mathematical background to get started: we really only need the concepts of sets and functions to present the basics of the theory. You should have come across the formal definitions of these concepts in previous courses, such as a typical discrete mathematics course. A large part of the discussion in this section and the next will be to fix notation and terminology.  

A set is a collection of mathematical objects. We do not care about the order that the objects are presented, nor any potential duplication of elements. The mathematical objects contained in a set $S$ are called the elements or members of a set, and write $x\in S$ to say that $x$ is an element of $S$ . We say that two sets are equal if they have exactly the same elements.  

The simplest way to present a set is as a list of all the elements of the set enclosed in braces, such as the set $\{1,2,3\}$ . For sets with large numbers of elements, or infinite sets, this presentation is tedious (or impossible!), so there are two alternatives. If there is a clear pattern to the elements, one can use ellipses to elide the majority of the set, leaving just enough to make the pattern of elements clear:  

$$
\{2,4,6,\ldots,100\}\qquad\mathrm{and}\qquad\{2,3,5,7,11,13,17\ldots\}
$$  

are clearly meant to represent the set of all even numbers from 2 to 100, and the set of all prime numbers respectively. However some sets are too complicated  

for this sort of presentation, and for these we use “set builder” notation. In set builder notation we simply specify the set by some property $P$ which defines the set:  

$$
\{x|x{\mathrm{~satisfies~}}P\}\qquad{\mathrm{or}}\qquad\{x:x{\mathrm{~satisfies~}}P\}.
$$  

For example, one could write the set of all prime numbers as  

$$
\{x|x{\mathrm{~is~prime}}\},
$$  

or the set of all numbers greater than 2 and less than or equal to 10 as  

$$
\{x:2<x\leq10\}.
$$  

This last example illustrates an ambiguity: which collection of numbers do we mean? Integers? Rational numbers? Real numbers? To resolve this ambiguity, we usually specify the set $S$ from which we take our elements, and use the notation  

$$
\{x\in S|x{\mathrm{~satisfies~}}P\}\qquad{\mathrm{or}}\qquad\{x\in S:x{\mathrm{~satisfies~}}P\}.
$$  

Therefore the interval of all real numbers greater than 2 and less than or equal to 10 would most clearly be represented by  

$$
\{x\in\mathbb{R}:2<x\leq10\}.
$$  

There are several special sets that come up with sufficient frequency to deserve their own notation. The most important is the empty set $\mathrm{\Delta\mathrm{\Omega}}\mathrm{\Delta}\mathrm{\Omega}\mathrm{\Delta\emptyset}\mathrm{\Delta\mathrm{\Omega}}$ , the set which contains no elements. The next most important are the various sets of numbers:  

$\begin{array}{r l}&{\mathbb{N}=\{1,2,3,4,\ldots\}}\\ &{\mathbb{Z}=\{\ldots,-3,-2,-1,0,1,2,3,\ldots\}}\\ &{\mathbb{Q}=\{p/q:p\in\mathbb{Z},q\in\mathbb{N},p\mathrm{~and~}q\mathrm{~coprime}\}}\\ &{\mathbb{R}=\{x:x\mathrm{~is~an~infnite~decimal}^{1}\}}\\ &{\mathbb{C}=\{x+i y:x,y\in\mathbb{R}\}}\end{array}$ natural numbers integers rational numbers real numbers complex numbers  

We say that a set $A$ is a subset of another set $B$ , and write $A\subseteq B$ , if every element of $A$ is an element of $B$ . For example, $\{2,4,6\}\subseteq\{1,2,3,4,5,6\}$ . Note that if $A$ is equal to $B$ , it is still a subset of $B$ , and that the empty set is always a subset of any other set. We say that $A$ is a proper subset of $B$ if $A\subset B$ and $A\neq B$ , and we denote this by $A\subset B$ .  

We can combine sets using a number of different set operations. The union of two sets $A$ and $B$ is the set containing all the elements of both sets combined, ie.  

Proving Equality of Sets: Often we have two sets, $A$ and $B$ , which we want to show are equal. $A$ very common technique to show that this is in fact the case is to show that each set is a subset of the other. We can then conclude that they are equal. In summary:  

$$
A\cup B=\{x:x\in A{\mathrm{~or~}}x\in B\}.
$$  

The intersection of $A$ and $B$ is the set containing the objects that are elements of both of the sets, ie.  

$$
A\cap B=\{x:x\in A{\mathrm{~and~}}x\in B\}.
$$  

Intersection and union are both commutative and associative operations, and are distributive with respect to one another:  

$$
{\begin{array}{c}{A\cup B=B\cup A}\\ {A\cap B=B\cap A}\\ {A\cup(B\cup C)=(A\cup B)\cup C=A\cup B\cup C}\\ {A\cap(B\cap C)=(A\cap B)\cap C=A\cap B\cap C}\\ {A\cup(B\cap C)=(A\cup B)\cap(A\cup C)}\\ {A\cap(B\cup C)=(A\cap B)\cup(A\cap C)}\\ {A\cup\emptyset=A}\\ {A\cap\emptyset=\emptyset}\end{array}}
$$  

If there is some natural universal set $U$ of elements which we are considering, we can define the complement of a set $A$ as the set of all things in $U$ not in $A$ , ie.  

$$
A^{c}=\{x\in U:x\notin A\}.
$$  

The complement of the complement is the original set, and complements interact with union and intersection via DeMorgan’s laws:  

$$
\begin{array}{c}{{(A^{c})^{c}=A}}\\ {{(A\cup B)^{c}=A^{c}\cap B^{c}}}\\ {{(A\cap B)^{c}=A^{c}\cup B^{c}}}\\ {{\emptyset^{c}=U}}\\ {{U^{c}=\emptyset.}}\end{array}
$$  

Note that sometimes the notation $\overline{{A}}$ is used for complements.  

Even in the absence of a universal set, we can define the set difference operation: $A\setminus B$ is everything in $A$ which is not in $B$ . That is  

$$
A\setminus B=\{x\in A:x\notin B\}.
$$  

If complements make sense, then we have $A\setminus B=A\cap B^{c}$ . We can also define the symmetric difference of $A$ and $B$ as the set of all things in either $A$ or $B$ , but not in both,  

$$
A\triangle B=\{x\in A\cup B:x\notin A\cap B\}
$$  

or equivalently  

$$
A\triangle B=(A\cup B)\setminus(A\cap B)=(A\setminus B)\cup(B\setminus A).
$$  

May 3, 2004  

Clearly $A\triangle B=B\triangle A$ .  

Perhaps the most important set operation for our purposes, since it appears in just about every core definition in abstract algebra, is the Cartesian product. The product of two sets, $A\times B$ is the set consisting of tuples $(x,y)$ , where $x\in A$ and $y\in B$ , ie.  

$$
A\times B=\{(x,y):x\in A,y\in B\}.
$$  

More generally, we define a product of $n$ sets to be the set of $n$ -tuples:  

$$
A_{1}\times A_{2}\times\cdot\cdot\cdot\times A_{n}=\{(a_{1},a_{2},\ldots,a_{n}):a_{k}\in A_{k},k=1,2,\ldots,n\}.
$$  

We also define  

$$
A^{n}=\underbrace{A\times A\times\cdot\cdot\cdot\times A}_{n{\mathrm{~times}}}
$$  

to be the set of all $n$ -tuples of elements of $A$ . This notation is familiar from calculus, where $\mathbb{R}^{n}$ is the set of all $n$ -tuples of real numbers. Note that $A\times B$ is not equal to $B\times A$ in general, although they are clearly closely related.  

If $A\subseteq C$ , and $B\subseteq D$ it is straightforward to see that $A\times B\subseteq C\times D$ .  

We denote the cardinality of a set $A$ by $|A|$ . For sets with a finite number of elements, the cardinality of $A$ is simply the number of elements in the set. For infinite sets, cardinality is a more complicated matter, but for the purposes of this course it really only matters whether a set is infinite or not. You should, however, be aware that there are countably infinite sets (such as $\mathbb{N}^{n}$ , $\mathbb{Z}^{n}$ and $\mathbb{Q}^{n}$ ) and uncountably infinite sets (such as $\mathbb{R}^{n}$ and $\mathbb{C}^{n}$ ) and that countable and uncountable sets have different cardinality.  

For finite sets, we have the following facts from basic counting theory: the inclusion-exclusion principle  

$$
|A\cup B|=|A|+|B|-|A\cap B|,
$$  

and the multiplication principle  

$$
|A\times B|=|A||B|.
$$  

Both of these will be of importance when exploring the structure of finite groups.  

# Exercises  

1.2.1. In this section many identities are stated without proof. Pick 8 of them and show why they hold. Be careful not to use any identity or fact which is dependent on what you are proving.  

1.2.2. Show that $|A\setminus B|=|A|-|A\cap B|$ .  

May 3, 2004  

# 1.3 Review: Functions  

A function $f$ from $A$ to $B$ is a rule which relates every element $x$ of $A$ to some unique element $y$ of $B$ . What is key here is that the function associates $x$ with precisely one element of $B$ . We write $y=f(x)$ . More formally, we denote the function with the notation  

$$
\begin{array}{r}{f:A\to B}\\ {x\mapsto y.}\end{array}
$$  

The set $A$ is the domain, the set $B$ the codomain, while the set  

$$
f(A)=\{f(x):x\in A\}
$$  

is the range of the function. The graph of the function is the subset  

$$
{\mathcal{G}}_{f}=\{(x,f(x)):x\in A\}
$$  

of $A\times B$ .  

From time to time, we will wish to specify an abstract function without specifying an exact formula or rule. In this case, we will just write $f:A\rightarrow B$ , specifying the domain and codomain, but nothing else. We will also write $\mathcal{F}(A,B)$ for the set of all functions from $A$ to $B$ . Some texts use $B^{A}$ instead for this set.  

Given a function $f:A\rightarrow B$ , and a subset $X\subseteq A$ , we define the image of $X$ to be the subset of $B$ given by  

$$
f(X)=\{f(x):x\in B\}.
$$  

If $Y\subseteq B$ , we also define the inverse image of $Y$ to be the subset of $A$ given by  

$$
f^{-1}(Y)=\{x\in A:f(x)\in Y\}.
$$  

In other words $f^{-1}(Y)$ is the set of elements of $A$ whose value lies in the set $Y$ .  

Given a function $g:A\rightarrow B$ and another function $f:B\to C$ , we define the composition of $f$ and $g$ to be the function $f\circ g:A\to C$ defined by $(f\circ g)(x)=f(g(x))$ .  

A function is one-to-one or injective if it satisfies the condition  

$$
f(x_{1})=f(x_{2}){\mathrm{~implies~}}x_{1}=x_{2}.
$$  

A function is onto or surjective if the range equals the entire codomain, or equivalently  

$$
f(A)=B.
$$  

A function which is both injective and surjective is called a bijective function.  

A bijective function automatically has an inverse function $f^{-1}:B\to A$ defined by $f^{-1}(b)=a$ if and only if $f(a)=b$ . The fact that $f$ is onto guarantees that $f^{-1}$ is defined on all of $B$ , while the fact that $f$ is injective ensures that $f^{-1}$ is a function. It follows from the definition that $(f\circ f^{-1})(x)=x$ and $(f^{-1}\circ f)(x)=x$ .  

# Proposition 1.2  

Let $A$ , $B$ $C$ and $D$ be sets, and $f:A\rightarrow B$ , $g:B\rightarrow C$ and $h:C\to D$ be functions. Then we have:  

(i) Composition of functions satisfies an associative law: $(h\circ g)\circ f=$ $h\circ(g\circ f)$ .   
(ii) If $f$ and $g$ are both one-to-one, then so is $g\circ f$ .   
(iii) If $f$ and $g$ are both onto, then so is $g\circ f$ .   
(iv) If $f$ and $g$ are both bijections, then so is $g\circ f$ .   
(v) If $f$ is a bijection, then so is $f^{-1}$ .   
(vi) If $f$ is a bijection $|A|=|B|$ .  

Proof: The proof is left as an exercise.  

# Example 1.5  

We could formally write the function $f(x)={\sqrt{x}}+1$ as:  

$$
\begin{array}{r}{f:[0,\infty)\to\mathbb{R}}\\ {x\mapsto\sqrt{x}+1.}\end{array}
$$  

As you would expect, the domain is $[0,\infty)$ , the codomain is $\mathbb{R}$ , the range is $[1,\infty)$ , and the graph is the set of points  

$$
\{(x,{\sqrt{x}}+1):x\in[0,\infty)\}.
$$  

The function is one-to-one, but is not surjective or bijective.  

# Exercises  

1.3.1. Prove Proposition 1.2.  

# 1.4 Permutations  

A permutation of a set $X$ is simply a re-arrangement of the elements, or more precisely a function that maps each element of $X$ to an element of $X$ with no $p$   
two distinct elements being mapped to the same element (and for infinite sets, we also need $p(X)=X$ ). Another way of saying this is that a permutation of $X$ is simply a bijection $p:X\rightarrow X$ .  

Normally we are interested only in permutations of finite sets, and we really only care how many elements there are to permute. Hence it is customary to consider permutations of the set $\{1,2,3,...,n\}$ .  

Since permutations are just functions,we can define them as we would any other function, by specifying the value that the function takes at each point in the domain. Unfortunately, unlike the usual functions you see in a calculus course, you usually can’t specify permutations using a formula.  

# Example 1.6  

The following function $p$ is a permutation of the set $\{1,2,3,4,5,6,7,8\}$ :  

$$
\begin{array}{c c c}{{p(1)=2\quad}}&{{p(2){=}4\quad}}&{{p(3)=6\quad}}&{{p(4){=}8}}\\ {{p(5)=7\quad}}&{{p(6){=}5\quad}}&{{p(7)=3\quad}}&{{p(8){=}1}}\end{array}
$$  

A more compact way of writing down a permutation is to write it as an array of numbers, with 1, through $n$ on the top row, and the respective image of each in the second row, like so:  

$$
p={\binom{1}{p(1)}}\quad p(2)\quad{\begin{array}{l l l l}{3}&{\dots}&{n}\\ {p(3)}&{\dots}&{p(n)}\end{array}}\quad
$$  

# Example 1.7  

The permutation $p$ of the previous example can be written as follows:  

$$
p={\binom{1}{2}}{\begin{array}{l l l l l l l l}{2}&{2}&{3}&{4}&{5}&{6}&{7}&{8}\\ {2}&{4}&{6}&{8}&{7}&{5}&{3}&{1}\end{array}}
$$  

We denote the set of all permutations of $\{1,2,3,\ldots,n\}$ by $S_{n}$ .  

# Example 1.8  

The set $S_{3}$ is  

$$
\left\{\left({\begin{array}{c c c}{1}&{2}&{3}\\ {1}&{2}&{3}\end{array}}\right),\left({\begin{array}{c c c}{1}&{2}&{3}\\ {3}&{1}&{2}\end{array}}\right),\left({\begin{array}{c c c}{1}&{2}&{3}\\ {2}&{3}&{1}\end{array}}\right),\left({\begin{array}{c c c}{1}&{2}&{3}\\ {1}&{3}&{2}\end{array}}\right),\left({\begin{array}{c c c}{1}&{2}&{3}\\ {2}&{1}&{3}\end{array}}\right),\left({\begin{array}{c c c}{1}&{2}&{3}\\ {3}&{2}&{1}\end{array}}\right)\right\}
$$  

Note that, as in the above example, the identity permutation $p(k)=k$ is always a permutation.  

Since every permutation is a one-to-one and onto function, there is an inverse function $p^{-1}$ associated with every permutation $p$ .  

We can “multiply” two permutations by applying the first permutation, and then using the second permutation to permute the result. If $p$ and $q$ are permutations of the same set, $p q(k)$ is the what you get from applying $q$ to $p(k)$ , ie. $p q(k)=q(p(k))$ , so $p q=q\circ p$ (note the reversal of terms in the product versus the composition).  

# Proposition 1.3  

Let $X$ be any set, and $p$ and $q$ be permutations of $X$ , then  

(i) $p^{-1}$ is a permutation of $X$ ,   
(ii) pq is a permutation of $X$ ,   
(iii) the product satisfies an associative law: $(p q)r=p(q r)$ .  

Proof:  

These follow immediately from Proposition 1.2: the inverse function of a bijection is a bijection, proving (i); the composition of bijective functions is a bijective function, proving (ii); and composition of functions is associative, so  

$$
(p q)r=r\circ(q\circ p)=(r\circ q)\circ p=p(q r),
$$  

proving (iii).  

# Example 1.9  

Let  

$$
p={\binom{1}{3}}\quad2\quad3{\Big)}\qquad{\mathrm{and}}\qquadq={\binom{1}{3}}\quad1\quad2{\Big)}.
$$  

We can find fairly easily: for example if $k=1$ , we know that $p(1)=3$ , and $p q$ $q(3)=2$ , so $p q(1)=2$ . Repeating for $k=2$ and 3, we get So we have  

$$
p q={\binom{1}{2}}\quad{\mathrm{1}}\quad{\mathrm{3}}\quad
$$  

# Example 1.10  

We listed all the elements of $S_{3}$ in Example 1.8. To simplify notation we will give each of these a symbol to identify it:  

$$
{\begin{array}{r l r l r l}&{p_{0}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {1}&{2}&{3}\end{array}\right)}}&&{p_{1}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {3}&{1}&{2}\end{array}\right)}}&&{p_{2}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {2}&{3}&{1}\end{array}\right)}}\\ &{p_{3}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {1}&{3}&{2}\end{array}\right)}}&&{p_{4}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {2}&{1}&{3}\end{array}\right)}}&&{p_{5}={\left(\begin{array}{l l l}{1}&{2}&{3}\\ {3}&{2}&{1}\end{array}\right)}}\end{array}}
$$  

It is easy to verify that $p_{0}^{-1}=p_{0}$ , $p_{1}^{-1}=p_{2}$ , $p_{2}^{-1}=p_{1}$ , $p_{3}^{-1}=p_{3}$ , $p_{4}^{-1}=p_{4}$ , and $p_{5}^{-1}=p_{5}$ .  

Just as with symmetries, we can write out a Cayley table for the products of these permutations:  

<html><body><table><tr><td></td><td>Po</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>P5</td></tr><tr><td>Po</td><td>Po</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>P5</td></tr><tr><td>P1</td><td>P1</td><td>P2</td><td>Po</td><td>P4</td><td>P5</td><td>P3</td></tr><tr><td>P2</td><td>P2</td><td>Po</td><td>P1</td><td>P5</td><td>P3</td><td>P4</td></tr><tr><td>P3</td><td>P3</td><td>P5</td><td>P4</td><td>po</td><td>P2</td><td>P1</td></tr><tr><td>P4</td><td>P4</td><td>P3</td><td>P5</td><td>P1</td><td>Po</td><td>P2</td></tr><tr><td>P5</td><td>p5</td><td>P4</td><td>P3</td><td>p2</td><td>P1</td><td>po</td></tr></table></body></html>  

May 3, 2004  

This product is not commutative.  

It’s probably not immediately obvious, but if you look closely you will see that the pattern of this Cayley table is exactly the same as the pattern of the Cayley table of Example 1.2, with the correspondences $p_{0}\leftrightarrow I$ , $p_{1}\leftrightarrow R_{1}$ , $p_{2}\leftrightarrow R_{2}$ , $p_{3}\leftrightarrow H_{0}$ , $p_{4}\leftrightarrow H_{1}$ , $p_{5}\leftrightarrow H_{2}$ . Indeed, the inverses of each element have the same pattern under these same correspondences.  

In other words, if we look at these two examples abstractly, we seem to be getting the same underlying mathematical object.  

This correspondence can be made even more concrete in the following way: if we label the vertices of the equilateral triangle of Example 1.2 with the numbers $^{1}$ , 2 and 3, starting at $(0,0)$ and working clockwise, we find that the symmetries of the triangle permute the vertices in exactly the same way that the corresponding permutations permute the corresponding numbers. $\diamondsuit$  

# 1.4.1 Cycles  

Even with the current notation, expressing and working with permutations can be cumbersome. There is another, alternative, notation which can speed up the process of working with permutations. This notation works by looking at the cycles withing a permutation. If $p$ is a permutation of the set $X$ , the cycle of an element $k$ of $X$ in $p$ is the sequence of elements $(k,p(k),p^{2}(k),\dots,p^{m}(k))$ (where $p^{l}$ is the product of $p$ with itself $\it{l}$ times) such that $m$ is the smallest number such that, $p^{m+1}(k)=k$ .  

Note that the order of the elements in a cycle is important, but not where we start in the cycle. For example, we regard $(k,p(k),p^{2}(k),\dots,p^{m}(k))$ , $(p(k)$ , $p^{2}(k),\ldots,p^{m}(k),k)$ , $(p^{2}(k),\ldots,p^{m}(k),k,p(k))$ , etc. as representing the same cycle. If $X$ is the set $\{1,2,\ldots n\}$ , it is usual to write a cycle starting with the smallest number in the cycle.  

A cycle with $m$ elements is called an $m$ -cycle. A 2-cycle is sometimes called a transposition, since it transposes two elements.  

# Example 1.11  

In the following permutation  

$$
\left({\begin{array}{c c c c c c c c c}{1}&{2}&{3}&{4}&{5}&{6}&{7}&{8}\\ {2}&{4}&{6}&{8}&{7}&{5}&{3}&{1}\end{array}}\right)
$$  

we have $1\rightarrow2$ , $2\rightarrow4$ , $4\longrightarrow8$ and $8\rightarrow1$ , so $(1,2,4,8)$ is a cycle. We could also write this cycle as $(2,4,8,1)$ , $(4,8,1,2)$ , or $(8,1,2,4)$ .  

The smallest element not in this cycle is 3, and we have $3\rightarrow6$ , $6\rightarrow5$ , $5\to7$ and $7\rightarrow3$ , so $(3,6,5,7)$ is another cycle.  

Since every element is in one of these two cycles, these are the only cycles in this permutation. $\diamondsuit$  

If we find all of the cycles of a permutation, we can represent the permutation as a whole as a product of its cycles. But to do that we need to understand how to multiply cycles.  

To work out how a product of cycles permutes a particular element $k$ , all you need do is work from left to right until you find the element in a cycle, and then find the element which follows it in that cycle. You continue from left to right starting with the the next cycle looking for an occurrence of the new element. If there is, then you find the element which follows it in the cycle. Continue on in this fashion until you run out of cycles. The final value of the element is the image of $k$ under the product of cycles.  

# Example 1.12  

Consider the permutation $p=(1,3,5)(2)(4,6)$ of the set $\{1,2,3,4,5,6\}$ . We can calculate $p(1)$ be looking at the first cycle, where we see that the element after 1 in that cycle is 3, and we also note that 3 does not occur in any cycle after the first, so $p(1)=3$ . Similarly, we have $p(2)=2$ , $p(3)=5$ , $p(4)=6$ , $p(5)=1$ and $p(6)=4$ . This permutation could also be written as  

$$
\left({\begin{array}{c c c c c c c}{1}&{2}&{3}&{4}&{5}&{6}\\ {3}&{2}&{5}&{6}&{1}&{4}\end{array}}\right).
$$  

Notice that there would be no difference in the above example if the cycle (2) was omitted. It is common practise to leave such single-element cycles out, particularly when the set which is being permuted is clear.  

# Example 1.13  

Consider the product of cycles $p=(1,3,5)(2,3)(4,6,5)$ in the set $\{1,2,3,4,5$ , $6\}$ . We can calculate $p(1)$ be looking at the first cycle, where we see that the element after 1 in that cycle is 3; however 3 occurs in the second cycle, and the element after it in the cycle is 2; and 2 does not occur in the remaining cycle, so $p(1)=2$ . Similarly, we have $3\rightarrow5$ in the first cycle, and $5\to4$ in the last cycle, so $p(3)=4$ . Calculating everything out, we have $p(2)=3$ , $p(4)=6$ , $p(5)=1$ and $p(6)=5$ . This permutation could also be written as  

$$
\left({\begin{array}{c c c c c c c}{1}&{2}&{3}&{4}&{5}&{6}\\ {2}&{3}&{4}&{6}&{1}&{5}\end{array}}\right),
$$  

or more simply in cycle notation as $(1,2,3,4,6,5)$ .  

Any permutation can be written as a product of the cycles it contains.  

# Theorem 1.4  

Every permutation of $S_{n}$ can be written as a product of disjoint cycles. (Two cycles are disjoint if they have not elements in common.)  

Proof:  

Let $p$ be a permutation of $S_{n}$ . We let $c_{1}$ be the cycle which includes 1,  

$$
c_{1}=\{1,p(1),p^{2}(1),\ldots,p^{m_{1}}(1)\},
$$  

May 3, 2004  

and we let $p_{1}$ be the permutation defined by  

$$
p_{1}(k)={\left\{\begin{array}{l l}{k}&{{\mathrm{if~}}k\in c_{1},}\\ {p(k)}&{{\mathrm{otherwise}}.}\end{array}\right.}
$$  

Then it is clear that $p=c_{1}p_{1}$ .  

Now if we have written $p=c_{1}\ldots c_{l}p_{l}$ , where $c_{1},\ldots$ , $c_{l}$ are disjoint cycles, and $p_{l}$ is a permutation which satisfies has $p_{l}(k)=k$ whenever $k$ is in one of the cycles, then one of two things must be true: either every element of $\{1,2,\ldots,n\}$ is an element of one of the cycles, or there is some smallest element $k_{l}$ which is not in any of the cycles.  

In the first case, we have that $p_{l}$ must be the identity permutation, so $p=$ $c_{1}\ldots c_{l}$ , and we are done.  

In the second case, we let $c_{l+1}$ be the cycle including $k_{l}$ ,  

$$
c_{l+1}=\{k_{l},p(k_{l}),p^{2}(k_{l}),\ldots,p^{m_{l}}(k_{l})\},
$$  

and let $p_{l+1}$ be the permutation defined by  

$$
p_{l+1}(k)={\left\{\begin{array}{l l}{k}&{{\mathrm{if~}}k{\mathrm{~is~an~element~of~any~cycle~}}c_{1},c_{2},\ldots,c_{l+1}}\\ {p(k)}&{{\mathrm{otherwise.}}}\end{array}\right.}
$$  

Then p = c1 . . . cl+1pl+1.  

Since $\{1,2,3,\ldots,n\}$ is a finite set, an induction argument using this construction proves the result.  

# Example 1.14  

The permutation  

$$
\left({\begin{array}{c c c c c c c c c}{1}&{2}&{3}&{4}&{5}&{6}&{7}&{8}\\ {2}&{4}&{6}&{8}&{7}&{5}&{3}&{1}\end{array}}\right)
$$  

can be written as $(1,2,4,8)(3,6,5,7)$ or $(3,6,5,7)(1,2,4,8)$ , or in many other ways. The first is the standard form. 3  

# Example 1.15  

The elements of $S_{3}$ can be represented in cycle form as follows:  

$$
{\begin{array}{r l}{{\binom{1}{1}}}&{2}&{3}\\ {1}&{2}&{3}\end{array}}=(1)(2)(3)\qquad{\begin{array}{r l}{{\binom{1}{3}}}&{2}&{3}\\ {3}&{1}&{2}\end{array}}=(1,3,2)\qquad{\begin{array}{r l}{{\binom{1}{2}}}&{2}&{3}\\ {2}&{3}&{1}\end{array}}=(1,2,3)}\\ {{\begin{array}{r l}{{\binom{1}{1}}}&{2}&{3}\\ {1}&{3}&{2}\end{array}}=(1)(2,3)\qquad{\begin{array}{r l}{{\binom{1}{2}}}&{2}&{3}\\ {2}&{1}&{3}\end{array}}=(1,2)(3)\qquad{\begin{array}{r l}{{\binom{1}{3}}}&{2}&{3}\\ {{\binom{1}{3}}}&{2}&{1}\end{array}}=(1,3)(2)
$$  

It is often convenient to simply always work with the cycle form of a permutation. We can calculate the product of two permutations in cycle notation  

by writing all long product of cycles, and then reducing to the standard form of the cycles.  

# Example 1.16  

Let $p=(1,4,6)(3,5)$ and $q=(1,2,4)(3,6,5)$ . Then $p q$ is given by the product of cycles $(1,4,6)(3,5)(1,2,4)(3,6,5)$ .  

Starting with 1, we see that $1\rightarrow4\rightarrow1$ , so the first cycle of the standard form is just (1).  

Looking at 2 next, we have $2\rightarrow4$ , so the next cycle starts $(2,4,\ldots)$ . Looking at 4, we get $4\rightarrow6\rightarrow5$ , so 5 is the next entry, and the cycle is starting $(2,4,5,\ldots)$ . Now starting with 5 we get $5\rightarrow3\rightarrow6$ , so 6 is next in the cycle. Continuing in this manner we get $6\rightarrow1\rightarrow2$ , and 2 is the start of the cycle, so the finished cycle is $(2,4,5,6)$ .  

The only remaining number is 3, so (3) must be the last cycle.  

Hence $p q\ =\ (1)(2,4,5,6)(3)$ , which we will usually just write as $p q\ =$ $(2,4,5,6)$ .  

Similarly we have that $q p=(1,2,4)(3,6,5)(1,4,6)(3,5)$ , and we have $1\rightarrow2$ , $2\rightarrow4\rightarrow6$ , $6\rightarrow5\rightarrow3$ and $3\rightarrow6\rightarrow1$ , so the first cycle is $(1,2,6,3)$ . Similarly, we have $4\rightarrow1\rightarrow4$ , so (4) is a cycle. Finally 5 is the only element remaining, so (5) is a cycle. Hence $q p=(1,2,6,3)(4)(5)=(1,2,6,3)$ . $\diamondsuit$  

# 1.4.2 Parity  

Informally, if we compare the permuations  

$$
{\left(\begin{array}{l l l}{1}&{2}&{3}\\ {3}&{1}&{2}\end{array}\right)}\qquad{\mathrm{and}}\qquad{\left(\begin{array}{l l l}{1}&{2}&{3}\\ {3}&{2}&{1}\end{array}\right)},
$$  

we observe that the first “rotates” the elements to the right, while the second “reflects” the elements. Indeed, if we consider the correspondence between these permutations and the symmetries of a triangle discussed in Example 1.10, we see that the first corresponds to a rotation, and the second to a reflection. In this section, we will generalize this idea to arbitrary permutations.  

The starting point of this discussion is a comparison between the following two products: if $p\in S_{n}$ we define  

$$
{\cal D}_{n}=\prod_{1\leq i<j\leq n}(j-i)
$$  

and  

$$
D(p)=\prod_{1\leq i<j\leq n}(p(j)-p(i)).
$$  

Given any pair of distinct elements $k,l\in\{1,2,\ldots,n\}$ , both of these products contain exactly one factor which is a difference of $k$ and $\it{l}$ . This is easy to see in the product $D_{n}$ , but a little thought will convince you that it is also the case for $D(p)$ . The difference between the two products is that in $D(p)$ it may not necessarily be the larger term minus the smaller term. Hence $D_{n}$ and $D(p)$ have the same magnitude, but may differ in sign.  

# Definition 1.2  

Let $p\in S_{n}$ . The parity of $p$ is  

$$
\mathrm{parity}(p)=\frac{D(p)}{D_{n}}.
$$  

Clearly the parity of $p$ is $^{1}$ if $D(p)>0$ and $-1$ if $D(p)<0$ .  

# Example 1.17  

Consider the permutations  

$$
p={\binom{1}{3}}\quad{\begin{array}{l l}{2}&{3}\\ {1}&{2}\end{array}}\quad\quad{\mathrm{and}}\quad\quad q={\binom{1}{3}}\quad{\begin{array}{l l}{2}&{3}\\ {2}&{1}\end{array}}.
$$  

In both cases  

$$
D_{3}=(3-1)(3-2)(2-1)=2\times1\times1=2.
$$  

Now  

$$
{\begin{array}{r l}&{D(p)=(p(3)-p(1))(p(3)-p(2))(p(2)-p(1))=(2-3)(2-1)(1-3)}\\ &{\qquad=-1\times1\times-2=2,}\end{array}}
$$  

so  

$$
\mathrm{parity}(p)=2/2=1.
$$  

On the other hand,  

$$
\begin{array}{c}{{D(q)=(q(3)-q(1))(q(3)-q(2))(q(2)-q(1))=(1-3)(1-2)(2-3)}}\\ {{{}}}\\ {{=-2\times-1\times-1=-2,}}\end{array}
$$  

so  

$$
\mathrm{parity}(q)=-2/2=-1.
$$  

Calculating the parity in the last example was fairly straightforward, but calculating the parity of a general permutation can be quite time consuming: a simple counting argument tells us that if $p\in S_{n}$ we have $n(n-1)/2$ terms in the product $D(p)$ . We need a better way to calculate the parity.  

It turns out that there is nothing particularly special about $D_{n}$ in the definition of parity. Let $(x_{1},x_{2},\ldots,x_{n})$ be a sequence of distinct numbers, and $p$ a permutation of $\{1,2,\ldots n\}$ . We define  

$$
p(x_{1},x_{2},\ldots,x_{n})=(x_{p(1)},x_{p(2)},\ldots,x_{p(n)}),
$$  

May 3, 2004  

and  

$$
D(x_{1},x_{2},\ldots,x_{n})=\prod_{1\leq i<j\leq n}(x_{j}-x_{i}).
$$  

The following technical lemma shows that we can use these instead to find the parity of $p$ .  

# Lemma 1.5  

If $p$ is a permutation in $S_{n}$ , then  

$$
{\mathrm{parity}}(p)={\frac{D(x_{p(1)},x_{p(2)},\ldots,x_{p(n)})}{D(x_{1},x_{2},\ldots,x_{n})}}.
$$  

We say that $p$ is an even permutation if parity $(p)=1$ , and $p$ is an odd permutation if parity $(p)=-1$ .  

Proof:  

See Section 1.6.2.  

Note that $D_{n}=D(1,2,\ldots,n)$ , and $D(p)=D(p(1),p(2),\ldots,p(n))$ . With this lemma in hand, we can easily prove the following important res  

# Theorem 1.6  

Let $p$ and $q\in S_{n}$ . Then  

$$
\mathrm{parity}(p q)=\mathrm{parity}(p)\mathrm{parity}(q).
$$  

Proof:  

The key observation here is that if we have permutations $p$ and $q$ , then  

$$
{\mathrm{parity}}(p)={\frac{D(p q)}{D(q)}}.
$$  

Letting $a_{k}=q(k)$ , so that  

$$
D(p q)=D(q(p(1)),q(p(2)),\ldots,q(p(n)))=D(a_{p(1)},\ldots,a_{p(n)}),
$$  

and  

$$
D(q)=D(a_{1},\ldots,a_{n}),
$$  

the lemma tells us that  

$$
{\mathrm{parity}}(p)={\frac{D(a_{p(1)},\ldots,a_{p(n)})}{D(a_{1},\ldots,a_{n})}}={\frac{D(p q)}{D(q)}}.
$$  

It is immediate form this that  

$$
\operatorname{parity}(p)\times\operatorname{parity}(q)={\frac{D(p q)}{D(q)}}\times{\frac{D(q)}{D_{n}}}={\frac{D(p q)}{D_{n}}}=\operatorname{parity}(p q).
$$  

Thinking in terms of cycles also helps us to calculate the parity of a permutation, as this result shows:  

# Theorem 1.7  

Let $c=(k_{1},k_{2},\ldots,k_{m})$ be a cycle. Then  

$$
\mathrm{parity}(c)=\left\{\begin{array}{l l}{{1}}&{{i f m i s o d d}}\\ {{-1}}&{{i f m i s e v e n.}}\end{array}\right.
$$  

Proof:  

First we observe that if $p=(1,2)$ , then all the factors in $D(p)$ are positive, except for $p(2)-p(1)=1-2=-1$ . Hence $D(p)$ is negative, so parity $(p)=-1$ . Now if $i,j\ >\ 2$ , and $i\neq j$ , then simple checking shows that $(i,j)\ =$ $(1,i)(2,j)(1,2)(1,i)(2,j)$ , and, given this fact, the previous theorem tells us  

$$
\begin{array}{l}{\mathrm{parity}((i,j))=\mathrm{parity}((1,i)(2,j))\times\mathrm{parity}(1,2)\times\mathrm{parity}((1,i)(2,j))}\\ {=-\mathrm{parity}((1,i)(2,j))^{2}}\\ {=-1}\end{array}
$$  

Finally, we observe that  

$$
c=(k_{1},k_{2},\ldots,k_{m})=(k_{1},k_{m})(k_{2},k_{m})\cdot\cdot\cdot(k_{m-1},k_{m}),
$$  

and so  

$$
{\begin{array}{r l}&{{\mathrm{parity}}(c)={\mathrm{parity}}((k_{1},k_{m}))\times{\mathrm{parity}}((k_{2},k_{m}))\times\cdots\times{\mathrm{parity}}((k_{m-1},k_{m}))}\\ &{\qquad=(-1)^{m-1}}\\ &{\qquad={\left\{\begin{array}{l l}{1}&{{\mathrm{if~}}m{\mathrm{~is~odd}}}\\ {-1}&{{\mathrm{if~}}m{\mathrm{~is~even}}.}\end{array}\right.}}\end{array}}
$$  

# Corollary 1.8  

A permutation $p$ is even iff when it is expressed as a product of cycles there are an even number of commas in the expression.  

Another interesting fact that can be squeezed out of the previous theorem is the following:  

# Proposition 1.9  

Any permutation $p$ can be written as a product of 2-cycles, and the number of 2-cycles is even iff $p$ is even.  

Proof:  

The first fact follows from the fact that every permutation can be written as a product of cycles, and equation (1.2) shows that every cycle is a product of 2-cycles.  

The second follows from the previous corollary, coupled with the fact that every 2-cycle has a single comma.  

# Example 1.18  

Let $p_{0}$ , $p_{1},\ldots,p_{5}$ be as in Example 1.10. Then parit $\mathrm{y}(p_{0})=\mathrm{parity}(p_{1})=$ $\mathrm{parity}(p_{2})=1$ , and $\mathrm{parity}(p_{3})=\mathrm{parity}(p_{4})=\mathrm{parity}(p_{5})=-1$ .  

Observe that the “reflections” have parity -1, while the “rotations” have parity 1.  

# Example 1.19  

The permutation  

$$
p={\left(\begin{array}{l l l l l l l l l l l l}{1}&{2}&{3}&{4}&{5}&{6}&{7}&{8}&{9}&{10}\\ {3}&{5}&{2}&{7}&{8}&{6}&{1}&{4}&{10}&{9}\end{array}\right)},
$$  

can be written as $p=(1,3,2,5,8,4,7)(9,10)$ , and since it has 7 commas in the expression, it has parity $-1$ . 3  

# 1.4.3 Permutation Matrices  

Another way of looking at permutations is very closely related to Equation 1.1. Since $x=(x_{1},x_{2},\ldots,x_{n})$ is a vector in $\mathbb{R}^{n}$ , the function it implicitly defines, $T_{p}:\mathbb{R}^{n}\longrightarrow\mathbb{R}^{n}$ , where  

$$
T_{p}x=(x_{p(1)},x_{p(2)},\ldots,x_{p(n)})
$$  

is a linear transformation:  

$$
\begin{array}{c}{{T_{p}(x+y)=(x_{p(1)}+y_{p(1)},x_{p(2)}+y_{p(2)},\ldots,x_{p(n)}+y_{p(n)})}}\\ {{{}}}\\ {{{}=(x_{p(1)},x_{p(2)},\ldots,x_{p(n)})+(y_{p(1)},y_{p(2)},\ldots,y_{p(n)})=T_{p}x+T_{p}y}}\end{array}
$$  

and  

$$
\begin{array}{r l}&{T_{p}(\lambda x)=(\lambda x_{p(1)},\lambda x_{p(2)},\ldots,\lambda x_{p(n)})}\\ &{\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=\lambda(x_{p(1)},x_{p(2)},\ldots,x_{p(n)})=\lambda T_{p}x.}\end{array}
$$  

Let $e_{k}$ be the $k$ th standard orthonormal basis vector in $\mathbb{R}^{n}$ , ie. $e_{k}$ is the vector with $0$ in every entry except the $k$ th entry, which is 1. By looking at the image of each standard basis vector $e_{k}$ under the transformation $T_{p}$ , we can find a corresponding $n\times n$ matrix which we will also call $T_{p}$ . We note that $T_{p}e_{k}$ has zeroes in every entry except the $p^{-1}(k)\mathrm{th}$ entry, which is $1$ . Hence $T_{p}e_{k}=e_{p^{-1}(k)}$ , so $T_{p}$ always takes basis vectors to basis vectors.  

# Example 1.20  

If $p=(1,2,3)\in S_{3}$ , then  

$$
T_{p}=\left[\begin{array}{l l l}{0}&{1}&{0}\\ {0}&{0}&{1}\\ {1}&{0}&{0}\end{array}\right]
$$  

# Proposition 1.10  

Let $p\in S_{n}$ be a permutation. Then  

(i) $T_{p}$ is an orthogonal matrix   
(ii) $T_{p}$ is the matrix with 1s in the $p^{-1}(k)t h$ row of the kth column, for $k=1,2,\ldots,n$ , and 0 everywhere else.   
(iii) $T_{p}$ is the matrix with 1s in the $p(k)t h$ column of the kth row, for $k=1,2,\ldots,n$ , and 0 everywhere else.   
(iv) $T_{e}=I_{n}$ .   
(v) $T_{p}T_{q}=T_{p q}$ .   
(vi) $T_{p}^{-1}=T_{p^{-1}}$ .  

Proof:  

(i) is immediate since every column of $T_{p}$ is a standard orthonormal basis vector, and each vector in the basis occurs exactly once.  

(ii) this is immediate from the fact that the $k$ th column is the column vector $e_{p^{-1}\left(k\right)}.$ .  

(iii) using part (ii), we know that is $k=p^{-1}(j)$ , then the entry in the $k$ th row and $j$ th column is 1. But $k=p^{-1}(j)$ if and only if $j=p(k)$ , so the $p(k)$ th column of the $k$ th row is $^{1}$ , and all other entries in the row are 0.  

(iv) the $k$ th row of $T_{e}$ is $e_{k}$ , so $T_{e}$ has $^{1}$ in diagonal entries and 0 everywhere else, so $T_{e}=I$ .  

(v) Looking at the standard basis vectors, we have  

$$
T_{p q}e_{k}=e_{(p q)^{-1}(k)}=e_{p^{-1}(q^{-1}(k))}=T_{p}e_{q^{-1}(k)}=T_{p}T_{q}e_{k}.
$$  

Since any vector $v$ is a linear combination of basis vectors, and the transformations $T_{p}$ , $T_{q}$ and $T_{p q}$ are all linear, we have that $T_{p q}v=T_{p}T_{q}v$ for any vector $v$ , and so $T_{p}T_{q}=T_{p q}$ .  

(vi) Since $T_{p}$ is orthogonal, it is invertible, and $T_{p}^{-1}e_{p^{-1}(k)}\ =\ e_{k}$ . Now $j=p^{-1}(k)$ if and only if $k=p(j)$ , so  

$$
T_{p}^{-1}e_{j}=e_{k}=e_{p(j)}=e_{(p^{-1})^{-1}(j)}=T_{p^{-1}}e_{j}.
$$  

As in part (v), it is sufficient to show that this occurs for every basis vector to be able to conclude that $T_{p}^{-1}=T_{p^{-1}}$ $\bigstar$  

# Example 1.21  

We know that in $S_{3}$ , if $p=(1,2,3)$ and $q=(1,2)$ , then $p q=(2,3)$ . The corresponding permutation matrices are  

$$
T_{p}={\left[\begin{array}{l l l}{0}&{1}&{0}\\ {0}&{0}&{1}\\ {1}&{0}&{0}\end{array}\right]}\qquadT_{q}={\left[\begin{array}{l l l}{0}&{1}&{0}\\ {1}&{0}&{0}\\ {0}&{0}&{1}\end{array}\right]}\qquadT_{p q}={\left[\begin{array}{l l l}{1}&{0}&{0}\\ {0}&{0}&{1}\\ {0}&{1}&{0}\end{array}\right]}
$$  

and matrix multiplication confirms that $T_{p}T_{q}=T_{p q}$ .  

# Exercises  

1.4.1. Let  

$$
p={\left(\begin{array}{l l l l l l}{1}&{2}&{3}&{4}&{5}&{6}\\ {4}&{2}&{5}&{1}&{6}&{3}\end{array}\right)}\qquad{\mathrm{~and~}}\qquadq={\left(\begin{array}{l l l l l l}{1}&{2}&{3}&{4}&{5}&{6}\\ {3}&{4}&{5}&{1}&{2}&{6}\end{array}\right)}.
$$  

Find $p q$ and $q p$ . Write both permutations using cycle notation. Write down the permutation matrices $T_{p}$ and $T_{q}$ . Determine the parity of $p$ and $q$ .  

1.4.2. Let $\ensuremath{p}=(1,5,3,2)(4,6,8)$ and $q=(1,7,4,3)(8,2)(5,6)$ . Find $p q$ and $q p$ . Write both permutations using array notation. Write down the permutation matrices $T_{p}$ and $T_{q}$ . Determine the parity of $p$ and $q$ .  

1.4.3. How many distinct permutations are there of the set $\{1,2,\dots,n\}\colon$ (Hint: they’re called permutations.)  

1.4.4. Let $p\in S_{3}$ . Use the Cayley table for $S_{3}$ to show that $p^{6}$ is always the identity permutation.  

1.4.5. Write down all the elements of $S_{4}$ in array, cycle and matrix form. Calculate the parity of each element. Find the inverse of each element. Choose 5 pairs of non-identity elements, and calculate their product.  

1.4.6. Let $c=(k_{1},k_{2},\ldots,k_{m}) $ be a cycle. What is $c^{-1}$ ? Use your answer to calculate the inverse of the permutation $p=(1,3,4)(2,5)$ .  

1.4.7. Show that $D_{n}=(n-1)!(n-2)!\dots...2!1!$ .  

1.4.8. Show that exactly half the permutations of $S_{n}$ are even, and half are odd.  

1.4.9. ( $^*$ ) Show that $S_{4}$ and the set of symmetries of a regular tetrahedron (see Exercise 1.1.5) correspond in the same way as $S_{3}$ and the set of symmetries of an equilateral triangle.  

Hint: you could do this by calculating all 576 entries in the Cayley table of each, and comparing the two; however it is more practical to find some way to classify the elements of each in a way which makes the correspondence clear.  

1.4.10. $(^{*}$ ) Let $\Omega\subseteq\mathbb{R}^{3}$ be the equilateral triangle with vertices $(1,0,0)$ , $(0,1,0)$ and $(0,0,1)$ . Show that every symmetry $S\in\mathrm{Sym}(\Omega)$ is a linear transformation, and that there is a permutation $p_{S}\in S_{3}$ such that $S=T_{p s}$ . Show that the correspondence $S\mapsto p_{S}$ preserves multiplication and inverses, ie. $T_{p_{S}p R}=S R$ , $T_{p_{S}^{-1}}=S^{-1}$ .  

1.4.11. $^{\prime*}$ ) Let $|A|$ denote the determinant of the matrix $A$ . Prove that $|T_{p}|=$ $\mathrm{parity}(p)$ .  

1.4.12. $(^{**})$ Write a computer program that calculates and prints out the Cayley table for $S_{4}$ . Generalize it to print out the Cayley table for $S_{n}$ for any $n$ .  

# 1.5 Modulo Arithmetic  

We say that two numbers $x$ and $y$ are equal (modulo $m$ ) if $x$ and $y$ differ by a multiple of $m$ , and we write  

$$
x\equiv y{\pmod{m}}
$$  

to denote this situation. Another equivalent (and useful) way to think of this situation is that $x$ and $y$ have the same remainder when you divide by $m$ . Since any number greater than $m$ is equal (modulo $m$ ) to a number less than $m$ , it is customary when working modulo $m$ to reduce your answer to a number in the range $[0,m)$ .  

For example  

$$
-1\equiv7\equiv1023{\pmod{8}},
$$  

and we would usually write any of these three numbers as 7 mod 8 if it were the solution to a problem.  

When we are working modulo $m$ , we can perform the operations of addition, multiplication and subtraction as normal, but we reduce our answers to the range $[0,m)$ . Indeed, in complicated expressions, one can reduce at intermediate steps to simplify calculations:  

$$
7\times6+4\times3\equiv42+12\equiv54\equiv6{\pmod{8}}
$$  

could be instead calculated as  

$$
\times6+4\times3\equiv42+12\equiv2+4\equiv6{\pmod{8}}.
$$  

Division is a trickier topic, but since we are usually performing modulo arithmetic with integers the na¨ıve way of defining modulo division does not make sense in most cases. Nevertheless, we will see later on that in some cases division does make sense.  

We can write out addition and multiplication tables for operations modulo some base, and we call these Cayley tables, just as before.  

# Example 1.22  

The addition and multiplication tables, modulo 6 are as follows:   


<html><body><table><tr><td>+</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>×</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>0</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>0</td><td>1</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>2</td><td>2</td><td>3</td><td>4</td><td>5</td><td>0</td><td>1</td><td>2</td><td>0</td><td>2</td><td>4</td><td>0</td><td>2</td><td>4</td></tr><tr><td>3</td><td>3</td><td>4</td><td>5</td><td>0</td><td>1</td><td>2</td><td>3</td><td>0</td><td>3</td><td>0</td><td>3</td><td>0</td><td>3</td></tr><tr><td>4</td><td>4</td><td>5</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>0</td><td>4</td><td>2</td><td>0</td><td>4</td><td>2</td></tr><tr><td>5</td><td>5</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>0</td><td>5</td><td>4</td><td>2</td><td>3</td><td>1</td></tr></table></body></html>  

# Exercises  

1.5.1. Write down the addition and multiplication tables modulo 5 and modulo 8.  

1.5.2. Recall that two natural numbers $p$ and $q$ are coprime if their highest common factor is 1. Show that if $k$ and $m$ are coprime, then $x k\equiv0$ (mod $m$ ) if and only if $x$ is an integer multiple of $m$ .  

# 1.6 Addendum: Technical Details  

We now provide the technical proofs that were omitted from earlier discussions in this chapter.  

# 1.6.1 Symmetry  

In Proposition 1.1 we make use of the following fact.  

Lemma 1.11   
If $T$ is a function from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ that preserves distance, then $T$ is one-to-one and onto.  

Proof:  

If $T(x_{1})=T(x_{2})$ then $d(T(x_{1}),T(x_{2}))=0$ so the fact that $T$ preserves distances means that $d(x_{1},x_{2})=0$ . But this implies that $x_{1}=x_{2}$ , so $T$ is one-to-one.  

If $x$ , $y$ and $z\in\mathbb{R}^{n}$ are the vertices of a triangle, then $T(x)$ , $T(y)$ and $T(z)$ are vertices of a triangle as well, and since $d(T(x),T(y))=d(x,y)$ , $d(T(x),T(z))=$ $d(x,z)$ , and $d(T(y),T(z))=d(y,z)$ , the triangles are congruent. From this observation, it follows that any parallelogram $x,y,$ $z$ , $w\in\mathbb{R}^{n}$ is congruent to the corresponding parallelogram $T(x),T(y),T(z),T(w)$ .  

If $T(0)~=~0$ then this means that in particular, the parallelogram $0,\ x$ , $x+y$ , $y$ is congruent to the parallelogram $0$ , $T(x)$ , $T(x+y)$ , $T(y)$ . Therefore, comparing opposite sides, $T(y)=T(x+y)-T(x)$ , or $T(x+y)=T(x)+T(y)$ . Also, the “triangle” with vertices $0$ , $x$ , $\lambda x$ is congruent to the triangle with vertices $0$ , $T(x)$ , $T(\lambda x)$ , but since the vertices of the first triangle are collinear, so must the vertices of the second, so $T(\lambda x)$ is a scalar multiple of $T(x)$ , and $d(T(\lambda x),0)=|\lambda|d(x,0)$ , so $T(\lambda x)=\pm\lambda x$ . However, if $T(\lambda x)=-\lambda x$ , then looking at the corresponding sides $x$ to $\lambda x$ and $T(x)$ to $T(\lambda x)$ , we would have $d(T(\lambda x),T(x))=|\lambda+1|d(x,0)$ , rather than $|\lambda-1|d(x,0)$ .  

Hence if $T(0)=0$ , $T$ is a linear transformation, and a linear transformation from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ which preserves distance is orthogonal and hence onto.  

If $T(0)=c$ , then the function $S(x)=T(x)-c$ preserves distances, and $S(0)=0$ , so $S$ is orthogonal and hence onto. But then $T(x)=S(x)+c$ , so given any $y$ , there is some $x$ such that $S(x)=y-c$ , and so $T(x)=S(x)+c=$ $(y-c)+c=y$ . Hence $T$ is onto.  

# 1.6.2 Parity  

Proof (Lemma 1.5):  

Given any number $k\in\{1,2,\ldots,n\}$ , let $a_{k}=p^{-1}(k)$ . Then given any $k>l$ , we have that $k=p(a_{k})$ and $l=p(a_{l})$ .  

If $a_{k}>a_{l}$ , in which case the corresponding terms in each of the sums $D_{n}$ , $D(p)$ , $D(x_{1},\ldots,x_{n})$ and $D(x_{p(1)},\ldots,x_{p(n)})$ are, respectively, $k-l$ , $p(a_{k})-$ $p(a_{l})$ , $x_{k}\mathrm{~-~}x_{l}$ , and $x_{p(a_{k})}-x_{p(a_{l})}$ , with the first two being equal and the second two being equal, and so these terms in the quotients $D(p)/D_{n}$ and $D(x_{p(1)},\ldots,x_{p(n)})/D(x_{1},\ldots,x_{n})$ , respectively, cancel each other out.  

On the other hand, if $a_{k}\ <\ a_{l}$ , the corresponding terms in each of the sums $D_{n}$ , $D(p)$ , $D(x_{1},\ldots,x_{n})$ and $D(x_{p(1)},\ldots,x_{p(n)})$ are, respectively, $k-l$ , $p(a_{l})-p(a_{k})$ , $x_{k}-x_{l}$ , and $x_{p(a_{l})}-x_{p(a_{k})}$ , with the first two being negatives and the second two being negatives, and so these terms in the quotients $D(p)/D_{n}$ and $D(x_{p(1)},\ldots,x_{p(n)})/D(x_{1},\ldots,x_{n})$ , respectively, give a factor of $-1$ .  

Hence the number of terms giving each of the factors $^{1}$ and $-1$ in each quotient are equal, so  

$$
{\mathrm{parity}}(p)={\frac{D(p)}{D_{n}}}={\frac{D(x_{p(1)},x_{p(2)},\ldots,x_{p(n)})}{D(x_{1},x_{2},\ldots,x_{n})}}.
$$  

# Assignment 1  

The following exercises are due Friday, Februrary 13.  

1.1 Exercises 1, 4.   
1.2 Exercise 2.   
1.4 Exercises 1, 2, 3, 6.   
1.5 Exercise 1.  

# Chapter 2  

# Groups  

Algebra concerns the abstraction of simple arithmetic operations to situations where the quantities involved are unknown. In this endeavour, we discover that there are certain rules which always apply, such as the commutative and associative laws of addition and multiplication, and that these laws allow us to manipulate and simplify algebraic expressions. As we learn more mathematics, we see similar rules appear over and over again.  

In abstract algebra, instead of concentrating on specific algebraic settings (such as algebra with numbers, vectors or, now, permutations or symmetries) we instead look at the rules of algebra and ask what we can infer from reasonable collections of such rules. We can then apply the knowledge so gained to a surprisingly wide collection of concrete situations which happen to satisfy such rules.  

You may have already seen such an approach in linear algebra, where one eventually considers abstract vector spaces (as opposed to concrete ones, such as $\mathbb{R}^{n}$ ). One then finds that, for example, that differentiable functions form a vector space, and that differentiation and integration are linear transformations, giving new (and quite important) insight into calculus.  

Our starting point, then will be the group, an object which encapsulates a reasonable set of rules for a single algebraic operation.  

# 2.1 Binary Operations  

A binary operation is a type of function that we shall be using regularly. A binary operation $*$ is simply a function  

$$
\begin{array}{c}{{*:A\times B\to C}}\\ {{(x,y)\mapsto x*y.}}\end{array}
$$  

The distinction lies in that instead of using “function-style” notation $*(x,y)$ , it is traditional to write binary operations “in-line” as $x*y$ . Often $A$ , $B$ and $C$  

are the same set, in which case we say that a binary operation $*:A\times A\to A$ is a binary operation on $A$ .  

A binary operation on $A$ is commutative if  

$$
x*y=y*x
$$  

for all $x$ , $y\in A$ . It is associative if  

$$
(x*y)*z=x*(y*z)=x*y*z.
$$  

for all $x$ , $y$ and $z\in A$ .  

# Lemma 2.1  

$L e t*:A\times A\to A$ be an associative binary operation. Then no matter where you put parentheses in the product $x_{1}*x_{2}*\cdots*x_{n}$ , you get the same result.  

Proof:  

We prove this by induction. Since the operation is associative, it is true for $n\leq3$ automatically.  

Now assume that this is true for all $n<k$ . We need to show that for any choice of $i$ and $j$ with $1\leq i<j<k$ , we have  

$$
(x_{1}*\cdot\cdot\cdot*x_{i})*(x_{i+1}*\cdot\cdot\cdot*x_{k})=(x_{1}*\cdot\cdot\cdot*x_{j})*(x_{j+1}*\cdot\cdot\cdot*x_{k})
$$  

(we do not need to worry about where the parentheses go inside each factor, since they are products of less than $k$ terms). Now we know that  

$$
{\mathfrak{x}}\cdot\cdot\cdot\ast{\boldsymbol{x}}_{k})=(x_{1}\ast\cdot\cdot\cdot\ast x_{i})\ast((x_{i+1}\ast\cdot\cdot\cdot\ast x_{j})\ast(x_{} 
$$  

since the second term is a product of less than $k$ terms. Similarly  

$$
(x_{1}*\cdot\cdot\cdot*x_{j})*(x_{j+1}*\cdot\cdot\cdot*x_{k})=((x_{1}*\cdot\cdot\cdot*x_{i})*(x_{i+1}*\cdot\cdot\cdot*x_{j}))*(x_{j+1}*\cdot\cdot\cdot*x_{k}).
$$  

But $*$ is associative, so  

$$
\begin{array}{r l}&{(x_{1}\ast\dots\ast x_{i})\ast\big((x_{i+1}\ast\dots\ast x_{j})\ast(x_{j+1}\ast\dots\ast x_{k})\big)}\\ &{\qquad=y_{1}\ast\big(y_{2}\ast y_{3}\big)}\\ &{\qquad=(y_{1}\ast y_{2})\ast y_{3}}\\ &{\qquad=\big((x_{1}\ast\dots\ast x_{i})\ast(x_{i+1}\ast\dots\ast x_{j})\big)\ast(x_{j+1}\ast\dots\ast x_{k}),}\end{array}
$$  

and we have proven equality of the two expressions.  

By induction, the result follows for any $k$ .  

An element $e$ of $A$ is an identity for the binary operation if  

$$
e*x=x\qquad{\mathrm{and}}\qquadx*e=x
$$  

for every $x\in A$ . More generally, one can have a left identity $e$ which merely satisfies  

$$
e*x=x
$$  

for every $x$ . A right identity is defined analagously.  

May 3, 2004  

# Lemma 2.2  

$I f*:A\times A\longrightarrow A$ is a binary operation, and $e$ is an identity for $*$ , then it is the only identity element.  

Proof:  

Assume that there is another element $e^{\prime}$ so that $e^{\prime}*x=x*e^{\prime}=x$ . Then in particular, if we let $x=e$ , we have $e^{\prime}*e=e$ . But by assumption, $e$ is an identity, and so $e^{\prime}*e=e^{\prime}$ . Hence $e=e^{\prime}$ . $\mid$  

Notationally, if $^*$ behaves in a “multiplication-like” fashion, or it is clear from context which binary operation we are using, we will often simply write $x y$ for $x*y$ .  

# Example 2.1  

The addition operation is a binary operation in the integers  

$$
\begin{array}{c}{+:\mathbb{Z}\times\mathbb{Z}\to\mathbb{Z}}\\ {(x,y)\mapsto x+y.}\end{array}
$$  

In this case we could write $+(2,3)=2+3=5$ . Addition is, of course, both associative and commutative. 0 is an identity for addition. In fact addition is also an associative and commutative binary operation on any of the standard number systems, and if $0$ is in the number system, then 0 is an identity. $\diamondsuit$  

# Example 2.2  

Multiplication is a binary operation on the set $\mathbb{R}$ , and it is associative and commutative, and 1 is an identity. Again, like addition, multiplication is communtative and associative on any of the standard number systems, and if 1 is in the number system, then 1 is an identity. $\diamondsuit$  

# Example 2.3  

If we consider the set $M_{n}(\mathbb{R})$ of $n\times n$ real-valued matrices, then matrix addition and matrix multiplication are binary operations. Both operations are associative, but only matrix addition is commutative. The zero matrix is an identity for addition, the identity matrix $I_{n}$ (the matrix with 1 down the diagonal and 0 elsewhere) is an identity for matrix multiplication.  

We also have scalar multiplication as a binary operation $\mathbb{R}\times M_{n}(\mathbb{R})\to$ $M_{n}(\mathbb{R})$ . This cannot be commutative or associative, but it does have 1 as a left identity. $\diamondsuit$  

# Example 2.4  

More generally, the inner or dot product on $\mathbb{R}^{n}$ is a binary operation $\mathbf{\partial}\cdot:\mathbb{R}^{n}\times\mathbf{\partial}$ $\mathbb{R}^{n}\to\mathbb{R}$ which is commutative, but cannot be associative (since the codomain is $\mathbb{R}$ , and one cannot take a dot product of an element of $\mathbb{R}$ and an element of $\mathbb{R}^{n}$ ). Similarly, there is no identity element of any sort. $\diamondsuit$  

# Example 2.5  

One can define arbitrary binary products which are of little or no interest. For example, $x*y=e^{x}(x+\sin(y))$ is a binary product. But it is neither associative, commutative, nor has an identity. So clearly simple binary operations are not enough to encapsulate the sorts of rules that we expect algebraic operations to have. $\diamondsuit$  

# Exercises  

2.1.1. Let $*:A\times A\to A$ be a commutative binary operation. If $e$ is a left identity, show that it also a right identity (and hence simply an identity).   
2.1.2. Let $X$ be any set, and let ${\mathcal{P}}(X)$ be the power set of $X$ (ie. the set of all subsets of $X$ ). Show that $\cup:{\mathcal{P}}(X)\times{\mathcal{P}}(X)\rightarrow{\mathcal{P}}(X)$ is an associative, commutative binary operation, and that $\mathrm{\o}$ is an identity for this operation. Similarly, show that $\cap:{\mathcal{P}}(X)\times{\mathcal{P}}(X)\to{\mathcal{P}}(X)$ is an associative, commutative binary operation, and that $X$ is an identity for this operation.   
2.1.3. Let $*:A\times A\to A$ be an associative and commutative binary operation. Show that for any product of $n$ elements $x_{1}$ , $x_{2},\ldots,x_{n}\in A$ , no matter what the order of elements in the product  

$$
x_{1}*x_{2}*\cdots*x_{n}
$$  

the result is the same.  

# 2.2 Groups  

If you look at the discussion of symmetries and permutations, you will note that not only was there a binary operation, but there was an inverse. We should have some model for this additional operation.  

A group, $\mathbf{G}=(G,*,e)$ , consists of a set $G$ , a binary operation $*:G\times G\to$ $G$ , and an element $e\in G$ satisfying the following three conditions:  

(i) $*$ is associative   
(ii) $e$ is an identity for $^*$   
(iii) every element $x\in G$ has an inverse element $x^{-1}\in G$ such that $x*x^{-1}=$ $x^{-1}*x=e$ .  

We call $*$ the group operation.  

Note that there is no requirement that the group operation is commutative. If it does happen to be commutative, then we say that the group is an commutative or Abelian group.  

This means that in general $x*y$ and $y*x$ are distinct elements, but sometimes they are not. If  

$$
x*y=y*x
$$  

for a particular $x$ and $y\in G$ , we say that $x$ and $y$ commute.  

A number of different notations are used when working with groups elements. Most commonly we will omit the group operation entirely and simply write $x y$ for $x*y$ , just as is done for multiplication. In this case we use the following clear notation for repeated applications of the group operations:  

$$
x^{k}=\underbrace{x x x\cdot\cdot\cdot x}_{k{\mathrm{~times}}}
$$  

for any natural number $k$ . To make this notation mesh nicely with the expected behaviour of power laws, we define  

$$
x^{-k}=(x^{-1})^{k}\qquad{\mathrm{and}}\qquadx^{0}=e.
$$  

When then have the standard power laws  

$$
x^{m}x^{k}=x^{m+k}\qquad\mathrm{and}\qquad(x^{m})^{k}=x^{m k},
$$  

for any integers $m$ and $k$ . Its also not hard to see that $(x^{k})^{-1}=x^{-k}$ . However, we have that  

$$
(x y)^{k}\neq x^{k}y^{k}
$$  

in general. In the case that $x$ and $y$ commute, then we do have equality.  

In the case of Abelian groups, we will sometimes instead use an additive notation. We use $^+$ for the group operation, and we customarily write the identity element as $0$ , and the inverse element of $x$ as $-x$ . We then use the notation  

$$
k x=\underbrace{x+x+\cdot\cdot\cdot+x}_{k{\mathrm{~times}}}
$$  

for any natural number $k$ , and  

$$
-k x=k(-x)\qquad{\mathrm{and}}\qquad0x=0.
$$  

We then have the natural rules that  

$$
k x+m x=(k+m)x,\qquadk(m x)=(k m)x\qquad{\mathrm{and}}\qquadk x+k y=k(x+y)
$$  

for any integers $k$ and $m$ .  

If the set $G$ has a finite number of elements, we say that the order of the group is the number of elements of $G$ . If $G$ is an infinite set, we say that the group has infinite order. We denote the order of the group by $|G|$ .  

# Example 2.6 (Addition and Multiplication)  

Since a principle motivation for the definition of groups are standard algebraic operations, it should be no surprise that the following are all Abelian groups:  

• the additive group of real numbers $(\mathbb{R},+,0)$  

• the additive group of complex numbers $(\mathbb{C},+,0)$ the additive group of rational numbers $(\mathbb{Q},+,0)$ the additive group of integers $(\mathbb{Z},+,0)$ the multiplicative group of real numbers $(\mathbb{R}\setminus\{0\},\times,1)$ • the multiplicative group of complex numbers $(\mathbb{C}\setminus\{0\},\times,1)$ • the multiplicative group of rational numbers $(\mathbb{Q}\setminus\{0\},\times,1)$ • the multiplicative group of integers $(\mathbb{Z}\setminus\{0\},\times,1)$ • the multiplicative group of natural numbers $(\mathbb{N},\times,1)$  

Note that for the multiplicative groups, we need to exclude $0$ , since $0$ has no multiplicative inverse.  

All of these groups have infinite order.  

# Example 2.7 (Modulo Addition)  

If $m$ is any natural number, the additive group of integers modulo $m$ is the group $\mathbb{Z}_{m}=(\{0,1,2,\ldots,m-1\},+,0)$ , where addition is performed modulo $m$ . To confirm that it is a group, we need to check that the axioms hold.  

Associativity follows from the fact that regular addition is associative and commutative. Given $x$ , $y$ and $z$ , we have $x+y=a+k m$ for some $a$ and $k$ , so $(x+y)+z\equiv a+z$ (mod $m$ ). But $y=a-x+k m$ , so $y+z\equiv a-x+z$ (mod $m$ ), and hence $x+(y+z)\equiv x+a-x+z\equiv a+z$ (mod $m$ ).  

The fact that $0$ is an identity is trivial: $0+x=x$ , so $0+x\equiv x$ (mod $m$ ) follows immediately.  

If $x\in\{1,2,\ldots,m-1\}$ , we know that $-x\equiv m-x$ (mod $m$ ), and so $(m-$ $x)+x\equiv0$ (mod $m$ ) and $x+(m-x)\equiv0$ (mod $m$ ). Also $0$ is its own inverse. So every element has an inverse.  

These groups are also clearly Abelian, since regular addition is commutative.   
The order of $\mathbb{Z}_{m}$ is $m$ .  

The previous example shows that there are groups of all orders except 0.  

# Example 2.8  

Multiplication modulo $m$ does not, in general, give a group structure. Multiplication modulo $m$ is associative, and 1 is an identity. We have to exclude 0 from the group, because it clearly does not have a multiplicative inverse, but even with this restriction, some other elements may not have multiplicative inverses.  

If you consider multiplication modulo 6, as in Example 1.22, you can see that there are no inverses for 2, 3, and 4, since none of them have a number which you can multiply them by to give 1. Indeed, there is a somewhat deeper problem in that some products give 0, which cannot be an element of the group.  

Multiplication modulo $m$ does sometimes give you a group, however. The multiplication table (omitting 0) for multiplication modulo 5 is as follows:  

<html><body><table><tr><td></td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>1</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>2</td><td>2</td><td>4</td><td>1</td><td>3</td></tr><tr><td>3</td><td>3</td><td>1</td><td>4</td><td>2</td></tr><tr><td>4</td><td>4</td><td>3</td><td>2</td><td>1</td></tr></table></body></html>  

A quick check shows that every element has an inverse. Hence $(\{1,2,3,4\},\times,1)$ is a group, where $\times$ is multiplication modulo 5. $\diamondsuit$  

# Example 2.9 (Symmetries of a Set)  

If $\Omega\subseteq\mathbb{R}^{n}$ , then $(\mathrm{Sym}(\Omega),\circ,I)$ is a group. The proof of this is the essential content of Proposition 1.1. $\diamondsuit$  

# Example 2.10 (Symmetric Group)  

The symmetric group is the group $S_{n}=(S_{n},\cdot,e)$ of all permutations, with the multiplication of permutations being the group operation, and $e(k)=k$ being the identity permutation. That this is a group is largely the content of Proposition 1.3. The only thing that needs to be checked is that the identity permutation is in fact a group identity, and that is fairly straightforward: if $p$ is any permutation in $S_{n}$ ,  

$$
(p e)(k)=e(p(k))=p(k)\qquad{\mathrm{and}}\qquad(e p)(k)=p(e(k))=p(k),
$$  

for all $k$ , so $e p=p e=p$ , and $e$ is therefore the identity for this group operation. $\diamondsuit$  

# Example 2.11 (Alternating Group)  

Let $A_{n}$ be the set of all even permutations. The alternating group is the group $A_{n}~=~(A_{n},\cdot,e)$ of all even permutations, with the multiplication of permutations being the group operation, and $e(k)=k$ being the identity permutation. We know that the product of two even permutations is an even permutation, and the product is associative, and from the previous example we know that $e$ is an identity. What remains to be checked is that if $p$ is an even permutation, so is $p^{-1}$ . We note that since parity $(p)=1$ ,  

$$
\operatorname{parity}(p^{-1})=\operatorname{parity}(p)\operatorname{parity}(p^{-1})=\operatorname{parity}(p p^{-1})=\operatorname{parity}(e)=1.
$$  

Hence $p^{-1}$ is an even permutation.  

The group $A_{n}$ has order $n!/2$ .  

# Example 2.12 (Matrix Groups)  

Recall that a matrix $A$ is invertible if and only if $\operatorname*{det}(A)\neq0$ . If we are going  

to find groups of matrices with matrix multiplication as the group operation, then they must be invertible at least.  

The following are all groups:  

the general linear group of $n\times n$ matrices $(G L_{n}(\mathbb{R}),\times,I_{n})$ , where  

$$
G L_{n}(\mathbb{R})=\{A\in M_{n}(\mathbb{R}):\operatorname*{det}(A)\neq0\}.
$$  

the orthogonal group of $n\times n$ matrices $(O_{n}(\mathbb{R}),\times,I_{n})$ , where $O_{n}(\mathbb{R})$ is the set of orthogonal matrices (ie. matrices whose columns form an orthonormal basis or, equivalently, which satisfy $A^{-1}=A^{t}$ ).  

the special linear group of $n\times n$ matrices $(S L_{n}(\mathbb{R}),\times,I_{n})$ , where  

$$
S L_{n}(\mathbb{R})=\{A\in M_{n}(\mathbb{R}):\operatorname*{det}(A)=1\}.
$$  

the special orthogonal group of $n\times n$ matrices $(S O_{n}(\mathbb{R}),\times,I_{n})$ , where $S O_{n}(\mathbb{R})$ is the set of orthogonal matrices with determinant 1.  

There isn’t anything particularly special about $\mathbb{R}$ -valued matrices in the above. Once can define $G L_{n}(\mathbb{F})$ , $S L_{n}(\mathbb{F})$ , $O_{n}(\mathbb{F})$ , and $S O_{n}(\mathbb{F})$ for any field $\mathbb{F}$ (such as the complex numbers $\mathbb{C}$ , or the rational numbers $\mathbb{Q}$ ).  

A unitary matrix is a complex-valued matrix which satisfies $A^{-1}=A^{*}$ , where $A^{*}$ is the conjugate transpose matrix of $A$ . More precisely, if $A\ =$ $[a_{i,j}]_{i,j=1}^{n}$ , then  

$$
A^{*}=[\overline{{a_{i,j}}}]^{t}.
$$  

We then have two additional complex matrix groups the unitary group of $n\times n$ matrices $(U_{n}(\mathbb{C}),\times,I_{n})$ , where $U_{n}$ is the set of unitary matrices. the special unitary group of $n\times n$ matrices $(S U_{n}(\mathbb{C}),\times,I_{n})$ , where $S U_{n}(\mathbb{C})$ is the set of unitary matrices with determinant 1.  

In all these cases, we know that matrix multiplication is associative, the identity matrix is an element of each group, and in each case there is a matrix inverse of each matrix. What we need to check in each case is that the product of two elements is an element of the group, and that the inverse of an element is an element of the group.  

In each case it is fairly easy to verify these two facts. The key identities that we use are as follows:  

(i) $|A B|=|A||B|$ . So if $|A|$ and $|B|\ne0$ , then $|A B|\ne0$ . Hence if $A$ and $B\in G L_{n}(\mathbb{R})$ , then so is $A B$ . Similarly if $|A|$ and $|B|=1$ , then $|A B|=|A||B|=1$ , so if $A$ and $B\in$ $S L_{n}(\mathbb{R})$ , then so is $A B$ .   
(ii) $(A^{-1})^{-1}=A$ , so if $A\in G L_{n}(\mathbb{R})$ , then so is $A^{-1}$ .   
(iii) $|A^{-1}|=|A|^{-1}$ , so if $|A|=1$ , $|A^{-1}|=1^{-1}=1$ . Hence if $A\in S L_{n}(\mathbb{R})$ , then so is A−1.   
(iv) if $A$ and $B\in O_{n}(\mathbb{R})$ , then $(A B)^{t}=B^{t}A^{t}=B^{-1}A^{-1}=(A B)^{-1}$ . Also $(A^{-1})^{t}=(A^{t})^{t}=A=(A^{-1})^{-1}$ , so $A^{-1}\in O_{n}(\mathbb{R})$ . This, combined with (1) and (2) also shows that $S O_{n}(\mathbb{R})$ is a group.   
(v) similarly, if $A$ and $B\in U_{n}(\mathbb{C})$ , then $(A B)^{*}=B^{*}A^{*}=B^{-1}A^{-1}=(A B)^{-1}$ . Also $(A^{-1})^{*}=(A^{*})^{*}=A=(A^{-1})^{-1}$ , so $A^{-1}\in U_{n}(\mathbb{C})$ . This, combined with (1) and (2) also shows that $S U_{n}(\mathbb{C})$ is a group.  

# Example 2.13  

The set of matrices  

$$
G=\left\{\left[\begin{array}{c c}{{1}}&{{0}}\\ {{0}}&{{1}}\end{array}\right],\left[\begin{array}{c c}{{0}}&{{1}}\\ {{1}}&{{0}}\end{array}\right],\left[\begin{array}{c c}{{-1}}&{{0}}\\ {{0}}&{{-1}}\end{array}\right],\left[\begin{array}{c c}{{0}}&{{-1}}\\ {{-1}}&{{0}}\end{array}\right]\right\}
$$  

is a group when given the standard matrix operations, and the identity is the identity matrix. The easiest way to verify this is simply to show that the group is closed under matrix multiplication and matrix inverse.  

Letting  

$$
I=\left[\begin{array}{c c}{{1}}&{{0}}\\ {{0}}&{{1}}\end{array}\right],\qquadA=\left[\begin{array}{c c}{{0}}&{{1}}\\ {{1}}&{{0}}\end{array}\right],\qquadB=\left[\begin{array}{c c}{{-1}}&{{0}}\\ {{0}}&{{-1}}\end{array}\right],C=\left[\begin{array}{c c}{{0}}&{{-1}}\\ {{-1}}&{{0}}\end{array}\right],
$$  

we have that $I^{-1}=I$ , $A^{-1}=A$ , $B^{-1}=B$ abd $C^{-1}=C$ , and if we draw up the Cayley table for matrix multiplication of these matrices, we get  

<html><body><table><tr><td></td><td>I</td><td>A</td><td>B C</td></tr><tr><td>I</td><td>I</td><td>A B</td><td>C</td></tr><tr><td>A</td><td>A</td><td>I C</td><td>B</td></tr><tr><td>B</td><td>B</td><td>C I</td><td>A</td></tr><tr><td>C</td><td>C</td><td>B A</td><td>I</td></tr></table></body></html>  

Note that the Cayley table and inverses of this group correspond to the Cayley table and inverses of the symmetries of the H-shaped set of Example 1.1, with $I\leftrightarrow I$ , $A\leftrightarrow H$ , $B\leftrightarrow V$ , and $C\leftrightarrow R$ . $\diamondsuit$  

# Example 2.14 (Free Groups)  

Let $a$ and $b$ two symbols, and $a^{-1}$ and $b^{-1}$ be the inverse of these two symbols. A word in the letters $a$ , $b$ , $a^{-1}$ and $b^{-1}$ is simply a list $w=w_{1}w_{2}\cdot\cdot\cdot w_{n}$ , where each $w_{k}$ is one of the 4 letters. A reduced word is a word where we have repeatedly cancelled any adjacent occurrences of a letter and its inverse.  

For example $w=a b a^{-1}a b^{-1}b^{-1}a b^{-1}b a$ is a word. The corresponding reduced word can be found by cancelling: $w=a b a^{-1}a b^{-1}b^{-1}a b^{-1}b a=a b b^{-1}b^{-1}a a=$ $a b^{-1}a a$ .  

The empty word $e$ is the word with no letters. The product of two words $v=v_{1}v_{2}\cdot\cdot\cdot v_{m}$ and $w=w_{1}w_{2}\cdot\cdot\cdot w_{n}$ is simply the concatenation of the two words:  

$$
v w=v_{1}v_{2}\cdot\cdot\cdot v_{m}w_{1}w_{2}\cdot\cdot\cdot w_{n}
$$  

The free group on 2 symbols, $F_{2}$ , is the set of all reduced words in $a$ , $b$ , $a^{-1}$ and $b^{-1}$ , where the group operation is to multiply two words, and then reduce the product, and the identity is the empty word. The inverse of a word $w=w_{1}w_{2}\cdot\cdot\cdot w_{n}$ is the word $w^{-1}=w_{n}^{-1}w_{n-1}^{-1}\cdot\cdot\cdot w_{1}^{-1}$  

In a similar manner, one can construct the free group $F_{n}$ on $n$ symbols. $\diamondsuit$  

# Exercises  

2.2.1. Let $(G,*,e)$ be a group, and let $x$ and $y$ be two elements of $G$ which commute. Prove that for any $k\in\mathbb{Z}$ , $(x y)^{k}=x^{k}y^{k}$ .  

2.2.2. Give an example of a group and two elements of that group such that  

$$
(x y)^{2}\neq x^{2}y^{2}.
$$  

Provide concrete calculations to demonstrate this fact for your example.  

2.2.3. Show that in each of the following cases, $(G,*,e)$ is a group.  

(i) $G=\mathbb{R}^{2}$ , $(x,y)*(x^{\prime},y^{\prime})=(x+x^{\prime},y+y^{\prime})$ , $e=(0,0)$ .   
(ii) $G=\{(x,y):x,y\in\mathbb{R},x\neq0\},(x,y)*(x^{\prime},y^{\prime})=(x x^{\prime},x^{\prime}y+y^{\prime}),$ $e=(1,0)$ .   
(iii) $G=\{x:x\in\mathbb{R},x\neq-1\}$ , $x*y=x+y+x y$ , e = 0.   
(iv) $G\ =\ S L_{2}(\mathbb{Z})\ =\ \left\{{\left[\frac{a}{c}\begin{array}{l}{b}\\ {d}\end{array}\right]}:a,b,c,d\in\mathbb{Z},a d-b c=1\right\},\ *\ {\mathrm{is~matrix}}$ multiplication, and $e$ is the identity matrix. $G=\left\{{\left[{\begin{array}{l l}{a}&{b}\\ {0}&{a}\end{array}}\right]}:a,b\in\mathbb{R},a\neq0\right\}$ , $^*$ is matrix multiplication, and $e$ is the identity matrix.   
(vi) $G={\mathcal{P}}(X)$ , the power set of some set $X$ , $*=\triangle$ , and $e=\emptyset$ .  

2.2.4. Let $m\geq2$ be a natural number, and  

$$
G=\{k\in\mathbb{Z}_{m}:k\neq0,\ k\ \mathrm{and}\ m\ \mathrm{are}\ \mathrm{coprime}\}.
$$  

Show that $(G,\times,1)$ is a group where $\times$ is performed modulo $m$ . Conclude that $(\mathbb{Z}_{p}\setminus\{0\},\times,1)$ is a group if and only if $p$ is prime.  

Hint: Use Exercise 1.5.2.  

2.2.5. Explain why $(\mathbb{N},\times,1)$ is not a group.  

2.2.6. (\*) Prove that $S L_{n}(\mathbb{Z})$ is a group.  

# 2.3 Working With Abstract Groups  

A lot of the content of group theory involves proving general facts about groups. The point of this section is to make you familiar with the sorts of techniques and proof methods involved. Unfortunately, even the most “obvious” and basic facts need careful checking, since we have stripped away most of the standard rules of algebra.  

Consider the cancellation law:  

# Proposition 2.3 (Cancellation Law)  

Let $(G,*,e)$ be a group, and $x$ , $y$ , and $z\in G$ . If $x*z=y*z$ , then $x=y$ .   
Similarly, if $z*x=z*y$ , then $x=y$ .  

Normally you would cancel like this in algebra without too much thought: the case $z=0$ for multiplication is really the only exceptional case in standard algebra. However we need to carefully justify that cancellation in fact works for groups.  

Proof:  

We have  

$$
{\begin{array}{r l r l}&{x=x*e}&&{{\mathrm{(identity~axiom)}}}\\ &{=x*(z*z^{-1})}&&{{\mathrm{(inverse~axiom)}}}\\ &{=(x*z)*z^{-1}}&&{{\mathrm{(associativity)}}}\\ &{=(y*z)*z^{-1}}&&{{\mathrm{(hypothesis)}}}\\ &{=y*(z*z^{-1})}&&{{\mathrm{(associativity)}}}\\ &{=y*e}&&{{\mathrm{(inverse~axiom)}}}\\ &{=y.}&&{{\mathrm{(identity~axiom)}}}\end{array}}
$$  

The second part is left as an exercise.  

Notice how each step is justified in terms of the axioms of a group. Needless to say, once you get more familiar with the way that group operations work, you will not need to justify each step, and you may be able to skip certain trivial steps. In the short term, however, you should be careful that you justify each step in any calculation.  

Here is another example: it should be fairly obvious that there can only be one inverse of any particular element. Nevertheless, we need to prove this result.  

# Proposition 2.4 (The Inverse is Unique)  

Let $(G,*,e)$ be a group, and $x$ , $y\in G$ . Then if $x*y=e$ , $y=x^{-1}$ . Similarly, if y ∗ x = e, y = x−1  

Proof:  

We have  

$$
{\begin{array}{r l r l}&{y=e*y}&&{{\mathrm{(identity~axiom)}}}\\ &{=(x^{-1}*x)*y}&&{{\mathrm{(inverse~axiom)}}}\\ &{=x^{-1}*(x*y)}&&{{\mathrm{(associativity)}}}\\ &{=x^{-1}*e}&&{{\mathrm{(hypothesis)}}}\\ &{=x^{-1}.}&&{{\mathrm{(identity~axiom)}}}\end{array}}
$$  

The second part is left as an exercise.  

Once we have basic facts like this, we can use them to simplify the proofs of other facts.  

# Proposition 2.5  

Let $(G,*,e)$ be a group, and $x$ , $y\in G$ . Then $(x*y)^{-1}=y^{-1}*x^{-1}$ .  

Proof:  

By Proposition 2.4, we need only show that $(x*y)*(y^{-1}*x^{-1})=e$ .  

$$
{\begin{array}{r l r l}{(x*y)*(y^{-1}*x^{-1})=(x*(y*y^{-1}))*x^{-1}}&&{{\mathrm{(associativity)}}}\\ &{=(x*e)*x^{-1}}&&{{\mathrm{(inverse~axiom)}}}\\ &{=x*x^{-1}}&&{{\mathrm{(identity~axiom)}}}\\ &{=e.}&&{{\mathrm{(inverse~axiom)}}}\end{array}}
$$  

Hence $(x*y)^{-1}=y^{-1}*x^{-1}$ .  

Notice in this example that the order of the product is reversed in the inverse. This is necessary if the elements do not commute.  

Here is another basic fact that needs to be verified.  

# Proposition 2.6 (Double Inverse)  

Let $(G,*,e)$ be a group, and $x\in G$ . Then $(x^{-1})^{-1}=x$ .  

Proof: Exercise.  

As mentioned in the previous section, if we use power-style notation, most of the usual power laws hold.  

# Proposition 2.7 (Power Laws for Groups)  

Let $(G,*,e)$ be a group, and $x\in G$ . Then  

(i) $x^{m}x^{n}=x^{m+n}$ ,   
(ii) $(x^{m})^{n}=x^{m n}$ ,   
(iii) if $y\in G$ and $x*y=y*x$ , then $(x*y)^{n}=x^{n}*y^{n}$ .  

May 3, 2004  

Let $(G,+,0)$ is an Abelian group, and $x$ , $y\in G$ . Then  

(i) $m x+n x=(m+n)x $ , (ii) $m(n x)=(m n)x$ , (iii) $n(x+y)=n x+n y$ .  

Proof: Exercise.  

# Exercises  

2.3.1. Prove the parts of the proofs from this section that were left as exercises.  

2.3.2. Some texts define groups slightly differently (and slightly more efficiently) as follows:  

A group, $\mathbf{G}=(G,*,e)$ , consists of a set $G$ , a binary operation $*:G{\times}G\to$ $G$ , and an element $e\in G$ satisfying the following three conditions:  

(i) $^*$ is associative   
(ii) $e$ is a (left) identity for $^*$ , ie. $e*x=x$ ,   
(iii) every element $x\in G$ has an inverse element $x^{-1}\in G$ such that $x^{-1}*x=e$ .  

Show that if you use these axioms, you can prove that $e$ is also a right inverse, and $x*x^{-1}=e$ , giving you the axioms of our definition of a group.  

This means that the two definitions are equivalent, so you can use either one.  

2.3.3. Let $(G,*,e)$ be a group, and $x$ , $y\in G$ . Show that if $y^{-1}x y=x^{k}$ , then y−nxmyn = xmkn.  

2.3.4. Let $(G,*,e)$ be a group. Show that $e^{-1}=e$ .  

# 2.4 Cayley Tables  

As we have seen, there are quite a number of groups around. We would like to develop some way that we can present groups abstractly, without worrying about any potential context.  

For finite groups, one way of doing this is by giving a Cayley table for the group.  

# Definition 2.1  

Let $(G,*,e)$ be a finite group of order $n$ , with some particular ordering $x_{1}$ , $x_{2},\ldots,x_{n}$ chosen for the elements of $G$ . Then a Cayley table of the group is an array where $x_{i}*x_{j}$ is in the ith row and $j$ th column.  

We do not need to specify the inverse as a separate table, since we can find the inverse of $x_{i}$ by looking for the $j$ such that the $j$ th entry of the $i$ th row is $e$ , so that $x_{i}*x_{j}=e$ , and hence $x_{j}=x_{i}^{-1}$ by Proposition 2.4.  

We have already seen a number of Cayley tables for binary operations which turned out to be groups. Indeed, in a number of situations, the Cayley tables turned out to be essentially the same.  

# Example 2.15  

Consider the Cayley tables of $(\mathbb{Z}_{2},+,0)$ and $(\mathbb{Z}_{3}\setminus\{0\},\times,1)$ .  

$$
{\begin{array}{r l}{{\frac{~+~{\left|~{\boldsymbol{0}}~{\boldsymbol{1}}~\right|}~}{~0~{\left|~{\boldsymbol{0}}~{\boldsymbol{1}}~\right|}~}}}&{{\frac{~\times~{\left|~{\boldsymbol{1}}~{\boldsymbol{2}}~}~}{~1~{\left|~{\boldsymbol{1}}~{\boldsymbol{2}}~}~\right|}}}\\ {{1}~{\left|~{\boldsymbol{1}}~{\boldsymbol{0}}~}~}&{{2~{\left|~{\boldsymbol{2}}~{\boldsymbol{1}}~}~}~}\end{array}}
$$  

These are the same table is you replace $^+$ by $\times$ , $0$ by 1 and $^{1}$ by 2.  

In situations like this, we can agree that the two groups in question are essentially the same.  

# Definition 2.2  

Two finite groups $(G,*,e)$ and $(H,\circ,i)$ are isomorphic if a Cayley table of $H$ can be obtained from a Cayley table of $G$ be replacing all occurrences of each symbol in $G$ by a corresponding symbol of $H$ , and each $^*$ by $\mathsf{O}$ .  

We write $G\cong H$ when $G$ and $H$ are isomorphic.  

The correspondence between any two isomorphic groups always has the two identities corresponding, and always has the inverse of a symbol in $G$ corresponding to the inverse of the corresponding symbol in $H$ .  

Isomorphism is clearly a transitive relation between groups. If $G$ and $H$ are isomorphic, and $H$ and $F$ are isomorphic, than $G$ and $F$ must also be isomorphic.  

This is not the final version of the definition of “isomorphic”, but it will do for now.  

# Example 2.16  

The groups $S_{3}$ and $\mathrm{Sym}(\Omega)$ , where $\Omega$ is an equilateral triangle, are isomorphic. $\diamondsuit$  

# Example 2.17  

Consider the Cayley tables of $\mathbb{Z}_{4}$ and the group of symmetries of the $\mathsf{H}$ - shaped set of Example 1.1:  

<html><body><table><tr><td>+</td><td>0</td><td>1</td><td>2</td><td>3</td><td></td><td>I</td><td>H V</td><td>R</td></tr><tr><td>0</td><td>0</td><td>1</td><td>2</td><td>3</td><td>I</td><td>I</td><td>H V</td><td>R</td></tr><tr><td>1</td><td>1</td><td>2</td><td>3</td><td>0</td><td>H</td><td>H</td><td>I R</td><td>V</td></tr><tr><td>2</td><td>2</td><td>3</td><td>0</td><td>1</td><td>V</td><td>V</td><td>R I</td><td>H</td></tr><tr><td>3</td><td>3</td><td>0</td><td>1</td><td>2</td><td>R</td><td>R</td><td>V H</td><td>I</td></tr></table></body></html>  

May 3, 2004  

These two groups are not isomorphic, since $I$ must correspond to $0$ , and one of $H$ , $V$ or $R$ must correspond to 1, and $1+1=2$ , but we have $H\circ H=I$ , $V\circ V=I$ , $R\circ R=I$ , so their products cannot correspond to the sum, and so there is no element that can correspond to 1. $\diamondsuit$  

It is immediate that for two groups to be isomorphic, they must have the same order, since otherwise the Cayley tables are different sizes, and so the cannot correspond.  

Similarly if two groups are isomorphic, either both are Abelian, or both fail to be Abelian, since there is no way the Cayley tables can correspond if one is Abelian and the other not.  

# Lemma 2.8  

$I f G$ and $H$ are finite groups, and $G\cong H$ , then:  

(i) $|G|=|H|$ , (ii) $G$ is Abelian if and only if $H$ is Abelian.  

This leads to the following:  

# Question 1  

How many different (ie. non-isomorphic) classes of groups are there for any given order?  

This question is one which we will spend a fair amount of time considering.  

Cayley tables can be used to answer this question, at least for groups of small order, but we need a few facts about Cayley tables first.  

# Proposition 2.9  

$H(G,*,e)$ is a finite group, then every element of $G$ occurs exactly once in each row and in each column of a Cayley table for $G$ .  

Proof:  

Let $G=\{x_{1},x_{2},\ldots,x_{n}\}$ . Assume that $x$ occurs twice in the $i$ th row, so that $x=x_{i}*x_{j}$ and $x=x_{i}*x_{k}$ for some $j\neq k$ . But then we have $x_{i}*x_{j}=x_{i}*x_{k}$ , and the cancellation law tells us that $x_{j}=x_{k}$ , so $j=k$ , which is a contradiction. Hence $x$ can occur at most once.  

If $x$ does not occur at all in the $\textit{\textbf{\i}}$ th row, then there must be some other element which occurs 2 or more times by the pidgeonhole principle, which is impossible. Hence $x$ must occur exactly once.  

A similar argument proves the result for columns.  

Another way of saying this is that a Cayley table is a Latin square: a Latin square is an array of symbols in which every symbol occurs exactly once in each row and in each column. Latin squares are significant in experimental design and statistics. However, not every Latin square is a Cayley table for a group.  

# Example 2.18  

The following binary operation does not give a group:  

<html><body><table><tr><td>*</td><td>1 a</td><td>b</td><td>C</td><td>d</td></tr><tr><td>1</td><td>1 a</td><td>b</td><td>C</td><td>d</td></tr><tr><td>a</td><td>a 1</td><td>d</td><td>b</td><td>C</td></tr><tr><td>b</td><td>b C</td><td>1</td><td>d</td><td>a</td></tr><tr><td>C</td><td>C d</td><td>a</td><td>1</td><td>b</td></tr><tr><td>d</td><td>d b</td><td>C</td><td>a</td><td>1</td></tr></table></body></html>  

The problem is that the operation it determines is not associative: $(a*b)*c=$ $d*c=a$ , while $a*(b*c)=a*d=c$ . However, this table clearly has the Latin square property. $\diamondsuit$  

# Theorem 2.10  

Let $n=1$ , 2, or 3. Then every group of order $n$ is isomorphic to $(\mathbb{Z}_{n},+,0)$ .  

Proof:  

Case $n=1$ : the group has one element which must be the identity, so $\mathit{G}=\{e\}$ . The only possible Cayley table is trivial  

$$
\frac{\left.*\right|{\boldsymbol{e}}}{e\mathrm{~\Big|~}e}
$$  

and this clearly is isomorphic to the Cayley table of $\mathbb{Z}_{1}$  

$$
\frac{\mathbf{+}\mathbf{\Gamma}\mathbf{|\mathbf{\Gamma}0}}{\mathbf{0}\mathbf{\Gamma}\mathbf{|\mathbf{\Gamma}0}}
$$  

Case $n=2$ : the group has two elements, one of which must be the identity, so $\boldsymbol{G}=\{\boldsymbol{e},\boldsymbol{a}\}$ . Entering in the elements which are products of the identity element we get  

$$
\frac{*\ \rvert\ e\quad a}{\textit{e}\ \lvert\textit{e}\ \rvert}
$$  

and clearly the only way to complete this table while keeping the Latin square property is to put an $e$ in the bottom right entry:  

$$
\frac{*\ \biggr\vert\ e\quad a}{\ e}
$$  

Again, this is clearly isomorphic to $\mathbb{Z}_{2}$ when you look at the Cayley table  

$$
\begin{array}{c}{{\frac{~+~{\left|~0~1~\right|}~}{~0~{\left|~0~1~\right|}~}}}\\ {{1~{\left|~1~0~\right|}~}}\end{array}
$$  

May 3, 2004  

Case $n=3$ : the group has three elements, one of which must be the identity, so $\boldsymbol{G}=\{\boldsymbol{e},\boldsymbol{a},\boldsymbol{b}\}$ . Entering in the elements which are products of the identity element we get  

$$
\begin{array}{l}{\frac{\textit{*}\left|\textit{e}\textsubscript{\textit{a}}\right\rangle}{\textit{e}}}\\ {\frac{\textit{a}}{\textit{b}}\left|\begin{array}{l}{a}\\ {b}\end{array}\right.}\end{array}
$$  

To preserve the Latin square property, the second entry of the second column must be $b$ , otherwise the second entry of the third column would be $b$ , which would break the Latin square property for the third column. This then implies that the last entries of the second row and second column must be $e$ , and the final entry of the array must be $a$ .  

$$
\begin{array}{r}{\frac{\textbf{*}\left|\textbf{\emph{e}}\textbf{\emph{a}}\textbf{\emph{b}}\right|}{e}}\\ {\frac{a}{b}\left|\textbf{\emph{a}}\textbf{\emph{b}}\right|}\\ {\frac{b}{b}\left|\textbf{\emph{b}}\textbf{\emph{e}}\textbf{\emph{a}}\right|}\end{array}
$$  

Again, the correspondence with the Cayley table for $\mathbb{Z}_{3}$ is clear:  

<html><body><table><tr><td>+</td><td>0 1</td><td>2</td></tr><tr><td>0</td><td>0 1</td><td>2</td></tr><tr><td>1</td><td>1 2</td><td>0</td></tr><tr><td>2</td><td>2 0</td><td>1</td></tr></table></body></html>  

We know that there are at least two non-isomorphic groups of order 4. It will turn out that these are the only two possibilities. We could prove this by finding all possible Cayley tables of groups of order 4, but as we will see, there are slicker ways to do this.  

# Exercises  

2.4.1. Find another example of a Latin square which is not the Cayley table of a group.   
2.4.2. Show that any group of order 4 is isomorphic to one of the two groups in Example 2.17.  

# 2.5 Generators  

While Cayley tables have their uses, there are clear limitations to their use once the groups get large, and for infinite groups they at best give a tiny snapshot of the group. Another way of presenting groups is required, which can deal with these larger groups.  

# Definition 2.3  

Let $(G,*,e)$ be a group. We say that a subset $X~=~\{x_{1},x_{2},.~.~.,x_{n}\}$ of $G$ generates $G$ if every element of $G$ can be written as a product of powers of elements of set $X$ (possibly with repetition). We say that the elements of $X$ are generators of $G$ .  

More generally, given a subset $X=\{x_{1},x_{2},...,x_{n}\}$ of $G$ , the set of elements that can be written as a product of powers of elements of $X$ (possibly with repetition) is the set generated by $X$ , and we denote it by $\langle x_{1},x_{2},\ldots,x_{n}\rangle$ or $\langle X\rangle$ .  

# Example 2.19  

The group $S_{3}$ is generated by the permutations $a=(1,2,3)$ and $b=(1,2)$ . One can easily verify that the identity permutation is $a^{0}$ , $a^{2}=(1,3,2)$ , $a b=$ $(2,3)$ and $a^{2}b=(1,3)$ . We could write $S_{3}=\langle(1,2,3),(1,2)\rangle$ .  

The group $S_{3}$ is also generated by $x=(1,2)$ and $y=(2,3)$ . This requires a little bit more checking, but $x^{0}=e$ , $y x=(1,2,3)$ , $x y=(1,3,2)$ , and $x y x=$ $(1,3)$ , so we also have $S_{3}=\langle(1,2),(2,3)\rangle$ .  

On the other hand, the permutation $a=(1,2,3)$ does not generate the whole group. The only elements we can get using just powers of $a$ are $e$ , $a$ , and $a^{2}$ , since $\boldsymbol{a}^{3}=\boldsymbol{e}$ . Hence $\langle(1,2,3)\rangle=\{e,(1,2,3),(1,3,2)\}$ . $\diamondsuit$  

# Example 2.20  

The group $(\mathbb{Z}_{4},+,0)$ is generated by $^{1}$ , since $1+1=2$ , $1+1+1=3$ and $1+1+1+1=0$ . It is also generated by $-1$ .  

However the set generated by 2 is simply $\{0,2\}$ .  

We note that the identity element is always in the set generated by any collection of elements.  

Generators help us understand the structure of a group by allowing us to represent general elements in terms of fewer symbols.  

# Example 2.21  

If $x=(1,2)$ and $y=(2,3)$ , then $S_{3}=\{e,x,y,x y,y x,x y x\}$ . In addition, we can see that $x^{2}=e$ , $\boldsymbol y^{2}=e$ and $y x y=x y x$ . For example, you could calculate the product of xyx and $y x$ using these facts as follows:  

$$
{\begin{array}{r l r l}{(x y x)(y x)=x(y x y)x}&{}&&{{\mathrm{(associativity)}}}\\ {=x(x y x)x}&{}&&{(y x y=x y x)}\\ {=(x x)y(x x)}&{}&&{{\mathrm{(associativity)}}}\\ {=e y e}&{}&&{(x^{2}=e)}\\ {=y}&{}&&{{\mathrm{(identity~axiom)}}}\end{array}}
$$  

May 3, 2004  

We can use this information to write the Cayley table of $S_{3}$ in terms of $x$ and y as follows:  

<html><body><table><tr><td></td><td>e</td><td>C</td><td>y</td><td>cy</td><td>yc</td><td>cyc</td></tr><tr><td>e</td><td>e</td><td>C</td><td>y</td><td>cy</td><td>yc</td><td>xyc</td></tr><tr><td>C</td><td>C</td><td>e</td><td>cy</td><td>y</td><td>cy.</td><td>yc</td></tr><tr><td>y</td><td>y</td><td>yx</td><td>e</td><td>cyc</td><td>C</td><td>cy</td></tr><tr><td>cy</td><td>xy</td><td>cyc</td><td>C</td><td>yc</td><td>e</td><td>y</td></tr><tr><td>yc</td><td>yc</td><td>y</td><td>cyc</td><td>e</td><td>cy</td><td>C</td></tr><tr><td>cyc</td><td>cyx</td><td>cy</td><td>yc</td><td>C</td><td>y</td><td>e</td></tr></table></body></html>  

Notice how in the above example the identities $x^{2}=e$ , $\boldsymbol y^{2}=e$ and $y x y=$ xyx help us calculate. Such identities are called relations. In fact, given the generators $x$ and $y$ and these three relations, we can recover the Cayley table for $S_{3}$ . This leads us to another way to present a group, which we will make more formal in a later section. In the mean-time we can use it informally as follows:  

# Example 2.22 (Cyclic Groups)  

The group $C_{n}$ consists of the set $C_{n}=\{1,a,a^{2},\dots,a^{n-1}\}$ and the group operation is determined by the relation $a^{n}=1$ .  

The group $(C_{4},\cdot,1)$ , then has the Cayley table  

$$
\begin{array}{c}{{\cdot\quad\left\vert\begin{array}{c c c c}{{1}}&{{a}}&{{a^{2}}}&{{a^{3}}}\\ {{1}}&{{a}}&{{a^{2}}}&{{a^{3}}}\end{array}\right.}}\\ {{a}}\\ {{a^{2}}}\\ {{a^{3}}}\end{array}\nonumber
$$  

Clearly $C_{4}$ and $\mathbb{Z}_{4}$ are isomorphic.  

# Example 2.23 (Dihedral Groups)  

The group $D_{2n}$ consists of the set  

$$
{\cal D}_{2n}=\{1,a,a^{2},\ldots,a^{n-1},b,a b,a^{2}b,\ldots,a^{n-1}b\}
$$  

and the group operation is determined by the relations $a^{n}=1$ , $b^{2}=1$ and $b a=a^{n-1}b$ . Note that $a^{n-1}a=a^{n}=1$ , so $a^{n-1}=a^{-1}$ , and we could write the third relation as $b a=a^{-1}b$ .  

For example, we can use these relations to show that  

$$
b a^{k}=a^{-1}b a^{k-1}=a^{-1}a^{-1}b a^{k-2}=\cdot\cdot\cdot=a^{-k}b.
$$  

May 3, 2004  

The group $(D_{8},\cdot,1)$ , then has the Cayley table  

<html><body><table><tr><td></td><td>1</td><td>a</td><td>a²</td><td>a3</td><td>b</td><td>ab</td><td>a²b</td><td>a3b</td></tr><tr><td>1</td><td>1</td><td>a</td><td>a2</td><td>a3</td><td>b</td><td>ab</td><td>a²b</td><td>ab</td></tr><tr><td>a</td><td>a</td><td>a2</td><td>a3</td><td>1</td><td>ab</td><td>a²b</td><td>a3b</td><td>b</td></tr><tr><td>a2</td><td>a2</td><td></td><td>1</td><td>a</td><td>a²b</td><td>α3b</td><td>b</td><td>ab</td></tr><tr><td>a3</td><td>a3</td><td>1</td><td>a</td><td>a2</td><td>a3b</td><td>b</td><td>ab</td><td>a?b</td></tr><tr><td>b</td><td>b</td><td>a3b</td><td>a?b</td><td>ab</td><td>1</td><td>a3</td><td>a2</td><td>a</td></tr><tr><td>ab</td><td>ab</td><td>b</td><td>a3b</td><td>a²b</td><td>a</td><td>1</td><td>a3</td><td>a2</td></tr><tr><td>a²b</td><td>a²b</td><td>ab</td><td>b</td><td>a3b</td><td>a2</td><td>a</td><td>1</td><td></td></tr><tr><td>ab</td><td>ab</td><td>a²b</td><td>ab</td><td>b</td><td>a3</td><td>a2</td><td>a</td><td>1</td></tr></table></body></html>  

If you were to write out the symmetry group of a square you would find that it is isomorphic to $D_{8}$ .  

In fact we will prove later that the symmetry group of a regular $n$ -gon is isomorphic to $D_{2n}$ . $\diamondsuit$  

# Definition 2.4  

Let $(G,*,e)$ be a group, and left $x\in G$ . The order $o(x)$ is the cardinality of the set it generates, $o(x)=|\langle x\rangle|$ .  

If $G=\langle x\rangle$ for any $x\in G$ , we say that $G$ is a cyclic group.  

# Example 2.24  

In $S_{3}$ , $e$ has order 1, $(1,2)$ , $(2,3)$ and $(1,3)$ have order 2, and the elements $(1,2,3)$ and $(1,3,2)$ have order 3.  

In $\mathbb{Z}_{4}$ , $0$ has order $^{1}$ , 2 has order 2, and 1 and $-1$ have order 4. Since $\mathbb{Z}_{4}=\langle1\rangle$ , it is a cyclic group. 3  

# Example 2.25  

For every $n\in\mathbb N$ , $(\mathbb{Z}_{n},+,0)$ is a cyclic group, since $\mathbb{Z}_{n}=\langle1\rangle$ .  

The orders of elements of a group can be used as a comparatively simple check to see if two groups may not be isomorphic. It is clear that if $G$ and $H$ are isomorphic groups, and $x\in G$ corresponds to $y\in H$ , then $o(x)=o(y)$ . Turning this idea around, we get the following theorem  

# Theorem 2.11  

If $G$ and $H$ are two groups, then if there is some n such that the number of elements of order $n$ in $G$ is different from the number of elements of order $n$ in $H$ , ie.  

$$
|\{x\in G:o(x)=n\}|\neq|\{y\in H:o(y)=n\}|
$$  

then $G$ and $H$ are not isomorphic.  

May 3, 2004  

Proof:  

Without loss of generality, we may assume that $|\{x\in G:o(x)=n\}|>|\{y\in$ $H:o(y)=n\}|$ . If $G$ and $H$ are isomorphic, then there must be some $x$ which corresponds to an element of $H$ which does not have order $y$ , because there are not enough elements of order $n$ in $H$ . But this cannot happen if the groups are isomorphic, giving a contradiction. $\mid$  

We will eventually see that the converse of this theorem is not true, so this does not give a good test for isomorphism of groups.  

The following simplified version of the theorem is often enough to prove that two groups are not isomorphic.  

# Corollary 2.12  

$I f G$ and $H$ are two groups, and there is some $x\in G$ such that $o(x)>o(y)$ for every $y\in H$ , then $G$ and $H$ are not isomorphic.  

This allows us to see that a couple of groups are not isomorphic very quickly:  

# Example 2.26  

If we look at $C_{4}=\{1,a,a^{2},a^{3}\}$ , we see that $o(a)=4$ . On the other hand, ${D_{4}}=\left\{{1,a,b,a b}\right\}$ has $o(1)=1$ , and $o(a)=o(b)=o(a b)=2$ . So by the corollary, $C_{4}\not\cong D_{4}$ . $\diamondsuit$  

# Example 2.27  

If we look at $C_{6}=\{1,a,a^{2},a^{3},a^{4},a^{5}\}$ , we see that $o(a)=6$ . On the other hand, $D_{6}=\{1,a,b,a b,a^{2},a^{2}b\}$ has $o(1)=1$ , $o(b)=o(a b)=o(a^{2}b)=2$ , and $o(a)=o(a)^{2}=3$ . So by the corollary, $C_{6}\not\cong D_{6}$ .  

One could also show this by simply observing that $C_{6}$ is Abelian, but $D_{6}$ is not. $\diamondsuit$  

Related to the above discussion is the following theorem.  

# Theorem 2.13  

$I f G$ is a group of order $n$ and there is some element $x\in G$ of order $n$ , then $G$ is a cyclic group.  

Proof:  

We have that $\langle x\rangle\subseteq G$ , and $|\langle x\rangle|=o(x)=n=|G|$ . Hence, since $G$ is finite, $\langle x\rangle=G$  

The following theorem is fairly obvious, but needs to be stated and proved.  

# Theorem 2.14  

If $G$ and $H$ are cyclic groups of the same order, then they are isomorphic.  

May 3, 2004  

Proof:  

We have that $G=\langle a\rangle=\{1=a^{n},a,a^{2},\dotsc a^{n-1}\}$ and $H=\langle b\rangle=\{1=$ $b^{n},b,b^{2},\dotsc,b^{n-1}\}$ , and a typical entry in their Cayley tables looks like  

<html><body><table><tr><td></td><td>ak</td><td>b</td></tr><tr><td></td><td>ak+l</td><td>b bk+l</td></tr></table></body></html>  

remembering that $a^{\scriptscriptstyle{\mathit{n}}}=1$ and $b^{n}=1$ . In any case, it is clear that we can get from one Cayley table to the other by simply replacing powers of $a$ with corresponding powers of $b$ .  

# Corollary 2.15  

Every group of order 1, 2, or 3 is a cyclic group.  

We give one last example, mainly to introduce a name.  

# Example 2.28  

The group $V$ consisting of the elements $\{1,a,b,a b\}$ with the relations ${a}^{2}=1$ , $b^{2}=1$ and $b a=a b$ is called the four-group or vierergruppe (which is German for “four-group”).  

The Cayley table of this group is  

<html><body><table><tr><td></td><td>1 a</td><td>b</td><td>ab</td><td></td></tr><tr><td>1</td><td>1</td><td>a</td><td>b</td><td>ab</td></tr><tr><td>a</td><td>a</td><td>1</td><td>ab</td><td>b</td></tr><tr><td>b</td><td>b</td><td>ab</td><td>1</td><td>a</td></tr><tr><td>ab</td><td>ab</td><td>b</td><td>a</td><td>1</td></tr></table></body></html>  

Given the Cayley table, you can see that this is isomorphic to the group of symmetries of the letter $H$ , via the correspondences: $1\leftrightarrow I$ , $a\leftrightarrow H$ , $b\leftrightarrow V$ , and $a b\leftrightarrow R$ .  

Also, since $\textstyle a^{2}=1$ implies $a=a^{-1}$ , we could have used the relation $b a=a^{-1}b$ instead of $b a=a b$ . Hence $V$ is the same group as $D_{4}$ . $\diamondsuit$  

# Exercises  

2.5.1. Show that $(1,2)$ and $(1,3)$ generate $S_{3}$ .  

2.5.2. Let $(G,*,e)$ be a group, and let $x$ , $y\in G$ . Show that  

(i) $o(x^{-1})=o(x)$ ,   
(ii) $o(x y)=o(y x)$ ,   
(iii) if $o(x)=1$ , then $x=e$ ,   
(iv) if $o(x)=n$ , then $x^{m}=e$ if and only if $n$ divides $m$ ,  

(v) if $o(x)=n$ , then $n$ is the smallest natural number such that $x^{\prime\iota}=e$ .  

2.5.3. Show that $a^{k}$ generates the cyclic group $C_{n}=\{1,a^{1},a^{2},\dots,a^{n-1}\}$ if and only if $k$ and $n$ are coprime. Show that the order of $a^{k}$ in $C_{n}$ is $k/\operatorname*{gcd}(k,n)$ .  

2.5.4. Let $(G,*,e)$ be a group, and $x$ , $y\in G$ . Show that if $x$ , $y$ and $x y$ all have order 2, then $x$ and $y$ commute.  

2.5.5. Show that $D_{6}$ and $S_{3}$ are isomorphic groups.  

2.5.6. Show that if $\Omega$ is a regular $n$ -sided polygon in $\mathbb{R}^{2}$ , then Sym $1(\Omega)\cong D_{6}$ .  

2.5.7. Let $G=\{1,a,b,a b,a^{2},a^{2}b\}$ with the relations $a^{3}=1$ , $b^{2}=1$ , and $a b=$ ba. Write out the Cayley table of this group, and show that $G$ is isomorphic to $C_{6}$ .  

2.5.8. Show that $(\mathbb{Z},+0)$ is generated by the elements 2 and 3. (\*) Show that it is generated by any pair of coprime numbers. (Hint: show that you can get 1 as a sum of multiples of the numbers.)  

# 2.6 Excursion: Introduction to Categories  

You may have noticed some similarities between the theory of groups as it has been presented to this point, and the theory of abstract vector spaces. In both cases the objects were defined in terms of axioms which must hold for various binary operations. In both cases we have a concept called isomorphism which tells us when two groups or two vector spaces are essentially the same.  

Indeed, if you think about it, the concept of generating sets and spanning sets are somewhat analogous: a set generates $G$ if you can get every element of $G$ by applying the group operations to the elements of the generating set; while a set spans a vector space $V$ if you can get every element of $V$ by applying linear operations (vector addition and scalar multiplication) to the elements of the spanning set.  

Clearly we should be careful not to take such analogies too far, since groups and vector spaces are different, but the analogies are useful for putting the next few sections into context.  

In the theory of vector spaces, there are three concepts we have not yet seen in the context of groups:  

direct sums of vector spaces: if $V$ and $W$ are vector spaces, we have the direct sum $V\oplus W$ which is the set $V\times W$ together with appropriate vector space operations.   
subspaces: a subspace of a vector space is a subset which is also a vector space.  

linear transformations: a linear transformation is a function between vector spaces which preserves the vector space operations.  

As you study more algebra you will notice that there are many similarities like these between the theories of various types of algebraic objects. Indeed, we even get such similarities in other areas of pure mathematics, such as analysis, topology and geometry. In many cases the proofs of basic facts in these theories are almost identical, but with appropriate change of terminology.  

Whenever you have similarities and patterns like this in mathematics, there must be something going on. Category theory is the theory which deals with and formalizes these similarities. There are a handful of useful results which have come out of category theory, but its primary significance is that it provides a framework for much of modern mathematical theory.  

We’ll consider categories in more depth later.  

# 2.7 Direct Products  

You may recall the definition of a direct sum from linear algebra, if you have two vector spaces $V$ and $W$ over the same scalar field, the direct product is the set $V\times W$ with vector addition  

$$
(v_{1},w_{1})+(v_{2},w_{2})=(v_{1}+v_{2},w_{1}+w_{2}),
$$  

scalar multiplication  

$$
\lambda(v,w)=(\lambda v,\lambda w)
$$  

and zero vector $(0,0)$ . Note that the vector space operations are defined simply by applying the appropriate vector space operation to each component.  

# Theorem 2.16  

Let $(G,*,e)$ and $(H,\circ,1)$ be two groups. If we define a binary operation $\bullet$ on $G\times H$ by  

$$
(g_{1},h_{1})\bullet(g_{2},h_{2})=(g_{1}*g_{2},h_{1}\circ h_{2}),
$$  

then $G\times H=(G\times H,\bullet,(e,1))$ is a group, and the inverse of $(g,h)$ is $(g^{-1},h^{-1})$ .  

Proof:  

The binary operation $\bullet$ is obviously well defined, so we only need to check that the three axioms hold.  

Associativity follows directly from the associativity of $G$ and $H$ :  

$$
{\begin{array}{r l}{\left((g_{1},h_{1})\bullet(g_{2},h_{2})\right)\bullet(g_{3},h_{3})=\left(g_{1}\ast g_{2},h_{1}\circ h_{2}\right)\bullet(g_{3},h_{3})}&{}\\ &{=\left((g_{1}\ast g_{2})\ast g_{3},(h_{1}\circ h_{2})\circ h_{3}\right)}\\ &{=\left(g_{1}\ast(g_{2}\ast g_{3}),h_{1}\circ(h_{2}\circ h_{3})\right)}\\ &{=\left(g_{1},h_{1}\right)\bullet(g_{2}\ast g_{3},h_{2}\circ h_{3})}\\ &{=\left(g_{1},h_{1}\right)\bullet\left(\left(g_{2},h_{2}\right)\bullet(g_{3},h_{3})\right)}\end{array}}
$$  

It’s straightforward to see that $(e,1)$ is an identity:  

$$
(g,h)\bullet(e,1)=(g\ast e,h\circ1)=(g,h)
$$  

and  

$$
(e,1)\bullet(g,h)=(e*g,e\circ h)=(g,h).
$$  

Finally, we observe that  

$$
(g^{-1},h^{-1})\bullet(g,h)=(g^{-1}\ast g,h^{-1}\circ h)=(e,1),
$$  

so by Proposition 2.4, $(g,h)^{-1}=(g^{-1},h^{-1})$ , and so every element of $G\times H$ has an inverse.  

So $G\times H$ is a group.  

If $G$ and $H$ are finite groups, the multiplication principle tells us that $G\times H$ is a finite group, and the order of $G\times H$ is $|G||H|$ .  

Consider the following examples:  

# Example 2.29  

The group $C_{2}\times C_{2}$ has 4 elements: $(1,1)$ , $(a,1)$ , $(1,a)$ and $(a,a)$ . We can draw up the Cayley table:  

<html><body><table><tr><td></td><td>(1,1) (a, 1) (1,a) (a,a)</td></tr><tr><td>(1,1) (1,1) (a, 1) (a, 1) (1,a) (1,a) (a,a) (a,a)</td><td>(a,1) (1,a) (a,a) (1, 1) (a,a) (1,a) (a,a) (1,1) (a, 1) (1,a) (a, 1) (1,1)</td></tr></table></body></html>  

Hopefully you can immediately see that this group is isomorphic to the vierergruppe $V$ via the correspondence $(1,1)\leftrightarrow1$ , $(a,1)\leftrightarrow a$ , $(1,a)\leftrightarrow b$ , and $(a,a)\leftrightarrow a b$ . Hence it is also isomorphic to the group of symmetries of the letter $H$ . $\diamondsuit$  

# Example 2.30  

The group $C_{2}\times C_{3}$ has 6 elemen $\mathrm{ts}\colon(1,1),(a,1),(1,b)(a,b),(1,b^{2})$ and $(a,b^{2})$ . We note that  

$$
\begin{array}{c}{{(a,b)^{2}=(1,b^{2})}}\\ {{(a,b)^{3}=(a,1)}}\\ {{(a,b)^{4}=(1,b)}}\\ {{(a,b)^{5}=(a,b^{2})}}\\ {{(a,b)^{6}=(1,1)}}\end{array}
$$  

So $C_{2}\times C_{3}=\langle(a,b)\rangle$ , and so it is a cyclic group. Hence $C_{2}\times C_{3}$ is isomorphic to the cyclic group of order $6$ , $C_{6}$ . $\diamondsuit$  

You may notice that in these example the groups are all Abelian. This is a consequence of the following proposition.  

# Proposition 2.17  

If $G$ and $H$ are Abelian groups, then so is $G\times H$ .  

Proof:  

Exercise.  

The converse to this proposition is also true, but it requires a lot more theory to get it in its nicest form.  

If we have several groups $G_{1}$ , $G_{2},\ldots,G_{n}$ , we can define the direct product $G_{1}\times G_{2}\times\cdot\cdot\cdot G_{n}$ in the obvious way. There is a fairly clear isomorphism between $(G_{1}\times G_{2})\times G_{3}$ , $G_{1}\times(G_{2}\times G_{3})$ , and $\vec{G}_{1}\times\vec{G}_{2}\times\vec{G}_{3}$ given by the correspondence  

$$
((g_{1},g_{2}),g_{3})\leftrightarrow(g_{1},(g_{2},g_{3}))\leftrightarrow(g_{1},g_{2},g_{3}).
$$  

So just as we consider the vector spaces $(V_{1}\oplus V_{2})\times V_{3}$ , $V_{1}\oplus(V_{2}\times V_{3})$ and $V_{1}\oplus V_{2}\times V_{3}$ as being the same vector space, we blur the distinction between the above direct products of groups and regard all three as the same group. With this in mind, the direct product is then associative. We will also write  

$$
G^{n}=\underbrace{G\times G\times\cdot\cdot\cdot\times G}_{n{\mathrm{~times}}}.
$$  

The following theorem will prove useful when we try to classify all the groups of a given order. It generalizes the isomorphism between $V$ and $C_{2}\times C_{2}$ .  

# Theorem 2.18  

Let $G$ be a finite group such that $x^{2}=1$ for every element $x\in G$ , and $|G|\geq2$ .   
Then $G$ is isomorphic to $C_{2}\times C_{2}\times\cdots\times C_{2}$ .  

Proof:  

We first observe that $G$ must be Abelian. Given any $x$ and $y\in G$ , we have  

$$
x y x y=(x y)^{2}=1.
$$  

But then  

$$
x=x1=x(x y x y)=x^{2}y x y x=y x y,
$$  

and so  

$$
y x=y(y x y)=y^{2}x y=x y.
$$  

We now find elements $a_{k}\in G$ by the following inductive construction:  

(i) Since $|G|\geq2$ , we can find some element $a_{1}\neq1$ . So $\langle a_{1}\rangle=\{1,a_{1}\}$ , since $a_{1}^{2}=1$ .   
(ii) Assume that we have found elements $a_{1}$ , $a_{2},\ldots,a_{r}$ , such that $a_{k}$ is not in $\langle a_{1},\ldots,a_{k-1}\rangle$ for all $k=2,\ldots,r$ . Then one of two things must be true: either $G=\langle a_{1},\ldots,a_{r}\rangle$ , or there is some $\boldsymbol{a}_{r+1}$ which is not in $\langle a_{1},\ldots,a_{r}\rangle$ . But then the elements $a_{1}$ , a2, . . . , ar, ar+1 satisfy the condition for r + 1.  

(iii) Proceeding inductively, we must eventually exhaust all the elements of $G$ .  

So we have that $G=\langle a_{1},a_{2},\ldots,a_{n}\rangle$ for elements $a_{1}$ , $a_{2},\ldots,a_{n}$ such that $a_{k}$ is not in $\langle a_{1},\ldots,a_{k-1}\rangle$ for all $k=2,\ldots,n$ . So any element $x\in G$ can be written as a product of powers of the elements $a_{1}$ , $a_{2},\ldots,a_{n}$ , and since $G$ is Abelian, we can move all the powers of $a_{1}$ to the front of the product, $a_{2}$ to the next term, and so on. So in general  

$$
x=a_{1}^{p(1)}a_{2}^{p(2)}\cdot\cdot\cdot a_{n}^{p(n)},
$$  

where $p(k)$ must be either $0$ or $^{1}$ . Moreover, this is the only way that the element $x$ can be written, since if we also have  

$$
x=a_{1}^{r(1)}a_{2}^{r(2)}\cdot\cdot\cdot a_{n}^{r(n)},
$$  

then  

$$
\begin{array}{l}{{1=a_{1}^{p(1)}a_{2}^{p(2)}\cdot\cdot\cdot a_{n}^{p(n)}(a_{1}^{r(1)}a_{2}^{r(2)}\cdot\cdot\cdot a_{n}^{r(n)})^{-1}}}\\ {{\quad=a_{1}^{p(1)-r(1)}a_{2}^{p(2)-r(2)}\cdot\cdot\cdot a_{n}^{p(n)-r(n)}.}}\end{array}
$$  

But this implies that  

$$
a_{n}^{p(n)-r(n)}=a_{1}^{r(1)-p(1)}a_{2}^{r(2)-p(2)}\cdot\cdot\cdot a_{n-1}^{r(n-1)-p(n-1)},
$$  

and so $p(n)-r(n)=0$ , since if $p(n)-r(n)=1$ , then $a_{n}$ would be generated by $a_{1}$ , $a_{2},\ldots,a_{n-1}$ , which contradicts our construction. So  

$$
1=a_{1}^{p(1)-r(1)}a_{2}^{p(2)-r(2)}\cdot\cdot\cdot a_{n}^{p(n-1)-r(n-1)},
$$  

and the same argument as for $n$ shows that $p(n-1)-r(n-1)=0$ .  

Proceeding inductively, we have that $p(k)-r(k)=0$ for all $k$ . Hence $p(k)=$ $r(k)$ for all $k$ , and so there is only one such way to write $x$ as a product of powers of $a_{1}$ , $a_{2},\ldots,a_{n}$ in that order.  

Now we can think of $C_{2}=\{1,a\}$ , and so  

$$
\underbrace{C_{2}\times C_{2}\times\dots\times C_{2}}_{n\mathrm{times}}=\{(a^{p(1)},a^{p(2)},\dots,a^{p(n)}):p(k)\in\{0,1\}\}.
$$  

But we have a correspondence  

$$
a_{1}^{p(1)}a_{2}^{p(2)}\cdot\cdot\cdot a_{n}^{p(n)}\leftrightarrow(a^{p(1)},a^{p(2)},\cdot\cdot..,a^{p(n)}),
$$  

and if you compare typical entries of the Cayley table you get  

<html><body><table><tr><td></td><td>r(1)_r(2) r(n) a1 a2 an</td></tr><tr><td>p(1) p(2) p(n) a2 a1 an</td><td>p(1)+r(1)p(2)+r(2) p(n)+r(n) a a2 an</td></tr></table></body></html>  

and  

<html><body><table><tr><td></td><td>(ar(1),ar(2),...,ar(n))</td></tr><tr><td>(ap(1),ap(2), ....,ap(n))</td><td>(ap(1)+r(1),ap(2)+r(2),...,ap(n)+r(n)</td></tr></table></body></html>  

and so the Cayley tables correspond. Hence $G\cong C_{2}^{n}$ .  

# Corollary 2.19  

Let $G$ be a finite group such that $x^{2}=1$ for every element $x\in G$ , and $|G|\geq2$ .   
Then $|G|=2^{n}$ for some $n$ .  

# Exercises  

2.7.1. Show that $D_{2n}$ and $C_{2}\times C_{n}$ are not isomorphic for $n>2$ .  

2.7.2. Write down the Cayley tables of $C_{2}\times C_{2}\times C_{2}$ and $C_{2}\times C_{4}$ . Show that these two groups are not isomorphic. Show that $D_{8}$ and $C_{2}\times C_{2}\times C_{2}$ are not isomporhic.   
2.7.3. Let $p$ and $q$ be prime numbers. Show that $C_{p}\times C_{q}\cong C_{p q}$ . More generally, show that if $p$ and $q$ are coprime, $C_{p}\times C_{q}\cong C_{p q}$ .   
2.7.4. Let $G$ be an Abelian group where $x^{3}=1$ for every $x\in G$ . Show that $G\cong C_{3}\times C_{3}\times\cdot\cdot\cdot\times C_{3}$ .   
2.7.5. ( $^{\ast\ast}$ ) Find a group $G$ such that $x^{3}=1$ for every $x\in G$ , but $G\not\cong C_{3}\times C_{3}\times$ C3  

# 2.8 Subgroups  

Groups often contain other groups. You should be at least intuitively aware of this fact, since the various additive groups of numbers are contained in one another:  

$$
\mathbb{Z}\subset\mathbb{Q}\subset\mathbb{R}\subset\mathbb{C}.
$$  

Knowing the groups which are contained within a particular group can tell you a lot about the group.  

# Definition 2.5  

Let $(G,*,e)$ be a group, and $H\subseteq G$ . We say $H$ is a subgroup of $G$ if $(H,*,e)$ is a group (where $*$ is restricted to $H$ ).  

We denote this relationship by writing $H\ \leq\ G$ . If $H\subset G$ , then we may write $H<G$ .  

In general, we do not expect an arbitrary subset of a group to be a group. In particular, we have to at least have $e\in H$ . Furthermore, for $*$ to be a binary operation when restricted to $H$ , it needs to be closed on $H$ . In other words, the product of elements of $H$ must again be an element of $H$ . Finally, we need that the inverse of every element of $H$ is an element of $H$ . But the good news is that we don’t need to check associativity: that is guaranteed by the associativity of $^*$ as an operation on $G$ . To summarize:  

# Theorem 2.20  

Let $G=(G,*e)$ be a group, and $H\subseteq G$ . If for every $x$ and $y\in H$ we have  

(i) $x*y\in H$ , and (ii) $x^{-1}\in H$ ,  

then $H$ is a subgroup.  

Proof:  

Condition (i) tells us that $*:H\times H\to H$ , so $^*$ is a binary operation on $H$ . Associativity is simple since $^*$ is associative on $G$ , so the axiom still holds for a subset. Since $x^{-1}\in H$ , we have both the inverse axiom, and the identity being an element of $H$ , since $e=x^{-1}*x\in H$ by (i). And $e$ is still an identity for $^*$ on $H$ , since it satisfies the identity axiom for all elements of $G$ , including those in $H$ .  

In fact, we can make this theorem even slicker by combining the two conditions into one:  

# Corollary 2.21  

Let $G=(G,*e)$ be a group, and $H\subseteq G$ . If for every $x$ and $y\in H$ we have $x y^{-1}\in H$ , then $H$ is a subgroup.  

Proof:  

We note that if $x\in H$ , then $x x^{-1}=e\in H$ , and hence $x^{-1}=e x^{-1}\in H$ , giving condition (i) of the theorem.  

Additionally, since $y^{-1}\in H$ , $x y=x(y^{-1})^{-1}\in H$ , giving condition (ii) of the theorem.  

Hence $H$ is a subgroup.  

Note that if we use additive notation for a group, the conditions of Theorem 2.20 become  

(i) $x+y\in H$ , and (ii) $-x\in H$ ,  

while the condition for Corollary 2.21 becomes $x-y\in H$ .  

We immediately note that a group $G$ is always a subgroup of itself, and the set containing just the identity $\{e\}$ is always a group. These two subgroups are  

called the trivial subgroups of $G$ . If $H$ is a subgroup of $G$ which not trivial, we say that $H$ is a proper subgroup.  

For finite groups, the Cayley table of a subgroup is simply the Cayley table of the whole group with every row and column corresponding to elements not in the subgroup being removed.  

# Example 2.31  

The cyclic group of order 3 $C_{3}=\{1,a,a^{2}\}$ has the subgroups $\{1\}$ , and $\{1,a,a^{2}\}$ . It has no proper subgroups.  

You can see that other subsets are not subgroups be inspection. For example, the set $\{1,a^{2}\}$ is not a subgroup because $a^{2}a^{2}=a^{4}=a$ , and $a$ is not an element of the set. $\diamondsuit$  

# Example 2.32  

The cyclic group of order 4 $C_{4}=\{1,a,a^{2},a^{3}\}$ has the subgroups $\{1\}$ , $\langle a^{2}\rangle=\{1,a^{2}\}$ and $\{1,a,a^{2},a^{3}\}$ .  

You can verify that $\{1,a^{2}\}$ is a subgroup by calculating every possible value of $x y^{-1}$ for $x$ and $y\in\{1,a^{2}\}$ :  

$$
\begin{array}{r l}{{1\cdot1^{-1}=1}}&{{\quad1\cdot(a^{2})^{-1}=a^{-2}=a^{2}}}\\ {{a^{2}\cdot1^{-1}=a^{2}}}&{{\ a^{2}\cdot(a^{2})^{-1}=1.}}\end{array}
$$  

In fact, we really only need to look at products where neither $x$ nor $y$ is $^{1}$ , since those always leave the other term alone. We will see shortly that the fact that this is a set generated by an element guarantees that it is a subgroup.  

These subgroups are isomorphic to $C_{1}$ , $C_{2}$ and $C_{4}$ respectively.  

# Example 2.33  

The vierergruppe $V=\{1,a,b,a b\}$ has the subgroups $\{1\}$ , $\langle a\rangle=\{1,a\}$ , $\langle b\rangle=\{1,b\}$ , $\langle a b\rangle=\{1,a b\}$ and $\{1,a,b,a b\}$ .  

These subgroups are isomorphic to $C_{1}$ , $C_{2}$ , $C_{2}$ , $C_{2}$ , and $V$ respectively.  

# Example 2.34  

The cyclic group or order $6$ , $C_{6}=\{1,a,a^{2},a^{3},a^{4},a^{5}\}$ has the subgroups $\{1\}$ , $\langle a^{3}\rangle=\{1,a^{3}\}$ , $\langle a^{2}\rangle=\{1,a^{2},a^{4}\}$ , and $C_{6}$ .  

These subgroups are isomorphic to $C_{1}$ , $C_{2}$ , $C_{3}$ , $C_{6}$ , and $V$ respectively.  

# Example 2.35  

Let $p$ be a prime number, and $C_{p}$ the cyclic group of order $p$ . Since $a^{k}$ generates $C_{p}$ for all $k\neq0$ , any subgroup which contains any element other than 1 must automatically contain all of $C_{p}$ . Hence $C_{p}$ only has the trivial subgroups $\{1\}$ and $C_{p}$ .  

# Example 2.36  

If $s\in\mathbb R$ , then the set $\{s n:n\in\mathbb{Z}\}$ is a subgroup of the additive group of real numbers $(\mathbb{R},+,0)$ . This follows because if we take two typical elements $n s$ and $_{m s}$ , then  

$$
n s-m s=(n-m)s,
$$  

and this is an element of the set $\{s n:n\in\mathbb{Z}\}$ .  

# Example 2.37  

We have that $S L_{n}(\mathbb{R})$ , $O_{n}(\mathbb{R})$ and $S O_{n}(\mathbb{R})$ are all proper subgroups of   
$G L_{n}(\mathbb{R})$ . In fact $S O_{n}(\mathbb{R})$ is also a proper subgroup of both $S L_{n}(\mathbb{R})$ and $O_{n}(\mathbb{R})$ . We know that these are subgroups, since we showed that they were groups   
under matrix multiplication in an earlier example. $\diamondsuit$  

# Example 2.38  

The alternating group $A_{n}$ is a subgroup of the corresponding symmetric group $S_{n}$ .  

# Example 2.39  

If $G$ and $H$ are groups, then the subset $\{(x,e):x\in G\}$ of $G\times H$ is a subgroup of $G\times H$ . Similarly, $\{(e,y):y\in H\}$ is a subgroup of $G\times H$ . $\diamondsuit$  

# Example 2.40  

If $G$ is any finite group and $x\in G$ , the set $\langle x\rangle$ is always a subgroup. This follows since $x^{n}(x^{m})^{-1}=x^{n-m}$ , which is a power of $x$ and so is an element of $\langle x\rangle$ , and Corollary 2.21 tells us $\langle x\rangle$ is a subgroup. $\diamondsuit$  

In fact, we can extend the last example to the set generated by any set of generators.  

# Theorem 2.22  

Let $(G,*,e)$ be a group, and $X\subseteq G$ . Then $\langle X\rangle$ is the smallest subgroup of $G$ which contains $X$ .  

Proof:  

First we must show that $\langle X\rangle$ is a subgroup. This set consists of all products of powers of elements of $X$ . If we have  

$$
x=x_{1}^{p_{1}}x_{2}^{p_{2}}\cdot\cdot\cdot x_{n}^{p_{n}}\qquad\mathrm{and}\qquady=y_{1}^{q_{1}}y_{2}^{q_{2}}\cdot\cdot\cdot y_{m}^{q_{m}},
$$  

where $x_{k}$ and $y_{l}\in X$ , $p_{k}$ and $q_{l}\in\mathbb{Z}$ , then we have that  

$$
\begin{array}{c}{(x_{n}^{-p_{n}}x_{n-1}^{-p_{n-1}}\cdot\cdot\cdot x_{1}^{-p_{1}})x=x_{n}^{-p_{n}}x_{n-1}^{-p_{n-1}}\cdot\cdot\cdot x_{1}^{-p_{1}}x_{1}^{p_{1}}x_{2}^{p_{2}}\cdot\cdot\cdot x_{n}^{p_{n}}}\\ {=x_{n}^{-p_{n}}x_{n-1}^{-p_{n-1}}\cdot\cdot\cdot x_{2}^{-p_{2}}x_{2}^{p_{2}}\cdot\cdot\cdot x_{n}^{p_{n}}}\\ {\vdots}\\ {=x_{n}^{-p_{n}}x_{n}^{p_{n}}=e,}\end{array}
$$  

Finding Subgroups: To find all the subgroups of a finite group, look at the subgroups generated by each element, then look at the subgroups generated by pairs of elements, then triples of elements, and so on. This procedure works because of Theorem 2.22.  

You can cut down the number of generating sets you need to check by noticing that if an element is in a subgroup, adding it to the set of generators of that subgroup gives nothing new.  

May 3, 2004  

so x−1 = x−pnx−pn−1 , which is a product of powers of elements of $X$ , so $x^{-1}\in\langle X\rangle$ . Similarly  

$$
x y=x_{1}^{p_{1}}x_{2}^{p_{2}}\cdot\cdot\cdot x_{n}^{p_{n}}y_{1}^{q_{1}}y_{2}^{q_{2}}\cdot\cdot\cdot y_{m}^{q_{m}}
$$  

is a product of powers of elements of $X$ , so $x y\in\left\langle X\right\rangle$ .  

So $\langle X\rangle$ satisfies conditions (i) and (ii) of Theorem 2.20, so it is a subgroup of $G$ .  

Now assume that there is some subgroup $H$ of $G$ with $X\subseteq H\subset\langle X\rangle$ . Then we can find some $x\in\langle X\rangle\setminus H$ , so  

$$
x=x_{1}^{p_{1}}x_{2}^{p_{2}}\cdot\cdot\cdot x_{n}^{p_{n}}
$$  

where $x_{k}\in X$ , $p_{k}\in\mathbb{Z}$ . A simple induction argument shows that $x_{k}^{p_{k}}\in H$ for all it is a product of elements of the s $k$ , no matter what power we have of $p_{k}$ . But this means that group $H$ , and similarly $x_{1}^{p_{1}}x_{2}^{p_{2}}\in H$ $x_{1}^{p_{1}}x_{2}^{p_{2}}x_{3}^{p_{3}}\in H$ , since Proceeding inductively, we have x1p x2p · · · xpnn ∈ H, and so x ∈ H. This is a contradiction, and hence there is no subgroup $H$ .  

Therefore $\langle X\rangle$ is the smallest subgroup of $G$ containing $X$ .  

It is worth noting that some texts actually define $\langle X\rangle$ to be the smallest subgroup of $G$ containing $X$ .  

# Example 2.41  

The dihedral group of order 8, $D_{8}=\{1,a,a^{2},a^{3},b,a b,a^{2},a^{3}\}$ has subgroups  

$$
\begin{array}{c c}{{\langle1\rangle=\{1\}\cong C_{1},}}&{{\langle a^{2}\rangle=\{1,a^{2}\}\cong C_{2},}}\\ {{\langle b\rangle=\{1,b\}\cong C_{2},}}&{{\langle a b\rangle=\{1,a b\}\cong C_{2},}}\\ {{\langle a^{2}b\rangle=\{1,a^{2}b\}\cong C_{2},}}&{{\langle a^{3}b\rangle=\{1,a^{3}b\}\cong C_{2},}}\\ {{\langle a\rangle=\{1,a,a^{2},a^{3}\}\cong C_{4},}}&{{\langle a^{2},b\rangle=\{1,a^{2},b,a^{2}b\}\cong V,}}\\ {{\langle a^{2},a b\rangle=\{1,a^{2},a b,a^{3}b\}\cong V,}}&{{\langle a,b\rangle=D_{8}.}}\end{array}
$$  

# Example 2.42  

Consider $S_{3}=\{e,(1,2,3),(1,3,2),(1,2),(2,3),(1,3)\}$ . We can find all the subgroups of this group by looking at the sets generated by each element:  

$$
\begin{array}{c}{{\langle e\rangle=e}}\\ {{\langle(1,2,3)\rangle=\langle(1,3,2)\rangle=\{e,(1,2,3),(1,3,2)\}}}\\ {{\langle(1,2)\rangle=\{e,(1,2)\}}}\\ {{\langle(2,3)\rangle=\{e,(2,3)\}}}\\ {{\langle(1,3)\rangle=\{e,(1,3)\}}}\end{array}
$$  

May 3, 2004  

Now we need to consider the sets generated by pairs of elements. For example, the set $\langle(1,2,3),(1,2)\rangle$ contains the elements $e$ , $(1,2,3)$ , $(1,3,2)$ and $(1,2)$ as well as  

$$
(2,3)=(1,2,3)(1,2)\qquad(1,3)=(1,2)(1,2,3).
$$  

So $\langle(1,2,3),(1,2)\rangle=S_{3}$ . In fact, when we look at all possible pairings, we discover that  

$$
\langle(1,2,3),(1,2)\rangle=\langle(1,2,3),(2,3)\rangle=\langle(1,2,3),(1,3)\rangle=S_{3}
$$  

and  

$$
\langle(1,2),(2,3)\rangle=\langle(1,2),(1,3)\rangle=\langle(2,3),(1,3)\rangle=S_{3}.
$$  

So $S_{3}$ is the only other subgroup.  

Notice in all the above examples of finite groups, the order of any subgroup divides the order of the group. This is always true, but we need some new ideas before we can prove it.  

# Exercises  

2.8.1. Let $X$ be a subset of a group. Show that any one of the following is sufficient to show that $X$ is not a subgroup:  

(i) $e\not\in X$ , (ii) there is an $x\in X$ with $x^{-1}\not\in X$ , (iii) there is an $x$ and $y\in X$ with $x y\notin X$ , (iv) there is an $x$ and $y\in X$ with $x y^{-1}\not\in X$ .  

2.8.2. For each of the following groups, find all its subgroups. For each subgroup, determine if it is isomorphic to a known group.  

(i) $D_{6}$   
(ii) $C_{8}$   
(iii) $C_{2}\times C_{4}$   
(iv) $C_{2}\times C_{2}\times C_{2}$   
(v) $D_{10}$   
(vi) $D_{12}$   
(vii) $A_{4}$  

2.8.3. Let $s\in\mathbb R$ . Show that $\{n+m s:n,m\in\mathbb{Z}\}$ is a subgroup of $\mathbb{R}$ .  

2.8.4. Show that N is not a subgroup of $(\mathbb{Z},+,0)$ .  

2.8.5. Show that every subgroup of a cyclic group is a cyclic group. Show that every subgroup of an Abelian group is an Abelian group.  

2.8.6. Show that every group has a cyclic subgroup.  

# 2.9 Homomorphisms  

Recall from abstract linear algebra that a linear transformation is a function from one vector space to another which preserves vector addition and scalar multiplication, ie. $T:V\to W$ is a linear transformation if and only if  

$$
T(v+w)=T(v)+T(w)\qquad\mathrm{and}\qquadT(\lambda v)=\lambda T(v),
$$  

for all $v$ , $w\in V$ and $\lambda\in\mathbb{F}$ .  

The analogue for groups should, then, be a function which preserves the group operation.  

# Definition 2.6  

Let $(G,*,e)$ and $(H,\star,1)$ be groups. $A$ function $\alpha:G\rightarrow H$ is a (group) homomorphism if  

$$
\alpha(x*y)=\alpha(x)\star\alpha(y)
$$  

for all $x$ , $y\in X$ .  

When using the multiplicative notation for groups, we will often simply write this condition as  

$$
\alpha(x y)=\alpha(x)\alpha(y).
$$  

It is immediate from this definition that group homomorphisms preserve the identity and inverse.  

# Proposition 2.23  

Let $G$ and $H$ be groups and $\alpha:G\rightarrow H$ a homomorphism. Then  

(i) $\alpha(e_{G})=e_{H}$ , (ii) $\alpha(x^{-1})=(\alpha(x))^{-1}$ for all $x\in X$ .  

Proof:  

(i) Let $y=\alpha(x)$ for some $x\in G$ , so that  

$$
y\alpha(e_{G})=\alpha(x)\alpha(e_{G})=\alpha(x e_{G})=\alpha(x)=y=y e_{H}.
$$  

The cancellation law then tells us that $\alpha(e_{G})=e_{H}$ .  

(ii) Given any $x\in G$ , we have that  

$$
\alpha(x^{-1})\alpha(x)=\alpha(x^{-1}x)=\alpha(e_{G})=e_{H}.
$$  

So $\alpha(x^{-1})=\alpha(x)^{-1}$ .  

Using this proposition we can show, using induction if needed, that  

$$
\alpha(x^{n})=\alpha(x)^{n}
$$  

for any $n\in\mathbb{Z}$ .  

May 3, 2004  

# Example 2.43  

Let $V=\{1,a,b,a b\}$ be the four-group, and $C_{4}=\{1,a,a^{2},a^{3}\}$ be the cyclic group of order 4. Consider the function $\alpha:V\rightarrow C_{4}$ definied by the following table:  

<html><body><table><tr><td>C</td><td>α(x)</td></tr><tr><td>1</td><td>1</td></tr><tr><td>a</td><td>a2</td></tr><tr><td>b</td><td>a?</td></tr><tr><td>ab</td><td>1</td></tr></table></body></html>  

Checking by hand, we can verify that this is indeed a homomorphism. For example ${a}^{2}=1$ in $V$ so $\alpha(a^{2})=\alpha(1)=1$ , and $\alpha(a)\alpha(a)=a^{2}a^{2}=1$ . $\diamondsuit$  

# Example 2.44  

If we look at the groups $\mathbb{Z}_{3}$ , where the group operation is addition modulo 3, and the group $C_{6}=\{1,a,a^{2},a^{3},a^{4},a^{5}\}$ , then we have the following homomorphisms from $\mathbb{Z}_{3}\to C_{6}$ :  

$$
\begin{array}{l}{\alpha(x)=1}\\ {\beta(x)=a^{2x}}\\ {\gamma(x)=a^{-2x}}\end{array}
$$  

To verify that $\beta$ is a homomoprhism, for example, we need to check that $\beta(x+$ $y)=\beta(x)\beta(y)$ :  

$$
{\begin{array}{r l}{\beta(x+y}&{{\mathrm{(mod~3)}})=a^{2(x+y\pmod{3})}=a^{2x+2y\pmod{6}}}\\ &{\beta(x)\beta(y)=a^{2x}a^{2y}=a^{2x+2y\pmod{6}},}\end{array}}
$$  

noting that in $C_{6}$ , $a^{x}a^{y}=a^{x+y}$ (mod 6). So $\beta$ is a homomorphism. You could also verify this fact by case-by-case checking of results.  

On the other hand, the function $\delta:\mathbb{Z}_{3}\to C_{6}$ defined by $\delta(x)=a^{x}$ is not a homomorphism, because  

Note: Observe here that $\mathbb{Z}_{3}$ uses additive notation, while $C_{6}$ uses multiplicative notation, and we need to use the appropriate form of notation when verifying that we have a homomorphism.  

$$
\delta(1+2{\pmod{3}})=\delta(0)=a^{0}=1,
$$  

but  

$$
\delta(1)\delta(2)=a^{1}a^{2}=a^{3}\neq1.
$$  

# Example 2.45  

Let $G L_{n}(\mathbb{R})$ be the group of all invertible real-valued matrices, and consider the determinant function $\operatorname*{det}:G L_{n}(\mathbb{R})\to R^{\times}$ given by  

$$
\operatorname*{det}(A)=|A|.
$$  

May 3, 2004  

Since  

$$
\operatorname*{det}(A B)=|A B|=|A||B|=\operatorname*{det}(A)\operatorname*{det}(B),
$$  

this is a homomorphism.  

If we once again consider the analogy with abstract linear algebra, you may recall that the image of a linear subspace under a linear transformation is a subspace of the range. If the analogy with linear algebra is to hold, the same thing ought to be true for subgroups.  

# Proposition 2.24  

If $G$ and $H$ are groups, $\alpha:G\rightarrow H$ is a homomorphism, and $K$ is a subgroup of $G$ , then $\alpha(K)$ is a subgroup of $H$ .  

Proof:  

Let $x$ , $y\in\alpha(K)$ , so that there are some $u$ and $v\in K$ such that $x=\alpha(u)$ and $y=\alpha(v)$ . Then, noting that $u v^{-1}\in K$ ,  

$$
x y^{-1}=\alpha(u)(\alpha(v))^{-1}=\alpha(u v^{-1})\in\alpha(K).
$$  

So by Corollary 2.21, $\alpha(K)\leq H$ .  

# Example 2.46  

We know from Example 2.33 that the subgroups of $V$ are $\{1\},\{1,a\},\{1,b\}$ , $\left\{1,a b\right\}$ and the whole group $V$ . The images of these sets under the homomorphism $\alpha$ of Example 2.43 are $\{1\},\{1,a^{2}\},\{1,a^{2}\},\{1\}$ and $\{1,a^{2}\}$ , respectively. $\diamondsuit$  

Since $G\leq G$ , the following is an immediate corollary of the proposition.  

# Corollary 2.25  

$I f G$ and $H$ are groups, and $\alpha:G\to H$ is a homomorphism, then $\alpha(G)$ is a subgroup of $H$ .  

In other words, the image of a homomorphism is a subgroup of the codomain We have a similar result for inverse images of subgroups.  

# Proposition 2.26  

If $G$ and $H$ are groups, $\alpha:G\rightarrow H$ is a homomorphism, and $K$ is a subgroup of $H$ , then $\alpha^{-1}(K)$ is a subgroup of $G$ .  

Proof:  

Let $x$ , $y\in\alpha^{-1}(K)$ , so that there are some $u$ and $v\in K$ such that $u=\alpha(x)$ and $v=\alpha(y)$ . Then,  

$$
\alpha(x y^{-1})=\alpha(x)(\alpha(y))^{-1}=u v^{-1}\in K,
$$  

so $x y^{-1}\in\alpha^{-1}(K)$ . Therefore by Corollary 2.21, $\alpha^{-1}(K)\leq G$ .  

You may also recall from linear algebra that the kernel of a linear transformation is a subspace of the domain. This leads us to the following definition and corollary.  

# Definition 2.7  

If $G$ and $H$ are groups, and $\alpha:G\rightarrow H$ is a homomorphism, then the kernel of $\alpha$ is the set  

$$
\ker\alpha=\left\{x\in X:\alpha(x)=e_{H}\right\}
$$  

of all elements of the group $G$ whose image is the identity.  

# Corollary 2.27  

If $G$ and $H$ are groups, and $\alpha:G\rightarrow H$ is a homomorphism, then $\ker\alpha$ is a subgroup of $G$ .  

Proof:  

Notice that $\ker\alpha=\alpha^{-1}(\{e_{H}\})$ , and $\left\{e_{H}\right\}\leq H$ , so by Proposition 2.26, $\ker\alpha\leq G$ .  

# Example 2.47  

In Example 2.43, the kernel of the homomorphism is $\left\{1,a b\right\}$ , which is a subgroup of $V$ , and the image of the homomorphism is $\{1,a^{2}\}$ , which is a subgroup of $C_{4}$ . $\diamondsuit$  

# Example 2.48  

The kernel of the determinant map from the previous example is the set of all matrices whose determinant is $^{1}$ , ie.  

$$
\ker\operatorname*{det}=S L_{n}(\mathbb{R}).
$$  

On the other hand, there are matrices whose determinant is any number you choose, so the image of $G L_{n}(\mathbb{R})$ under det is all of $\mathbb{R}\backslash\{0\}$ . 3  

# Example 2.49  

Parity can be regarded as a function that takes permutations to elements of the multiplicative group of integers $\mathbb{Z}\backslash\{0\}$ . Theorem 1.6 tells us that this is a homomorphism.  

The kernel of parity is the subgroup $A_{n}$ of all permutations whose parity is 1, ie.  

$$
\mathrm{kerparity}=A_{n}.
$$  

On the other hand, the image of $S_{n}$ under the parity homomorphism is simply the set $\{1,-1\}$ , which is a subgroup of the multiplicative group of integers. $\diamondsuit$  

Just as we are particularly interested in functions which are one-to-one, onto, or bijective, we are interested in homomorphisms which are one-to-one, onto, or bijective.  

# Definition 2.8  

Let $G$ and $H$ be groups, and $\alpha:G\rightarrow H$ a homomorphism. If $\alpha$ is one-to-one (or injective), then we say that it is a monomorphism. If $\alpha$ is onto (or sujective), then we say that it is an epimorphism. If $\alpha$ is a bijection, then we say that it is an isomorphism.  

If $\alpha:G\to G$ is an isomorphism, then we call $\alpha$ an automorphism. The set of all automorphisms of $G$ is denoted $\operatorname{Aut}(G)$ .  

This definition of isomorphism agrees with the definition that we have been using, but can also be applied to infinite groups. The concept of “correspondence” of elements that we have been informally using to translate between the two Cayley tables is, formally, a bijection; while the fact that the Cayley tables correspond means that the group operation is preserved by the bijection, giving us a homomorphism. If there is an isomorphism between two groups $G$ and $H$ , then we say that the groups are isomorphic and write $G\cong H$ as usual.  

# Example 2.50  

Consider the additive group of reals numbers, $(\mathbb{R},+,0)$ , and the multiplicative group of positive real numbers, $(\mathbb{R}^{+},\times,1)$ . The exponential function  

$$
\exp:x\mapsto e^{x}
$$  

is a homomorphism between these two groups, since  

$$
\exp(x+y)=e^{x+y}=e^{x}e^{y}=\exp(x)\exp(y).
$$  

Furthermore, the exponential function is one-to-one (a fact you should be familiar with from elementary calculus), and the range of the exponential function is all positive real numbers. Hence exp is an isomorphism, and the two groups are isomorphic.  

In fact, this isomorphism is not the only possible choice. Any function of the form  

$$
x\mapsto a^{x}
$$  

for $a>0$ is an isomorphism. Going in the reverse direction, the corresponding logarithms  

$$
x\mapsto\log_{a}x,
$$  

regarded as functions from $\mathbb{R}^{+}$ to $\mathbb{R}$ , are also isomorphisms.  

The following proposition collects some useful facts about homomorphisms.  

# Proposition 2.28  

Let $G$ , $H$ and $K$ be groups, and let $\alpha:G\rightarrow H$ and $\beta:H\to K$ be homomorphisms.  

(i) $\beta\circ\alpha:G\rightarrow K$ is a homomorphism, (ii) if $\alpha$ and $\beta$ are monomorphisms, so is $\beta\circ\alpha$ ,  

May 3, 2004  

(iii) if $\alpha$ and $\beta$ are epimorphisms, so is $\beta\circ\alpha$ , (iv) if $\alpha$ and $\beta$ are isomorphisms, so is $\beta\circ\alpha$ , (v) if $\alpha$ is an isomorphism, so is the inverse function $\alpha^{-1}$ , (vi) $\alpha$ is a monomorphism if and only if ker $\alpha=\{e\}$ ,  

Proof:  

(i) We observe that  

$$
\beta(\alpha(x y))=\beta(\alpha(x)\alpha(y))=\beta(\alpha(x))\beta(\alpha(y)),
$$  

so $\beta\circ\alpha$ is a homomorphism.  

(ii–iv) These follow immediately from (i) which tells us that the composition is a homomorphism, and parts (ii–iv) of Proposition 1.2 which tell us that the composition is, respectively, one-to-one, onto, and bijective.  

(vi) If $\alpha$ is an isomorphism, then for any $x$ , $y\in H$ , we have $u$ and $v\in G$ such that $\alpha(u)=x$ and $\alpha(v)=y$ , so  

$$
x y=\alpha(u)\alpha(v)=\alpha(u v).
$$  

But $u=\alpha^{-1}(x)$ and $v=\alpha^{-1}(y)$ , so  

$$
\alpha^{-1}(x y)=\alpha^{-1}(\alpha(u v))=u v=\alpha^{-1}(x)\alpha^{-1}(y).
$$  

Hence $\alpha^{-1}$ is a homomorphism.  

Furthermore, parts (v) of Proposition 1.2 tells us that $\alpha^{-1}$ is a bijection, so $\alpha^{-1}$ is an isomorphism.  

(v) If $\alpha$ is a monomorphism and $\alpha(x)=e$ , then $\alpha(x)=\alpha(e)$ , and since $\alpha$ is injective, $x=e$ . So $\ker\alpha=\{e\}$ .  

On the other hand, if $\ker\alpha=\{e\}$ , then if we have $x$ and $y\in G$ such that $\alpha(x)=\alpha(y)$ , then  

$$
\alpha(x y^{-1})=\alpha(x)\alpha(y)^{-1}=\alpha(x)\alpha(x)^{-1}=e,
$$  

so $x y^{-1}\in\ker\alpha$ , and hence $x y^{-1}=e$ . But then  

$$
x=x y^{-1}y=e y=y,
$$  

so $\alpha$ is injective and hence a monomorphism.  

# Exercises  

2.9.1. Find all the homomorphisms from $C_{2}$ to $V$ . Describe the kernel and image of each.  

May 3, 2004  

2.9.2. Let $\mathbb{Z}$ be the additive group of integers. Show that for each $m\in\mathbb{Z}$ , the function $\alpha_{m}:\mathbb{Z}\to\mathbb{Z}$ , given by  

$$
\alpha_{m}(x)=m x
$$  

is a homomorphism. Show that if $m\neq0$ then $\alpha_{m}$ is a monomorphism.   
Show that it is an epimorphism if and only if $m=\pm1$ .  

2.9.3. Let $\mathbb{Q}$ be the additive group of rational numbers. Show that for each $r\in\mathbb{Q}$ , the function $\alpha_{r}:\mathbb{Q}\to\mathbb{Q}$ given by  

$$
\alpha_{r}(x)=r x
$$  

is a homomorphism. Show that if $r\neq0$ then $\alpha_{r}$ is an automorphism.  

2.9.4. Consider the group $(G,*,e)$ , where $G=\{(x,y):x,y\in\mathbb{R},x\neq0\}$ , $(x,y)*$ $(x^{\prime},y^{\prime})=(x x^{\prime},x y^{\prime}+y)$ and $e=(1,0)$ , and the group $(H,\cdot,I_{2})$ , where  

$$
H=\left\{{\left[\begin{array}{l l}{a}&{b}\\ {0}&{1}\end{array}\right]}:a,b\in\mathbb{R},a\neq0\right\},
$$  

is matrix multiplication, and $I_{2}$ is the 2 by 2 identity matrix. Show that $G$ and $H$ are isomorphic.  

2.9.5. Show that every isomorphism $\alpha:(\mathbb{R},+,0)\to(\mathbb{R}^{+},\times,1)$ satisfies  

$$
\alpha(x)=a^{x}
$$  

for some number $a>0$ .  

Show that every isomorphism $\alpha:(\mathbb{R}^{+},\times,1)\to(\mathbb{R},+,0)$ satisfies  

$$
\alpha(x)=\log_{a}x
$$  

for some number $a>0$ .  

2.9.6. Let $G$ and $H$ be two groups. Show that each of the following is a homomorphism:  

(i) $\alpha:G\rightarrow G\times H$ , where $\alpha(g)=(g,1)$ , (ii) $\alpha:H\rightarrow G\times H$ , where $\alpha(h)=(1,h)$ , (iii) $\alpha:G\times H\to G$ , where $\alpha(g,h)=g$ , (iv) $\alpha:G\times H\to H$ , where $\alpha(g,h)=h$ , (v) $\alpha:G\times H\to H\times G$ , where $\alpha(g,h)=(h,g)$ , (vi) $\alpha:G\to G\times G$ , where $\alpha(g)=(g,g)$ .  

Which of these are monomorphisms, which are epimorphisms, and which are isomorphisms?  

2.9.7. Find all the automorphisms of $V$ .  

2.9.8. Let $G$ be a group. Show that $(\operatorname{Aut}(G),\circ,\operatorname{id})$ is a group, where $\mathsf{O}$ is composition and id : $G\rightarrow G$ is the identity function $\operatorname{id}(x)=x$ for all $x\in G$ .  

# Assignment 2  

The following exercises are due Friday, March 5th.  

2.1 Exercises 1, 2.   
2.2 Exercises 2, 3, 5.   
2.3 Exercises 2, 3, 4.   
2.4 Exercises 1.  

# Assignment 3  

The following exercises are due Friday, March 12th.  

2.5 Exercises 1, 2, 7.   
2.7 Exercises 1, 4.   
2.8 Exercises 2, 3.   
2.9 Exercises 1, 2, 4, 7.  

# Chapter 3  

# The Structure of Groups  

We are interested in understanding the structure of groups, particularly finite groups, as a way of potentially distinguishing groups. In this section we will see a number of ways of looking at structure within a group.  

# 3.1 The Subgroup Lattice  

At a very coarse level, if two groups are isomorphic then their subgroups must be in bijective correspondence with one another.  

# Proposition 3.1  

Let $G$ and $H$ be two groups, and let $\operatorname{Sub}(G)$ and $\operatorname{Sub}(H)$ be the set of subgroups of $G$ and $H$ respectively. If $G\cong H$ then there is a bijection between $\operatorname{Sub}(G)$ and $\operatorname{Sub}(H)$ .  

Proof:  

Since $G\cong H$ there is an isomorphism $\alpha:G\rightarrow H$ , and its inverse function $\alpha^{-1}:H\rightarrow G$ is also an isomorphism by Proposition 2.28. Since by Proposition 2.24, the image $\alpha(K)$ of a subgroup $K$ of $G$ is a subgroup of $H$ , we can define a function  

$$
\begin{array}{r}{\overline{{\alpha}}:\operatorname{Sub}(G)\rightarrow\operatorname{Sub}(H)}\\ {K\mapsto\alpha(K).}\end{array}
$$  

The function $\overline{{\alpha}}$ is one-to-one, since if we have two subgroups $K_{1}$ and $K_{2}$ of $G$ such that $\overline{{\alpha}}(K_{1})=\overline{{\alpha}}(K_{2})$ , then  

$$
K_{1}=\alpha^{-1}(\alpha(K_{1}))=\alpha^{-1}(\overline{{{\alpha}}}(K_{1}))=\alpha^{-1}(\overline{{{\alpha}}}(K_{12}))=\alpha^{-1}(\alpha(K_{2}))=K_{2},
$$  

since $\alpha^{-1}\circ\alpha$ is the identity function.  

The function $\overline{{\alpha}}$ is onto, since if $K$ is a subgroup of $H$ , then $\alpha^{-1}(K)$ is a subgroup of $G$ , and  

$$
\overline{{{\alpha}}}(\alpha^{-1}(K))=\alpha(\alpha^{-1}(K))=K
$$  

since $\alpha\circ\alpha^{-1}$ is the identity function.  

So $\alpha$ is a bijection.  

# Corollary 3.2  

If $G$ and $H$ are finite groups with different numbers of subgroups, then $G$ and $H$ cannot be isomorphic.  

# Example 3.1  

From Example 2.32 $C_{4}$ , we know that $C_{4}$ has 3 subgroups. On the other hand, Example 2.33 tells us that $V$ has 5 subgroups, so $V$ is not isomorphic to C4.  

However, it is certainly conceivable that two groups may have the same number of subgroups, but fail to be isomorphic. In this case we need to investigate the relationships between subgroups of a group. For instance, if we have a group $G$ and subgroups $H$ and $K$ of $G$ such that $K\subseteq H$ , then it is immediate from Corollary 2.21 that $K$ is a subgroup of $H$ . So a good starting point is to consider which subgroups are contained in other subgroups.  

# Example 3.2  

From Example 2.32 $C_{4}=\{1,a,a^{2},a^{3}\}$ has the subgroups $\{1\}$ , $\langle a^{2}\rangle=\{1,a^{2}\}$ and $\{1,a,a^{2},a^{3}\}$ . We can easily see that $\{1\}\le\langle a^{2}\rangle\le C_{4}$ . $\diamondsuit$  

Noting that each of these is subgroups is contained in the next, we can represent this situation diagramatically as follows:  

$$
\begin{array}{c}{C_{4}}\\ {\big\vert}\\ {\langle a^{2}\rangle}\\ {\big\vert}\\ {\{1\}}\end{array}
$$  

This sort of diagram is a graphical representation of the subgroup lattice of the group. The idea is that if a subgroup is contained in another, with no intermediate subgroups, we write it higher on the page and join the two subgroups with a line. We also try to draw the diagram so that subgroups with the same number of elements are the same distance up the page.  

Here are some more examples.  

# Example 3.3  

From Example 2.33, the vierergruppe $V=\{1,a,b,a b\}$ has the subgroups $\{1\}$ , $\langle a\rangle=\{1,a\}$ , $\langle b\rangle=\{1,b\}$ , $\langle a b\rangle=\{1,a b\}$ and $\{1,a,b,a b\}$ .  

The subgroup lattice of $V$ is:  

![](images/8773e05f665b273ec2641823e3ca8d4f5f9fbb0389a3c4c2f735c31c61effa35.jpg)  

# Example 3.4  

From Example 2.34, the cyclic group or order 6, $C_{6}=\{1,a,a^{2},a^{3},a^{4},a^{5}\}$ has the subgroups $\{1\}$ , $\langle a^{3}\rangle=\{1,a^{3}\}$ , $\langle a^{2}\rangle=\{1,a^{2},a^{4}\}$ , and $C_{6}$ . The subgroup lattice of $C_{6}$ is:  

![](images/40822d5b45c3ebf01c304fe1e400bb1a8161a85fd2c61f86f16d00be04484d0c.jpg)  

# Example 3.5  

Let $p$ be a prime number, and $C_{p}$ the cyclic group of order $p$ . From Example 2.35 $C_{p}$ only has the trivial subgroups $\{1\}$ and $C_{p}$ . The subgroup lattice of $C_{p}$ is always:  

![](images/60e34af28397b8ce65ef54fe375d35680de919add8cc6e99fa8ec90ed4496e00.jpg)  

# Example 3.6  

From Example 2.41, the dihedral group of order 8, $D_{8}=\{1,a,a^{2},a^{3},b,a b,a^{2},a^{3}\}$ has subgroups $\{1\}$ , $\langle a^{2}\rangle~=~\{1,a^{2}\}$ , $\langle b\rangle~=~\{1,b\}$ , $\langle a b\rangle~=~\{1,a b\}$ , $\langle a^{2}b\rangle\ =$ $\{1,a^{2}b\}$ , $\langle a^{3}b\rangle=\{1,a^{3}b\}$ , $\langle a\rangle=\{1,a,a^{2},a^{3}\}$ , $\langle a^{2},b\rangle=\{1,a^{2},b,a^{2}b\}$ , $\langle a^{2},a b\rangle=$ $\{1,a^{2},a b,a^{3}b\}$ , and $D_{8}$ . The subgroup lattice of $D_{8}$ is:  

May 3, 2004  

![](images/2c834da439322756aa383a6b11324d94259609edc46c08d02934f0926272882c.jpg)  

Notice that in these diagrams, there is always a unique smallest subgroup which is bigger than any pair of subgroups. This is a corollary of Theorem 2.22.  

# Corollary 3.3  

$I f G$ is a group and $H$ and $K$ are subgroups of $G$ , then $\langle H\cup K\rangle$ is the smallest subgroup which contains both $H$ and $K$ .  

This subgroup is usually quite different from the union of the two sets. Indeed, we have the following:  

# Proposition 3.4  

Let $H$ and $K$ be subgroups of $G$ . Then $H\cup K$ is a subgroup if and only if either $H\subseteq K$ or $K\subseteq H$ .  

Proof:  

If $H\subseteq K$ , then $H\cup K=K$ , so $H\cup K$ is a subgroup. Similarly, if $K\subseteq H$ , then $H\cup K=H$ , so $H\cup K$ is a subgroup.  

Conversely, if neither $H$ nor $K$ is a subset of the other, then there is some $x\in H\backslash K$ and $y\in K\setminus H$ . Also $x^{-1}\in H$ , since $H$ is a subgroup. But then if $x y\in H$ , we have $x^{-1}(x y)=y\in H$ , but since $y\in K\setminus H$ , this means that $y\notin H$ , which is a contradicition. Therefore $x y\notin H$ . But a similar argument shows that $x y\notin K$ . So $x y\notin H\cup K$ . So $H\cup K$ is not a subgroup of $G$ . $\mid$  

The following theorem shows us that there is also a subgroup which is contained in both $H$ and $K$ .  

# Theorem 3.5  

Let $(G,*,e)$ be a group, and $H$ and $K$ subgroups of $G$ . Then $H\cap K$ is the largest subgroup of $G$ which is contained in both $H$ and $K$ .  

Proof:  

We first need to show that $H\cap K$ is a subgroup. If $x$ , $y\in H\cap K$ , then $x y^{-1}\in H$ , since $H$ is a subgroup, and $x y^{-1}\in K$ , since $K$ is a subgroup. Therefore $x y^{-1}\in H\cap K$ , and so by Corollary 2.21, $H\cap K$ is a subgroup of $G$ .  

Since $H\cap K$ is the largest set contained in both $H$ and $K$ , it must also be the largest subgroup contained in both. $|$  

We will denote $\langle H\cup K\rangle$ by $H\lor K$ and call it the $j o i n$ of $H$ and $K$ . We will denote $H\cap K$ by $H\wedge K$ , and call it the meet of $H$ and $K$ . The reason for this terminology will be come clear when we look at abstract lattices.  

The significance of the lattice of subgroups is that if two groups do not have similar lattices of subgroups, they cannot be isomorphic, so it provides a nice pictorial way of demonstrating that two groups are distinct. To show this, we first need to show that homomorphisms preserve the relationship of inclusion of subgroups.  

# Proposition 3.6  

If $G$ and $H$ are groups, $\alpha:G\rightarrow H$ is a homomorphism, and $K_{1}$ and $K_{2}$ are subgroups of $G$ with $K_{1}\subseteq K_{2}$ , then $\alpha(K_{1})$ is a subgroup of $\alpha(K_{2})$ .  

Furthermore, if $\alpha$ is a monomorphism, and $K_{1}\subset K_{2}$ , then $\alpha(K_{1})$ not equal to $\alpha(K_{2})$ .  

Proof:  

We know from Proposition 2.24 that $\alpha(K_{1})$ and $\alpha(K_{2})$ are subgroups of $H$ , and it is immediate from the definition of the image of a set under a function that $\alpha(K_{1})\subseteq\alpha(K_{2})$ if $K_{1}\subseteq K_{2}$ . So $\alpha(K_{1})\leq\alpha(K_{2})$ .  

If $\alpha$ is a monomorphism in addition, then since there is some $g\in K_{2}$ , but not in $K_{1}$ , we cannot have $\alpha(h)=\alpha(g)$ for any $h\in K_{1}$ (otherwise $\alpha$ would not be one-to-one). Hence $\alpha(K_{1})$ is properly contained in $\alpha(K_{2})$ . $\mid$  

# Corollary 3.7  

If $G$ and $H$ are groups, $\alpha:G\to H$ is a homomorphism, and $K_{1}$ and $K_{2}$ are subgroups of $G$ , then $\alpha(K_{1}\vee K_{2})=\alpha(K_{1})\vee\alpha(K_{2})$ and $\alpha(K_{1}\wedge K_{2})=$ ${\alpha(K_{1})\wedge\alpha(K_{2})}$ .  

Proof:  

We know that $\alpha(K_{1})\vee\alpha(K_{2})$ is the smallest subgroup which contains both $\alpha(K_{1})$ and $\alpha(K_{2})$ , but since $K_{1}$ and $K_{2}\subseteq K_{1}\vee K_{2}$ , we have that $\alpha(K_{1})$ and $\alpha(K_{2})\subseteq\alpha(K_{1}\vee K_{2})$ , hence $\alpha(K_{1})\vee\alpha(K_{2})\subseteq\alpha(K_{1}\vee K_{2})$ .  

Conversely, if $\alpha^{-1}(\alpha(K_{1})\vee\alpha(K_{2}))$ is a subgroup of $G$ which contains both $K_{1}$ and $K_{2}$ , so ${\cal K}_{1}\vee{\cal K}_{2}\subseteq\alpha^{-1}(\alpha({\cal K}_{1})\vee\alpha({\cal K}_{2}))$ , and hence  

$$
\alpha(K_{1}\vee K_{2})\subseteq\alpha(\alpha^{-1}(\alpha(K_{1})\vee\alpha(K_{2})))=\alpha(K_{1})\vee\alpha(K_{2}).
$$  

Hence $\alpha(K_{1})\vee\alpha(K_{2})=\alpha(K_{1}\vee K_{2})$ .  

The proof of the case for $\wedge$ is left as an exercise.  

We will say that two groups $G$ and $H$ have corresponding, or isomorphic, subgroup lattices if there is a bijection $f$ from $\operatorname{Sub}(G)$ to $\operatorname{Sub}(H)$ so that $f(K_{1})\vee$ $f(K_{2})=f(K_{1}\vee K_{2})$ , and $f(K_{1})\wedge f(K_{2})=f(K_{1}\wedge K_{2})$ .  

# Corollary 3.8  

If $G$ and $H$ are two groups whose subgroup lattices do not correspond, then $G$ and $H$ are not isomorphic.  

Proof:  

If $G$ and $H$ are isomorphic, then Proposition 3.1 tells us that $\overline{{\alpha}}$ is a bijection from $\operatorname{Sub}(G)$ to $\operatorname{Sub}(H)$ , and Corollary 3.7 says that $\overline{{{\alpha}}}(K_{1}\vee K_{2})=$ $\overline{{\alpha}}(K_{1})\vee\overline{{\alpha}}(K_{2})$ and $\overline{{{\alpha}}}(K_{1}\wedge K_{2})=\overline{{{\alpha}}}(K_{1})\wedge\overline{{{\alpha}}}(K_{2})$ . So isomorphic groups have corresponding subgroup lattices.  

The contrapositive of this result is the corollary.  

# Example 3.7  

The groups $C_{4}$ and $V$ have different subgroup lattices, so they are not isomorphic. 3  

Note that the converse of the corollary is not true. We know, for example, that if $p$ is prime, the groups $C_{p}$ all have corresponding subgroup lattices, yet the groups are clearly not isomorphic.  

# Exercises  

3.1.1. Find the subgroup lattice of the group $D_{6}$ .   
3.1.2. Find the subgroup lattice of the group $C_{8}$ .   
3.1.3. Find the subgroup lattice of the group $C_{2}\times C_{4}$ .   
3.1.4. Find the subgroup lattice of the group $C_{2}\times C_{2}\times C_{2}$ .   
3.1.5. Find the subgroup lattice of the group $D_{10}$ .   
3.1.6. Find the subgroup lattice of the group $D_{12}$ .   
3.1.7. Find the subgroup lattice of the group $A_{4}$   
3.1.8. Complete the proof of Corollary 3.7.  

# 3.2 Extension: Lattices  

The pattern that subgroups of a group make under inclusion is a particular example of a general phenomenon. The key idea is that we know when one subgroup is “larger” than another, that we can find the largest thing smaller than two subgroups (the meet) and that we can find the smallest thing larger than the two subgroups (the join).  

This idea is essentially the same as what happens with general subsets of a set. We know when one subset is “larger” than another, we can find the largest thing smaller than two subsets (the intersection) and we can find the smallest thing larger than the two subsets (the union).  

To explore this similarity further, we need to introduce a general concept that we can use to model the idea of one thing being larger then another.  

# Definition 3.1  

Let $X$ and $Y$ be sets. A relation between $X$ and $Y$ is a subset $R$ of $X\times Y$ , where $x\in X$ and $y\in Y$ are considered to be related by $R$ if and only i ${\vec{\mathbf{\sigma}}}(x,y)\in R$ . We write xRy if $(x,y)\in R$ . If $X=Y$ we say that $R$ is a relation on $X$ .  

Note that you should not confuse this definition of relation with the notion of a relation on the elements of a group.  

The concept of a relation is extremely general, and can be used to model a great many fundamental mathematical concepts.  

# Example 3.8  

If $X$ is any set, equality can be regarded as the relation $R=\{(x,x):x\in$ $X\}\subseteq X\times X$ . Here $x R y$ if and only if $(x,y)\in R$ if and only if $x=y$ . 3  

# Example 3.9  

If $X$ and $Y$ are any sets and $f:X\to Y$ is a function, the graph $R=$ $\left\{(x,f(x)):x\in X\right\}\subset X\times Y$ is a relation where $x R y$ if and only if $y=f(x)$ . In fact functions are sometimes defined in this way as a special case of the concept of a relation. $\diamondsuit$  

# Example 3.10  

In the real numbers, the set $L=\{(x,y):x\leq y\}\in\mathbb{R}\times\mathbb{R}$ is a relation where $x L y$ if and only if $x\leq y$ . $\diamondsuit$  

# Example 3.11  

If ${\mathcal{P}}(X)$ is the power set of some set $X$ , then the set $\subseteq=\{(A,B):A$ is a subset of $B\}$ is a relation where $A\subseteq B$ if and only if $A$ is a subset of $B$ . 3  

# Example 3.12  

If $G$ is a group, and $X$ is the set of subgroups of $G$ , then the set $\underline{{<}}=\left\{(A,B):\right.$ $A$ is a subgroup of $B\}$ is a relation where $A\le B$ if and only $A$ is a subgroup of $B$ . 3  

Because of the generality of relations, we need to impose some additional conditions to make them useful in modelling particular situations.  

# Definition 3.2  

Let $X$ and $Y$ be sets, and $R$ a relation on $X$ . Let $x,y$ and $z\in X$ . We say that $R$ is reflexive if $x R x$ for all $x\in X$ . We say that $R$ is symmetric if xRy implies $y R x$ . We say that $R$ is antisymmetric if $x R y$ and yRx implies $x=y$ .  

We say that $R$ is transitive if $x R y$ and $y R z$ implies $x R z$ .  

A partial order on $X$ is a relation on $X$ which is reflexive, transitive and antisymmetric.  

# Example 3.13  

The $\leq$ , $<$ , $>$ and $\geq$ relations on any set of numbers. All are antisymmetric and transitive, but not symmetric. The relations $\leq$ and $\geq$ are reflexive, but $<$ and $>$ are not. $\diamondsuit$  

# Example 3.14  

The relation $\subseteq$ of ${\mathcal{P}}(X)$ is a partial order.  

# Example 3.15  

The relation $\leq$ on the set of subgroups of a group is a partial order.  

Notice that if $\preceq$ is a partial order on a set $X$ , and $x$ , $y\in X$ , then it may be the case that neither $x\preceq y$ nor $y\preceq x$ . If this is the case, we say that $x$ and $y$ are incomparable elements of $X$ .  

# Example 3.16  

The sets $\{1\}$ and $\{2,3\}$ are incomparable elements of $\mathcal{P}(\{1,2,3\})$ under the subset partial order $\subseteq$ . $\diamondsuit$  

So a partial order seem to encapsulate the general idea of something being bigger than something else. Now we need to model the idea of a meet and a join.  

# Definition 3.3  

$I f X$ is a set, $\preceq$ is a partial order on $X$ , and $x$ and $y\in X$ , then $a\in X$ is a lower bound for $x$ and $y$ if $a\preceq x$ and $a\preceq y$ . We say that $a$ is the greatest lower bound of $x$ and $y$ if given any lower bound $z$ of $x$ and $y$ , we have that $z\preceq a$ .  

Similarly, $a$ is an upper bound for $x$ and $y$ if $x\preceq a$ and $y\preceq a$ . We say that $a$ is the least upper bound of $x$ and $y$ if given any upper bound $z$ of $x$ and $y$ , we have that $a\preceq z$ .  

In general, we denote the greatest lower bound of $x$ and $y$ by $x\wedge y$ , and the least upper bound of $x$ and $y$ by $x\vee y$ .  

A lattice $(X,\preceq)$ is a set with a partial order such that every pair of elements has a greatest lower bound and a least upper bound.  

The examples of partial orders earlier in this section all have the lattice property.  

# Example 3.17  

The pair $(\mathbb{R},\leq)$ is a lattice. Since for any $x$ and $y$ , we have $x\leq y$ or $y\le x$ (or both), then $x\vee y=\operatorname*{min}x,y$ and $x\wedge y=\operatorname*{max}x,y$ . $\diamondsuit$  

# Example 3.18  

If $X$ is any set, then the pair $({\mathcal{P}}(X),\subseteq)$ is a lattice. Given any two subsets $A$ and $B$ of $X$ , we have $A\land B=A\cup B$ and $A\lor B=A\cap B$ . $\diamondsuit$  

# Example 3.19  

The subgroup relation $\leq$ on the set of subgroups ${\mathrm{Sub}}(G)$ of a group $G$ makes $(\operatorname{Sub}(G),\leq)$ a lattice, since if $H$ and $K\in{\mathrm{Sub}}(G)$ we have $H\vee K=\langle H\cup K\rangle$ and $H\wedge K=H\cap K$ . $\diamondsuit$  

# Example 3.20  

Let $X=\{0,x,y,1\}$ , and let $\preceq$ be the partial order defined by $0\preceq x,0\preceq y$ , $0\preceq x$ , $0\preceq1$ , $x\preceq1$ , $y\preceq1$ and $y\preceq1$ . Then $(X,\preceq)$ is a lattice, and we can represent it diagramatically as  

![](images/4bac0bf63e0b5640016e657cc24f04a6158d60a3ceeb13c0967195f1488e40f2.jpg)  

In the previous section, we showed that if the subgroup lattice of two groups didn’t agree, then the groups could not be isomorphic. The heart of the result was showing that the isomorphism $\alpha$ between the groups produced a function between the subgroups $\overline{{\alpha}}$ which preserved meet and join.  

# Definition 3.4  

Let $(X,\preceq)$ and $(Y,\leq)$ be two partially ordered sets. A function $\alpha:X\rightarrow Y$ is an order-preserving function if whenever we have $x$ and $y\in X$ such that $x\preceq y$ , we have $\alpha(x)\leq\alpha(y)$ .  

$H\left(X,\preceq\right)$ and $(Y,\leq)$ are lattices, then $\alpha:X\rightarrow Y$ is a lattice homomorphism if $\alpha(x\vee y)=\alpha(x)\vee\alpha(y)$ , and $\alpha(x\wedge y)=\alpha(x)\wedge\alpha(y)$ . If $\alpha$ is a bijective lattice homomorphism, we say that it is a lattice isomorphism.  

# Example 3.21  

Let $(X,\preceq)$ be as in Example 3.20, and $V$ be the four-group. The function $\alpha:X\to\operatorname{Sub}(V)$ defined by the table  

<html><body><table><tr><td>t</td><td>α(t)</td></tr><tr><td>0</td><td>{e}.</td></tr><tr><td>C</td><td>{e,a}</td></tr><tr><td>y</td><td>{e,b}</td></tr><tr><td>2</td><td>{e, ab}</td></tr><tr><td>1</td><td>V</td></tr></table></body></html>  

May 3, 2004  

is a lattice isomorphism. Perhaps the easiest way to grasp this fact is to observe that the diagrams for each lattice correspond. 3  

# Example 3.22  

Consider the homomorphism $\alpha:V\rightarrow C_{4}$ of Example 2.43. The corresponding map between subgroups of these groups is $\overline{{\alpha}}$ , and is given by the following table  

<html><body><table><tr><td>H α(H) {e}</td></tr><tr><td>[e}. {e,a} {e,a²} {e,b} {e,a²} {e,ab} {e}</td></tr></table></body></html>  

is a lattice homomorphism.  

The essential content of Proposition 3.6 and Corollary 3.7 is then:  

# Proposition 3.9  

If $G$ and $H$ are groups, and $\alpha:G\rightarrow H$ is a homomorphism, then ${\overline{{\alpha}}}:{\mathrm{Sub}}(G)\to$ $\operatorname{Sub}(H)$ is an order-preserving map.  

# Corollary 3.10  

$I f G$ and $H$ are groups, and $\alpha:G\rightarrow H$ is a homomorphism, then ${\overline{{\alpha}}}:{\mathrm{Sub}}(G)\to$ $\operatorname{Sub}(H)$ is a lattice homomorphism.  

So we can then re-phrase Corollary 3.8 in the following way:  

# Corollary 3.11  

If $G$ and $H$ are groups, and the subgroup lattices of $G$ and $H$ are not isomorphic, then $G$ and $H$ are not isomoprhic.  

We can use Exercise 3.2.1 to prove the following proposition.  

# Proposition 3.12  

Let $(X,\preceq)$ and $(Y,\leq)$ be lattices, and $\alpha:X\rightarrow Y$ a lattice homomorphism.   
Then $\alpha$ is an order-preserving function.  

Proof:  

See Exercise 3.2.2.  

In fact, we could prove Corollary 3.7 directly, and use this proposition to conclude that Proposition 3.6 must hold.  

So the abstract concept of a lattice helps us understand the structure of subgroups within a group, just as the abstract structure of a group helps us understand concrete situations such as the symmetries of a set.  

# Exercises  

3.2.1. Let $\preceq$ be a partial order on $X$ . Show that if $x\preceq y$ that $x\wedge y=x$ and $x\vee y=y$ .  

3.2.2. Prove Proposition 3.12.  

3.2.3. Consider the natural numbers N with the “divides” relation $x\mid y$ if and only if $y=k x$ for some $k\in\mathbb N$ (ie. if $x$ divides $y$ ). Show that $|$ is a partial order, and that $x\wedge y$ is the greatest common divisor of $x$ and $y$ and $x\vee y$ is the least common multiple of $x$ and $y$ .  

3.2.4. Show that if $\preceq$ is a partial order on $X$ , then the reverse relation $\succeq$ defined by $x\succeq y$ if and only if $y\preceq x$ is a partial order. Show that $(X,\preceq)$ is a lattice if and only if $(X,\succeq)$ is a lattice.  

3.2.5. Let $F(D,\mathbb{R})$ be the set of real-valued functions on some fixed domain $D\subseteq\mathbb{R}$ (ie. the typical functions considered in calculus). Show that the relation defined by $f\leq g$ if $f(x)\leq g(x)$ for all $x\in D$ is a partial order.  

Give an example of two functions which are incomparable.  

Show that $(F(D,\mathbb{R}),\leq)$ is a lattice, where $f\wedge g$ and $f\vee g$ are the functions defined by  

$$
(f\wedge g)(x)=\operatorname*{min}f(x),g(x)\qquad{\mathrm{and}}\qquad(f\vee g)(x)=\operatorname*{max}f(x),g(x)
$$  

respectively.  

# 3.3 The Centre and Centralizers  

Abelian groups are much nicer to work with algebraically than non-Abelian groups. However, even in the case of non-Abelian groups there may be large parts of the group which commute with each other.  

Recall that two elements $x$ and $y\in G$ commute with one another if  

$$
x y=y x.
$$  

Multiplying both sides on the right by $x^{-1}$ , we can state this equivalently as saying that $x$ and $y$ commute if and only if  

$$
x y x^{-1}=y,
$$  

or, multiplying on the other side, if and only if  

$$
x^{-1}y x=y.
$$  

The set of all elements which commute with every other element of the group is called the centre of the group, denoted by $Z(G)$ . At the very least we have the identity $e\in Z(G)$ , but it is potentially much larger.  

# Example 3.23  

The centre of $D_{6}$ is $\{1\}$ . Clearly 1 always commutes with any element of $D_{6}$ . For the other elements we can always find an element with which it does not commute. For example, $a$ does not commute with $b$ , since $b a=(a^{2})b\neq a b$ . The same equation shows that $a^{2}$ does not commute with $b$ . So $a$ , $a^{2}$ and $b$ are not in the centre. Similarly $(a b)a=a^{3}b=a^{2}(a b)$ , so $a b$ does not commute with $a$ , and $(a^{2}b)a=a^{4}b=a^{2}(a^{2}b)$ , so $a^{2}b$ does not commute with $a$ . $\diamondsuit$  

# Example 3.24  

The centre of $D_{8}$ is $\{1,a^{2}\}$ . This follows since $a^{n}a^{2}=a^{n+2}=a^{2}a^{n}$ and  

$$
(a^{n}b)a^{2}=a^{n}a^{3}b a=a^{n+3}a^{3}b=a^{n+6}b=a^{n+2}b=a^{2}(a^{n}b).
$$  

for $n=0,1,2,3$ . So $a^{2}\in Z(D_{8})$ .  

On the other hand $(a^{n}b)a=a^{n}a^{3}b=a^{3}(a^{n}b)$ for $n=0,1,2,3$ , so $a$ , $a^{3}$ , and $a^{n}b$ are not in the centre. $\diamondsuit$  

# Example 3.25  

The centre of $G L_{n}(\mathbb{R})$ is the set  

$$
\mathbb{R}^{\times}I_{n}=\left\{\left[{\begin{array}{c c c c}{a}&{0}&{\cdot\cdot\cdot0}\\ {0}&{a}&{\cdot\cdot\cdot0}\\ {\vdots}&{\vdots}&{\ddots}\\ {0}&{0}&{\cdot\cdot\cdot a}\end{array}}\right]:a\in\mathbb{R}^{\times}\right\}.
$$  

It is easy to verify with matrix multiplication that every element of this set commutes with every matrix.  

To simplify calculations, we will just look at the case $n=2$ . To see that these are the only possible elements of the centre, we note that if  

$$
{\left[\begin{array}{l l}{a}&{b}\\ {c}&{d}\end{array}\right]}\in Z(G L_{2}(\mathbb{R}))
$$  

then we must have  

$$
{\begin{array}{r l}{\left[a\ }&{b}\\ {c\ }&{d}\end{array}\right]}{\left[\begin{array}{l l}{0}&{1}\\ {1}&{0}\end{array}\right]}={\mathord{\left[\begin{array}{l l}{0}&{1}\\ {1}&{0}\end{array}\right]}}{\left[\begin{array}{l l}{a}&{b}\\ {c}&{d}\end{array}\right]},
$$  

or,  

$$
{\Bigg[}{\begin{array}{l l}{b}&{a}\\ {d}&{c}\end{array}}{\Bigg]}={\Bigg[}{c}\quad d{\Bigg]}
$$  

and so we conclude that $a=d$ and $b=c$ . We also must have  

$$
\left[\begin{array}{c c}{{a}}&{{b}}\\ {{b}}&{{a}}\end{array}\right]\left[\begin{array}{c c}{{0}}&{{-1}}\\ {{1}}&{{0}}\end{array}\right]=\left[\begin{array}{c c}{{0}}&{{-1}}\\ {{1}}&{{0}}\end{array}\right]\left[\begin{array}{c c}{{a}}&{{b}}\\ {{b}}&{{a}}\end{array}\right],
$$  

May 3, 2004  

or,  

$$
{\left[{\begin{array}{l l}{b}&{-a}\\ {a}&{-b}\end{array}}\right]}={\left[{\begin{array}{l l}{-b}&{-a}\\ {a}&{~b}\end{array}}\right]}
$$  

so $b=-b$ . Hence $b=0$ , and so a matrix is in the centre only if it is in the set $\mathbb{R}^{\times}I_{2}$ . 3  

The centre is an extremely nice subset of the group.  

# Proposition 3.13  

Let $G$ be a group. Then $Z(G)$ is an Abelian subgroup of $G$ .  

Proof:  

Given $x$ , $y\in Z(G)$ , we observe that for any $z\in G$ we have  

$$
(x y)z=x z y=z(x y),
$$  

so $x y\in Z(G)$ . Similarly,  

$$
x^{-1}z=x^{-1}z x x^{-1}=z x^{-1},
$$  

so $x^{-1}\in Z(G)$ . Hence $Z(G)$ is a subgroup of $G$ .  

It is immediate that $Z(G)$ is also Abelian, since every element of $Z(G)$ commutes with every element of $G$ , and $Z(G)$ is a subgroup. $\mid$  

You can think of the centre as being a measure of how close the group $G$ is to being Abelian. If $Z(G)=G$ , then the group is Abelian, while if $Z(G)$ is a large subgroup, then $G$ can be thought of having a large Abelian component. On the other hand, if $Z(G)=\{e\}$ , the group is very far from being Abelian.  

# Proposition 3.14  

Let $G$ and $H$ be isomorphic groups. Then $Z(G)$ and $Z(H)$ are isomorphic groups.  

Proof:  

We know that there is an isomorphism $\alpha:G\rightarrow H$ . If $x\in Z(G)$ , then for any $y\in H$ , we have that there is some unique $u\in G$ such that $\alpha(u)=y$ , and  

$$
\alpha(x)y=\alpha(x)\alpha(u)=\alpha(x u)=\alpha(u x)=\alpha(u)\alpha(x)=y\alpha(x).
$$  

So $\alpha(x)\in Z(H)$ . An identical argument shows that if $x\in Z(H)$ , then $\alpha^{-1}(x)\in$ $Z(G)$ .  

Hence $\alpha(Z(G))=Z(H)$ , and the restriction of $\alpha$ to $Z(G)$ is an isomorphism between the centres.  

# Corollary 3.15  

If $G$ and $H$ have centres which are not isomorphic, then $G$ and $H$ are not isomorphic.  

May 3, 2004  

More generally, rather than asking which elements commute with the entire group we might ask which elements commute with some subset of the group. If $X\subset G$ is any subset of the group $G$ , then the centralizer of $X$ is the set of all elements which commute with every element of $X$ , ie.  

$$
Z_{G}(X)=\{y\in G:y x=x y\}=\{y\in G:y^{-1}x y=x\}=\{y\in G:y x y^{-1}=x\}.
$$  

This set always contains at least the identity element $e$ .  

# Proposition 3.16  

Let $G$ be a group, and $X\subset G$ . Then $Z_{G}(X)$ is a subgroup of $G$ . If $Y\subseteq X$ , then $Z_{G}(X)$ is a subgroup of $Z_{G}(Y)$ .  

Proof:  

The proof of the first part of this proposition is essentially the same as the proof of the first part of Proposition 3.13, but with $z\in X$ rather than $z\in G$ .  

The second part follows from the fact that if $z x=x z$ for every $x\in X$ , then it must also hold for every element of $Y$ , since $Y\subseteq X$ . Therefore $Z_{G}(X)\subseteq Z_{G}(Y)$ .  

In particular, this proposition implies that $Z(G)=Z_{G}(G)\subseteq Z_{G}(X)$ for all $X$ .  

The case where $X$ contains a single element $g$ is important enough to have its own, slightly different notation. We define  

$$
Z_{G}(g)=\{y\in G:y g=g y\}=\{y\in G:y^{-1}g y=g\}=\{y\in G:y g y^{-1}=g\}.
$$  

# Example 3.26  

The consider the element $a$ of $D_{6}$ . Then $Z_{D_{6}}(a)=\{e,a,a^{2}\}$ . The elements $b$ , $a b$ and $a^{2}b$ are not elements of $Z_{D_{6}}(a)$ . Similarly, we have:  

$$
\begin{array}{c}{{Z_{D_{6}}(e)=D_{6},}}\\ {{{}}}\\ {{Z_{D_{6}}(a^{2})=\{e,a,a^{2}\},}}\\ {{{}}}\\ {{Z_{D_{6}}(b)=\{e,b\},}}\\ {{{}}}\\ {{Z_{D_{6}}(a b)=\{e,a b\},}}\\ {{Z_{D_{6}}(a^{2}b)=\{e,a^{2}b\},}}\end{array}
$$  

Once again, these subsets have very nice properties.  

# Proposition 3.17  

Let $G$ be a group and $g\in G$ . Then $\langle g\rangle$ is a subgroup of $G$ . Furthermore, $Z_{\cal G}(g)={\cal G}$ if and only if $g\in Z(G)$  

Proof:  

We note that $g\in Z_{G}(g)$ , and since $\langle g\rangle$ is the smallest subgroup of $G$ containing $g$ , we have that $\langle g\rangle$ must be a subgroup of $Z_{G}(g)$ .  

If $g\in Z(G)$ , $g x=x y$ for all $x\in G$ , so $Z_{\cal G}(g)={\cal G}$  

On the other hand, if $Z_{\cal G}(g)={\cal G}$ , then that implies that for any $x\in G$ $x y=y x$ , and so $g\in Z(G)$ .  

You can think of the centralizer of $g$ as measuring how close $g$ is to being an element of the centre, or how close it is to commuting with everything.  

The centralizer plays a key role in the discussion of conjugacy later in this chapter. We finish up with one last result which links the centre and centralizers of elements.  

# Proposition 3.18  

Let $G$ be a group. Then $Z(G)$ is the intersection of all the subgroups $Z_{G}(g)$ .  

Proof:  

We know that $Z(G)\subseteq Z_{G}(g)$ for every $g$ , so  

$$
Z(G)\subseteq\bigcap_{g\in G}Z_{G}(g).
$$  

On the other hand, if $x\in Z_{G}(g)$ for every $g\in G$ , then $x g=g x$ for every $g\in G$ , and so $x\in Z(G)$ . Hence  

$$
\bigcap_{g\in G}Z_{G}(g)\subseteq Z(G).
$$  

# Exercises  

3.3.1. Find the centre of the group $C_{2}\times D_{6}$ .   
3.3.2. Find the centre of the group $D_{10}$ .   
3.3.3. Show that $Z(D_{2n})=\{1\}$ if $n$ is odd, and $Z(D_{2n})=\{1,a^{n/2}\}$ if $n$ is even.   
3.3.4. Find the centralizers of each element of $D_{8}$ .   
3.3.5. Find the centralizers of each element of $C_{2}\times D_{6}$ .   
3.3.6. Find the centralizers of each element of $S_{4}$ . What is the center of this   
group?   
3.3.7. Show that $Z_{G}(X)=Z_{G}(\langle X\rangle)$ .  

May 3, 2004  

# 3.4 Cosets  

There are other subsets of groups which it seems should have some significance. For example in $S_{3}$ , the set of “reflection permutations”, ie. those permutations with odd parity, is $\{(1,2),(1,3),(2,3)\}$ and is not a subgroup. Nevertheless, the elements have a commonality. To understand such a situation, we need to introduce some new notation.  

If $*:A\times B\to C$ is any binary relation, then given any $x\in X$ and $Y\subseteq B$ , we define  

$$
x*Y=\{x*y:y\in Y\}\subseteq C.
$$  

Similarly, if $y\in B$ and $X\subseteq A$ , we define  

$$
X*y=\{x*y:x\in X\}\subseteq C.
$$  

And analagously, we define  

$$
X*Y=\{x*y:x\in X,y\in Y\}\subseteq C.
$$  

We will often omit the operation $*$ and simply write $x Y$ , $X y$ and $X Y$ respectively.  

If $^*$ is a binary operation on $A$ , and $X\subseteq A$ , we will sometimes write  

$$
X^{n}=\underbrace{X*X*\cdot\cdot*X}_{n{\mathrm{~times}}}.
$$  

This is far from ideal notation, since it conflicts with the Cartesian product  

$$
X^{n}=\underbrace{X\times X\times\cdot\cdot\cdot\times X}_{n{\mathrm{~times}}}.
$$  

However, it is usually clear from context which of the two possibilities we mean.  

If $(G,*,e)$ is a group, and $X\subseteq G$ then we can also write  

$$
X^{-1}=\{x^{-1}:x\in X\}.
$$  

Using this notation, we can write Theorem 2.20 and Corollary 2.21 as follows:  

# Corollary 3.19  

Let $G=(G,*e)$ be a group, and $H\subseteq G$ . If $H^{2}\subseteq H$ and $H^{-1}\subseteq H$ , then $H\leq G$ .  

# Corollary 3.20  

Let $G=(G,*e)$ be a group, and $H\subseteq G$ . If $H H^{-1}\subseteq H$ , then $H\leq G$ .  

In fact, if $H$ is a finite subset of $G$ , we can use an even weaker condition:  

# Proposition 3.21  

Let $G=(G,*e)$ be a group, and $H$ be a finite subset $G$ . If $H^{2}=H$ , then $H\leq G$ .  

To prove this fact, we need a lemma which we will use often in what follows:  

# Lemma 3.22  

Let $G=(G,*e)$ be a group, $H\subseteq G$ , and $g\in G$ . Then the right translation by $g$ function $\rho_{g}:H\to H g$ defined by $\rho_{g}(x)=x g$ is a bijection.  

Similarly, the left translation by $g$ function $\lambda_{g}:H\to g H$ defined by $\lambda_{g}(x)=g x$ is a bijection.  

We also have $|H|=|g H|=|H g|$ .  

Proof:  

That $\rho_{g}$ is onto is trivial, for  

$$
\rho_{g}(H)=\{\rho_{g}(x):x\in H\}=\{x g:x\in H\}=H\{
$$  

On the other hand, if $\rho_{g}(x_{1})=\rho_{g}(x_{2})$ , then this means that $x_{1}y=x_{2}y$ , and the cancellation law (Proposition 2.3) says that $x_{1}=x_{2}$ . Hence $\rho_{g}$ is one-to-one, and so $\rho_{g}$ is a bijection.  

The result for $\lambda_{g}$ is similar.  

Since we have bijections between $H$ and $_{H g}$ , and $H$ and $g H$ , all three sets must have the same cardinality (Proposition 1.2).  

With this in hand, the proof is simple. Proof (Proposition 3.21):  

The hypothesis $H^{2}=H$ implies that $x y\in H$ for all $x$ , $y\in H$ , so we need only prove that $x^{-1}\in H$ .  

Let $|H|=n$ . Lemma 3.22 tells us $|H x|=|H|=n$ . Since $H$ is finite this, together with the fact that $H x\subseteq H^{2}=H$ , implies that $H x=H$ .  

Hence there must be some element $y\in H$ such that $y x=x$ . By the cancellation law, we have $y=e$ , so $x\in H$ .  

But now there must also be some $z\in H$ such that $z x=e$ , so $z=x^{-1}$  

Hence $H$ is a subgroup of $G$ .  

Getting back to the example at the start of this section, if $H=\langle(1,2,3)\rangle=$ $\{e,(1,2,3),(1,3,2)\}$ , then we can write  

$$
\{(1,2),(1,3),(2,3)\}=(1,2){\cal H}.
$$  

In fact, we also have  

$$
\mathrm{~,~}(2,3)\}=(1,3)H=(2,3)H=H(1,2)=H(1,
$$  

This situation is important enough to give it a name.  

# Definition 3.5  

Let $G$ be a group, and $H$ a subgroup of $G$ . Sets of the form $x H$ are called left cosets of $H$ and sets of the form $H x$ are called right cosets of $H$ , where $x$ is any element of $G$ .  

If $G$ is Abelian, then $H x=x H$ , and we simply call the set a coset of $H$ .  

So $\{(1,2),(1,3),(2,3)\}$ is both a left and right coset of $\{e,(1,2,3),(1,3,2)\}$ .  

# Example 3.27  

Consider $C_{6}=\{1,a,a^{2},a^{3},a^{4},a^{5}\}$ . We know that $H=\langle a^{3}\rangle$ is a subgroup, and its cosets are itself $H=a^{3}H$ , $a H=a^{4}H=\{a,a^{4}\}$ , and $a^{2}H=a^{5}H=$ $\{a^{2},a^{5}\}$ .  

Because cyclic groups are Abelian, the left and right cosets are the same. $\diamondsuit$  

Notice in the examples so far that several different choices for $x$ give the same coset. This is typical.  

# Theorem 3.23  

Let $G$ be a group, $H$ a subgroup of $G$ , and $x$ and $y\in H$ . Then $x H=y H$ if and only if $x^{-1}y\in H$ . On the other hand, $H x=H y$ if and only if $x y^{-1}\in H$ .  

Furthermore, either $x H=y H$ or $x H\cap y H=\theta$ . Similarly, either $H x=H y$ or $H x\cap H y=O$ .  

Proof:  

A typical element of $y H$ is of the form $y z$ , for some $z\in H$ . We note that $x^{-1}y z\in H$ , as well, and so multiplying on the left by $x$ we have $x(x^{-1}y z)=$ $y z\in x H$ . So $y H\subseteq x H$ . Similarly, since $(x^{-1}y)^{-1}=y^{-1}x\in H$ , given a typical element $x z\in x H$ we have $y^{-1}x z\in H$ , and so $y(y^{-1}x z)=y z\in y H$ . Hence $x H\subseteq y H$ , and we conclude that $x H=y H$ .  

Conversely, if $x^{-1}y\notin H$ , we have that $y\notin x H$ , since if that were the case $y=x z$ for some $z\in H$ , but $y=x x^{-1}y$ , and the cancellation law implies that then $z=x^{-1}y$ , so $z\not\in H$ , which is a contradiction. However, $y=y e\in y H$ , so $x H$ and $y H$ are not equal.  

Assume that $x H\neq y H$ , so that $x y^{-1}\notin H$ . If there were some $z\in x H\cap y H$ , then we would have $z=x u=y v$ for some $u$ and $v\in H$ . So $u v^{-1}\in H$ . But  

$$
u v^{-1}=x^{-1}x u v^{-1}=x^{-1}y v v^{-1}=x^{-1}y\notin H.
$$  

This is a contradiction, so there can be no such element $z$ .  

Analagous arguments show that $H x=H y$ if and only if $x y^{-1}\in H$ , and either $H x=H y$ or $H x\cap H y=\emptyset$ .  

Every element of the group must lie in some coset, since $x=x e\in x H$ , so the cosets of a subgroup $H$ break the group $G$ up into a collection of disjoint subsets. Furthermore, we know that each of these subsets has the same cardinality. This is nice to know for infinite groups, but it is really useful when dealing with finite groups.  

# Theorem 3.24 (Lagrange)  

If $G$ is a finite group, and $H\leq G$ , then the number of left cosets of $H$ and the number of right cosets of $H$ both equal $|G|/|H|$ .  

Proof:  

Assume that there are $n$ left cosets, and that $g_{1}H$ , $g_{2}H,\ldots,g_{n}H$ is a complete list of the distinct left cosets of $H$ , so $G$ is a disjoint union of these sets. The inclusion-exclusion principle tells us that when we have a disjoint union of finite sets, the cardinality of the union is the sum of the cardinality of each set, ie.  

$$
|G|=|g_{1}H|+|g_{2}H|+\cdot\cdot\cdot+|g_{n}H|.
$$  

But Lemma 3.22 tells is that $|g_{1}H|=|g_{2}H|=\cdot\cdot\cdot=|g_{n}H|=|H|$ , so  

$$
|G|=\underbrace{|H|+|H|+\cdots+|H|}_{n{\mathrm{~times}}}=n|H|.
$$  

Hence the number of left cosets is $n=|G|/|H|$ .  

The argument for right cosets is analagous.  

The number of cosets of a subgroup is significant enough to be given its own name and notation.  

# Definition 3.6  

If $H$ is a subgroup of a group $G$ , the index $[G:H]$ is the number of left (or right) cosets of $H$ .  

The index is sometimes denoted $|G:H|$ . The following corollaries of Lagrange’s theorem are almost trivial, but they are important enough that we state them explicitly. We will use these some of these facts as much, if not more often, than Lagrange’s theorem itself.  

# Corollary 3.25  

If $H$ is a subgroup of a finite group $G$ , $[G:H]=|G|/|H|$ .  

# Corollary 3.26  

If $H$ is a subgroup of a finite group $G$ , then both $[G:H]$ and $|H|$ divide $|G|$ .  

Proof:  

The numbers $[G:H]=|G|/|H|$ , and $|H|=|G|/[G:H]$ are both natural numbers, so $|H|\mid|G|$ and $[G:H]\mid|G|$ .  

# Corollary 3.27  

If $G$ is a finite group, and $x\in G$ , then the order of the $x$ , $o(x)$ , divides $|G|$ .  

Proof:  

We know that $o(x)=\vert\langle x\rangle\vert$ , and we also know that $\langle x\rangle$ is a subgroup. Hence $|\langle x\rangle|$ divides $|G|$ , and so $o(x)$ divides $G$ .  

This last corollary has some immediate consequences, both for properties of particular elements, and for classifying groups.  

# Corollary 3.28  

If $G$ is a finite group, and $n=|G|$ , then given any $x\in G$ , $x^{\pi}=e$ .  

Proof:  

Assume that $o(x)\ =\ k$ , so that in particular $\boldsymbol{x^{k}}\ =\ e$ . By the previous corollary, $o(x)$ divides $n$ , so we have $n=k m$ for some integer $m$ . But then  

$$
x^{n}=x^{k m}=(x^{k})^{m}=e^{m}=e.
$$  

# Exercises  

3.4.1. Identify the cosets of the subgroup $H=\{1,a^{2}\}$ of the group $D_{8}$ . What is the index of $H$ in $G$ ?   
3.4.2. Verify Lagrange’s Theorem for $D_{8}$ .   
3.4.3. Show that if $X\subseteq G$ is not a subgroup, then an element may lie in more than one set of the form $a X$ . Show that every element lies in at least one such set.   
3.4.4. Let $C_{n}$ be a cyclic group, and $m$ be a number that divides $n$ . Show that there is an element of order $m$ in $C_{n}$ .   
3.4.5. Show that if $G$ is a finite group, then the function $\alpha:{\mathrm{Sub}}(G)\to\mathbb{N}$ given by $\alpha(G)=|G|$ is an order-preserving function from the partial ordered set $(\operatorname{Sub}(G),\leq)$ to the partially ordered set $(\mathbb{N},|)$ .  

# 3.5 Classifying Groups of Small Order  

We will use the corollaries of Lagrange’s theorem to show that groups of low order fall into a few distinct isomorphism classes. The aim of this section is to show that every group of order less than or equal to 8 is isomorphic to one of a few standard groups.  

# Corollary 3.29  

Let $G$ be a group with $|G|=p$ , a prime number. Then $G$ has no proper subgroups, and $G$ is cyclic.  

Proof:  

If $H$ is a subgroup of $G$ , then $|H|$ divides $|G|=p$ , so $|H|$ must either be 1 or $p$ . If $|H|=1$ , then $H=\{e\}$ , since every subgroup must contain the identity element. On the other hand, if $|H|=p$ , then $H=G$ , since $H$ must contain every element of the group. So $G$ has no proper subgroups.  

Now assume that $x\in G$ , and $\boldsymbol{x}\neq\boldsymbol{e}$ . Then $H=\langle x\rangle$ is a subgroup of $G$ , and $H$ contains at least two elements $x$ and $e$ . Hence $|H|\neq1$ , and so $|H|=p$ , which implies $H=G$ . So $G$ is generated by $x$ , and hence is a cyclic group. $\mid$  

This last corollary means that every group of prime order is isomorphic to $C_{p}$  

We can also show that we have found all groups of order 4.  

# Proposition 3.30  

Let $G$ be a group such that $|G|=4$ . Then either $G\cong C_{4}$ or $G\cong V\cong C_{2}\times C_{2}$ .  

Proof:  

We know that the order of every element of $G$ divides 4, so the order of an element which is not the idientity must be 2 or 4.  

If there is an element $x$ with $o(x)=4$ , then $x$ must generate $G$ . Hence $G$ is cyclic, so $G\cong C_{4}$ .  

If there is no element of order 4, then we must have $x^{2}=e$ for every $x\in G$ But then Theorem 2.18 tells us that $G\cong C_{2}\times C_{2}$ .  

This table summarizes the typical groups of each order that we have discovered, up to order 12.  

<html><body><table><tr><td>Order</td><td>Known Groups C1</td></tr><tr><td>1 2 3 4 5 6 7 8 9 10 11 12</td><td>C2 C3 C4, C2 x C2 C5 C6, D6, ? C7 Cs, C2 x C2 x C2, C4 x C2, D8, ? C9, C3 x C3, ? C10, D10, ? C11 C12, C2 x C6, D12, A3, ?</td></tr></table></body></html>  

There may be other groups of order 6, 8, 10 and 12 that we do not yet know of, indicated by the question marks in the table.  

The following general result tells us that there are no more groups of order 6 and 10 than the ones listed on the table.  

# Proposition 3.31  

Let $G$ be a group with $|G|=2p$ , where $p$ is a prime number greater than 2.   
Then either $G$ is cyclic, or $G\cong D_{2p}$ .  

Proof:  

The factors of $2p$ are $^{1}$ , $2$ , $p$ and $2p$ , so the order of each element must be one of those factors. If $G$ has an element of order $2p$ it is cyclic.  

Assume that $G$ does not have an element of order $2p$ . If $G$ does not have an element of order , then every non-identity element of $G$ must have order $p$ 2, which means that $x^{2}=e$ for every element of $G$ . That imples that $G$ is a product of copies of $C_{2}$ by Theorem 2.18, and in particular that $|G|=2^{n}$ , which is impossible if $p>2$ .  

Hence there is some element $a\in G$ with $o(a)=p$ . Choose any element $b\not\in\langle a\rangle$ . Then $G$ breaks into the two right cosets  

$$
\begin{array}{c}{{\langle a\rangle=\{1,a,a^{2},\ldots,a^{p-1}\},}}\\ {{\langle a\rangle b=\{b,a b,a^{2}b,\ldots,a^{p-1}b\}.}}\end{array}
$$  

Therefore $b^{2}$ and $(b a)^{2}$ must lie in one of these two cosets.  

If $b^{2}\in\langle a\rangle b$ , then $b^{2}=a^{k}b$ for some $k$ , and the cancellation law tells us that $b=a^{k}$ , which is a contradiction. Hence $b^{2}\in\langle a\rangle$ , so $b^{2}=a^{k}$ for some $k$ . If $k\neq0$ , then $b^{2}\neq e$ , so $o(b)>2$ and hence $b$ must have order $p$ . But then since $p$ is odd, $p-1$ is even, and  

$$
b^{p}=b^{p-1}b=(b^{2})^{(p-1)/2}b=(a^{k})^{(p-1)/2}b=a^{k(p-1)/2}b\neq e.
$$  

So $b$ cannot have order $p$ . Hence $b^{2}=e$ .  

The same argument with $b a$ in the place of $b$ shows that $(b a)^{2}=e$ . Hence,  

$$
\begin{array}{c}{{b a b a=1}}\\ {{b a b=a^{-1}=a^{p-1}}}\\ {{b a=a^{p-1}b^{-1}=a^{p-1}b.}}\end{array}
$$  

So $G=\{1,a,a^{2},\ldots,a^{p-1},b,a b,a^{2}b,\ldots,a^{p-1}b\}$ and the relations  

$$
a^{p}=1,b=1,{\mathrm{and}}b a=a^{p-1}b,
$$  

hold, which is precisely the definition of $D_{2p}$ . Hence $G\cong D_{2p}$ .  

There may still be groups of order 8, 9 and 12 which we do not know about. We need to introduce a new concept to fully analyse these situations, so we defer them to a later section.  

# Exercises  

3.5.1. Show that every Abelian group of order 8 is isomorphic to one of $C_{8}$ , $C_{4}\times C_{2}$ , or $C_{2}\times C_{2}\times C_{2}$ .  

3.5.2. For any natural number $n$ , define the quaternion group to be the group  

$$
Q_{4n}=\{1,a,a^{2},\ldots,a^{2n-1},b,a b,a^{2}b,\ldots,a^{2n-1}b\}
$$  

where the Cayley table is determined by the relations $a^{2n}=1$ , $b^{2}=a^{n}$ , and $b^{-1}a b=a^{-1}$ .  

Show that $Q_{4}\cong C_{2}\times C_{2}$ .  

Show that $Q_{8}$ is not isomorphic to any one of $C_{8}$ , $C_{2}\times C_{2}\times C_{2}$ , $C_{2}\times C_{4}$ , or $D_{8}$ . In other words, $Q_{8}$ is a new group of order 8.  

Find the subgroup lattice for $Q_{8}$ .  

# 3.6 Excursion: Equivalence Relations  

Underlying the concept of cosets is the notion of an equivalence relation. Equivalence relations are a powerful mathematical concept which occur with regularity throughout abstract mathematics. Moreover, you are familiar with some of them already, even though you may have not seen the concept formally explained.  

# Definition 3.7  

An equivalence relation on a set $X$ is a relation $R$ on $X$ which is reflexive, transitive and symmetric. Equivalence relations are often denoted by the symbol $\sim$ .  

# Example 3.28  

The equality relation is an equivalence relation. Indeed, it is the prototypical equivalence relation. $\diamondsuit$  

# Example 3.29  

Let $M_{n}(\mathbb{R})$ be the set of $n$ by $n$ real-valued matrices. Recall that $A$ and $B$ are equivalent matrices if there is an orthogonal matrix $U$ such that $A=U^{-1}B U$ . The relation $\sim$ defined by $A\sim B$ if and only if $A$ and $B$ are equivalence matrices is an equivalence relation. $\diamondsuit$  

# Example 3.30  

Let $m$ be any natural number. The relation $\equiv$ on $\mathbb{Z}$ defined by $x=y$ if and only if $m$ divides $x-y$ is an equivalence relation. Another way of looking at it, is that it holds if and only if  

$$
x\equiv y{\pmod{m}}.
$$  

A third, useful, way of looking at this situation is that the set of all integers divisible by $m$ is the subset $H=m\mathbb{Z}=\{m n:n\in\mathbb{Z}\}$ of $\mathbb{Z}$ , and this is a subgroup, so $x=y$ if and only if $x-y\in H$ , and $x-y$ is the additive notation for $x y^{-1}$ . So this is the same condition we were using to test whether two elements were in the same coset. $\diamondsuit$  

We now get to the key example which links this section with what we have just discussed by generalizing the above example.  

# Example 3.31  

Let $G$ be a group, $H$ a subgroup of $G$ , and $x$ , $y\in G$ . We define $x\equiv_{R}$ $y$ (mod $H$ ) if $x y^{-1}\in H$ .  

To prove this, you need to check that each of the three axioms for an equivalence relation hold.  

Firstly, $x x^{-1}=e\in H$ , so $x\equiv_{R}x$ (mod $H$ ), and $\equiv_{R}$ (mod $H$ ) is reflexive.  

Secondly, if $x\equiv_{R}y$ (mod $H$ ), then $x y^{-1}\in H$ , then $y x^{-1}=(x y^{-1})^{-1}\in H$ , so $y\equiv_{R}x$ (mod $H$ ), and $=\boldsymbol{R}$ (mod $H$ ) is symmetric.  

Finally, if $x\equiv_{R}y$ (mod $H$ ) and $y\equiv_{R}z$ (mod $H$ ), then $x y^{-1}$ and $y z^{-1}\in H$ , so $x z^{-1}=x y^{-1}y z^{-1}\in H$ , so $x\equiv_{R}z$ (mod $H$ ), $\mathrm{and}\equiv_{R}$ (mod $H$ ) is transitive.  

There is of course the equivalent “left” relation $x\equiv_{L}y$ (mod $H$ ) if $x^{-1}y\in$ $H$ , and this too is an equivalence relation. $\diamondsuit$  

Notice how the three group axioms correspond to the three equivalence relation axioms in the previous example. Notice also that with these equivalence relations, the cosets are precisely the sets of elements which are equivalent to one-another.  

# Definition 3.8  

Let $\sim$ be an equivalence relation on $X$ . The equivalence class of $x\in X$ is the set  

$$
[x]_{\sim}=\{y\in X:x\sim y\}.
$$  

When the equivalence relation is clear, we typically write just $[x]$ .  

# Example 3.32  

For equality, the equivalence class $[x]_{=}=\{x\}$ .  

# Example 3.33  

$\mathrm{For}\equiv\ (\mathrm{mod}\ m)$ ), the equivalence class of a number $n$ , $[n]_{\equiv}$ is the set of all numbers with the same remainder when divided by $m$ , or equivalently,  

$$
[n]=\{\ldots,n-2m,n-m,n,n+m,n+2m,\ldots\}
$$  

# Example 3.34  

The equivalence class of $x$ under the equivalence relation $=\boldsymbol{R}$ (mod $H$ ) is the right coset $H x$ .  

The equivalence class of $x$ under the equivalence relation $\equiv_{L}$ (mod $H$ ) is the left coset $x H$ . $\diamondsuit$  

A key fact about equivalence classes is that they partition the set $X$ into a disjoint collection of subsets whose union is the whole set.  

# Lemma 3.32  

Let $\sim$ be an equivalence relation on $X$ . We have $x\sim y$ if and only if $[x]=[y]$ . Also, either $[x]=[y]$ or $[x]\cap[y]=O$ .  

Proof:  

Assume that $x\sim y$ , then given any $z\in[y]$ , we have $y\sim z$ , and since $\sim$ is transitive, $x\sim z$ , so $z\in[x]$ . Hence $[x]\subseteq[y]$ . On the other hand, we know that by symmetry $x\sim y$ implies $y\sim x$ . If $z\in[x]$ , we have $x\sim z$ and since $\sim$ is transitive, $x\sim z$ , so $z\in[x]$ . Hence $[x]\subseteq[y]$ . Hence $[x]=[y]$ .  

If $[x]=[y]$ , then $y\in[y]$ , since $y\sim y$ by reflexivity, and so $y\in[x]$ . Hence $x\sim y$ by definition.  

If $[x]\cap[y]\neq\emptyset$ , then there must be some $z$ in both sets, ie. $x\sim z$ and $y\sim z$ . Now since $\sim$ is symmetric, $y\sim z$ implies $z\sim y$ , and since $\sim$ is transitive, $x\sim y$ . The first part of this lemma allows us to conclude that $[x]=[y]$ .  

Theorem 3.23 is now a trivial corollary of this general fact about equivalence relations. Similarly, Lagrange’s Theorem follows from the following general fact.  

# Theorem 3.33  

Let $X$ be a finite set, and $l e t\sim b e$ an equivalence relation on $X$ . Choose elements $x_{1}$ , $x_{2},\ldots,x_{n}\in X$ so that no two are equivalent, and so for every $x\in X$ , there is some $x_{k}$ such that $[x]=[x_{k}]$ (ie. this is a complete set of equivalence class representatives). Then  

$$
\left|X\right|=\sum_{k=1}^{n}\left|[x_{k}]\right|
$$  

Proof:  

We have that $X$ is the disjoint union of the equivalence classes of the $x_{k}$ , ie.  

$$
X=[x_{1}]\cup[x_{2}]\cup\cdot\cdot\cdot\cup[x_{n}],
$$  

which follows from the fact that every $x$ lies in one of the $[x_{k}]$ , and if $k\neq j$ ,  

$$
[x_{j}]\cap[x_{k}]=\emptyset,
$$  

since $x_{k}\not\sim x_{j}$ .  

Repeated application of the inclusion-exclusion principle then tells us that  

$$
|X|=|[x_{1}]|+|[x_{2}]|+\cdot\cdot\cdot+|[x_{n}]|,
$$  

since the intersections are empty.  

We will use this general theorem in the next section.  

# Exercises  

3.6.1. Verify that Example 3.30 is an equivalence relation. 3.6.2. Verify that Example 3.29 is an equivalence relation. 3.6.3. Let $A$ and $B\in M_{n}(\mathbb{R})$ , and define a relation by $A\sim B$ if $A=X^{-1}B X$ for some invertible matrix $X$ . SHow that this is an equivalence relation.  

3.6.4. Can a relation be symmetric and transitive, but not reflexive? Give a proof or counterexample to justify your position.  

3.6.5. ( $*$ ) Show that if is the equivalence relation of Example 3.29, and $\sim$  

$$
A={\left[\begin{array}{l l}{1}&{0}\\ {0}&{2}\end{array}\right]}
$$  

then $[A]_{\sim}$ is the set of all symmetric $2\times2$ matrices with eigenvalues 1 and 2.  

# 3.7 Conjugacy Classes  

There is another equivalence relation which is of interest which is a generalization of the idea of conjugate matrices (see Example 3.29).  

# Definition 3.9  

Let $G$ be a group and let $x$ and $y\in G$ . We say that $y$ is a conjugate of $x$ if $y=u^{-1}x u$ for some $u\in G$ . In this case, we write $x\sim y$ .  

# Example 3.35  

In the group $D_{6}=\{1,a,a^{2},b,a b,a^{2}b\}$ we have that $a\sim a^{2}$ , since $b^{-1}a b=$ $b a b=a^{2}b b=a^{2}$ . Similarly, we have that $b\sim a b$ since $a^{-1}b a=a^{2}b a=a^{2}a^{2}b=$ $a b$ . $\diamondsuit$  

# Proposition 3.34  

If $G$ is a group, the conjugacy relation $\sim$ is an equivalence relation on $G$ .  

Proof:  

We need to show that $\sim$ is reflexive, symmetric and transitive.  

Since $x=e^{-1}x e$ , we have that $x\sim x$ , so $\sim$ is reflexive.  

If $x\sim y$ , then there is an element $u$ such that $y\ =\ u^{-1}x u$ . But then $x=u u^{-1}x u u^{-1}=u y u^{-1}=(u^{-1})^{-1}y u^{-1}$ . So $y\sim x$ , and $\sim$ is symmetric.  

If $x\sim y$ and $y\sim z$ , then $y=u^{-1}x u$ and $z=v^{-1}y v$ for some $u$ and $v$ in $G$ , so $z=v^{-1}u^{-1}x u v=(u v)^{-1}x(u v)$ . Hence $x\sim z$ , and $\sim$ is transitive. $\mid$  

We will call the equivalence class of $x\in G$ the conjugacy class of $x$ . We will denote the conjugacy class of an element $x\in G$ by $C(x)$ . In other words,  

$$
C(x)=\{y\in G:y=u^{-1}x u{\mathrm{~for~some~}}u\in G\}. 
$$  

# Example 3.36  

The conjugacy classes of the four-group are $\{e\},\ \{a\},\ \{b\},\ \{a b\}$ . In other words, each element is in its own conjugacy class. $\diamondsuit$  

# Example 3.37  

The conjugacy classes of the group $D_{6}=\{1,a,a^{2},b,a b,a^{2}b\}$ are $\{1\}$ , $\{a,a^{2}\}$ , $\{b,a b,a^{2}b\}$ . $\diamondsuit$  

The reason that the four-group has a conjugacy class for each element is because it is an Abelian group. In fact if $x$ commutes with everything in $G$ , then the only element of the conjugacy class of $x$ is itself.  

# Proposition 3.35  

Let $G$ be a group and $x\in G$ . Then $C(x)=\{x\}$ if and only if $x\in Z(G)$ .  

Proof:  

Assume $x\in Z(G)$ . Let $y\in C(x)$ , so that there is some $u\in G$ such that $y=u^{-1}x u$ . But then, since $x u=u x$ ,  

$$
y=u^{-1}x u=u^{-1}u x=x.
$$  

So $C(x)=\{x\}$ .  

On the other hand, if $C(x)=\{x\}$ , then for every $u\in G$ , we have $u^{-1}x u=x$ , which means $x$ and $u$ commute. Hence $x\in Z(G)$ .  

Notice in this case that we are saying that $u^{-1}x u=v^{-1}x v$ for every $u$ and $v\in G$ . This is not going to be the case in general, but it is of interest to know for which $u$ and $v$ it occurs. For example, if these quantities were never equal for different $u$ and $v$ , it would show that the conjugacy class of $x$ has many different elements.  

# Proposition 3.36  

Let $G$ be a group and $x\in G$ . Then $u^{-1}x u=v^{-1}x v$ if and only if $u v^{-1}\in Z_{G}(x)$ .  

Proof:  

If $u^{-1}x u=v^{-1}x v$ , then we have  

$$
\begin{array}{c}{{u^{-1}x u=v^{-1}x v}}\\ {{x u=u v^{-1}x v}}\\ {{x u v^{-1}=u v^{-1}x.}}\end{array}
$$  

So we can see that $u v^{-1}$ commutes with $x$ , and so $u v^{-1}\in Z_{G}(x)$ .  

On the other hand, if $u v^{-1}\in Z_{G}(x)$ , we can run the calculation backwards:  

$$
\begin{array}{c}{{x u v^{-1}=u v^{-1}x}}\\ {{x u=u v^{-1}x v}}\\ {{u^{-1}x u=v^{-1}x v,}}\end{array}
$$  

and so we have the result.  

Recalling Theorem 3.23, this proposition cas the following corollary:  

May 3, 2004  

# Corollary 3.37  

Let $G$ be a group and $x\in G$ . Then $u^{-1}x u=v^{-1}x v$ if and only if $u$ and $v$ are in the same right coset of $Z_{G}(x)$ , ie. if $Z_{G}(x)u=Z_{G}(x)v$ .  

Turning this around, it means that $u^{-1}x u\neq v^{-1}x v$ if and only if $u$ and $v$ are in different right cosets of $Z_{G}(x)$ , which means that there is a distinct conjugate of $x$ for each distinct right coset of $Z_{G}(x)$ . If $G$ is finite, then Lagrange’s Theorem tells us that there are exactly $|G:Z_{G}(x)|=|G|/|Z_{G}(x)|$ distinct right cosets of $Z_{G}(x)$ . This proves the following:  

# Corollary 3.38  

Let $G$ be a finite group and $x\in G$ . Then  

$$
|C(x)|=|G:Z_{G}(x)|.
$$  

Note that this agrees with Proposition 3.35, since $x\in Z(G)$ if and only if $Z_{G}(x)=G$ , but this happens if and only if $|C(x)|=|G:Z_{G}(x)|=|G|/|G|=1$ . In other words, if and only if the only conjugate of $x$ is itself.  

# Example 3.38  

For the group $D_{6}$ we know that $a$ has conjugacy class $\{a,a^{2}\}$ , and we know that $Z_{D_{6}}(a)=\{1,a,a^{2}\}$ . It is simple to verify that  

$$
|G|/|Z_{D_{6}}(a)|=6/3=2=|C(a)|.
$$  

If we look at the other elements of $D_{g}$ , we observe that  

$$
\begin{array}{r l r}{\vert C(e)\vert=2}&{}&{\vert G\vert/\vert Z_{D_{6}}(e)\vert=6/6=1}\\ {\vert C(a^{2})\vert=2}&{}&{\vert G\vert/\vert Z_{D_{6}}(a^{2})\vert=6/3=2}\\ {\vert C(b)\vert=3}&{}&{\vert G\vert/\vert Z_{D_{6}}(b)\vert=6/2=3}\\ {\vert C(a b)\vert=3}&{}&{\vert G\vert/\vert Z_{D_{6}}(a b)\vert=6/2=3}\\ {\vert C(a^{2}b)\vert=3}&{}&{\vert G\vert/\vert Z_{D_{6}}(a^{2}b)\vert=6/2=3.}\end{array}
$$  

We can now apply Theorem 3.33 to the conjugacy equivalence relation.  

# Theorem 3.39 (The Class Equation)  

Let $G$ be a finite group, and let $g_{1}$ , $g_{2}$ , $g_{n}\in G$ be chosen so that no two are conjugate, and every conjugacy class of $G$ occurs as one of the conjugacy classes $C(g_{k})$ (ie. the elements $g_{k}$ are a complete set of conjugacy class representatives). Then  

$$
|G|=\sum_{k=1}^{n}|C(g_{k})|=\sum_{k=1}^{n}|G:Z_{G}(g_{k})|.
$$  

In fact, if we assume that $Z(G)=\{g_{1},g_{2},...,g_{m}\}$ , then  

$$
|G|=|Z(G)|+\sum_{k=m+1}^{n}|G:Z_{G}(g_{k})|.
$$  

May 3, 2004  

Proof:  

Since conjugacy is an equivalence relation, Theorem 3.33 applies and tells us that  

$$
|G|=|C(g_{1})|+|C(g_{2})|+\cdots+|C(g_{n})|=\sum_{k=1}^{n}|C(g_{k})|.
$$  

But then Corollary 3.38 tells us that $|C(g_{k})|=|G:Z_{G}(g_{k})|$ , and so we can re-write the equation as  

$$
|G|=\sum_{k=1}^{n}|G:Z_{G}(g_{k})|.
$$  

Now we know that if $g_{k}\in Z(G)$ , then $|C(g_{k})|=1$ , so if $Z(G)=\{g_{1},g_{2},...,g_{m}\}$ , then  

$$
\begin{array}{l}{|G|=|C(g_{1})|+|C(g_{2})|+\dots+|C(g_{m})|+|C(g_{m+1})|+|C(g_{m+2})|+\dots+|C(g_{n})|}\\ {\displaystyle=\underbrace{1+1+\dots+1}_{m\mathrm{~times}}+|C(g_{m+1})|+|C(g_{m+2})|+\dots+|C(g_{n})|}\\ {\displaystyle\quad=|Z(G)|+\sum_{k=m+1}^{n}|C(g_{k})|}\\ {\displaystyle=|Z(G)|+\sum_{k=m+1}^{n}|G:Z_{G}(g_{k})|.}\end{array}
$$  

# Example 3.39  

For the group $D_{6}$ we have that $^{1}$ , $a$ , and $b$ are representatives of each conjugacy class, and  

$$
|C(1)|+|C(a)|+|C(b)|=1+2+3=6=|D_{6}|.
$$  

# Example 3.40  

For the group $D_{8}$ , we know from Example 3.24 that the centre $Z(D_{8})=$ $\{1,a^{2}\}$ , so $C(1)=\{1\}$ and $C(a^{2})=\{a^{2}\}$ .  

We know that $Z_{D_{8}}(a)$ contains $\langle a\rangle$ so  

$$
|Z_{D_{8}}(a)|\geq|\langle a\rangle|=o(a)=4,
$$  

and from Lagrange’s theorem $|Z_{D_{8}}(a)|$ divides 8, so we conclude that $|Z_{D_{8}}(a)|$ is 4 or 8. But $a\not\in Z(D_{8})$ , so $Z_{D_{8}}(a)\neq D_{8}$ , and hence $|Z_{D_{8}}(a)|=4$ . Hence $a$ has two conjugates, itself, and $b^{-1}a b=b a b=a^{3}b^{2}=a^{3}$ . So $C(a)=\{a,a^{3}\}$ .  

Now $Z_{D_{8}}(b)$ contains $\left<b\right>$ , so $|Z_{D_{8}}(b)|\ge o(b)=2$ , and $|Z_{D_{8}}(b)|$ divides 8. Also $b$ is not in the centre of $D_{8}$ , so $|Z_{D_{8}}(b)|\neq8$ , and hence $|Z_{D_{8}}(b)|$ is $2$ or 4. Now we know that the centre is a subgroup of $Z_{D_{8}}(b)$ , so $^{1}$ and $a^{2}$ are in the subgroup, as is $b$ itself. Hence $|Z_{D_{8}}(b)|>2$ , and we conclude that $|Z_{D_{8}}(b)|=4$ . Hence $b$ has two conjugates, itself and $a^{-1}b a=a^{-1}a^{3}b=a^{2}b$ . So $C(b)=\{b,a^{2}b\}$ .  

Looking at $Z_{D_{8}}(a b)$ , an analagous argument tells us that $|Z_{D_{8}}(a b)|=4$ , and the conjucacy class is $C(a b)=\{a b,a^{\3}b\}$ .  

We can verify that the class equation holds in this example: a complete collection of equivalence class representatives is $^{1}$ , $a^{2}$ , $a$ , $b$ and $a b$ , and  

$$
|Z(D_{8})|+|C(a)|+|C(b)|+|C(a b)|=2+2+2+2=8=|D_{8}|.
$$  

We conclude with one last proposition which can help identify which elements are in different conjugacy classes.  

# Proposition 3.40  

Let $G$ be a finite group. If $x$ and $y\in G$ are conjugate, then $o(x)=o(y)$ .  

Proof:  

Let $x=u^{-1}y u$ . Then if $o(y)=n$ ,  

$$
\begin{array}{r l}&{x^{n}=(u^{-1}y u)^{n}}\\ &{\quad=\underbrace{(u^{-1}y u)(u^{-1}y u)\cdot\cdot\cdot(u^{-1}y u)}_{=u^{-1}\underbrace{y y\cdot\cdot\cdot y}_{\begin{array}{l}{\end{array}}}\\ {\quad=u^{-1}y^{n}\cdot u}\end{array}}n\mathrm{~times}u}\\ &{\quad=u^{-1}y^{n}u}\\ &{\quad=u^{-1}u}\\ &{\quad=e.}\end{array}
$$  

So $o(x)$ divides $n$ . However, if $o(x)=m$ , then a similar calculation shows that  

$$
\begin{array}{c}{{e=x^{m}}}\\ {{=u^{-1}y^{m}u,}}\end{array}
$$  

so $y^{m}=u u^{-1}y^{m}u u^{-1}=u e u^{-1}=e$ . Hence $n$ divides $m$ , and so $n=m$ .  

Note that the converse is not true, since in the group $D_{8}$ , the elements $a^{2}$ , $b$ and $a b$ all have order 2, but all are in distinct conjugacy classes.  

Notice that every term in the class equation divides the order of the group. We can use this to prove the following interesting result that will be useful when we look once again at groups of small order.  

# Theorem 3.41  

Let $G$ be a finite group of prime power order, ie. $|G|=p^{n}$ where $p$ is prime and $n\geq1$ . Then $|Z(G)|=p^{m}$ for some $m\geq1$ .  

Proof:  

Lagrange’s theorem tells us that $|Z(G)|=p^{m}$ for some $m\geq0$ .  

Let $x_{1}$ , $x_{2},\ldots,x_{k}$ be a complete set of conjugacy class representatives, and let $Z(G)=\{x_{1},\ldots,x_{l}\}$ . Now let $n_{i}=|C(x_{i})|=|G:Z_{G}(x_{i})|$ , so $n_{i}\mid p^{n}$ . Now for $i>l$ , we must have $n_{i}>1$ , so $n_{i}$ must be a multiple of $p$ . Therefore,  

$$
p^{n}=|{\cal G}|=|{\cal Z}({\cal G})|+n_{l+1}+n_{l+2}+\cdot\cdot\cdot n_{k}=l+j p
$$  

for some integer $j$ . But therefore $\it{l}$ is a multiple of $p$ , and since $e\ \in\ Z(G)$ , $|Z(G)|\ge1$ . So we conclude that $|Z(G)|\geq p$ , and hence $m\geq1$ .  

In other words, groups of prime power must have more than just the identity in their centres.  

# Exercises  

3.7.1. In a group of order 15, what does the Class equation say are possible sizes of the conjugacy classes? Potentially how many different ways can a group of order 15 be divided into conjugacy classes of these sizes (remembering that the number of conjugacy classes of size 1 has to equal $|Z(G)|$ , which has to divide 15).  

Note: in actual fact, there turns out to be only one group of order 15, so there is just one way to do it once this is taken into account; however you should use the class equation to give you all the potentially possible ways that it could be done.  

3.7.2. Find the conjugacy classes of $D_{10}$ , and verify that the class equation holds.  

3.7.3. Find the conjugacy classes of $D_{12}$ , and verify that the class equation holds.  

3.7.4. Find the conjugacy classes of $A_{4}$ , and verify that the class equation holds.  

3.7.5. Let $G$ be a finite group, and $x\in G$ . Show that the conjugacy classes $C(x)$ and $C(x^{-1})$ have the same number of elements.  

3.7.6. Show that any group $G$ of even order must contain an element of even order, and use the previous exercise to conclude that there is at least one element $x\in G$ other than $e$ such that $C(x)=C(x^{-1})$ .  

# 3.8 Normal Subgroups  

It turns out that many of the ideas relating to centralizers and conjugacy can be applied to subgroups instead of individual elements.  

# Proposition 3.42  

Let $G$ be a group and $H$ a subgroup of $G$ . Given any $x\in G$ , the set  

$$
x^{-1}H x=\{x^{-1}y x:y\in H\}
$$  

of conjugates of elements of $H$ is a subgroup of $G$ .  

Proof:  

Given any elements $u$ and $v\in x^{-1}H x$ , we have $y$ and $z\in H$ such that $u=x^{-1}y x$ and $v=x^{-1}z x$ . Then  

$$
u v^{-1}=x^{-1}y x(x^{-1}z x)^{-1}=x^{-1}y x x^{-1}z^{-1}x=x^{-1}y z^{-1}x,
$$  

which is an element of $x^{-1}H x$ , since $y z^{-1}\in H$ .  

Hence $x^{-1}H x$ is a subgroup of $G$ .  

If $x\in H$ , then $x^{-1}H x=H$ , but if $x\notin H$ we may potentially get something else.  

# Example 3.41  

In the group $D_{6}$ , consider the subgroup $H=\{1,b\}$ . Since $a^{-1}1a=1$ , and $a^{-1}b a=a^{-1}a^{2}b=a b$ , we have  

$$
a^{-1}H a=\{1,a b\}.
$$  

Similarly, we have  

$$
\begin{array}{c}{{(a^{2})^{-1}H a^{2}=\{1,a^{2}b\}}}\\ {{b^{-1}H b=\{1,b\}}}\\ {{(a b)^{-1}H(a b)=\{1,a^{2}b\}}}\\ {{(a^{2}b)^{-1}H(a^{2}b)=\{1,a b\}}}\end{array}
$$  

On the other hand, the subgroup $K=\{1,a,a^{2}\}$ has $x^{-1}K x=K$ for any $x$ . For example, since $b^{-1}1b=1$ , $b^{-1}a b=b a b=a^{2}b^{2}=a^{2}$ , and $b^{-1}a^{2}b=b a^{2}b=$ $b^{2}a=a$ , we have $b^{-1}K b=K$ . Similar arguments give the remaining cases. $\diamondsuit$  

We will say that two subgroups $H$ and $K$ of $G$ are conjugate if there is some $x\in G$ such that  

$$
K=x^{-1}H x,
$$  

and we will write $K\sim H$ if this is the case. It is equivalent to say that $K$ and $H$ are conjugate if and only if there is some $x\in G$ such that $x K=H x$ .  

# Proposition 3.43  

Let $G$ be a group. The conjugacy relation $\sim$ is an equivalence relation on the set $\operatorname{Sub}(G)$ of all subgroups of $G$ . Furthermore if two subgroups are conjugate, they are isomorphic.  

Proof:  

We need to show that $\sim$ is reflexive, symmetric and transitive.  

Given a subgroup $H$ , we have $H\ \sim\ H$ immediately from the fact that $e^{-1}H e=H$ .  

If $K$ and $H$ are subgroups with $K\sim H$ , then there is some $x\in G$ so that $K=x^{-1}H x$ . But then $H=x K x^{-1}=(x^{-1})^{-1}K x^{-1}$ , and so $H\sim K$ .  

Finally, if $H$ , $K$ and $F$ are subgroups, with $H\sim K$ and $K\sim F$ , then there are elements $x$ and $y\in G$ such that $K=x^{-1}H x$ and $\boldsymbol{F}=\boldsymbol{y}^{-1}\boldsymbol{K}\boldsymbol{y}$ . But then $F=y^{-1}(x^{-1}H x)y=(x y)^{-1}H(x y)$ , and so $F\sim H$ .  

So conjugacy of subgroups is an equivalence relation.  

If $K$ and $H$ are conjugate, with $K=x^{-1}H x$ , we define a function $\alpha:H\rightarrow K$ by $\alpha(y)=x^{-1}y x$ . This function is a homomorphism, since  

$$
\alpha(y)\alpha(z)=x^{-1}y x x^{-1}z x=x^{-1}y z x=\alpha(y z).
$$  

It is also onto, since $K=\alpha(H)$ by definition. Finally, it is one-to-one since if $\alpha(y)=\alpha(z)$ , then $x^{-1}y x=x^{-1}z x$ , and using the cancellation law on the left and right gives $y=z$ .  

So $\alpha$ is an isomorphism from $H$ to $K$ , and $H$ and $K$ are isomorphic.  

# Example 3.42  

Continuing the example of $D_{6}$ from above, we have that $\{1\}$ is only conjugate with itself, $\{1,a,a^{2}\}$ is only conjugate with itself, $\{1,b\}\sim\{1,a b\}\sim\{1,a^{2}b\}$ , and $D_{6}$ is only conjugate with itself. $\diamondsuit$  

We recall that elements of a group whose conjugacy class was just themselves were special: they formed the centre of the group. Subgroups which are conjugate only with themselves are also special.  

# Definition 3.10  

Let $G$ be a group and $K$ a subgroup of $G$ . If the only subgroup conjugate to $K$ is $K$ itself, that is, for any $x\in G$  

$$
x^{-1}K x=K,
$$  

then we say that $K$ is a normal subgroup, and we write $K\triangleleft G$ .  

Another way of representing the condition that $K$ is normal is that  

$$
K x=x K
$$  

for every $x\in G$ , or in other words that the corresponding left- and right- cosets of $K$ are identical.  

# Example 3.43  

In the group $D_{6}$ we have that the subgroups $\{1\}$ , $\{1,a,a^{2}\}$ and $D_{6}$ are normal. The subgroups $\{1,b\}$ , $\left\{1,a b\right\}$ and $\{1,a^{2}b\}$ are not normal. $\diamondsuit$  

# Lemma 3.44  

If $G$ is a group, then $\{e\}$ and $G$ are always normal subgroups of $G$ . If $G$ is Abelian, then every subgroup of $G$ is normal.  

Proof:  

We know $x^{-1}e x=x^{-1}x=e$ for every $x$ , so $x^{-1}\{e\}x=\{e\}$ for all $x$ . We also know that $x^{-1}G x=G$ for any $x$ , since $x\in G$ .  

If $G$ is Abelian, we recall that $K x=x K$ for any subgroup and any $x\in G$ (see Definition 3.5), hence $K$ is always normal.  

Not every element of a group is in the centre, and not every subgroup is normal. Similarly, just as the centralizer gives us information about the conjugacy classes of elements, we have an analagous concept for conjugacy classes of subgroups.  

# Definition 3.11  

Let $G$ be a group, and $H$ a subgroup of $G$ . We define the normalizer of $H$ to be the set  

$$
N_{G}(H)=\{x\in G:H=x^{-1}H x\}
$$  

With this definition, we can duplicate most of the key results about centralizers and conjugacy classes.  

# Theorem 3.45  

Let $G$ be a group and $H$ a subgroup of $G$ . Then  

(i) $N_{G}(H)$ is a subgroup of $G$ ,   
(ii) $H$ and $Z(G)$ are subgroups of $N_{G}(H)$ ,   
(iii) $N_{G}(H)=G$ if and only if $H$ is normal,   
(iv) $x^{-1}H x=y^{-1}H y$ if and only if $x$ and $y$ are in the same right coset of $N_{G}(H)$ ,   
(v) The number of distinct conjugacy classes of $H$ is $|G:N_{G}(H)|$ .  

Proof:  

The proofs of these facts are analogous to the proofs of the corresponding results for centralizers, and are left as an exercise.  

Normal subgroups play a key role in the theory of groups, and we now turn to study them in more detail. We start with some ways of testing whether a subgroup is normal or not, and discovering normal subgroups of a group.  

# Theorem 3.46  

Let $G$ be a group and $K$ a subgroup of $G$ . Then the following are equivalent:  

(i) $K$ is normal, (ii) $x^{-1}K x=K$ for all $x\in G$ ,  

May 3, 2004  

(iii) $K x=x K$ for all $x\in G$ ,   
(iv) $N_{G}(H)=G$ ,   
(v) $x^{-1}y x\in K$ for all $y\in K$ and $x\in G$ ,   
(vi) $K$ is a union of some of the conjugacy classes of elements of $G$ ,  

Proof:  

We have already seen that (i), (ii), (iii) and (iv) are equivalent.  

If $K$ is normal then $x^{-1}K x=K$ , so for any $y\in K$ we have that $x^{-1}y x\in K$ . Conversely, if $x^{-1}y x\in K$ for all $y\in K$ and $x\in G$ , then if we fix $x$ we have that  

$$
x^{-1}K x=\{x^{-1}y x:y\in K\}\subseteq K.
$$  

On the other hand, given any $y\in K$ , we have that $(x^{-1})^{-1}y x^{-1}\in K$ , and so  

$$
x^{-1}((x^{-1})^{-1}y x^{-1})x=x^{-1}x y x^{-1}x=y,
$$  

nd so $K\subseteq x^{-1}K x$ . Hence $x^{-1}K x=K$ for every $x\in G$ and so $K$ is normal.  

So we have just shown that (i) and (v) are equivalent.  

Another way of stating (v) is that if $y\in K$ then every conjugate of $y$ is in $K$ , so that $C(y)\subseteq K$ . Hence $K$ must be the union of all the conjugacy classes of its elements, ie.  

$$
K=\bigcup_{y\in K}C(y)
$$  

So (v) implies (vi).  

Conversely, if $K$ is a union of conjugacy classes, then given any element $y\in K$ , the conjugacy class $C(y)$ of $y$ must be a subset of $K$ , and so we have that $x^{-1}y x\in C(y)\subseteq K$ . Therefore $x^{-1}y x\in K$ , and (vi) implies (v). $|$  

# Example 3.44  

In the group $D_{6}$ , we have conjugacy classes $\{1\}$ , $\{a,a^{2}\}$ and $\{b,a b,a^{2}b\}$ . We can clearly see that each of the normal subgroups are unions of conjugacy classes:  

$$
\begin{array}{c}{{\{1\}=\{1\}}}\\ {{\{1,a,a^{2}\}=\{1\}\cup\{a,a^{2}\}}}\\ {{D_{6}=\{1\}\cup\{a,a^{2}\}\cup\{b,a b,a^{2}b\}.}}\end{array}
$$  

Notice that not every union of conjugacy classes gives a normal subgroup, because some unions of conjugacy classes aren’t subgroups. For example, the set $\{1,b,a b,a^{2}b\}=\{1\}\cup\{b,a b,a^{2}b\}$ is not a subgroup. 3  

# Example 3.45  

The group $D_{8}$ has conjugacy classes  

$$
\{1\},\{a^{2}\},\{a,a^{3}\},\{b,a^{2}b\},\{a b,a^{3}b\}.
$$  

May 3, 2004  

The trivial subgroups $\{1\}$ and $D_{8}$ are automatically normal, but in addition, we have that the subgroups  

$$
\begin{array}{c}{{\{1,a^{2}\}=\{1\}\cup\{a^{2}\}}}\\ {{\{1,a,a^{2},a^{3}\}=\{1\}\cup\{a^{2}\}\cup\{a,a^{3}\}}}\\ {{\{1,a^{2},b,a^{2}b\}=\{1\}\cup\{a^{2}\}\cup\{b,a^{2}b\}}}\\ {{\{1,a^{2},a b,a^{3}b\}=\{1\}\cup\{a^{2}\}\cup\{a b,a^{3}b\}}}\end{array}
$$  

are all normal, as they can be written as unions of conjugacy classes as shown.   
These are the only possible normal subgroups.  

There are a number of conditions which guarantee that a subgroup is normal.  

# Theorem 3.47  

Let $G$ be a group, and $H$ a subgroup of $G$ . If any of the following holds, $H$ is normal:  

(i) $H=\{e\}$ or $G$ ,   
(ii) $H\subseteq Z(G)$ ,   
(iii) $|G:H|=2$ ,   
(iv) $H$ is the only subgroup of order $|H|$ in $G$ .  

Proof:  

We have already seen (i) is true. (ii) Recall that for any element $x\in Z(G)$ , $C(x)=\{x\}$ , so if $H\subseteq Z(G)$ , then  

$$
H=\bigcup_{x\in H}\{x\}=\bigcup_{x\in H}C(x),
$$  

so $H$ is a union of conjugacy classes, and so $H$ is normal.  

(iii) If $|G:H|=2$ , then $H$ has two left cosets, $H$ itself and $x H$ , where $x\notin H$ . Similarly, it has two right cosets $H$ and $H x$ . Now since every element of $G$ is either in $H$ or $x H$ , we have that $x{H}=G\backslash{H}$ . But we similarly have that $H x=G\setminus H$ . Hence $x H=H x$ , and corresponding left and right cosets of $H$ are equal. Hence $H$ is normal.  

(iv) We know that every conjugate of $H$ is a subgroup of $G$ , and we must have $|x^{-1}H x|=|H|$ . Hence if $H$ is the only subgroup of order $|H|$ , we must have $H=x^{-1}H x$ for all $x\in G$ . So $H$ is normal. $\mid$  

# Example 3.46  

In the dihedral group ${\cal D}_{2n}=\{1,a,a^{2},\ldots,a^{n-1},b,a b,\ldots,a^{n-1}n\}$ , the subgroup $\langle a\rangle=\{1,a,a^{2},\dotsc,a^{n-1}\}$ has order $|\langle a\rangle|=o(a)=n$ . So $|D_{2n}:\langle a\rangle|=$ $|D_{2n}|/|\langle a\rangle|=2n/n=2$ .  

So $\langle a\rangle$ is always a normal subgroup of $D_{2n}$  

Normal subgroups also have a nice relationship with the lattice structure of subgroups.  

# Proposition 3.48  

Let $G$ be a group, $K$ a normal subgroup of $G$ , and $H$ an arbitrary subgroup of $G$ . Then $H K$ is a subgroup of $G$ , and furthermore $H\vee K=H K=K H$ .  

Proof:  

We start by showing that $H K$ is a group. Given $x$ and $y\in H$ and $u$ and $v\in K$ , we have that $x u$ and $y v$ are typical elements of $H K$ . Now  

$$
x u(y v)^{-1}=x u v^{-1}y^{-1}=x y^{-1}(y^{-1})^{-1}u v^{-1}y^{-1},
$$  

and we know $x y^{-1}\in H$ , and $(y^{-1})^{-1}u v^{-1}y^{-1}\in K$ , since it is a conjugate of $u v^{-1}\in K$ . So $x u(y v)^{-1}\in H K$ , and hence $H K$ is a subgroup of $G$ .  

Now we need to show that $\langle H\cup K\rangle=H K$ . We do this by showing that $H K$ is the smallest subgroup of $G$ containing both $H$ and $K$ . First observe that since $e\in K$ , $H=H e\subseteq H K$ . Similarly $K=e K\subseteq H K$ . So $H K$ contains both $H$ and $K$ . Now assume that $F$ is a subgroup which contains both $H$ and $K$ . Then given any $x\in H$ and $u\in K$ , then $x$ and $u\in F$ and so $x u\in F$ . Hence $H K\subseteq F$ . So $H K$ is the smallest subgroup which contains both $H$ and $K$ .  

A similar argument shows that $K H$ is a subgroup of $G$ and that $H\lor K=K H$ as well.  

So we have that $H K=\langle H\cup K\rangle=H\vee K=K\vee H=K H.$  

We can use this fact to show that normal subgroups form a sub-lattice within the lattice of subgroups of a group.  

# Theorem 3.49  

Let $G$ be a group, and let $H$ and $K$ be normal subgroups of $G$ . Then $H\vee K=$ $\langle H\cup K\rangle$ and $H\wedge K=H\cap K$ are both normal.  

Proof:  

We know that $H\vee K=H K$ , so we will show that $H K$ is a normal subgroup of $G$ . If $x\in H$ and $u\in K$ and $z\in G$ , then $x u$ is a typical element of $H K$ and we have that  

$$
z^{-1}x u z=z^{-1}x z z^{-1}u z,
$$  

and $z^{-1}x z\in H$ , $z^{-1}u z\in K$ , and so $z^{-1}x u z\in H K$ . Hence $H K$ is normal.  

Also, given any $y\in H\cap K$ , and any $x\in G$ , we have that $x^{-1}y x\in H$ and $x^{-1}y x\in K$ , so $x^{-1}y x\in H\cap K$ , and so $H\cap K$ is normal.  

# Corollary 3.50  

$I f G$ is a group, then the set of normal subgroups of $G$ is a lattice.  

We will use the symbol $\triangleleft$ to represent the order that gives this lattice. In other words, $H\triangleleft K$ if and only if both $H$ and $K$ are normal and $H\leq K$ . This extends the use of $\triangleleft$ to indicate a normal subgroup of a group.  

# Example 3.47  

The normal subgroup lattice of $D_{8}$ is:  

![](images/2e9f4d470aaa24815370c6821c4463b26216a78be65437334936df83d0b066d8.jpg)  

The lattice diagrams for the subgroup lattice and the normal subgroup lattice are sometimes combined by representing the normal subgroup lattice by thicker lines or double lines.  

![](images/77bf535c830e2112ea8122c9442037c136c19affa94ea3aca7f0b3d5e7512e4d.jpg)  

Part of the importance of normal subgroups is that they are closely related to homomorphisms. In fact every homomorphism gives you a normal subgroup.  

# Theorem 3.51  

Let $G$ and $H$ be groups and $\alpha:G\to H$ a homomorphism. If $K$ is a normal subgroup of $H$ , then $\alpha^{-1}(K)$ is a normal subgroup of $G$ .  

In particular, $\ker\alpha$ is always normal.  

Proof:  

We know that $\alpha^{-1}(K)$ is a subgroup of $G$ . Given any $x\in G$ , and any $y\in\alpha^{-1}(K)$ , we have that  

$$
\alpha(x^{-1}y x)=(\alpha(x))^{-1}\alpha(y)\alpha(x),
$$  

and $\alpha(y)\in K$ , so $(\alpha(x))^{-1}\alpha(y)\alpha(x)\in K$ . Therefore, $x^{-1}y x\in K$ , and so $K$ is normal.  

We recall that $\ker\alpha\:=\:\alpha^{-1}(\{e\})$ , and $\{e\}$ is always normal, so $\ker\alpha$ is normal.  

# Corollary 3.52  

If $G$ and $H$ are groups, and $\alpha:G\rightarrow H$ is an isomorphism, then a subgroup $K$ of $G$ is normal if and only if $\alpha(K)$ is normal.  

# Corollary 3.53  

If $G$ and $H$ are isomorphic, then the lattice of normal subgroups of $G$ and the lattice of normal subgroups of $H$ are isomorphic.  

# Example 3.48  

Consider the homomorphism $\alpha:D_{8}\to V$ defined by $\alpha(a)=a$ , and $\alpha(b)=b$ . Looking at the image of each element, we see get the following table:  

<html><body><table><tr><td></td><td>α(x)</td></tr><tr><td>1 a</td><td>1</td></tr><tr><td>a2</td><td>a 1</td></tr><tr><td>3 a?</td><td>a</td></tr><tr><td>b ab</td><td>b</td></tr><tr><td></td><td>ab</td></tr><tr><td>a²b</td><td></td></tr><tr><td></td><td>b</td></tr><tr><td>a3b</td><td>ab</td></tr></table></body></html>  

Since $V$ is Abelian, every subgroup is normal, and the inverse images of each subgroup are the normal subgroups  

$$
\begin{array}{r l}&{~\alpha^{-1}(\{1\})=\{1,a^{2}\}}\\ &{~\alpha^{-1}(\{1,a\})=\{1,a,a^{2},a^{3}\}}\\ &{~\alpha^{-1}(\{1,b\})=\{1,b,a^{2},a^{2}b\}}\\ &{~\alpha^{-1}(\{1,a b\})=\{1,a b,a^{2},a^{3}b\}}\\ &{~\alpha^{-1}(V)=D_{8}.}\end{array}
$$  

# Exercises  

3.8.1. Prove Theorem 3.45.  

3.8.2. Let $H=\langle X\rangle$ be a subgroup of a group $G$ . Show that $H$ is normal if and only if $g^{-1}x g\in H$ for all $x\in X$ .  

3.8.3. Find the normal subgroup lattice of $D_{10}$ .  

3.8.4. Find the normal subgroup lattice of $D_{12}$ .  

3.8.5. Find the normal subgroup lattice of $A_{4}$ .  

May 3, 2004  

3.8.6. Given elements $x$ and $y\in G$ , their commutator is the element  

$$
[x,y]=x^{-1}y^{-1}x y.
$$  

The derived or commutator subgroup is the subgroup generated by all the commutators of elements of $G$  

$$
G^{\prime}=\langle\{[x,y]:x,y\in G\}\rangle
$$  

(i) Show that if $x$ and $y$ commute, then $[x,y]=e$ . Conclude that the commutator subgroup of an Abelian group is always $\{e\}$ .   
(ii) Show that $G^{\prime}$ is normal.   
(iii) Find the commutator subgroup of $D_{8}$ .   
(iv) If $H$ and $K$ are normal subgroups of $G$ , show that the commutator $[x,y]$ of any pair $x\in H$ and $y\in K$ lies in $H\wedge K$ . Show that if $H\land K=\{e\}$ , then any element of $H$ commutes with any element of $K$ .  

# 3.9 Groups of Small Order, Part II  

In previous sections we have discovered that all the groups of order less than 8 are isomorphic to one of a small collection of groups. We do not know what groups there are of order 8, however.  

We have identified $C_{8}$ , $C^{2}\times C^{4}$ , $C_{2}\times C_{2}\times C_{2}$ and $D_{8}$ , but there could potentially be more. In fact, in Exercise 3.5.2, there was the following definition.  

# Definition 3.12  

For any natural number $n$ , the quaternion group is the group with elements  

$$
Q_{4n}=\{1,a,a^{2},\ldots,a^{2n-1},b,a b,a^{2}b,\ldots,a^{2n-1}b\}
$$  

where the Cayley table is determined by the relations $a^{2n}=1$ , $b^{2}=a^{n}$ , and $b^{-1}a b=a^{-1}$ .  

In Exercise 3.5.2 it was shown that $Q_{8}$ is not isomorphic to any other known group of order 8. In fact, $Q_{8}$ completes the set of isomorphism classes of groups of order 8.  

# Theorem 3.54  

$_{I f G}$ is a group of order 8, then $G$ is isomorphic to one of $C_{8}$ , $C_{2}{\times}C_{4}$ , $C_{2}{\times}C_{2}{\times}C_{2}$ , $D_{8}$ or $Q_{8}$ .  

Proof:  

If $G$ has an element of order 8, then Theorem 2.13 tells us that $G$ is cyclic, and so $G\cong C_{8}$ .  

If every element of $G$ other then $e$ has order 2, then Theorem 2.18 tells us that $G$ is a direct product of cyclic groups of order 2. So $G\cong C_{2}\times C_{2}\times C_{2}$ .  

If $G$ is not isomorphic to one of these two groups, it must have an element of order 4, and no elements of order 8. Let $a$ be this element of order 4 in $G$ , and let $H=\langle a\rangle$ . Since $|G:H|=|G|/|H|=8/4=2$ , so if $b\in G\setminus H$ , then $H$ has two cosets $H$ and $H b$ , and $G$ is the disjoint union of $H$ and $H b$ . So  

$$
G=\{e,a,a^{2},a^{3},b,a b,a^{2}b,a^{3}b\}.
$$  

If $G$ is Abelian, then $a b=b a$ , and $b$ must either be of order 2 or 4. If $b^{2}=1$ , then $G\cong C_{4}\times C_{2}$ where the isomorphism is given by $\alpha(u^{k},v^{l})=a^{k}b^{l}$ , where $C_{4}=\langle u\rangle$ and $C_{2}=\langle v\rangle$ . If $b^{2}\neq e$ , then we must have $b^{2}=a^{k}$ , since if $b^{2}=a^{k}b$ , then the cancellation law tells us that $b=a^{k}\in H$ , which is a contradiction. Furthermore if $k=1$ or $k=3$ , then $b^{4}=a^{2}\neq e$ , which is a contradiction. So then $b^{2}=a^{2}$ . But in this case, ab has order 2, and $G\cong C_{4}\times C_{2}$ where the isomorphism is given by $\alpha(u^{k},v^{l})=a^{k}(a b)^{l}$ .  

So the only Abelian groups of order 8 are $C_{8}$ , $C_{4}\times C_{2}$ and $C_{2}\times C_{2}\times C_{2}$ .  

If $G$ is not Abelian, we note that Theorem 3.47 says that since $|G:H|=2$ , $H$ is normal in $G$ , which means that $b^{-1}a b\in H$ , so  

$$
b^{-1}a b=a^{k}
$$  

where $k$ is one of $0$ , $^{1}$ , $2$ or 3. But $k\neq0$ , since otherwise $a\in C(e)=\{e\}$ , which cannot happen. If $k=2$ , then  

$$
b^{-1}a^{2}b=b^{-1}a b b^{-1}a b=a^{2}a^{2}=a^{4}=e,
$$  

so $a^{2}\in C(e)$ , which is also impossible. If $k=1$ , then $b^{-1}a b=a$ implies $a b=b a$ , so $G$ is Abelian. So the only remaining possibility is that $k=3$ .  

As before, the order of $b$ must be either 2 or 4. If $b$ has order 2, then $b^{2}=1$ . This means that $b=b^{-1}$ , and so $b^{-1}a b=a^{3}$ implies that $b a=a^{3}b$ . So $G$ is determined by the relations $a^{4}=1$ , $b^{2}=1$ and $b a=a^{3}b$ , and $G$ is isomorphic to $D_{8}$ under the trivial isomorphism $\alpha(a^{k}b^{l})=a^{k}b^{l}$ .  

Finally, if $b$ has order 4, then the same argument as the Abelian case tells us that $b^{2}=a^{2}$ . We then observe that since $b^{3}=b_{-1}$ , we have that $G$ is determined by the relations $a^{4}=1$ , $b^{2}=a^{2}$ and $b^{-1}a b=a^{-1}$ , and $G$ is isomorphic to $Q_{8}$ under the trivial isomorphism $\alpha(a^{k}b^{l})=a^{k}b^{l}$ . $\mid$  

The next lowest order that we don’t have full information on is groups of order 9. We know that we have $C_{9}$ and $C_{3}\times C_{3}$ , but there could potentially be other groups of order 9.  

# Theorem 3.55  

Let $G$ be a group of order 9. Then $G$ is isomorphic to one of $C_{9}$ or $C_{3}\times C_{3}$ .  

Proof:  

From Lagrange’s theorem, the elements of $G$ all have orders dividing 9, so they have order 1, 3 or 9. If there is an element of order 9, then Theorem 2.13 tells us that $G\cong C_{9}$ . So if $G$ is not isomorphic to $C_{9}$ , then every element other than the identity must have order 3.  

Choosing an element $a\neq e$ , we have that the subgroup $H=\langle a\rangle$ has order 3. Now choose $b\notin H$ . Now $b^{2}\notin H$ , since that would imply $b^{2}=a^{k}$ , where $k=1$ or 2, and in either case $b$ would have order 6. Similarly $b^{2}\notin H b$ since then we would have $b^{2}=a^{k}b$ , where $k=1$ or 2, and then since $G$ is Abelian $b^{3}=a^{k}b^{2}=a^{2k}b\neq e$ , since $b\notin H$ . So the right cosets of $H$ are $H$ , $H b$ and $H b^{2}$ , and therefore  

$$
G=\{e,a,a^{2},b,a b,a^{2}b,b^{2},a b^{2},a^{2}b^{2}\}.
$$  

Theorem 3.41 tells us that the centre of $G$ has $|Z(G)|=3$ or $|Z(G)|=9$ . If $|Z(G)|=9$ , then $G$ is Abelian, and $G\cong C_{3}\times C_{3}$ via the isomorphism $\alpha(u^{k},u^{l})=a^{k}b^{l}$ , where $C_{3}=\langle u\rangle$ .  

So assume that $|Z(G)|=3$ . Without loss of generality in the above discussion, we could have assumed that we chose $a\in Z(G)$ , so that $H=Z(G)$ . Therefore $a$ commutes with every element of $G$ , so in particular, $b a=a b$ . But since $G=\langle a,b\rangle$ , $G$ is Abelian and $|Z(G)|=|G|=9$ . So every group of order 9 is Abelian. $\bigstar$  

At this point we have classified every group of order up to and including 11. It turns out that any group of order 12 is isomorphic to one of $C_{12}$ , $C_{6}\times C_{2}$ , $D_{12}$ , $Q_{12}$ or the alternating group $A_{4}$ . However, the proof of this fact requires considerably more powerful techniques than we have available right now.  

# Exercises  

3.9.1. Show that if $p$ is a prime number, and $G$ is a group with $|G|=p^{2}$ , then $G$ is isomorphic to one of $C_{p^{2}}$ or $C_{p}\times C_{p}$ . Hint: generalize the case of $|G|=9$ .   
3.9.2. Explain why there is one group of order 13 and two groups of order 14.   
3.9.3. Show that $C_{15}\cong C_{5}\times C_{3}$ .  

# 3.10 Extension: Cayley Graphs  

Cayley graphs are closely related to generators, and give a nice way of picturing groups. Unfortunately, they are not that useful in distingishing between groups which are not isomorphic, but they are of some independent interest, particularly when considering how one can computerize calculations involving groups.  

# Definition 3.13  

Let $(G,*,e)$ be a group, and let $S~=~\{g_{1},a_{2},...,g_{n}\}$ be a finite set which generates $G$ , ie. $G=\langle S\rangle$ . The Cayley graph of $G$ with the generating set $S$ is the graph $\Gamma_{G}=(G,E)$ with vertices being the elements of $G$ , and two vertices $x$ and $y\in G$ are connected by an edge if $x=y g$ for some $g\in S\cup S^{-1}$ .  

Definitions vary somewhat from source to source. Some may define the Cayley graph as a directed graph with directed edges of the form $(x,x g)$ for $g\in S\cup S^{-1}$ , while others allow a double edge $(x,y)$ if both $x=y g$ and $y=x y$ .  

# Example 3.49  

The cyclic group $C_{4}=\{1,a,a^{2},a^{3}\}$ has a generating set $S\ =\ \{a\}$ . The Cayley graph of $G$ with this generating set has edges $(1,a)$ , $(a,a^{2})$ , $(a^{2},a^{3})$ and $(a^{3},1)$ . In other words, it looks like this:  

![](images/a9f8b52f76106fa0afbab60af6b155e4515f3efd1458f5f9d47c72c7918ab5ea.jpg)  

This group is also generated (somewhat redundantly) by the set $S=\{a,a^{2}\}$ . This gives a Cayley graph which looks like this:  

![](images/8370a57627284da86c92fde13883143ee78821e75b773c1bd2095addc965560d.jpg)  

# Example 3.50  

The four-group $V=\{1,a,b,a b\}$ has a generating set $S=\{a,b\}$ . The Cayley graph of $G$ with this generating set has edges $(1,a)$ , $(a,a b)$ , $(1,b)$ and $(b,a b)$ . In other words, it looks like this:  

![](images/0805be99c8e466e6282562779d3ab4a3acac70ca5e9ae999bd2709b5e879ad60.jpg)  

This group is also generated by the set $S=\{a,b,a b\}$ . This gives a Cayley graph which looks like this:  

![](images/24d583968479cc41dffc630264340b9e6eb76ab356daea5d24b91c6de59eff5e.jpg)  

Notice that in each case, the graphs look the same, even though the groups are not isomorphic.  

May 3, 2004  

# Proposition 3.56  

Let $G$ be a finite group. The Cayley graph with generating set taken to be all of $G$ is the complete graph on $|G|$ vertices.  

Proof:  

Given any pair of vertices $x$ , $y$ , the element $g=y^{-1}x$ is in the generating set $G$ , and $y g=y y^{-1}x=x$ , so there is an edge joining $x$ and $y$ . Hence the Cayley graph is a complete graph.  

With careful selection of the generating set, however, the Cayley graph can reveal a lot about the structure of the group.  

# Example 3.51  

The group of integers $(\mathbb{Z},+,0)$ is generated by the element 1. The Cayley graph is infinite, but the region near 0 looks like:  

![](images/a7a72dce6c8a61548ff4fdeb9790b5497c1be4e25badb48a4086130a79fc2f43.jpg)  

Cayley graphs have some regularities that normal graphs do not.  

# Proposition 3.57  

Let $G$ be a group, and $S$ a finite set which generates $G$ . The Cayley graph of $G$ with generating set $S$ has the following properties:  

(i) the graph is connected. (ii) every vertex is an end of exactly $|S\cup S^{-1}|$ edges  

Proof:  

(i) Every element $x$ can be written as a product $x=x_{1}x_{2}\cdot\cdot\cdot x_{n}$ of elements $x_{k}\in S\cup S^{-1}$ , so $e,x_{1},x_{1}x_{2},\ldots,x_{1}x_{2}\cdot\cdot\cdot x_{n-1},x$ is a path from $e$ to $x$ . So the Cayley graph is connected.  

(ii) Given any $x\in G$ , if we consider the set  

$$
X=\{x g:g\in S\cup S^{-1}\},
$$  

then we note that if $x g_{1}=x g_{2}$ , the cancellation law says that $g_{1}=g_{2}$ . Therefore every edge $(x,x g)$ is distinct, and $|X|=|S\cup S^{-1}|$ . Furthermore, if there is some $y$ such that $(y,x)$ is an edge, then $x=y g$ for some $g\in S{\cup}S^{-1}$ , so $y=x g^{-1}\in X$ . So $X$ is exactly the set of all vertices connected to $x$ by one edge. So $x$ is an end of exactly $|S\cup S^{-1}|$ edges.  

# Exercises  

3.10.1. Draw the Cayley graph of $C_{6}$ with generating set $\{a\}$ . Draw the Cayley graph of $C_{6}$ with generating set $\{a^{2},a^{3}\}$ .   
3.10.2. Draw the Cayley graph of $D_{6}$ with generating set $\{a,b\}$ .   
3.10.3. Draw the Cayley graph of $D_{8}$ with generating set $\{a,b\}$ .   
3.10.4. Draw the Cayley graph of $C_{2}\times C_{4}$ with generating set $\{(a,1),(1,b)\}$ .   
3.10.5. Draw the region near 0 of the Cayley graph of $\mathbb{Z}$ with generating set $\{2,3\}$ .   
3.10.6. Draw the region near 0 of the Cayley graph of $\ensuremath{\mathbb{Z}}^{2}$ with generating set $\{(1,0),(0,1)\}$ . Draw the same region if the generating set is $\{(1,0),(0,1),(1,1)\}$ .   
3.10.7. Draw the region near $e$ of the Cayley graph of the free group $F_{2}$ with generating set $\{a,b\}$ .  

# Assignment 4  

The following exercises are due Monday, April 19th. Since this is a long assignment, it will be worth twice as much as the other 3 assignments.  

3.1 Exercises 2, 4, 6.   
3.2 Exercises 1, 3, 5.   
3.3 Exercises 1, 3, 5.   
3.4 Exercises 1, 4, 5.   
3.5 Exercises 2.   
3.6 Exercises 4.   
3.7 Exercises 1, 2, 5, 6.   
3.8 Exercises 2, 3, 6.   
3.9 Exercises 1, 2.   
3.10 Exercises 1, 2, 5, 6.  

# Chapter 4  

# Constructing Groups  

In Chapter 2 we saw a simple way that we could combine two groups to get a third group: the direct product. In this chapter we look at other ways to construct new groups from already known groups.  

# 4.1 Quotient Groups  

Let $G$ be a group, and $N$ a normal subgroup of $G$ . We can (at least potentially) define a binary operation $*$ on the set of cosets of $N$ by  

$$
N x*N y=N x y.
$$  

The difficulty with this definition is that there may be many different choices for $x^{\prime}$ and $y^{\prime}$ , so that $N x^{\prime}=N x$ and $N y=N y^{\prime}$ , and it is not immediate why we should have $N x y=N x^{\prime}y^{\prime}$ . However they are in fact equal since $x^{\prime}=u x$ and $y^{\prime}=v y$ for some $u$ and $v\in N$ , and since $N$ is normal, we have $z=x v x^{-1}\in N$ , so  

$$
x^{\prime}y^{\prime}=u x v y=u x v x^{-1}x y=(u z)x y.
$$  

so $N x^{\prime}y^{\prime}=N(u z)x y=N x y$ . So $^*$ is a well-defined binary operation, and it is only well-defined if $N$ is normal.  

# Proposition 4.1  

Let $G$ be a group and $N$ a normal subgroup of $G$ . Let $G/N$ be the set of all cosets of $N$ in $G$ . Then $(G/N,*,N)$ is a group.  

Proof:  

We first observe that $G/N$ is associative, since for any right cosets $N x$ , N y and $N z\in G/N$ , we have  

$$
N x*N y)*N z=N x y*N z=N x y z=N x*
$$  

$$
*N z=N x y z=N x*N y z=N x*(N y*N z)
$$  

The set $N$ is an identity, since $N=N e$ , and so  

$$
N e*N x=N e x=N x\qquad{\mathrm{and}}N x*N e=N x e=N x.
$$  

Finally, $N x^{-1}$ is the inverse of $N x$ , since  

$$
N x*N x^{-1}=N x x^{-1}=N e=N\qquad{\mathrm{and}}\qquadN x^{-1}*N x=N x^{-1}x=N e=N.
$$  

In fact, there is another way of looking at this product. If we consider cosets $N x$ and $N y$ , then we have that the product of the cosets as sets is  

$$
(N x)(N y)=(x N)(N y)=(x N^{2})y=(x N)y=(N x)y=N x y,
$$  

recalling that $N x=x N$ since $N$ is normal, and $N^{2}=N$ since $N$ is a subgroup. In other words, $N x*N y$ is given by the product of sets $(N x)(N y)$ . In some texts this is used as the definition of the product.  

# Definition 4.1  

$I f G$ is a group, and $N$ a normal subgroup of $G$ , then we call $G/N$ the quotient group of $N$ in $G$ .  

# Example 4.1  

Let $D_{6}=\{1,a,a^{2},b,a b,a^{2}b\}$ as usual. If $N=\{1,a,a^{2}\}$ , then $N$ is normal, and the cosets are $N$ and $N b$ . Then the Cayley table of $D_{6}/N$ is simply  

<html><body><table><tr><td>*</td><td>N</td><td>Nb</td></tr><tr><td>N</td><td>N</td><td>Nb</td></tr><tr><td>Nb</td><td>Nb</td><td>N</td></tr></table></body></html>  

Clearly, $D_{6}/N\cong C_{2}$ .  

# Example 4.2  

The additive group of integers is Abelian, so every subgroup is normal. If we have a subgroup of the form  

$$
N=m\mathbb{Z}=\{m x:x\in\mathbb{Z}\},
$$  

then we observed in Example 3.33 that the cosets of this subgroup are the sets of numbers which have the same remainer modulo $m$ , or more concretely,  

$$
\mathbb{Z}/N=\{N,N+1,N+2,\ldots,N+(m-1)\}.
$$  

The group operation on these cosets is just  

$$
\left(N+x\right)+\left(N+y\right)=N+\left(x+y\right)=N+z,
$$  

where $z=x+y$ (mod $m$ ). In other words $\mathbb{Z}/m\mathbb{Z}\cong Z_{m}$ . Indeed, this is a common way of defining addition modulo $m$ . $\diamondsuit$  

From the theory developed in the previous chapters, we can immediately conclude the following.  

# Proposition 4.2  

Let $G$ be a group, and $N$ a subgroup of $G$ . Then  

(i) if $|G|$ is finite, then $|G/N|=|G|/|N|$ ,   
(ii) the function $\alpha(x)\ =\ N x$ is a homomorphism from $G$ to $G/N$ , and $\ker\alpha=N$ .  

Proof:  

(i) This is immediate from the fact that $|G/N|=[G:N]$ and Lagrange’s Theorem.  

(ii) That $\alpha$ is a homomorphism follows immediately from the fact that  

$$
\alpha(x)*\alpha(y)=N x*N y=N x y=\alpha(x y).
$$  

It is immediate that $N\subseteq\ker\alpha$ , since if $x\in N$ , $\alpha(x)=N x=N$ . Similarly, if $\alpha(x)=N$ , then $N x=N$ , which only happens when $x\in N$ . So $\ker\alpha=N$ . $\mid$  

# Corollary 4.3  

$A$ subgroup $N$ of a group $G$ is normal if and only if it is the kernel of some homomorphism.  

The relationship between quotient groups and homomorphisms is significantly deeper than this corollary, however. These relationships are encapsulated in a trilogy of theorems called the Isomorphism Theorems. Unfortunately there is little consensus about which of the three should be first, second and third.  

# Theorem 4.4 (First Isomorphism Theorem)  

Let $G$ and $H$ be groups, and $\alpha:G\rightarrow H$ a homomorphism. Then  

$$
\alpha(G)\cong G/\ker\alpha.
$$  

Proof:  

For simplicity of notation, let $N=\ker\alpha$ .  

We would like to define a function $\beta:G/\ker\alpha\rightarrow\alpha(G)$ by $\beta(N x)=\alpha(x)$ , but it’s not clear that this is a well-defined function. To verify that this definition is good, we need to show that if $N x=N y$ then $\beta(N x)=\beta(N y)$ so that the value of $\beta$ does not depend on the choice of $x$ .  

Now if $N x=N y$ we have that $x y^{-1}\in N$ , so $\alpha(x y^{-1})=e$ , and hence  

$$
\alpha(y)=e\alpha(y)=\alpha(x y^{-1})\alpha(y)=\alpha(x y^{-1}y)=\alpha(x).
$$  

So we conclude that if $N x=N y$ then $b e t a(N x)=\beta(N y)$ , and so $\beta$ is welldefined.  

Furthermore, $\beta$ is a homomorphism, since  

$$
\beta(N x*N y)=\beta(N x y)=\alpha(x y)=\alpha(x)\alpha(y)=\beta(N x)\beta(N y).
$$  

We also have that $\beta$ is onto, since if $y\in\alpha(G)$ , then $y=\alpha(x)$ for some $x\in G$ , but then $y=\beta(N x)$ .  

Finally, if $\beta(N x)=\beta(N y)$ , then $\alpha(x)=\alpha(y)$ , so  

$$
\alpha(x y^{-1})=\alpha(x)(\alpha(y))^{-1}=\alpha(x)(\alpha(x))^{-1}=e.
$$  

This means that $x y^{-1}\in\ker\alpha=N$ , so $N x=N y$ . Hence $\beta$ is one-to-one.  

So $\beta$ is an isomorphism, and we conclude that $G/\ker\alpha\cong\alpha(G)$ .  

We will now turn to look at how the subgroup structure of $G$ and the subgroup structure of $G/N$ are related. Letting $\alpha:G\to G/N$ be given by $\alpha(x)=N x$ , we have that if $H\leq G$ , then $\alpha(H)\leq G/N$ and if $K\subseteq G/N$ then $\alpha^{-1}(K)\leq G$ from Propositions 2.24 and 2.26. A deeper question is if there is any relationship between normal subgroups of $G$ and normal subgroups of $G/N$ .  

# Proposition 4.5  

Let $G$ be a group and $N$ a normal subgroup of $G$ . Then every subgroup of $N$ is equal to $K/N$ where for some $K$ with $N\leq K\leq G$ . Furthermore $K/N$ is normal if and only if $K$ is normal.  

Proof:  

Let $\alpha:G\to G/N$ be given by $\alpha(x)=N x$ .  

Let $H$ be a subgroup of $G/N$ , and let $K=\alpha^{-1}(H)$ , so that $K$ is a subgroup of $G$ , and since $N=\alpha^{-1}(\{e\})\subseteq\alpha^{-1}(H)$ , so $N\leq K$ . So $H=\alpha(K)$ and if we restrict $\alpha$ to $K$ , the First Isomorphism Theorem tells us that  

$$
\alpha(K)\cong K/N.
$$  

Furthermore, recall from the proof of the First Isomorphism Theorem that this isomorphism is given by $\beta(N x)=\alpha(x)$ , and $\alpha(x)=N x$ , so $\beta$ is just the identity map, and so $H=K/N$ .  

If $K$ is normal in $G$ , then $x^{-1}K x=K$ for each $x\in G$ , and so  

$$
(N x^{-1})*\alpha(K)*(N x)=\alpha(x^{-1})*\alpha(K)*\alpha(x)=\alpha(x^{-1}K x)=\alpha(K),
$$  

so $H=\alpha(K)$ is normal in $G/N$ .  

Conversely, if $H=K/N$ is normal in $G/N$ , then Theorem 3.51 tells us immediately that $\alpha^{-1}(H)=K$ is normal in $G$ .  

Now if $N\triangleleft K\triangleleft G$ , as in the last part of the Proposition, we note that $N$ is normal when regarded as a subgroup of $K$ also, and so we can take three quotients: $G/N$ , $G/K$ and $K/N$ . The Second Isomorphism Theorem gives us a relationship between these three quotients.  

# Theorem 4.6 (Second Isomorphism Theorem)  

Let $G$ be a group and $N$ and $K$ normal subgroups of $G$ with $N\leq K$ . Then  

$$
(G/N)/(K/N)\cong G/K.
$$  

Proof:  

We first note that since $K$ is normal, the previous proposition tells us that $K/N$ is normal in $G/N$ , and so $(G/N)/(K/N)$ is defined.  

We would like to define a function $\alpha:G/N\to G/K$ by $\alpha(N x)=K x$ , but once again we must be careful that this well-defined, since there are multiple possible choices for $x$ which give the same coset $N x$ . If $N x=N y$ , then we recall that $x y^{-1}\in N$ , and so $x y^{-1}\in K$ as well. So $K=K(x y^{-1})$ ,  

$$
\alpha(N y)=K y=(K x y^{-1})y=K x=\alpha(N y).
$$  

So this is a well-defined function.  

Furthermore, $\alpha$ is a homomorphism, since  

$$
{}^{r}x*N y)=\alpha(N x y)=K x y=K x*K y=\alpha(N
$$  

We observe that $\alpha(N x)=K$ if and only if $x\in K$ , or equivalently, $N x\in K/N$ . But this means that k $\mathrm{er}\alpha=K/N$ . We also have that since every coset of $K$ is of the form $K x$ for some $x\in G$ , we have that $\alpha(G/N)=\{\alpha(N x):x\in G\}=$ $\{K x:x\in G\}=G/K$ , so $\alpha$ is onto.  

Now the First Isomorphism Theorem tells us that  

$$
\alpha(G/N)\cong(G/N)/\ker\alpha,
$$  

But we know that $\alpha(G/N)=G/K$ and $\ker\alpha=K/N$ , so  

$$
G/K\cong(G/N)/(K/N).
$$  

Note that this theorem essentially says that the quotient operation cancels in the way that you would expect a quotient to cancel: if $G$ , $K$ and $N$ were numbers you would expect the same equation to hold.  

We can also ask what happens if $K$ is a general subgroup of $G$ . In this case we can’t talk about $K/N$ , since we may not have $N\subseteq K$ . However we do know that the meet of $K$ and $N$ is a subgroup of $K$ . Indeed we have that for any $x\in K$ , and $y\in K\land N$ we have that $x^{-1}y x\in K$ , since $K$ is a subgroup. But we also have that $x^{-1}y x\in N$ , since $N$ is normal. Hence $K\wedge N$ is a normal subgroup of the subgroup $K$ . So we can consider the quotient group $K/(K\wedge N)$ . Similarly, although $N$ is not a subgroup of $K$ , we know that $N\vee K$ contains $N$ , and since $N$ is normal, we can consider the quotient group $(K\vee N)/N$ .  

# Theorem 4.7 (Third Isomorphism Theorem)  

Let $G$ be a group, $K$ a subgroup of $G$ and $N$ a normal subgroup of $G$ . Then  

$$
{\cal K}/(K\wedge N)\cong(K\vee N)/N.
$$  

Proof:  

Recall from Proposition 3.48 that $K\vee N=N K$ when $N$ is normal.  

We define a function $\alpha:K\to G/N$ by $\alpha(x)=N x$ . This is a homomorphism since  

$$
\alpha(x y)=N x y=N x*N y=\alpha(x)*\alpha(y).
$$  

The image of $\alpha$ is the set  

$$
\alpha(K)=\{N x:x\in K\}=N K/N=(N\vee K)/N.
$$  

Furthermore, the kernel of $\alpha$ is the set of $x$ such that $\alpha(x)=N$ , ie. all $x\in K$ such that $N x=N$ . But $N x=N$ if ans only if $x\in N$ , so $x\in K\cap N=K\land N$ . So the First Isomorphism Theorem tells us that  

$$
\alpha(K)=K/\ker\alpha,
$$  

and so  

$$
(N\vee K)/N\cong K/(K\wedge N).
$$  

As you may expect, quotient groups can be used to shed some light on the structure of finite groups.  

# Theorem 4.8  

Let $G$ be a finite group which is not Abelian, and $Z(G)$ the centre of $G$ . Then $G/Z(G)$ cannot be cyclic.  

Proof:  

We first recall that $Z(G)$ is always normal, so $G/Z(G)$ is defined. If $G/Z(G)$ is cyclic, then we can find an element $t$ so that $Z(G)t$ generates $G/Z(G)$ , and so every coset of $Z(G)$ is of the form $Z(G)t^{k}$ for some $k$ . But then given arbitrary elements $x$ and $y\in G$ , we have that $x\ =\ u t^{k}$ and $y\ =\ v t^{l}$ for some $u$ and $v\in Z(G)$ . So now, since $u$ and $v$ commute with all elements of $G$ ,  

$$
x y=u t^{k}v t^{l}=u v t^{k}t^{l}=u v t^{l}t^{k}=v t^{l}u t^{k}=y x.
$$  

So $G$ is Abelian, which is a contradiction.  

# Corollary 4.9  

If $p$ is a prime number and $G$ is a group with order $p^{2}$ , then $G$ is Abelian.  

Proof:  

We know from Theorem 3.41 that the order of $Z(G)$ is either $p$ or $p^{2}$ . But if $|Z(G)|=p$ , then $G$ is not Abelian, and we have that $|G/Z(G)|=|G|/|Z(G)|=$ $p^{2}/p\:=\:p$ , so $G/Z(G)$ must be a cyclic group of order $p$ . But the previous theorem showed that this cannot happen.  

Hence $|Z(G)|=p^{2}$ , and so $G$ is Abelian.  

This corollary allows us to slightly simplify the proof of Theorem 3.55, since the last paragraph can be replaced by a reference to this corollary. Indeed, one can generalise that theorem to all prime numbers.  

# Theorem 4.10  

If $p$ is a prime number and $G$ is a group with order $p^{2}$ , then $G$ is isomorphic to one of $C_{p^{2}}$ or $C_{p}\times C_{p}$ .  

Proof:  

The previous corollary tells us that $G$ is Abelian. If $G$ has an element of order $p^{2}$ , then $G\cong C_{p^{2}}$ .  

Otherwise every element of $G$ other then the identity $e$ has order $p$ . Let $a$ be such an element, and let $N=\langle a\rangle=\{1,a,a^{2},\dots,a^{p-1}\}$ . Lagrange’s Theorem tells us that the quotient group $G/N$ has order $p$ , so $G/N\cong C_{p}$ . So there is some element $b$ with order $p$ , such that $G/N$ is generated by the coset $N b$ , so $G/N=\{N,N b,N b^{2},\dots,N b^{n-1}\}$ . Hence $G=\{a^{k}b^{l}:k,l=0,1,\ldots,p-1\}$ with $a^{p}=e$ and $b^{p}=e$ . Hence $G$ is isomorphic to $C_{p}\times C_{p}$ via the isomorphism  

$$
\alpha(u^{k},u^{l})=a^{k}b^{l}.
$$  

# Exercises  

4.1.1. Consider the group $D_{8}$ . The subgroup $Z(D_{8})=\{1,a^{2}\}$ is normal, since it is the centre of $D_{8}$ . Write down the Cayley table of $D_{8}/Z(D_{8})$ . Explain why $D_{8}/Z(D_{8})\cong V$ .   
4.1.2. Consider the group $D_{12}$ . The subgroup $Z(D_{12})=\{1,a^{3}\}$ is normal, since it is the centre of $D_{12}$ . Write down the Cayley table of $D_{12}/Z(D_{12})$ . Explain why $D_{12}/Z(D_{12})\cong D_{6}$ ?   
4.1.3. Generalize the above two results, and show that if $n$ is even, then $D_{2n}/Z(D_{2n})\cong$ Dn .   
4.1.4. Consider the normal subgroup $N=\{1,a^{2},a^{4}\}$ of the group $D_{12}$ . Write down the Cayley table of $D_{12}/N$ . Show $D_{12}/N\cong V$ .  

4.1.5. Let $G$ and $H$ be groups. Show that the set  

$$
K=\{(g,e):g\in G\}
$$  

is a normal subgroup of $G\times H$ . Let $\pi:G\times H\to H$ be defined by $\pi(g,h)=$ $h$ (this a homomorphism by Exercise 2.9.6). Use this homomorphism to show that  

$$
(G\times H)/K\cong H.
$$  

4.1.6. Consider the additive group $\ensuremath{\mathbb{Z}}^{2}$ . Show that $\alpha(x,y)=3x+2y$ is a homomorphism from $\mathbb{Z}^{2}\to\mathbb{Z}$ , and $\ker\alpha=K=\{(2k,-3k):k\in i n t e g e r s\}$ . Show that  

$$
\mathbb{Z}^{2}/K\cong\mathbb{Z}.
$$  

4.1.7. Let $\alpha:G\rightarrow H$ be a homomorphism. Use the first Isomorphism Theorem to show that $|\alpha(G)|$ divides both $|G|$ and $|H|$ . Show that if $|G|$ and $|H|$ have a greatest common divisor of $^{1}$ , then $\alpha(x)=e$ for all $x\in G$ .   
4.1.8. Let $G$ be a group. Recall the commutator subgroup $G^{\prime}$ defined in Exercise 3.8.6 is normal. Show that $G/G^{\prime}$ is Abelian.   
4.1.9. Let $G$ be a group, and $N$ be a normal subgroup of $G$ such that $G/N$ is Abelian. Show that the commutator subgroup $G^{\prime}$ defined in Exercise 3.8.6 is a normal subgroup of $N$ .  

# 4.2 Automorphism Groups  

Recall that an automorphism of a group $G$ is an isomorphism from $G$ to itself. The set of all automorphisms of $G$ is denoted by $\operatorname{Aut}(G)$ . This set is never empty since at the very least the identity map defined $\operatorname{id}(x)=x$ is always an automorphism.  

# Proposition 4.11  

$I f G$ is a group, then $(\operatorname{Aut}(G),\circ,i d)$ is a group, where $\bigcirc$ is function composition.  

Proof:  

Function composition of two automorphisms gives another automorphism, since if $\alpha$ and $\beta\in\operatorname{Aut}(G)$ , then $\beta\circ\alpha:G\rightarrow G$ is an isomoprhism by Proposition 2.28, so $\beta\circ\alpha\in\operatorname{Aut}(G)$ .  

We already know that function composition is associative, so that group axiom holds.  

The identity map id is an identity under composition, since for any $x\in G$ ,  

$$
{\mathrm{(id~}}\circ\alpha{\mathrm{)}}(x)={\mathrm{id}}(\alpha(x))=\alpha(x)\qquad{\mathrm{and}}\qquad(\alpha\circ{\mathrm{id}})(x)=\alpha({\mathrm{id}}(x))=\alpha(x),
$$  

so we conclude that ${\mathrm{id}}\circ\alpha=\alpha\circ{\mathrm{id}}=\alpha$ .  

Since $\alpha$ is an isomorphism, it has an inverse function which is also an isomorphism from $G$ to $G$ by Proposition 2.28. We know that for any inverse function, $\alpha^{-1}\circ\alpha=\mathrm{id}=\alpha\circ\alpha^{-1}$ , so $\alpha^{-1}$ is an inverse for $\alpha$ in the set of automorphisms.  

# Example 4.3  

Consider the group $C_{4}$ . Any automorphism has to preserve the order of each of the elements, and since $a^{2}$ is the only element of order 2, $\alpha(a^{2})=a^{2}$ for every automorphism $\alpha$ . However, an automorphism could potentially swap $a$ and $a^{3}$ . Indeed, there are two automorphisms: id and the function $\alpha(x)=x^{-1}$ , or more concretely,  

<html><body><table><tr><td></td><td>α(x)</td></tr><tr><td>1</td><td>1</td></tr><tr><td>a a?</td><td>a² Q3</td></tr><tr><td>a3</td><td>a</td></tr></table></body></html>  

Verifying that this is an isomorphism is easy, since  

$$
\alpha(a^{k}a^{l})=\alpha(a^{k+l})=a^{-k-l}=a^{-k}a^{-l}=\alpha(a^{k})\alpha(a^{l}),
$$  

and it is clearly bijective.  

Since there are only two elements, $\mathrm{Aut}(C_{4})\cong C_{2}$  

# Example 4.4  

Consider the four-group $V$ . $V$ has 3 elements of order 2, $a$ , $b$ and $a b$ , so an isomorphism could possibly interchange those elements. In fact any permutation of these three elements gives rise to a distinct automorphism.  

For example, the function $\alpha$ given by  

<html><body><table><tr><td>C</td><td>α(x)</td></tr><tr><td>1</td><td>1</td></tr><tr><td>a</td><td>ab</td></tr><tr><td>b</td><td>b</td></tr><tr><td>ab</td><td>a</td></tr></table></body></html>  

is an automorphism: it is clearly bijective, $\alpha(x x)=1=\alpha(x)\alpha(x)$ for any $x\in V$ , since $x^{2}=1$ for every element, $\alpha(1x)=\alpha(1)\alpha(x)$ for any $x\in V$ , and the remaining cases are covered by  

$$
\begin{array}{c}{{\alpha(a b)=a=(a b)b=\alpha(a)\alpha(b)}}\\ {{\alpha(a(a b))=b=(a b)a=\alpha(a)\alpha(a b)}}\\ {{\alpha(b(a b))=a b=b a=\alpha(b)\alpha(a b).}}\end{array}
$$  

So $\alpha$ is indeed an automorphism.  

Given that every automorphism corresponds to a permutation of a set with 3 elements, and function composition will correspond to composition of the permutations, we have that $\operatorname{Aut}(V)\cong S_{3}$ . $\diamondsuit$  

As the previous example illustrates, finding all the automorphisms of a group can be potentially difficult. However, non-Abelian groups have a collection of automorphisms which are easy to find.  

# Proposition 4.12  

If $G$ is a group, then the conjugation by $\boldsymbol{x}$ function  

$$
\alpha_{x}(y)=x^{-1}y x
$$  

is an automorphism.  

Proof:  

We first note that  

$$
\alpha_{x}(y z)=x^{-1}y z x=x^{-1}y x x^{-1}z x=\alpha(y)\alpha(z),
$$  

May 3, 2004  

so $\alpha:G\rightarrow G$ is a homomorphism. Now $\alpha_{x}(y)=e$ if and only if  

$$
x^{-1}y x=e.
$$  

But this implies that $y=x x^{-1}=e$ . So $\ker\alpha=\{e\}$ , and so $\alpha_{x}$ is one-to-one.   
Finally, $G$ is a normal subgroup of itself, so $x^{-1}G x=G$ , and so $\alpha_{x}(G)=G$ .  

Hence $\alpha_{x}$ is an automorphism.  

We call such automorphisms inner automorphism, and let $\operatorname{Inn}(G)=\{\alpha_{x}:$ $x\in G\}$ be the set of all inner automorphisms of $G$ . Of course, not every one of these automorphisms need be distinct, since if $x\in Z(G)$ , then  

$$
\alpha_{x}(y)=x^{-1}y x=\operatorname{id}(y),
$$  

for every $y$ , and so $\alpha_{x}=y$ . So in particular, the collection of inner automorphisms is just $\{{\mathrm{id}}\}$ if $G$ is Abelian. In fact the set of inner automorphisms is very closely related to the centre.  

# Theorem 4.13  

Let $G$ be a group. Then ${\mathrm{Inn}}(G)$ is a subgroup of $\operatorname{Aut}(G)$ , the function $\beta:x\mapsto$ $\alpha_{x}-1$ is a homomorphism from $G$ onto $\operatorname{Inn}(G)$ , and  

$$
\operatorname{Inn}(G)\cong G/Z(G).
$$  

Proof:  

We start by observing that the function $\beta$ is a homomorphism:  

$$
\beta(x y)(z)=\alpha_{(x y)^{-1}}(z)=x y z y^{-1}x=x(\alpha_{y^{-1}}(z))x=\alpha_{x^{-1}}(\alpha_{y^{-1}}(z))=(\beta(x)\circ\beta(y))(z)
$$  

for all $z\in G$ , so $\beta(x y)=\beta(x)\circ\beta(y)$ .  

Now the image of $G$ under $\beta$ is precisely ${\mathrm{Inn}}(G)$ , so $\operatorname{Inn}(G)$ is a subgroup. Furthermore, $\beta(x)=\operatorname{id}$ if and only if $\beta(x)(z)=z$ for all $z\in G$ , or equivalently,  

$$
x z x^{-1}=z
$$  

or all $z\in G$ . So $x\in\ker\beta$ if and only if $x\in Z(G)$ . Therefore $\ker\beta=Z(G)$ .  

The First Isomorphism Theorem then tells us that  

$$
\beta(G)\cong G/\ker\beta,
$$  

so  

$$
\operatorname{Inn}(G)\cong G/Z(G).
$$  

# Corollary 4.14  

If $G$ is a finite group which is not Abelian, then ${\mathrm{Inn}}(G)$ is never cyclic.  

May 3, 2004  

Proof:  

This follows immediately from the above theorem and Theorem 4.8.  

# Example 4.5  

The group $D_{6}$ has centre $Z(D_{6})=\{1\}$ , so ${\mathrm{Inn}}(D_{6})\cong D_{6}$ . In fact, since $D_{6}$ has 2 elements of order 3, and 3 elements of order 2, if $\alpha\in\mathrm{Aut}(D_{6})$ we must have  

$$
\begin{array}{l}{{\alpha(a)=a\mathrm{~or~}a^{2}}}\\ {{\alpha(b)=b,a b\mathrm{~or~}a^{2}b}}\end{array}
$$  

and since $\alpha(a^{k}b^{l})=(\alpha(a))^{k}(\alpha(b))^{l}$ , these choices completely determine the automorphism. Therefore $|\mathrm{Aut}(D_{6})|\le6$ , and since $\vert\mathrm{Inn}(D_{6})\vert=\vert D_{6}\vert=6$ , every automorphism of $D_{6}$ is inner. $\diamondsuit$  

# Example 4.6  

The group $D_{8}$ has centre $Z(D_{8})=\{1,a^{2}\}$ , so $|\operatorname{Inn}(G)|=|D_{8}|/|Z(D_{8})|=$ $8/2=4$ . But we know that $D_{8}/Z(D_{8})$ cannot be cyclic, so $\operatorname{Inn}(G)\cong V$ . 3  

It’s worthwhile noting that if $H$ is a normal subgroup of $G$ , then the inner automorphisms of $G$ are automorphisms of $H$ when restricted to just $H$ . This follows because if $x\in G$ , we have that  

$$
H=x^{-1}H x=\alpha_{x}(H),
$$  

so $\alpha_{x}$ , regarded as a function defined on $H$ , must be a bijective homomorphism onto $H$ , ie. an element of $\operatorname{Aut}(H)$ .  

A characteristic subgroup $H$ of a group $G$ is a subgroup which is invariant under every automorphism of $G$ , in other words $\alpha(H)=H$ for all $\alpha\in\operatorname{Aut}(G)$ . Every characteristic subgroup is automatically normal, since if $x\in G$ , so $\alpha_{x}\in$ $\operatorname{Inn}(G)$ , then  

$$
H=\alpha_{x}(H)=x^{-1}H x.
$$  

The centre of $G$ is always characteristic, since any isomorphism always maps the centre to the centre. The trivial subgroups $G$ and $\{e\}$ are also always characteristic.  

# Example 4.7  

In the group $D_{6}$ , the subgroup $H=\langle a\rangle$ is characteristic, since any automorphism must map 1 to $^{1}$ and elements of order 3 to elements of order 3, ie. the set $\{a,a^{2}\}$ maps onto $\{a,a^{2}\}$ . 3  

# Proposition 4.15  

If $G$ is a group, $N$ is a normal subgroup of $G$ and $H$ is a characteristic subgroup of $N$ , then $H$ is a normal subgroup of $G$ .  

Proof:  

Since $\alpha_{x}\in\operatorname{Inn}(G)$ is an automorphism of $N$ , and $H$ is characteristic, then  

$$
x^{-1}H x=\alpha_{x}(H)=H.
$$  

The inner automorphisms which come from characteristic subgroups are also interesting.  

# Theorem 4.16  

Let $G$ be a group, and let $H$ be a characteristic subgroup of $G$ . Then the set of inner automorphisms of the form $\{\alpha_{x}:x\in H\}$ is a normal subgroup of $\operatorname{Aut}(G)$ .  

Proof:  

We first note that since $\beta(x)=\alpha_{x^{-1}}$ is a homomorphism from $G$ to ${\mathrm{Inn}}(G)$ , $\beta(H)=\{\alpha_{x}:x\in H\}$ is a subgroup of $\operatorname{Aut}(G)$ .  

Let $x\in H$ . Given any automorphism $\alpha$ , we have that for any $z\in G$ ,  

$$
\begin{array}{r l}&{(\alpha^{-1}\circ\alpha_{x}\circ\alpha)(z)=\alpha^{-1}(\alpha_{x}(\alpha(z)))}\\ &{\qquad=\alpha^{-1}(x^{-1}\alpha(z)x)}\\ &{\qquad=\alpha^{-1}(x^{-1})\alpha^{-1}(\alpha(z))\alpha^{-1}(x)}\\ &{\qquad=(\alpha^{-1}(x))^{-1}z\alpha^{-1}(x)}\\ &{\qquad=\alpha_{\alpha^{-1}(x)}(z).}\end{array}
$$  

But since $H$ is characteristic, $\alpha^{-1}(x)\in H$ , so $\alpha^{-1}\circ\alpha_{x}\circ\alpha\in\beta(H)$ . Hence $\beta(H)$ is a normal subgroup of $\operatorname{Aut}(G)$ .  

# Corollary 4.17  

If $G$ is a group, then $\operatorname{Inn}(G)$ is a normal subgroup of $\operatorname{Aut}(G)$ .  

# Exercises  

4.2.1. Find the automorphism group of $C_{5}$ . Does $C_{5}$ have any non-trivial inner automorphisms?   
4.2.2. Find the automorphism group of $C_{6}$ .   
4.2.3. Find $\operatorname{Aut}(Q_{8})$ .   
4.2.4. Show that the additive group of integers has only two automorphisms: id and $\iota(x)=-x$ . Conclude that $\mathrm{Aut}(\mathbb{Z})\cong C_{2}$ .   
4.2.5. Let $\mathbb{Z}_{m}$ be the additive group of integers modulo $m$ , and let $\alpha:\mathbb{Z}_{m}\rightarrow\mathbb{Z}_{m}$ be a homomorphism. Show that if $\alpha(1)=k$ , then $\alpha(x)=k x$ .  

May 3, 2004  

Show that $\alpha$ is an automorphism if and only if the greatest common divisor of $k$ and $m$ is $^{1}$ .  

Let $\alpha_{k}(x)=k x$ on $\mathbb{Z}_{m}$ . Show that $\alpha_{k}\circ\alpha_{j}=\alpha_{k j}$ .  

Show that $\operatorname{Aut}(\mathbb{Z}_{m})$ is isomorphic to the multiplicative group $\mathbb{Z}_{m}^{*}=\{x:$ $\operatorname*{gcd}(x,m)=1\}$ .  

Show that $\operatorname{Aut}(\mathbb{Z}_{8})\cong V$ .  

Show that if $m=p q$ where $p$ and $q$ are distinct primes, then $|\operatorname{Aut}(\mathbb{Z}_{m})|=$ $(p-1)(q-1)$ .  

4.2.6. Show that if $(G,+,0)$ is any finite Abelian group (written using additive notation), then the function  

$$
\alpha_{k}(x)=\underbrace{x+x+x+\dots+x}_{k{\mathrm{~times}}}
$$  

is a homomorphism from $G$ to $G$ . Show that if $k$ and $|G|$ have a greatest common divisor of $^{1}$ , then $\alpha_{k}(x)=0$ if and only if $x=0$ . Show that in this case, $\alpha_{k}$ is an automorphism.  

Show that $\alpha_{k}\cup\alpha_{j}=\alpha_{k j}$ , so that the function $\beta:\mathbb{Z}_{|G|}^{*}\to\operatorname{Aut}(G)$ defined by $\beta(k)=\alpha_{k}$ is a homomorphism.  

4.2.7. Show that if $G$ is a group, the function $\iota(x)=x^{-1}$ is an automorphism if and only if $G$ is Abelian.  

4.2.8. Show that if $G\cong H$ , then $\operatorname{Aut}(G)\cong\operatorname{Aut}(H)$ .  

4.2.9. Show that if $n$ is odd then $\mathrm{Inn}(D_{2n})\cong D_{2n}$ , while if $n$ is even then $\mathrm{Inn}(D_{2n})\cong D_{n}$ (Hint: you may use Exercise 4.1.3 to prove this).  

# 4.3 Extension: Category Theory  

If you think about the basic outlines of the theory which we have developed so far, you should notice some similarities between the theories of groups, vector spaces, partial orders and lattices. At a very abstract level we have:  

<html><body><table><tr><td>Sets</td><td>Functions</td></tr><tr><td>Groups</td><td>Homomorphisms</td></tr><tr><td>Vector Spaces</td><td>Linear Transformations</td></tr><tr><td>Partially Ordered Sets</td><td>Order-preserving Functions</td></tr><tr><td>Lattices</td><td>Lattice Homomorphisms</td></tr></table></body></html>  

There are also similarities beyond this: in all cases there is the notion of “isomorphism” between appropriate types of sets and the notion of a “sub-” object (like a subgroup or subspace), for example.  

The model that we should keep in mind ofr what we are about to define is simply a minimal set of axioms which sets and functions will satisfy:  

1. each function has a domain and codomain,  

2. if $\operatorname{dom}f=\operatorname{cod}g$ we can compose the functions,  

3. function composition is associative,  

4. for each set $X$ , there is an identity function $\operatorname{id}_{A}:X\to X$ , and this identity function has the property $f\circ\operatorname{id}_{A}=f$ and $1\mathrm{d}_{A}\circ g=g$ .  

Notice that group homomorphisms also satisfy all of these conditions.  

Definition 4.2   
$A$ category $\boldsymbol{\mathscr{C}}$ consists of a set of objects, $\mathcal{O}$ ; a set of arrows or morphisms   
$\mathcal{A}$ , two functions  

$$
\quad\cot:{\mathcal{A}}\rightarrow{\mathcal{O}}\qquad\ a n d\qquad\ \operatorname{dom:}{\mathcal{A}}\rightarrow{\mathcal{O}}
$$  

which assign to each arrow an object called, respectively, the domain and codomain of the arrow; a function  

$$
i d:{\mathcal{O}}\to{\mathcal{A}},
$$  

which assigns to each object $A$ an identity arrow $i d_{A}$ ; and a composition operation that assigns each to pair of arrows $(\alpha,\beta)$ with d $\tan\alpha=\cot\beta$ an arrow $\gamma=\alpha\circ\beta$ with $\operatorname{cod}\gamma=\operatorname{cod}\alpha$ and $\operatorname{dom}\gamma=\operatorname{dom}\beta$ .  

We will write $f:A\rightarrow B$ to denote that an arrow $f$ has domain $A$ and codomain $B$ , or diagramatically, write:  

$$
A\rightarrow B
$$  

These have to satisfy the following axioms:  

(i) Associativity: if $f:B\rightarrow A$ , $g:C\rightarrow B$ , and $h:D\to C$ , then $(f\circ g)\circ h=f\circ(g\circ h)$ ,   
(ii) Identity: for any $f:A\rightarrow B$ , $f\circ i d_{A}=f$ ; and for any $g:B\rightarrow A$ , $i d_{A}\circ g=g$ ,  

Notice that these axioms are very similar to the definition of a group, but with added complexity because of the neccessity of dealing with the domains and codomains, and with no inverse axiom.  

Categories are very closely related to directed graphs, and we can often represent parts of a category graphically. Many key facts in category can be represented by succinctly by commuting diagrams. The key property of a commuting diagram is that any path following the arrows through a diagram that start from the same object and ends at the same object are equal. For example, the associativity axiom can be represented by the following commuting diagram:  

XXX Picture   
Similarly, the following two diagrams represent the identity axioms:   
XXX Picture  

As is the case for associative binary operations, the associativity axiom for categories means that it doesn’t matter where we put the parentheses in a composition of multiple arrows. We can also show that for each $A$ , id $A$ is unique.  

# Example 4.8  

The following are all categories:  

1. Set: the category with objects being all sets contained in some universe $U$ and arrows being all functions on those sets.   
2. Grp: the category with objects being all groups contained in some universe $U$ , and arrows being all group homomorphisms.   
3. Abl: the category with objects being all Abelian groups contained in some universe $U$ , and arrows being group homomorphisms between them.   
4. $\mathbf{Vec}(\mathbb{F})$ : the category with objects being all vector spaces over a field $\mathbb{F}$ (contained in some universe $U$ ), and arrows being linear transformations between them.   
5. Lat: the category with objects being all lattices contained in some universe $U$ , and arrows being lattice homomorphisms.   
6. $\mathbf{set}_{*}$ : the category whose objects are pointed sets: pairs $(X,x)$ , where $X$ is a set contained in some universe $U$ , and $x\in X$ is some distinguished point; and whose arrows are functions which map distingushed points to distinguished points: if $(X,x)$ and $(Y,y)$ are pointed sets, then $f:X\to Y$ is a morphism if and only if $f(x)=y$ .  

There are, of course, many, many other categories. Indeed, whenever you encounter a new mathematical object, particularly in algebra, you should ask yourself “what is the category that goes with this?” If you can establish this, then you can immediately get a number of basic results for free.  

For a good theory which encompasses the fundamentals of functions on sets and group homomorphisms, we need to have more than just composition and identity. We also need to determine analogues of injective functions (or group monomorphisms), surjective functions (or group epimorphisms), and most importantly bijection (or group isomorphisms).  

# Definition 4.3  

Let $\boldsymbol{\mathscr{C}}$ be a category with objects $\boldsymbol{\mathcal{O}}$ and arrows $\mathcal{A}$ . An arrow $\alpha:A\rightarrow B$ is invertible if there is an arrow $\alpha^{-1}:B\rightarrow A$ such that $\alpha^{-1}\circ\alpha=i d_{A}$ and $\alpha\circ\alpha^{-1}=i d_{B}$ . We say that two objects $A$ and $B$ are isomorphic if there is an invertible arrow $\alpha:A\rightarrow B$ .  

An arrow $\alpha:A\rightarrow B$ is monic if whenever there are arrows $\beta_{1}$ and $\beta_{2}:C\to$ $A$ such that $\alpha\circ\beta_{1}=\alpha\circ\beta_{2}$ , then $\beta_{1}=\beta_{2}$ (ie. we can cancel $\alpha$ on the left). An arrow $\alpha:A\rightarrow B$ is epi if whenever there are arrows $\beta_{1}$ and $\beta_{2}:B\rightarrow C$ such that $\beta_{1}\circ\alpha=\beta_{2}\circ\alpha$ , then $\beta_{1}=\beta_{2}$ (ie. we can cancel $\alpha$ on the right).  

$A$ right inverse of an arrow $\alpha:A\rightarrow B$ is an arrow $\rho:B\rightarrow A$ such that $\alpha\circ\rho=i d_{B}$ . A left inverse of an arrow $\alpha:A\rightarrow B$ is an arrow $\lambda:B\rightarrow A$ so that $\lambda\circ\alpha=i d_{A}$ . A right inverse of $\alpha$ is also called a section of $\alpha$ , while a left-inverse is called a retraction of $\alpha$ .  

If an object $A$ has the property that for any object $B$ we have a exactly one arrow $B\rightarrow A$ , it is said to be terminal. If instead it has the property that there is exactly one arrow from $A\rightarrow B$ , then it is said to be initial.  

# Example 4.9  

In the categories of Set and Grp, we have that following correspondence:   


<html><body><table><tr><td></td><td>Set</td><td>Grp</td></tr><tr><td>invertible arrow</td><td>bijective function</td><td>isomorphism</td></tr><tr><td>monic arrow</td><td>injective function</td><td> monomorphism</td></tr><tr><td>epi arrow</td><td>surjective function</td><td>epimorphism</td></tr><tr><td>terminal object</td><td>any set with one element</td><td>any group with one element</td></tr><tr><td>initial object</td><td>the empty set</td><td>any group with one element</td></tr></table></body></html>  

We can prove a number of facts immediately:  

# Proposition 4.18  

If $\boldsymbol{\mathscr{C}}$ is a category, then  

(i) if an arrow $\alpha$ has a right inverse, then it is epi,   
(ii) if an arrow $\alpha$ has a left inverse, then it is monic,   
(iii) an arrow $\alpha$ has both a left and right inverse if and only if it is invertible, (iv) if an arrow is invertible, it is both epi and monic.  

Proof:  

(i) Let $\alpha:A\rightarrow B$ and let $\rho:B\rightarrow A$ be a right inverse of $\alpha$ . Then given any arrows $\beta_{1}$ and $\beta_{2}:B\rightarrow C$ such that $\beta_{1}\circ\alpha=\beta_{2}\circ\alpha$ , we have that  

$$
{\begin{array}{r l}&{\beta_{1}=\beta_{1}\circ\mathrm{id}}\\ &{\quad=\beta_{1}\circ\alpha\circ\rho}\\ &{\quad=\beta_{2}\circ\alpha\circ\rho}\\ &{\quad=\beta_{2}\circ\mathrm{id}}\\ &{\quad=\beta_{2}.}\end{array}}
$$  

So $\alpha$ is epi.  

(ii) The proof of this is left as an exercise. (iii) If $\alpha$ is invertible, then the inverse is both a left and right inverse, so $\alpha$ has both a left and right inverse.  

On the other hand, if $\alpha:A\rightarrow B$ and $\rho:B\rightarrow A$ and $\lambda:B\rightarrow A$ be right and left inverses of $\alpha$ , respectively, then  

$$
\begin{array}{l}{\rho=\mathrm{id}\circ\rho}\\ {\quad=\lambda\circ\alpha\circ\rho}\\ {\quad=\lambda\circ\mathrm{id}}\\ {\quad=\lambda}\end{array}
$$  

So $\rho=\lambda$ is an inverse for $\alpha$ .  

(iv) This follows immediately from the first three parts.  

In the previous chapter, key results came from looking at an object in another category which corresponded to the group, such as looking at the subgroup lattice of a group in the category of lattices. These sorts of correspondences between categories were first recognised in the study of algebraic topology, and are extremely powerful. Indeed, in some sense they are what justifies the study of categories as a distinct topic.  

# Definition 4.4  

Let $\zeta_{1}$ and $\zeta_{2}$ be two categories with object sets $\mathcal{O}_{1}$ and $\mathcal{O}_{2}$ , and arrow sets $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ , respectively. A functor $\mathcal{F}:\mathcal{C}_{1}\longrightarrow\mathcal{C}_{2}$ is a pair of functions ${\mathcal{F}}_{\mathcal{O}}:{\mathcal{O}}_{1}\to$ $\mathcal{O}_{2}$ and $\mathcal{F}_{A}:\mathcal{A}_{1}\rightarrow\mathcal{A}_{2}$ such that if $A\in{\mathcal{O}}_{\infty}$ , $\alpha$ and $\beta\in{\mathcal{A}}_{\infty}$ , then  

$$
\begin{array}{r l}&{\mathrm{dom}\mathcal{F}_{A}(\alpha)=\mathcal{F}_{\mathcal{O}}(\mathrm{dom}\alpha)\qquad\mathrm{cod}\mathcal{F}_{A}(\alpha)=\mathcal{F}_{\mathcal{O}}(\mathrm{cod}\alpha)}\\ &{\quad\mathcal{F}_{A}(\alpha\circ\beta)=(\mathcal{F}_{A}(\alpha)\circ\mathcal{F}_{A}(\beta)\qquad\mathcal{F}_{A}(i d_{A})=i d_{\mathcal{F}_{\mathcal{O}}}(A)}\end{array}
$$  

whenever $\alpha\circ\beta$ is defined.  

We usually don’t distinguish between the functor, and the functions on the sets of objects and arrows, simply representing each of them by a single symbol $\mathcal{F}$ .  

# Example 4.10  

Let Grp and Lat be the categories of groups and lattices defined earlier. Then we know that the function $\mathcal{F}_{\mathcal{O}}$ defined by ${\mathcal{F}}_{{\mathcal{O}}}(G)={\mathrm{Sub}}(G)$ is a function from the objects of Grp to the objects of Lat. Corollary 3.10 tells us that if we have a group homomorphism $\alpha:G\rightarrow H$ , then we have a corresponding lattice homomorphism ${\overline{{\alpha}}}:\operatorname{Sub}(G)\to\operatorname{Sub}(H)$ . So we define $\mathcal{F}_{\mathcal{A}}(\alpha)=\overline{{\alpha}}$ . This gives the first two of the four conditions that we need for $\mathcal{F}$ to be a functor.  

It is easily verified that $\mathcal{F}_{\mathcal{A}}(\mathrm{id}_{G})=\mathrm{id}_{\mathcal{F}_{\mathcal{O}}(G)}$ , while it is a little more work to check the remaining composition condition. They do hold, however, so we have a functor between the categories.  

This is not the only possible functor between these two categories, since we could also consider a functor which maps a group $G$ to the lattice consisting of the power set ${\mathcal{P}}(G)$ with meet and join being intersection and union. $\diamondsuit$  

# Example 4.11  

The maps ${\mathcal{F}}(G)=G$ and $\mathcal{F}(\alpha)=\alpha$ give a functor $\mathcal{F}:\mathbf{Grp}\longrightarrow\mathbf{Set}$ .  

Functors such as the one in the last example are called forgetful functors because we are forgetting about all the extra structure that a group has and treating it just as a set, and regarding homomoprhisms simply as functions. Whenever we have a category whose objects and arrows are specializations of another categories objects and arrows we get a forgetful functor which strips this additional structure away.  

# Example 4.12  

There is a forgetful functor $\mathcal{F}:\mathbf{Abl}\longrightarrow\mathbf{Grp}$ , since Abelian groups are simply groups with an additional requirement of commutativity, and homomorphisms between Abelian groups are still homomoprhisms.  

When looking at the question of whether or not two objects within a category are isomorphic or not, functors can help us say that that the two objects are not isomorphic.  

# Proposition 4.19  

Let $\mathcal{C}$ be a category, and let $A$ and $B$ be two objects in $\boldsymbol{\mathscr{C}}$ . If there is another category $\mathcal{D}$ and a functor $\mathcal{F}:\mathcal{C}\rightarrow\mathcal{D}$ such that ${\mathcal{F}}(A)$ and ${\mathcal{F}}(B)$ are not isomorphic in $\mathcal{D}$ , then $A$ and $B$ are not isomorphic in $\boldsymbol{\mathscr{C}}$ .  

Proof:  

If $A$ and $B$ are isomorphic, then we have an invertible arrow $\alpha:A\rightarrow B$ and its inverse $\alpha^{-1}:B\rightarrow A$ . If $\mathcal{F}:\mathcal{C}\rightarrow\mathcal{D}$ is any functor, then  

$$
\mathcal F(\alpha)\circ\mathcal F(\alpha^{-1})=\mathcal F(\alpha\circ\alpha^{-1})=\mathcal F(\mathrm{id}_{B})=\mathrm{id}_{\mathcal F(B)}.
$$  

Similarly ${\mathcal{F}}(\alpha^{-1})\circ{\mathcal{F}}(\alpha)=\operatorname{id}_{{\mathcal{F}}(A)}$ , and so ${\mathcal{F}}(\alpha):{\mathcal{F}}(A)\rightarrow{\mathcal{F}}(B)$ has an inverse arrow $\mathcal{F}(\alpha^{-1})$ , and hence ${\mathcal{F}}(A)$ is isomorphic to $\mathcal{F}(B)$ .  

Hence if ${\mathcal{F}}(A)$ is not isomorphic to ${\mathcal{F}}(B)$ , then $A$ and $B$ are not isomorphi  

This very general result is at the core of many of the techniques we have for distinguishing groups which are not isomorphic. For example, the fact that two groups with different subgroup lattices are not isomorphic is an immediate corollary of this proposition, together with the functor of Example 4.10. The fact that two groups of different orders are not isomorphic is an immediate corollary of this proposition, using the forgetful functor from Grp to Set.  

Unfortunately, this doesn’t help us in showing when two groups are isomorphic, since there are many functors available, so checking every possible functor is impossible.  

The category of groups is not the only category of interest in mathematics, of course, so it is useful to observe that in every category we find many naturally occurring groups:  

# Proposition 4.20  

Let $\boldsymbol{\mathscr{C}}$ be a category, and $A$ an object in $\boldsymbol{\mathscr{C}}$ . Then the triple $(\operatorname{Aut}(A),\circ,i d_{A})$ , where $\operatorname{Aut}(A)$ is the set of invertible arrows from $A$ to $A$ , is a group. We call this the automorphism group of $A$ .  

# Proof:  

From the definition of a category, when $\cup$ is restricted to $\operatorname{Aut}(A)$ , it is an associative binary operation, and id $A$ is an identity. So the only thing that needs to be checked is that there is an inverse for every element, but this is guaranteed by the assumption that our arrows are all invertible.  

# Example 4.13  

In the category Grp, $\operatorname{Aut}(G)$ is precisely the automorphism group of $G$ , as discussed earlier. $\diamondsuit$  

# Example 4.14  

In the category of finite dimensional real vector spaces and linear transformations, we have that ${\mathrm{Aut}}(V)$ is the set of all invertible linear transformations from $V$ to $V$ . If $V$ has dimension $n$ , then this group is isomorphic to $G L_{n}(\mathbb{R})$ . $\diamondsuit$  

# Example 4.15  

In the category Set, the group $\operatorname{Aut}(X)$ consists of all bijections of $X$ onto itself, or in other words, the group of all permutations of $X$ . If $|X|=n$ , then $\operatorname{Aut}(X)\cong S_{n}$ . $\diamondsuit$  

# Example 4.16  

One can consider a category whose objects are all subsets of $\mathbb{R}^{n}$ , and whose arrows are isometries which map one subset onto another.  

In this category, the automorphism group of an object is the set of all symmetries of the object. $\diamondsuit$  

As the above examples illustrate, the concept of an automorphism group generalises the concept of symmetries that we first introduced in Chapter 1. Whenever you see a new type of mathematical object being introduced, there is usually a category associated with it, and hence there is some sort of automorphism group associated with each object. Given the wide variety of categories that are of interest in mathematics, this underlines the importance of group theory.  

# Proposition 4.21  

Let $\boldsymbol{\mathscr{C}}$ be a category, and $A$ and $B$ two objects in $\boldsymbol{\mathscr{C}}$ . If $A$ and $B$ are isomorphic, then $\operatorname{Aut}(A)$ and $\operatorname{Aut}(B)$ are isomorphic.  

Proof:  

Let $\alpha:A\rightarrow B$ be an invertible arrow. Then we define a function $\overline{{\alpha}}$ : $\operatorname{Aut}(A)\to\operatorname{Aut}(B)$ by  

$$
\overline{{\alpha}}(\beta)=\alpha\circ\beta\circ\alpha^{-1}.
$$  

One can easily verify that $\overline{{\alpha}}(\beta)$ is an invertible arrow, and hence the function is indeed into $\operatorname{Aut}(B)$ , and furthermore  

$$
\overline{{\alpha}}(\beta)\circ\overline{{\alpha}}(\gamma)=\alpha\circ\beta\circ\alpha^{-1}\circ\alpha\circ\gamma\circ\alpha^{-1}=\alpha\circ\beta\circ\gamma\circ\alpha^{-1}=\overline{{\alpha}}(\beta\circ\gamma),
$$  

so $\overline{{\alpha}}$ is a homomorphism. Similarly, ${\overline{{\alpha^{-1}}}}:\operatorname{Aut}({\underline{{B}}})\to\operatorname{Aut}(A)$ is a homomorphism, and furthermore $\overline{{\alpha}}\circ\overline{{\alpha^{-1}}}=\operatorname{id}_{\operatorname{Aut}(B)}$ and $\alpha^{-1}\circ{\overline{{\alpha}}}=\operatorname{id}_{\operatorname{Aut}(A)}$ , so $\overline{{\alpha}}$ is an isomorphism. $\mid$  

This means that in any time you have a category, you can use the automoprhism groups to distinguish non-isomoprhic objects in the category via the contrapositive of this result: if the automorphism groups are not isomorphic, the objects are not isomorphic.  

# Definition 4.5  

$I f{\cal C}$ is a category, and $A$ is an object in $\boldsymbol{\mathscr{C}}$ , then an action of a group $G$ on $A$ is a homomorphism $\alpha:G\to\operatorname{Aut}(A)$ .  

For most categories of interest the objects of the category are sets with additional structure and the arrows are functions with additional conditions that they must satisfy. This means that $\alpha(g)$ is a function of some sort from $A$ to $A$ . To avoid confusion, it is common to write $\alpha(g)=\alpha_{g}$ , so that we can use the less confusing notation $\alpha_{g}(x)$ instead of $(\alpha(g))(x)$ to represent the image of an element of $A$ under this automorphism. In this case we have that $\alpha_{g h}=\alpha_{g}\circ\alpha_{h}$ .  

# Example 4.17  

Given a group $G$ , the map $\alpha:G\to\operatorname{Aut}(G)$ defined by $\alpha(g)=\alpha_{g}$ , where $\alpha_{g}(x)=g^{-1}x g$ is an action of $G$ on itself. The image of the homomorphism is $\operatorname{Inn}(G)$ , and the kernel is $Z(G)$ . $\diamondsuit$  

# Example 4.18  

Given a group $G$ then for each $g\in G$ we have bijective functions $\lambda_{g}(x)=$ $g^{-1}x$ and $\rho_{g}(x)=x g$ . Since $\lambda_{g h}=\lambda_{g}\circ\lambda_{h}$ , and $\rho_{g h}=\rho_{g}\circ\rho_{h}$ , the functions $\lambda:g\mapsto\lambda_{g}$ and $\rho:g\mapsto\rho_{g}$ are actions of $G$ on itself, when we consider it as an object in the category Set. We call these the $l e f t$ and right actions of $G$ on itself, respectively. $\diamondsuit$  

In a category in which the objects are sets with additional structure, and the arrows are functions which satisfy some additional conditions, we define the orbit of an element $x$ under an action of a group $G$ to be the set  

$$
O(x)=\{\alpha_{g}(x):g\in G\}.
$$  

We can define an equivalence relation using an action by $x\sim y$ if and only if $y=\alpha_{g}(x)$ for some $g\in G$ . It is easy to verify that this is indeed an equivalence relation, since the facts that $x=\alpha_{e}(x)$ ; that if $x=\alpha_{g}(y)$ then $y=\alpha_{g^{-1}}(x)$ ; and that if $x=\alpha_{g}(y)$ and $y=\alpha_{g}(z)$ , then  

$$
x=\alpha_{g}(y)=\alpha_{g}(\alpha_{h}(z))=\alpha_{g h}(z)
$$  

give reflexivity, symmetry and transitivity, respectively. We then have that the orbit of $x$ is simply the equivalence class of $x$ , ie.  

$$
O(x)=[x]_{\sim}.
$$  

# Example 4.19  

The orbit of an element $x\in G$ under the action of Example 4.17 is the conjugacy class of $x$ , ie. $O(x)=C(x)$ . $\diamondsuit$  

# Example 4.20  

The orbit of an element $x\in G$ under the left action of $G$ is $G$ .  

However if we restrict the left action to some subgroup $H$ of $G$ , so we only consider functions of the form $\lambda_{h}$ for $h\in H$ , then $\lambda$ is an action of $H$ on $G$ , and then $O(x)$ is the right coset $H x$ . $\diamondsuit$  

# Example 4.21  

In the category of Example 4.16, consider closed disc of radius 1 with centre at the origin as the object $A$ . The automorphism group of this object consists of rotations $R_{\theta}$ and reflections $H_{\psi}$ of the disc, as discussed in Example 1.3. There is an action of the additive group of real numbers $\alpha:\mathbb{R}\to\operatorname{Aut}(A)$ given by $\alpha_{x}=R_{y}$ , where $x=2\pi k+y$ , with $k\in\mathbb{Z}$ and $y\in\left[0,2\pi\right)$ .  

In this case the orbit of the point $(1,0)$ is the unit circle, since we can find a rotation that maps that point to any other on the unit circle, and every rotation is in the image of the action.  

Indeed, the orbit of any point in the disc will be a circle.  

The previous example should help you understand why an orbit is called an orbit.  

# 4.4 Semidirect Products  

Group automorphisms and the actions of groups allow us to generalise the notion of the direct product introduced in Chapter 2. Let $G$ and $H$ be groups, and let $\alpha$ be an action of $H$ on $G$ . We supply a binary operation $*$ for the set  

$$
G\times H=\{(g,h):g\in G,h\in H\}
$$  

by  

$$
(g_{1},h_{1})\ast(g_{2},h_{2})=(\alpha_{h_{2}}(g_{1})g_{2},h_{1}h_{2}).
$$  

May 3, 2004  

# Proposition 4.22  

Let $G$ and $H$ be groups, and let $\alpha$ be an action of $H$ on $G$ . The binary operation $*$ defined above makes $\left(G\times H,*,(e,e)\right)$ a group. We call this group a semidirect product of $G$ and $H$ , and denote it symbolically by $G\rtimes_{\alpha}H$ . If $H$ is a subgroup of $\operatorname{Aut}(G)$ acting in the obvious way, then we call this group the extension of $G$ by $H$ .  

Moreover  

(i) the subset $G^{\prime}=\{(g,e):g\in G\}$ is a characteristic subgroup of the semidirect product which is isomorphic to $G$ ;   
(ii) $(G\rtimes_{\alpha}H)/G^{\prime}\cong H;$   
(iii) the subset $H^{\prime}=\{(e,h):h\in H\}$ is a subgroup of the semidirect product which is isomorphic to $H$ ;   
(iv) the conjugate of an element $(g,e)$ of $G^{\prime}$ by an element $(e,h)$ of $H^{\prime}$ is the element $(\alpha_{h}(g),e)$ of $G^{\prime}$ . In other words, conjugation by elements of $H^{\prime}$ is equivalent to the action $\alpha$ on $G$ .  

Proof:  

Showing that the semidirect product is a group is left as an easy exercise. It is also left as an exercise to show that $G^{\prime}\cong G$ and $H^{\prime}\cong H$ .  

We note that the inverse of the element $(g,h)$ is the element $(\alpha_{h^{-1}}(g^{-1}),h^{-1})$ . The function $\beta(g,h)=h$ from $G\rtimes_{\alpha}H$ to $H$ is a homomorphism, since  

$$
\begin{array}{r l r}&{}&{\beta((g_{1},h_{1})*(g_{2},h_{2}))=\beta(\alpha_{h_{2}}(g_{1})g_{2},h_{1}h_{2})}\\ &{}&{\qquad=h_{1}h_{2}}\\ &{}&{\qquad=\beta(g_{1},h_{1})\beta(g_{2},h_{2})).}\end{array}
$$  

Now  

$$
\ker\beta=\{(g,h):\beta(g,h)=h=e\}=\{(g,e):g\in G\}=G^{\prime},
$$  

so $G^{\prime}$ is a normal subgroup, and furthermore the First Isomorphism Theorem says that since $\beta$ is onto,  

$$
H\cong(G\rtimes_{\alpha}H)/\ker\beta=(G\rtimes_{\alpha}H)/G^{\prime}
$$  

Finally, by calculation,  

$$
\begin{array}{c l}{(e,h)^{-1}\ast(g,e)\ast(e,h)=(e,h^{-1})\ast(g,e)\ast(e,h)}\\ &{=(\alpha_{e}(e)g,h^{-1}e)\ast(e,h)}\\ &{=(g,h^{-1})\ast(e,h)}\\ &{=(\alpha_{h}(g)e,h^{-1}h)}\\ &{=(\alpha_{h}(g),e).}\end{array}
$$  

# Example 4.22  

For any $G$ and $H$ , there is always the trivial action $\alpha_{h}(g)=g$ . Under this action, the semidirect product $G\rtimes_{\alpha}H$ has product  

$$
(g_{1},h_{1})*(g_{2},h_{2})=(\alpha_{h_{2}}(g_{1})g_{2},h_{1}h_{2})=(g_{1}g_{2},h_{1}h_{2}).
$$  

In other words, this is simply the direct product of the two groups.  

# Example 4.23  

If $G$ is any Abelian group, there is an action of the multiplicative group $\{1,-1\}\cong C_{2}$ given by $\alpha_{1}(g)=g$ and $\alpha_{-1}(g)=g^{-1}$ . It is trivial that $\alpha_{1}$ is a homomorphism, and $\alpha_{-1}(g h)=(g h)^{-1}=h^{-1}g^{-1}=g^{-1}h^{-1}=\alpha_{-1}(g)\alpha_{-1}(h)$ , since $G$ is Abelian.  

The semidirect product of $G$ and this group with this action is a group of order $2|G|$ , and if $G$ has any element of order greater than 2, this is distinct from the direct product. Indeed, if it is distinct from the direct product, the semidirect product is not Abelian, since if $g_{1}$ is an element of order greater than 2, we have  

$$
(g_{1},1)*(e,-1)=(g_{1}^{-1},-1),
$$  

but  

$$
(e,-1)*(g_{1},1)=(g_{1},-1),
$$  

and these are only equal if $g_{1}^{-1}=g_{1}$ , which implies that $g_{1}$ has order 2. So these elements do not commute.  

One can show that as a particular example of this action giving a semidirect product, we have $D_{2n}\cong C_{n}\rtimes_{\alpha}C_{2}$ , where the generators $a$ and $b$ of $D_{2n}$ correspond to the elements $(u,1)$ and $(1,-1)$ , respectively. $\diamondsuit$  

# Assignment 5  

The following exercises are due on day of final.  

4.1 Exercises 1, 2, 4, 5, 7.   
4.2 Exercises 1, 4, 5, 8, 9.  

# Index  

Cayley table, 3   
centralizer, 84   
centre, 81   
codomain, 10, 130   
communtator, 110   
commutative, $\delta$ , 30   
commuting diagram, 130   
commuting elements, 33   
complement, $\delta$   
complex numbers, 7   
composition, 130 of two functions, 10   
conjugacy class, 96   
conjugate subgroups, 102   
coprime, 25   
coset, 87 left, 87 right, 87   
cycle, 14   
DeMorgan’s laws, $\delta$   
distributive, 8   
domain, 10, 130   
element, $\it{6}$   
epimorphism, 66   
equivalence class, 94   
four-group, 50   
function, 10 inverse, 10   
functor, 133 forgetful, 134   
generated, 46   
graph of a function, 10   
$m$ -cycle, 14   
-morphism auto-, 66 epi-, 66 homo-, 62, 62–68 iso-, 66 mono-, 66   
action, 136 left, 136 right, 136   
arrow, 130 epi, 132 identity, 130 inverse left, 132 right, 132 invertible, 131 monic, 131   
associative, 8, 30   
automorphism, 66 inner, 126   
bijective function, 10   
binary operation, 29 on a set, 30   
bound greatest lower, 78 least upper, 78 lower, 78 upper, 78   
cancellation law, 39   
cardinality, 9   
cartesian product, $g$   
category, 130   
Cayley graph, 112   
group, 29, 32, 41 Abelian, 32, 61 alternating, 35 automorphism of an object, 135 commutative, 32 cyclic, 48, 61 dihedral, 60, 73 free, 38, 115 general linear, 36 orthogonal, 36 quaternion, 92, 110 special linear, 36 special orthogonal, 36 special unitary, 36 symmetric, 35 unitary, 36   
group extension, 138   
group operation, 32   
homomorphism, 62, 62–68   
identity, 30 left, 30 right, 30   
image, 10 inverse, 10   
incomparable elements, 78   
index of a subgroup, 89   
injective function, 10   
integers, 7   
intersection, $\delta$   
inverse element, 32, 41   
isometries, 2   
isomorphic, 42, 66 objects, 131   
isomorphism, 66   
join of subgroups, 75   
kernel, 65   
Latin square, 43   
lattice, 78 subgroup, 72   
May 3, 2004   
letters, 37   
matrix unitary, 36   
meet of subgroups, 75   
member, $\it6$   
modulo equality, 24   
monomorphism, 66   
morphism, 130   
natural numbers, 7   
normalizer, 104   
object initial, 132 terminal, 132   
objects, 130   
one-to-one function, 10   
onto function, 10   
orbit, 136   
order of a group, 33 of an element, 48 partial, 78   
parity, 18   
permutation, 11 even, 19 identity, 12 odd, 19   
product semidirect, 138   
quotient group, 118   
range, 10   
rational numbers, 7   
real numbers, 7   
relation, 77 antisymmetric, 77 symmetric, 77 equivalence, 93 reflexive, 77 transitive, 78   
relations, 47  

# INDEX  

retraction, 132   
section, 132   
set, $\it6$ empty, 7 generating, 46 pointed, 131   
set difference, $\delta$   
set operations, 7   
subgroup, 56 characteristic, 127 commutator, 110 derived, 110 normal, 103 proper, 58 trivial, 58   
subset, 7 proper, 7   
surjective function, 10   
symmetric difference, $\delta$   
symmetry, 2 identity, 2   
translation left, 87 right, 87   
transposition, 14   
union, 7   
universal set, 8   
vierergruppe, 50   
word, 37 reduced, 37  