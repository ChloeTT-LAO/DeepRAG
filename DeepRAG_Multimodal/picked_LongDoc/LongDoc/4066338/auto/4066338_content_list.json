[
    {
        "type": "text",
        "text": "Gender and Ethnicity Classification Using Partial Face in Biometric Applications ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Jamie Lyle Clemson University, jlyle@g.clemson.edu ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Follow this and additional works at: https://tigerprints.clemson.edu/all_dissertations ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Part of the Computer Sciences Commons ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Gender and Ethnicity Classification Using Partial Face in Biometric Applications ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A Dissertation   \nPresented to   \nthe Graduate School of   \nClemson University ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Computer Science ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "by Jamie Lyle December 2014 ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Accepted by: Dr. Damon L. Woodard, Committee Chair Dr. Shaundra Daily Dr. Juan Gilbert Dr. Jason Hallstrom ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "As the number of biometric applications increases, the use of non-ideal information such as images which are not strictly controlled, images taken covertly, or images where the main interest is partially occluded, also increases. Face images are a specific example of this. In these non-ideal instances, other information, such as gender and ethnicity, can be determined to narrow the search space and/or improve the recognition results. Some research exists for gender classification using partial-face images, but there is little research involving ethnic classifications on such images. Few datasets have had the ethnic diversity needed and sufficient subjects for each ethnicity to perform this evaluation. Research is also lacking on how gender and ethnicity classifications on partial face are impacted by age. If the extracted gender and ethnicity information is to be integrated into a larger system, some measure of the reliability of the extracted information is needed. This study will provide an analysis of gender and ethnicity classification on large datasets captured by non-researchers under day-to-day operations using texture, color, and shape features extracted from partial-face regions. This analysis will allow for a greater understanding of the limitations of various facial regions for gender and ethnicity classifications. These limitations will guide the integration of automatically extracted partial-face gender and ethnicity information with a biometric face application in order to improve recognition under non-ideal circumstances. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Overall, the results from this work showed that reliable gender and ethnic classification can be achieved from partial face images. Different regions of the face hold varying amount of gender and ethnicity information. For machine classification, the upper face regions hold more ethnicity information while the lower face regions hold more gender information. All regions were impacted by age, but the eyes were impacted the most in texture and color. The shape of the nose changed more with respect to age than any of the other regions. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Contents ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Title Page i Abstract ii List of Tables v List of Figures . vi ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1 Introduction 1   \n1.1 Biometrics Overview 3   \n1.2 Previous Work 5 ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2 Research Design . 15 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.1 Data 15   \n2.2 Preprocessing 20   \n2.3 Feature Extraction Methods 23   \n2.4 Feature Reduction Methods 24   \n2.5 Classification Methods 25   \n2.6 General Experiment Setup 28 ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 Color 32 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.1 Feature Extraction Methods 32   \n3.2 Color Spaces 33   \n3.3 Experiment Setup 36   \n3.4 Analysis . 37   \n3.5 Gender . 42   \n3.6 Ethnicity 50   \n3.7 Conclusions 57 ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4 Shape 59 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4.1 Feature Extraction Methods 59   \n4.2 Experiment Setup 60   \n4.3 Analysis 61   \n4.4 Gender 61   \n4.5 Ethnicity 65   \n4.6 Conclusions 70 ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5 Texture 72 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5.1 Feature Extraction Methods 72   \n5.2 Experiment Setup 74   \n5.3 Analysis . 75   \n5.4 Gender 76   \n5.5 Ethnicity 83   \n5.6 Conclusions 88 ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "6 Application . 91 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "6.1 Experiment Setup 91   \n6.2 Analysis 93   \n6.3 Results . 102   \n6.4 Conclusions 107 ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "7 Conclusions and Future Work 110 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "7.1 Reliability 110   \n7.2 Age 112   \n7.3 Application 112   \n7.4 Future Work 113 ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Glossary 115 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "References 116 ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "List of Tables ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "1.1 Gender and ethnicity results for partial face experiments in literature . 7   \n1.2 Short description and details from databases in literature and experiments . 8   \n1.3 Details of partial face experiments found in literature. 9   \nDemographic breakdown of FRGC and MORPH experiment sets 17   \n2.2 Facial region details by dataset. 21   \n3.1 MORPH image counts by age and demographic. 41   \n3.2 Gender performance using color and full face 48   \n3.3 Ethnic performance using color and full face, FRGC 54   \n3.4 Ethnic performance using color and full face, MORPH 54   \n4.1 Points used per region for shape features . 60   \n4.2 Gender performance using shape and full face 64   \n4.3 Ethnic performance using shape and full face, FRGC 68   \n4.4 Ethnic performance using shape and full face, MORPH 68   \n5.1 Gender performance using LBP texture and full face 82   \n5.2 Ethnic performance using LBP texture and full face, FRGC 87   \nEthnic performance using LBP texture and full face, MORPH 87   \n6.1 Demographic breakdown of Pinellas experiment sets 92   \n6.2 Region/Feature/Classifier combinations chosen 93   \n6.3 Confusion matrices from classifying the gallery set on color combinations 94   \n6.4 Confusion matrices from classifying the gallery set on texture combinations 95   \n6.5 Confusion matrix from classifying the gallery set on shape combination 95   \n6.6 Performance details on facial recognition fusion experiments, baseline 101   \n6.7 Average comparisons per probe, baseline . 101   \n6.8 Weighted sum fusion with color . 103   \n6.9 Weighted sum fusion with shape and other information . 103   \n6.10 Weighted sum fusion with texture 104   \n6.11 Weighted sum fusion with mixed color and texture features 104   \n6.12 Weighted sum fusion best mix . 105   \n6.13 Score fusion with color . 106   \n6.14 Score fusion with texture 107   \n6.15 Score fusion with mixed gender, ethnicity, color and texture 107 ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "List of Figures ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "1.1 Typical biometric system 4 ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.1 Example images from the FRGC database. 16   \n2.2 Distribution of FRGC according to age, gender, and ethnicity 17   \n2.3 Example images from the MORPH database. 18   \n2.4 Distribution of MORPH according to age, gender, and ethnicity 19   \n2.5 Example images from the Pinellas database. 19   \n2.6 Distribution of Pinellas according to age, gender, and ethnicity 20   \n2.7 Example feature points detected by VeriLook SDK 21   \n2.8 Example facial regions for experiments 22   \n2.9 Point subset used to extract facial regions 23   \n2.10 Preprocessing method 24   \n2.11 Example ROC and DET curves 29   \n2.12 Example CMC curve . 30 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.1 RGB and HSI histogram visualization 33 ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.2 YIQ and YCbCr histogram visualization 34   \n3.3 LUV and LCH histogram visualization 36   \n3.4 Average global feature vector for MORPH color features 38   \n3.5 Mean difference between FRGC global color region vectors by demographic 39   \n3.6 Nearest neighbor gender classification on FRGC color features . 44   \n3.7 Nearest neighbor gender classification on MORPH color features 45   \n3.8 ANN and SVM gender classification on color features . 46   \n3.9 Easy and hard subjects in color gender classification, MORPH 47   \n3.10 Gender performance on color by age . 49   \n3.11 Nearest neighbor ethnic classification on FRGC color features 51   \n3.12 Nearest neighbor ethnic classification on MORPH color features 52   \n3.13 ANN and SVM ethnic classification on color features 55   \n3.14 Hard subjects in color ethnicity classification, MORPH . 55   \n3.15 Ethnic performance on color by age . 57 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4.1 Example of shape feature calculation 60 ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4.2 Gender nearest neighbor results on shape 62   \n4.3 Gender ANN and SVM results on shape 63   \n4.4 Hard subjects in shape gender classification, MORPH 64   \n4.5 Gender and ethnic performance on shape by age 66   \n4.6 Ethnic nearest neighbor results on shape . 66   \n4.7 Easy and hard subjects in shape ethnic classification, MORPH 67   \n4.8 Ethnic ANN and SVM results on shape 69   \n5.1 Average global feature vector for MORPH HOG features . 77   \n5.2 Average global feature vector for MORPH LBP features 77   \n5.3 Mean difference between MORPH region-wide global texture vectors by demographic 78   \n5.4 Nearest neighbor gender classification on texture features 79   \n5.5 ANN and SVM gender classification on texture features 81   \n5.6 Easy and hard subjects in texture gender classification, MORPH 82   \n5.7 Gender performance on texture by age. 84   \n5.8 Nearest neighbor ethnic classification on texture features . 85   \n5.9 ANN and SVM ethnic classification on texture features 86   \n5.10 Easy and hard subjects in texture ethnic classification, MORPH 87   \n5.11 Ethnic performance on texture by age 89   \n6.1 Age distribution of Pinellas experiment sets 96   \n6.2 Pinellas images with negative ages 97   \n6.3 Pinellas images with ages below 16 . 97   \n6.4 Pinellas images with ages over 100 98   \n6.5 Gender and ethnic performance on chosen combinations by age 98   \n6.6 Score distribution for straight whole face experiment 100   \n6.7 Baseline performance on identification and verification 101   \n6.8 Best weighted sum fusion experiments 106   \n6.9 Best score fusion experiments . 108 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Chapter 1 ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Introduction ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The world today is becoming more and more identity-driven. As the number of applications which require identification increases, the chance of identity-theft also rises. Modern biometric applications were developed as a solution to this problem. Identification can now take place by who you are, physically and behaviorally, as opposed to what you have or what you know. Biometric applications, which uniquely identify the individual, are useful, but not perfect under all conditions. Sometimes the data available to the application is not ideal and identification fails. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "An example of this non-ideal information is grainy surveillance footage of a gas station robbery. The police probably will not be able to identity the suspect on the video, but they can determine certain information. This information could be the approximate height, gender, or ethnicity of the suspect. These descriptions will not identify the individual to the police, but they will narrow the possible suspects to the people who match whatever information is extracted. This type of information, which describes the individual but does not uniquely identity them, is known as soft biometric information. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Soft biometric information covers a wide range of details about a person. It can include height, weight, hair color, eye color, gender, age, and even clothing color. Some soft biometric traits are more permanent than others, gender versus clothing color for example. Some traits change naturally over time, such as height, weight, and age. Even though some traits are not entirely stable, certain applications might only need short-term information where the soft biometrics would be unlikely to change, or they could use the more permanent information such as gender and ethnicity. Which traits are the most useful depend on the application and the type of data it collects. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Soft biometric information can be used in various aspects of culture today including advertising, computer interaction customization, surveillance and tracking, as well as biometric applications. Advertisers today target ads to specific populations. Web advertisements can appear based on previous browsing behaviors. Targeted advertising even shows up in movies; for example, shoppers are identified based on iris scans and their purchase history made available in order to personalize the store’s interaction with them [50]. Most likely, soft biometric information would allow targeted ads based on demographic groups as opposed to a specific identity, but the idea remains the same. Human Computer Interaction (HCI) is a growing field of study which can benefit from the use of soft biometrics. Suppose the computer could determine an age and take appropriate steps based on that age, such as enlarge the font for an older individual or enable parental controls for a younger individual. Law enforcement uses soft biometric information to describe subjects as mentioned previously and algorithms exist that can track an individual across camera views using soft biometric information [19].The main focus of this paper will be an analysis of gender and ethnicity classifications on various regions of the face, culminating in using the results of the analysis in an effort to improve the performance of a biometric application using face information. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Park and Jain [56] group the uses of soft biometric information in biometric and computer applications into four main categories. The first category is using soft biometric information to supplement existing biometric systems to improve identification accuracy. The second category is enabling faster image retrieval which could be used in a biometric application or an image database. The next category mentioned is enabling matching or retrieval of facial images that are occluded or captured at an angle. The last category is using soft biometric information to provide more descriptive evidence about the similarity or dissimilarity between facial images, which could be useful in judicial settings. These categories were created with face images in mind, but can generalize well to other biometric modalities. This work seeks to further the understanding of the use of partial face in the first two categories to enable future use in all categories. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The face modality is a well-researched modality in the biometrics community. Most human recognition takes place based on the face and it is a relatively easy modality to acquire. The face is a good source of gender and ethnicity information, which is the soft biometric information of interest in this work. Partial face has not been researched as in-depth as full face for both gender and ethnicity classification, but deserves more attention. As surveillance increases and awareness of surveillance increases, the difficulty of acquiring a good quality full face image increases as well. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The probability that some part of the face will be occluded or blocked from the camera is high. Therefore, an analysis of what information, namely gender and ethnicity, can reliably be found in different regions of the face is needed. Since surveillance is not limited to any certain age group, an investigation of how age impacts the partial-face gender and ethnicity classification is needed as well. Before the details of the analysis are discussed, a short overview of biometric systems is included for background information. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "1.1 Biometrics Overview",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Biometrics, within the security and computing fields, is defined as the science of identifying an individual based on physiological or behavioral characteristics [6]. Physiological characteristics belonging to an individual that have been used for biometric purposes include fingerprints, face, iris, and hands. Behavioral characteristics include voice, signature, and the way a person walks (gait). The specific characteristic used is known in the literature as a biometric modality or simply a modality. Biometric applications can use a single modality, such as in face recognition systems, or can use multiple modalities, such as a system that combines face and gait information to identify an individual. The type of modality used will depend on the problem the application is trying to solve. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The two main types of problems for biometric applications are verification and identification. Applications that deal with both problem types have a similar structure, which can be seen in Figure 1.1. First, the system needs data, which is captured by a sensor. The sensor could be a camera or an audio recorder. The data is preprocessed to get it in the right form for feature extraction. In facial recognition this step includes face detection, alignment, and contrast enhancement. Once the data is in the right format, the system extracts features from the data. These could be texture, shape, or color features from an image. The features are then used to generate a template. If the system is in the learning or enrollment phase, the templates are stored in a database along with an identity. If the system is in testing phase, the template, also known as the probe, is compared against other stored templates, known collectively as the gallery. A match score is given for each comparison. The system uses these scores to make a decision. The decision is different for verification and identification systems. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Access control is an example of a verification problem. Only certain individuals are permitted access and their identities must be verified before they can gain access. The person who wishes access claims an identity. The biometric system processes the data and compares the probe to the stored template that corresponds to the claimed identity. The system has a threshold for match scores in its decision module. If the score is above the threshold, the system decides that the person and the identity are the same and grants access. If the score is below the threshold, the system decides that the person and the claimed identity are not a match and denies access. ",
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/ad125f92dd45885149553bd0db3c81d5e782a7a9aefc9ef17300e59c46026aef.jpg",
        "img_caption": [
            "Figure 1.1: Typical biometric system "
        ],
        "img_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In an identification problem, the unknown person makes no identity claim. Since no claim is made, the probe is compared against multiple, possibly all, gallery templates stored during the enrollment phase. The match scores are sorted. The corresponding identities are ranked from the most likely match to the least likely. The most likely identity can be returned or a list of most likely identities can be returned for a human expert to make the final decision. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Some cases arise where identity is not the main goal or is not plausible with the given data. In those cases classification by some other criteria, such as age or gender, is desired. This soft biometric classification may be the end result of the system or it could be incorporated into a biometric application in some way. Classifications on the probe could be used as another feature to increase confidence in a possible identity match. They could also be used as a filter to only compare the probe with identities in the gallery that have matching soft biometric information. A soft biometric classification system starts the same as either a verification or identification system. A sensor collects the data which is then preprocessed. Features are extracted from the preprocessed data. In place of enrollment, a classification system undergoes a training phase. The training can either be supervised, where class labels are provided for the training data, or unsupervised, where labels are not provided. The trained system will take the features extracted from the data and output a class label, such as male or female. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "With a general understanding of biometrics and classification systems, the proposed analysis will be placed in context with previous research involving partial-face classification of gender and ethnicity. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "1.2 Previous Work ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Much of biometric research focuses on using the face for recognition purposes. The face is very visible in most cultures and is a large part of how humans recognize one another. In many instances, not even the entire face is needed for a human to recognize an individual. When looking at face recognition by machines, the quality of the face image is an important factor. Blurred images [39], occluded faces, and low or uncontrolled lighting [7] can all negatively impact the results of facial recognition algorithms. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Identity is not the only information to be found in the human face. Previous studies have shown that the face holds both gender [24, 29, 72] and ethnicity [28, 29, 30, 45] information in addition to identity. In the event that a face recognition system is not confident of its result, gender and ethnicity information can be used to increase the confidence of the system. In an image of a face though, there can be noise which would inhibit an accurate determination of gender or ethnicity. Studies have been performed to show the usefulness of various subregions of the face for gender [5, 9, 35, 47] and ethnicity [40, 47] determination; however, most of the studies are on a small scale and do not carry out experiments on all regions of the face. The goal of the research detailed herein is to gain an understanding of the limitations of partial-face regions for gender and ethnicity classifications. This understanding will including learning how reliable each part of the face is for gender and ethnicity classification and how each region is impacted by age. Knowing these limitations, a method using machine-based gender and ethnicity classifications made on partial face is proposed to improve performance in a facial biometric application. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "1.2.1 Reliability ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "What parts of the face hold reliable gender and ethnicity information for machine applications? ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The first question to be considered is which parts of the face hold reliable gender and ethnicity information for machine applications. It is possible that a subset of the face holds more gender or ethnicity information than another subset. Some regions of the face may be gender or ethnic neutral. Another consideration is what type of features encode the gender or ethnicity information. Texture, color, and shape features can be found in facial images. Knowing which regions of the face hold reliable gender and ethnicity information and what features encode the information best will provide a foundation for improving gender and ethnicity classifications based on partial face. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "1.2.1.1 Face Regions ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Previous studies have focused mainly on gender when working with partial face [5, 8, 35, 36, 44, 55, 69]. Details for partial face studies mentioned can be found in Table 1.1. Some details of the databases used can be found in Table 1.2. Most of these studies use human-defined regions to subdivide the face, such as the eyes, nose, mouth, and chin. Regions containing the eyes ranked among the top two regions for several studies [5, 8, 44]. These results were derived from three different datasets, so the good performance of the eye region is not a peculiarity of a specific dataset. It is more likely that the eye region holds more gender information than other regions tested. Other best performing regions were the nose in FERET experiments and the mouth in XM2VTS experiments [5]. The nose performed the worst in the XM2VTS experiments. This could be due to the different ethnic compositions of the datasets, or the authors suggested that the nose was more susceptible to illumination than the mouth. The differences between the best performing regions in the various datasets suggest that more research is needed in this area. It could be that the sample/experiment size was not large enough. Most of the studies perform experiments with less than 1,000 subjects, as seen in Table 1.3. Lapedriza et al. [37] have one of the larger studies based on image number, but the number of subjects is unknown and their research focuses more on features outside the face such as the ears, forehead, and hair, instead of the eyes, nose, mouth, and chin. Aside from the nose [5], mouth [5, 36] and eyes [5, 8, 44, 47] mentioned above, another best performing region of the face is the jaw or chin [35, 36]. ",
        "page_idx": 13
    },
    {
        "type": "table",
        "img_path": "images/dd3b76c443851d46efb3c2414e6be2e54e4eb6108f5304985919f52c437823ef.jpg",
        "table_caption": [
            "Table 1.1: Details and results for partial face experiments. Traits are gender (G) and ethnicity (E) with (x) classes. Results from best performing subregion were included if applicable. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Study</td><td>Feature</td><td>Classifier</td><td>Rate</td><td>Modality</td><td>Trait</td><td>Dataset</td></tr><tr><td>Ozbudak [55]</td><td>DWT+PCA</td><td>FLD</td><td>93% (52%)</td><td>Face (w.o. nose)</td><td>G</td><td>FERET, SUMS</td></tr><tr><td>Kawano [36]</td><td>FDF</td><td>LDA</td><td>(83.7%)</td><td>Pace)</td><td>G</td><td>Softopia Japan</td></tr><tr><td>Andreu [5]</td><td>Pixels+PCA</td><td>SVM</td><td>(80.0%)</td><td>(Mach)</td><td>G</td><td>XM2VTS</td></tr><tr><td>Andreu [5]</td><td>Pixels+PCA</td><td>SVM</td><td>95.2% (86.4%)</td><td>Face (Nose)</td><td>G</td><td>FERET</td></tr><tr><td>Buchala [8]</td><td>Pixels+PCA</td><td>SVM(RBF)</td><td>86.5% (92.3%)</td><td>Face (Composite)</td><td>G</td><td>FERET, AR, BioID</td></tr><tr><td>Lu [44]</td><td>Pixels</td><td>SVM(RBF)</td><td>92.5% (92.9%)</td><td>Face (Upper)</td><td>G</td><td>CAS-PEAL</td></tr><tr><td>Lu [43]</td><td>2D-PCA</td><td>SVM(RBF)</td><td>(95.3%)</td><td>(Faco)</td><td>G</td><td>CAS-PEAL</td></tr><tr><td>Hu [31]</td><td>Curvature</td><td>SVM</td><td>(93.5%)</td><td>(Facon)</td><td>G</td><td>own, UND (3D)</td></tr><tr><td>Lapedriza [38]</td><td>FraeFacents</td><td>SVM</td><td>94.2%</td><td>Extermal</td><td>G</td><td>FRGC</td></tr><tr><td>Manesh [47]</td><td>Gabor</td><td>SVM(RBF)</td><td>94.0%</td><td>Face Fusion</td><td>G</td><td>FRGC, CAS-PEAL</td></tr><tr><td>Lapedriza [37]</td><td>Face Fragments</td><td>JointBoost</td><td>96.8% (96.7%)</td><td>Face (External)</td><td>G</td><td>Controlled FRGC</td></tr><tr><td>Lapedriza [37]</td><td>Face Fragments</td><td>JointBoost</td><td>91.7% (90.6%)</td><td>Face (External)</td><td>G</td><td>Uncontrolled FRGC</td></tr><tr><td>Thomas [68]</td><td>Texture Texture ,</td><td>DT</td><td>75%</td><td>Iris</td><td>G</td><td>Custom</td></tr><tr><td>Lyle [46]</td><td>Color</td><td>ANN/SVM</td><td>97.3%</td><td>Periocular</td><td>G</td><td>FRGC</td></tr><tr><td>Lyle [46]</td><td>Texture</td><td>ANN/SVM</td><td>90%</td><td>Periocular</td><td>G</td><td>MBGC</td></tr><tr><td>Merkow [48]</td><td>Texture, pixels</td><td>LDA+SVM</td><td>85%</td><td>Periocular</td><td>G</td><td>web</td></tr><tr><td>Manesh [47]</td><td>Gabor</td><td>SVM(RBF)</td><td>97.4%</td><td>Face Fusion</td><td>E(2)</td><td>CAS-PEAL FRGC,</td></tr><tr><td>Qiu [59] </td><td>fGabos</td><td>AdaBoost</td><td>86%</td><td>Iris</td><td>E(2)</td><td>CASLA,UPOL,</td></tr><tr><td>Qiu [60]</td><td>Iris-Textons</td><td>SVM</td><td>91%</td><td>Iris</td><td>E(2)</td><td>CASIA-BioSecure</td></tr><tr><td>Li [40] </td><td>Eyelash direction</td><td>1-NN</td><td>93.2%</td><td>Periocular</td><td>E(2)</td><td>CMU-PIER, UBIRISv1</td></tr><tr><td>Lyle [46]</td><td>Texture,</td><td>ANN/SVM</td><td>94%</td><td>Periocular</td><td>E(2)</td><td>FRGC</td></tr><tr><td>Lyle [46]</td><td>Texture</td><td>ANN/SVM</td><td>89 % </td><td>Periocular</td><td>E(2)</td><td>MBGC</td></tr></table></body></html>\n\n",
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/c22fda7d92258dd65f72bea0ed7278fb059dcb324ec0354f42a87ea171fcdda5.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 1.2: Short description and details from databases found in previous work in partial-face gender and ethnicity classification and those to be used. "
        ],
        "table_body": "\n\n<html><body><table><tr><td>Database</td><td>Images</td><td>Subjects</td><td>Short Description</td></tr><tr><td>BioID</td><td>1,521</td><td>23</td><td>Grayscale face images captured under real-world con- ditions</td></tr><tr><td>UPOL</td><td>384</td><td>64</td><td>Color iris images</td></tr><tr><td>CMU-PIER</td><td>321</td><td>107</td><td>Iris images collected for ethnicity classification based on eyelash direction</td></tr><tr><td>MBGC</td><td>149 (videos)</td><td>114</td><td>Multi-modal face and iris database captured under near infrared illumination, video and stills</td></tr><tr><td>AR</td><td>4,000</td><td>126</td><td>Color face image captured under varying illumina- tion and facial expressions. Occlusions were created with sunglasses and scarves.</td></tr><tr><td>UBIRIS (v1)</td><td>1,877</td><td>241</td><td>Noisy iris images captured under visible light condi- tions (color)</td></tr><tr><td>XM2VTS</td><td>1,180</td><td>295</td><td>Multi-modal face database including high-quality color face images</td></tr><tr><td>Softopia Japan</td><td>1,200</td><td>300</td><td>Color face images, equal male and female subjects</td></tr><tr><td>SUMS</td><td>400</td><td>400</td><td>Grayscale face images from equal male and female subjects from Stanford University Medical students</td></tr><tr><td>FRGC</td><td>39,329</td><td>568</td><td>High-quality color face images from students and fac- ulty at University of Notre Dame</td></tr><tr><td>CASIA (v3)</td><td>22,051</td><td>700</td><td>Grayscale iris images captured under near infrared illumination</td></tr><tr><td>CAS-PEAL</td><td>30,871</td><td>1,040</td><td>Grayscale Chinese face database captured under varying pose, expression, accessory, and lighting con- ditions</td></tr><tr><td>FERET</td><td>14,126</td><td>1,199</td><td>Color face images captured under semi-controlled conditions.</td></tr><tr><td>MORPH</td><td>55,134</td><td>13,314</td><td>Color face images collected from public records taken under real-world operating conditions (semi-</td></tr><tr><td>Pinellas</td><td>1,447,607</td><td>403,619</td><td>controlled) Color face images collected from Pinellas County Sheriff's Office</td></tr></table></body></html>\n\n",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Other studies broke the face into regions differently, both larger and smaller than the previously mentioned human-defined regions. The area around the eye still performs well in these studies [44, 47]. In a study by Manesh et al. [47], the eyebrows and the area between the eyes performed best for gender classification. The upper region of the face, which includes the eyes, also performed well in a study by Lu et al. [44]. The drawback of using the larger regions is that it is unclear which smaller portion of the face actually encodes the gender information. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Ethnicity research on partial face are not as widespread as gender research, but some research has been done in the past. The eyes are still a good choice for ethnicity classification according to ",
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/887121467d1a66f91986cb99905cf3940d1612c1d077ff4231eff060928dbb34.jpg",
        "table_caption": [
            "Table 1.3: Details of partial face experiments found in literature. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Study</td><td>Trait</td><td>Type</td><td>Subjects</td><td>Images</td><td>Datasets</td></tr><tr><td>Ozbudak [55] Kawano [36] Andreu [5] Andreu [5] Buchala [8] Lu [43, 44] Hu [31] Lapedriza [38] Manesh [47]</td><td>Gender Gender Gender Gender Gender Gender Gender Gender Gender, Ethnicity Lapedriza [37] Gender Gender</td><td>Texture Shape Pixels Pixels Pixels Pixels Shape Texture Texture Texture Texture</td><td>300 203 834 一 321 1,691 一</td><td>480 1,200 1,378 2,147 400 800 945 2,640 1,691 3,440/1,886</td><td>2 1 1(XM2VTS) 1(FERET) 3 1 2 1 2 1</td></tr></table></body></html>\n\n",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Manesh et al. [47], but their results suggest that the cheeks are also useful for determining ethnicity. Manesh et al. [47] also have the largest number of subjects in their gender and ethnicity experiments with 1,691 subjects from the FERET and CAS-PEAL datasets. However, both datasets were needed to gain the desired ethnic diversity. The majority of each ethnic class was found in only one of the databases. The authors took steps to try to minimize the influence of using two different datasets, but their system could have learned the image properties of the database instead of the ethnic class. Studies on just the iris and periocular regions have been performed in both gender [46, 48, 68] and ethnicity [40, 46, 59, 60]. This is the region of the face where most ethnic research has been performed. The eye region is able to obtain at least 85% on most binary gender and ethnicity classifications, indicating that it holds a large amount of gender and ethnicity. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Taking all this previous work under consideration, experiments will be performed on the eyes, nose, mouth, and chin regions of the face in reliability experiments. Most of the facial regions, at one point or another have all been the best performing region on some dataset. It is hoped that experiments within this work will have some measure of agreement and be able to relate back to previous works. Regions including the eyes are expected to perform well for both gender and ethnicity classification across all datasets as they are among the best regions for the majority of previous studies. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "1.2.1.2 Features ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Various types of features have been used in the past for gender and ethnicity classification. The main types of features found in images can be categorized into pixels, texture, shape, color, and key-point. Pixel intensity values are one of the most basic features used for these purposes [44, 48]. This type of feature is commonly used in combination with a projection into a different space, such as Principal Component Analysis (PCA) [5] or Independent Component Analysis (ICA) [32], for feature reduction. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Local texture features are another type of feature that can be extracted from facial images. Image texture can be characterized as a set of repeating patterns. Within the face, texture can range from very fine skin texture to larger texture caused by hair and wrinkles. Texture feature extraction techniques used in partial-face research include Gabor features [47, 59], iris texture [60, 68], Histograms of Oriented Gradient (HOG) [46], Discrete Cosine Transform (DCT) [46], and the texture of the face fragments used by Lapedriza et al. [37] is derived using Gaussian derivative filters. The Local Binary Patterns (LBP) feature extraction method is another widely used texture representation that has been used for gender and ethnicity classification in both full face [41, 66, 75] and partial [46, 48]. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Aside from local texture information, shape information can also be found in the face. 3D shape information has been utilized for gender classification [31]. Active Appearance Models (AAMs), which combine shape and texture information, can be used to represent a face. AAMs model the statistical shape of an object, as well as texture, and can be used to find a shape, such as the face, in an image as well as for matching or classification [15]. AAMs have been used successfully for gender classification [64]. Another approach which uses the distance and angles between facial landmarks is called face metrology. This method has been used to successfully predict gender in facial images [10]. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Another type of feature representation is Scale Invariant Feature Transform (SIFT). This technique is different from the others in that it only looks at specific key-points on the image instead of calculating statistics over the entire image or finding a sequence of points (shape). SIFT has been used to successfully perform gender classification using full or occluded facial images [69]. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "The last category is color. Local Color Histograms (LCH) are a simple color representation that have been used for both gender and ethnic classification [46]. Color is often an important cue for humans in determining ethnicity. One drawback to using color is that it can be easily changed by various factors such as temperature, cosmetics, or illumination. Because of this, it might be useful to combine color features with features from another category, such as texture or shape. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Since texture is a large part of the information to be found in partial face regions, several different texture representations will be investigated to see if any one is better suited to a particular region than another. The LBP feature extraction technique was chosen due mainly to its success in gender classification using full face. HOG is another interesting texture representation which was first developed for pedestrian detection [18]. Its discriminating capability has been shown to be successful in facial recognition [20, 65], as well as periocular gender and ethnicity classification [46]. Local Phase Quantization (LPQ) is the last local texture representation chosen for investigation. LPQ is a recently proposed texture descriptor [54] which is said to be robust to image blurring. It has been used successfully for facial recognition [2, 11, 12]. It is expected that its descriptive capabilities will also work well for gender and ethnicity classification on partial-face images. LPQ was also chosen because the robustness to blur would mean that image quality might not be as important in order to achieve good performance. Local Color Histograms (LCH) will be used to represent color information based on previous performance [46]. Shape features will be investigated using a version of face metrology [10] adapted to partial face. A more detailed description of these methods can be found in the following chapters. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "1.2.1.3 Classification ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Early work in gender classification of faces relied on neural networks [1, 16, 22, 24, 67] to classify the data. The classifiers most widely used for gender and ethnicity determination in more recent works are still various forms of of Artificial Neural Networks (ANN) as well as Support Vector Machines (SVM) which can be seen in Table 1.1. In order to give a comparison, both SVM and ANN classifiers will be investigated as classifiers for this work. A simpler classifier, nearest neighbor, will also be used as a baseline with which to compare ANN and SVM classification. A more detailed description of these classification methods can be found in Section 2.5. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "1.2.2 Impact of Age ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "How is machine classification of gender and ethnicity in partial face impacted by age? ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "The second question to be considered is how age impacts gender and ethnicity classification for each part of the face. The human face changes as an individual ages. Younger faces are fuller, lacking the fine lines and wrinkles the face acquires as it ages [14]. The skin starts to sag as the face ages, losing it elasticity, and fine lines become more apparent. The eyebrows can even fall to the level of the brow. The eyes seem to sink into the face and “crow’s feet” appear at the eye corners. The cheeks hollow as the fat deposits sink or are redistributed in the face. These changes are natural in the aging process and can occur at different times for different individuals. How do these changes affect gender and ethnicity classification using partial face? Is there a specific region of the face that works best for a certain age range or one that works well across various ages? Being able to characterize how performance is impacted by age will allow for better design choices in a biometric application when the demographics of the population are known in advance. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "1.2.2.1 Facial Regions ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Research in gender classification on full face shows that gender classification is impacted by age. The results of multiple studies [26, 62] indicate that gender classification is easier on adult faces than younger or more senior faces. The results in [26] were $10\\%$ higher on adult faces than seniors or children, with subjects ranging from 0 to 93 years of age. Successful recognition is possible on younger faces though. One machine classification method [71] achieved better gender recognition in toddler faces than humans performing the same task. Not very many researchers have looked at how age affects gender classification using partial face images. Kawano et al. [36] only looked at how age affected their best performing partial-face region, which was the jaw. They found that the best gender performance on the jaw was on subjects in their 30’s. The error rate for subjects that were either older or younger increased. This is very similar to full face results; however, it would be interesting to see if the trend holds for other regions of the face, not just the jaw. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Not as much research has been done on ethnicity classification and the impact that age has on performance. Ethnicity results in [27] on full face were not as affected by age as the gender results mentioned above, but the age range of the subjects is different from the gender experiments. The subjects in the dataset used for the ethnicity experiments were limited mostly to adult faces with ages ranging from 16 to 67 years of age, while the gender study included children as well as adults up to 93 years old. No partial face research was found which looked at ethnicity classification in the presence of age. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "A specific study of how gender and ethnicity classifications on the various facial regions are affected by age would be beneficial. Knowing how each individual part is affected by age would contribute to the confidence level for gender and ethnicity classifications. If there is a specific part of the face in which gender and ethnicity information is least impacted by age, that would be the best region when age is a factor. Ethnicity classification on partial face is expected to be less impacted by age than gender classifications based on the literature for classification of full face. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "1.2.2.2 Features ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The feature extraction methods used when looking at gender and ethnicity over an age range include many of the same ones mentioned in the previous section. Pixel intensity values, HOG features, and LBP features are among the features compared by Guo et al. [26]. A different feature, named Biologically Inspired Features (BIF), is used by Guo et al. [26, 27] for both gender and ethnicity classification in the presence of aging. BIF are based, at the lowest level, on the results of Gabor filtering on an image. The other feature extraction most used in the presence of aging is Active Appearance Models (AAM) used by Wang et al. [71] to classify the gender of toddlers. Shape features are important because the change in features from infants to adults are mainly shape-based. Texture is the next big change as the adult face ages. This is when the skin loses its elasticity and fat layer underneath, allowing the wrinkles to appear. The main features used for gender and ethnicity classification over age fall into the categories of pixels, shape, and texture. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Experiments on how age impacts performance will use the same features mentioned in Section 1.2.1.2. These features belong to the color, texture, and shape categories. Since facial texture changes as an adult ages, it is expected that results for texture features will be impacted by age. Color also changes some with age, but will probably not be as impacted by the age range present in the datasets used for experiments. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "1.2.3 Application ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "To what extent can automatically determined gender and ethnicity information improve performance in a biometric experiment using the face or partial face modality? ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The final question to consider is how to use gender and ethnicity classifications on partial faces to improve the performance of a biometric application. Various studies have investigated fusing soft biometric information with a biometric experiment in an effort to improve performance [3, 33, 34, 45, 76]. Jain et al. [34] were the first to introduce the terminology “soft biometrics” ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "and used gender, ethnicity, and height information to improve the performance of a fingerprint experiment. They achieved an improvement of approximately 5% when including the soft biometric information. In a later experiment involving facial features [33], the extracted gender and ethnicity information failed to improve facial performance, while the simulated height information improved performance by approximately 5% again. Jain et al. [33] concluded that soft biometric information improves performance only if the information is complementary, or independent, of the primary modality used in the experiment. ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Another way to utilize the soft biometric information is to use it to retrieve a candidate list from the stored templates, similar to indexing a database. Park et al. [56] use facial marks, such as freckles, moles, scars, and birthmarks, along with gender and ethnicity information to filter the stored templates in their experiments. Using these soft biometric traits, they were able to increase performance by $1.5\\%$ . An improvement is seen, even though the soft biometric information was not independent of the facial features used in the biometric recognition experiment. However, the gender and ethnicity information used here was not automatically extracted in the course of the experiment. ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "If the soft biometric information does not need to be independent of the primary modality, it is possible that gender and ethnicity information can improve the performance of a face experiment. The best methods from the previous sections will be combined with face features in an effort to improve performance. The machine-based soft biometric classifications will be used as a filter for the stored templates to see what improvement can be achieved. These combinations can be useful for both recognition and verification applications. ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Each of these research questions will be addressed in the subsequent chapters, along with the design of the experiments for each question (Chapter 2). The analysis on reliability and impact of age will be performed for the three main types of features found in face images: color (Chapter 3), shape (Chapter 4), and texture (Chapter 5). The application of the research to use the acquired gender and ethnicity information to improve an everyday face biometric system (Chapter 6) will utilize conclusions from the analysis. Overall conclusions from each chapter will provide the basis for future work (Chapter 7.) ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Chapter 2 ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Research Design ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "2.1 Data ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Three databases will be used for experiments in this work: the Facial Recognition Grand Challenge database, the Craniofacial Longitudinal Morphological Face database, and a face database collected by the Pinellas County Sheriff’s office. The collection and composition of each database are discussed in the following sections as well as the motivation behind including these databases. ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "2.1.1 Facial Recognition Grand Challenge ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The Facial Recognition Grand Challenge database (FRGC) [57] was collected at the University of Notre Dame. The database is composed of full frontal still images. Images were collected from 568 subjects under varying lighting conditions and with different facial expressions, see example images in Figure 2.1. The images were collected on campus over the 2002-2003 and 2003-2004 academic school years. As a result, the majority of subjects in the database are between the ages of 18 and 27 years old. The approximate ages of the subjects at the start of collection in 2002 can be seen in Figure 2.2a. With a median age of 19, this database will not be used in age experiments; however, the distribution of the subjects by gender, Figure 2.2c, and ethnicity, Figure 2.2b, show enough diversity for FRGC to be used in reliability experiments for gender and ethnicity classification. ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The FRGC database was chosen for reliability experiments for several reasons. First, the FRGC database is widely used in facial recognition research [49, 58] and gender classification [37, ",
        "page_idx": 22
    },
    {
        "type": "image",
        "img_path": "images/88a882821cacf2cb0d85db8ebb3b6ef03aeaf8425471ca4ea9a01729041ad694.jpg",
        "img_caption": [
            "Figure 2.1: Example images from the FRGC database. "
        ],
        "img_footnote": [],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "38, 47]. This database was included to provide a basis of comparison with some of the previous fullface and partial-face studies performed in gender and ethnicity classification. Second, the images are high-resolution. The high-resolution allows for skin texture to be captured in detail. Experiments on the high resolution images will provide a baseline for experiments performed on the other databases which are captured at a lower resolution. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Not all the data in FRGC is usable, and the data that is usable must first be processed. The subjects corresponding to the Unknown ethnicity label cannot be used in ethnicity experiments, so those images are discarded for both gender and ethnicity experiments. Only one subject exists in the database in the Middle Eastern ethnicity label. Since distinct subjects cannot be used for training and testing with only one subject, this class will be discarded. Images taken under uncontrolled lighting conditions were discarded, as well as images where the subject was wearing glasses. Four images were used for each subject. Some subjects did not have four which met these requirements and were excluded. The total number of subjects for FRGC experiments is 535. With four images per subject, the number of face images in FRGC experiments is 2,140. The average interocular distance for these images is 270.32 pixels. Images were not sorted on expression, so a subject can have images with either a neutral or smiling expression. A breakdown of the subjects in the experiment set can be seen in Table 2.1. Other considerations and preprocessing steps will be discussed later. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "2.1.2 Craniofacial Longitudinal MORPHological Face ",
        "text_level": 1,
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "The Craniofacial Longitudinal Morphological Face database (MORPH) [63] is a collection of facial images taken from public records. The images were taken under real-world conditions and not by researchers under controlled conditions. Example images from this database can be seen in Figure 2.3. The MORPH database was formed for the purpose of studying age progression. It contains multiple images of an individual spanning both large and small age gaps. The MORPH database has two subsets, Album 1 and Album 2. The MORPH public release [61], used for this work, contains all of Album 1 and a subset of Album 2. This version contains over 55,000 images of more than 13,000 subjects. Subjects in the dataset range from 16 to 77 years old with a median age of 33. Gender and ethnicity information are included along with the age information for this dataset, as seen in Figure 2.4, providing ground truth for later experiments. The resolution of images in MORPH is either $200\\times240$ or $400\\times480$ . ",
        "page_idx": 23
    },
    {
        "type": "image",
        "img_path": "images/9d6a7a8e0c4f6f43fb0aa4a5ebd558b995fbb744cf4f61baf16fe9be7ad67b6a.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 24
    },
    {
        "type": "table",
        "img_path": "images/9f36c589039c6dc092215273edd28771a0d23fc492a2b11c265f5fa2940d5317.jpg",
        "table_caption": [
            "Figure 2.2: Distribution of the FRGC dataset according to a) age, b) ethnicity, and c) gender. ",
            "Table 2.1: Breakdown of subjects in the FRGC and MORPH experiment sets by gender and ethnicity. The average number of images per subject in each class is given in parentheses. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td colspan=\"4\">FRGC</td><td colspan=\"4\">MORPH</td></tr><tr><td colspan=\"2\">Ethnicity</td><td></td><td>Gender</td><td></td><td>Ethnicity</td><td></td><td>Gender</td></tr><tr><td>White</td><td></td><td>379 (4)</td><td>Male 309 (4)</td><td>White</td><td>2,666 (2.7)</td><td>Male</td><td>11,182 (2.7)</td></tr><tr><td>Asian</td><td></td><td>119 (4)</td><td>Female 226 (4)</td><td>Asian</td><td></td><td>50 (2.6) Female</td><td>2,132 (2.7)</td></tr><tr><td>Hispanic</td><td></td><td>15 (4)</td><td></td><td>Hispanic</td><td>529 (2.8)</td><td></td><td></td></tr><tr><td> Indian</td><td></td><td>12 (4)</td><td></td><td>Black</td><td>10,056 (2.7)</td><td></td><td></td></tr><tr><td>Black</td><td></td><td>10 (4)</td><td></td><td>Native American</td><td>13 (3.1)</td><td></td><td></td></tr><tr><td>Total</td><td></td><td>535 (4)</td><td></td><td>535 (4)</td><td></td><td>13,314 (2.7)</td><td>13,314 (2.7)</td></tr></table></body></html>\n\n",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "The MORPH database, chosen for reliability and age experiments, was included for several reasons. MORPH Album 2 has been used in previous work [27] for ethnicity estimation on faces. MORPH includes five ethnicity labels that provide the diversity needed for ethnicity classification. Also, since the data was captured under real-world conditions and not in a research environment [51], results from MORPH should provide a more accurate measure of performance of the proposed methods in real-world applications. The demographic distribution provides the opportunity to see if performance is impacted by any specific demographic, which is the final reason MORPH was chosen for this work. ",
        "page_idx": 24
    },
    {
        "type": "image",
        "img_path": "images/272dd320af713de3e8a39f3c0218588e570d85dc034a14c72f46c3eebcf37dc7.jpg",
        "img_caption": [
            "Figure 2.3: Example images from the MORPH database. "
        ],
        "img_footnote": [],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "As with the FRGC database, some of the data in MORPH cannot be used. Images of subjects belonging to the Unknown class will be discarded. The Unknown class is not useful for supervised learning techniques. The image list for MORPH was created by selecting a maximum of four images per subject. Some subjects do not have four images in the database. These images were examined to determine if the point fit was good and the subject was not wearing glasses. A subject was excluded from the experiment only if all of his or her candidate images did not meet the criteria mentioned above. The experiments on the MORPH data set included 35,601 face images of 13,314 distinct subjects. The breakdown of subjects in the chosen experiment set can be seen in Table 2.1 along with the average number of images per subject. The average interocular distance for images used in MORPH experiments was 93.51 pixels. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "2.1.3 Pinellas ",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The Pinellas database is a collection of booking (mug shot) images from the Pinellas County Sheriff’s Office1 in Florida. Example images can be seen in Figure 2.5. With approximately 1.4 million facial images from over 400,000 subjects, this database is one of the largest facial databases ",
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/043eb5c7a81aa46e0f35da951275cc08e872b6a74f71ebcdeea55c6a6cbdf2f7.jpg",
        "img_caption": [
            "Figure 2.4: Distribution of the MORPH dataset according to a) age, b) ethnicity, and c) gender. "
        ],
        "img_footnote": [],
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/8aa6c9ce26b3088b55906b7d85830743f892adae82ac416e263bf1c9de8e84fa.jpg",
        "img_caption": [
            "Figure 2.5: Example images from the Pinellas database. "
        ],
        "img_footnote": [],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "to date. The distribution of images according to age, gender, and ethnicity can be seen in Figure 2.6.   \nImages in the database average $480\\times600$ pixels. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "This dataset was chosen for its size as well as the ethnic and age diversity present in the images. Pinellas is much larger than MORPH and provides plenty of images for the application experiments. It was also included for reasons similar to MORPH. The data was captured under realworld conditions, at the sheriff’s office, and so can provide a more accurate measure of performance of the proposed methods in the real world. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "As with the MORPH and FRGC databases, images of subjects belonging to the Other class will be excluded from experiments. Further details on the selection of images from this dataset can be found in Chapter 6. ",
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/e2550e40df88c84e095aac99f06e3fd00060e94f383209ebbe2e027f4d026c28.jpg",
        "img_caption": [
            "Figure 2.6: Distribution of the Pinellas dataset according to a) age, b) ethnicity, and c) gender. "
        ],
        "img_footnote": [],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "2.2 Preprocessing ",
        "text_level": 1,
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The first steps of preprocessing are to annotate the facial images and correct for in-plane rotation. All images were processed with a script using the VeriLook 5.4 Standard SDK. Included in the SDK is the FaceExtractor component which extracts 68 facial feature points. These points include locations for the eyes, nose, mouth, and chin, and can be seen in Figure 2.7. The fit of the points was another consideration for including images in the experiment sets. If all 68 points were not detected in an image, it was automatically discarded. Using the eye centers from the detected feature points, the faces are rotated so the eyes are level in the image plane. The rest of the points are updated as well to be used later for separating the face into its subregions and extracting the face. More information on VeriLook can be found at the end of this chapter. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Figure 2.8 shows example facial regions of the chin, mouth, nose, nose tip, and eyes. A subset of the 68 points detected by VeriLook, shown in Figure 2.9, were used to extract facial regions. The eye regions are centered on the eyes (points 4 and $\\it5$ ) with a width equal to the distance between the centers and a height equal to the vertical distance from the eye centers to the center of the nose (12 ). The nose tip region starts midway between the eye centers (4, $\\it5$ ) and the nose center (12 ) and extends to midway between the nose center and the mouth center (11 ). The left and right boundaries correspond to the $x$ -coordinates of the eye centers. The nose region starts at the $y$ -coordinate of point $\\it6$ and extends to midway between the nose center (12 ) and the mouth center $(11)$ . The ",
        "page_idx": 27
    },
    {
        "type": "table",
        "img_path": "images/b25c4e34acf2c7b8e72d3005b0ffbf3ba5766d1c6c76fd50889d6f4a49ebc506.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td colspan=\"3\">FRGC</td><td colspan=\"2\">MORPH</td><td colspan=\"2\">Pinellas</td></tr><tr><td rowspan=\"3\">Face</td><td>Aug size</td><td>Ecp size</td><td>Aug size</td><td>Ecp size</td><td>Aug size</td><td>Ecp size</td></tr><tr><td>482×486</td><td>480×480</td><td>195×196</td><td>200×200</td><td>246×248</td><td>250×250</td></tr><tr><td>220×120</td><td>220×120</td><td>91x41</td><td>90×40</td><td>114x68</td><td>120×70</td></tr><tr><td>Eyes Nose</td><td>146×172</td><td>150×170</td><td>69×68</td><td>70×70</td><td>80×94</td><td>80×100</td></tr><tr><td>N Tip</td><td>220×119</td><td>220×120</td><td>91x47</td><td>90×50</td><td>114×64</td><td>120×70</td></tr><tr><td>Mouth</td><td>261×144</td><td>260×150</td><td>102×59</td><td>100×60</td><td>124×70</td><td>130×70</td></tr><tr><td>Chin</td><td>261x90</td><td>260×90</td><td>102×37</td><td>100×40</td><td>124×45</td><td>130×50</td></tr></table></body></html>\n\n",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Table 2.2: The average size of the regions from raw images and the resolution the regions were resized to for experiments. The images used in calculations were those that had a face and all 68 points detected by VeriLook script: 39,250 of 39,328 for FRGC, 53,964 of 55,608 for MORPH, and 1,436,799 of 1,447,607 for Pinellas. ",
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/33ba262fb68b2c3905910bb6f9cd1a29293f4f52e57ddff64006e22ab4b30b09.jpg",
        "img_caption": [
            "Figure 2.7: Example feature points detected by VeriLook SDK a) good fit on FRGC image, b) bad fit on MORPH image "
        ],
        "img_footnote": [],
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/9465c59e43674d373bae3a7ccb028a1ae6d7a6bd190956cec10339867e6ca8fb.jpg",
        "img_caption": [
            "Figure 2.8: Example facial regions used for experiments, extracted from an FRGC image. "
        ],
        "img_footnote": [],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "left and right boundaries are 5 pixels outside the $x$ -coordinates of points 7 and $\\delta$ on the outline of the nose. The mouth region starts halfway between the mouth $(11)$ and nose ( $12$ ) centers and extends to halfway between the mouth center ( $11$ ) and the chin ( $\\boldsymbol{\\mathcal{Z}}$ ). The left and right boundaries are midway between the $x$ -coordinates of the mouth corners $(9,\\ 10)$ and the closest points on the face contour (1, 3 ). The chin region keeps the same left and right boundaries of the mouth region. The chin region starts midway between the mouth center $(11)$ and chin ( $\\mathcal{L}$ ) and extends 5 pixels below the chin tip (2 ). Table 2.2 show the average resolution of each of the facial regions for each dataset. Based on these calculations, an experiment size was chosen for each region and dataset. All face regions were resized to the experiment resolution before preprocessing continued and feature extraction began. The extracted face region for baseline comparison experiments extends over the entire are were points were detected, from the eyebrows to the chin. Prior to further processing an elliptical mask of neutral color is placed around the face to minimize the effect of the background on performance. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "The next preprocessing step is to enhance the contrast of the image. Two methods are used. For texture-based features that will be extracted from a grayscale image, a simple histogram equalization is performed. The contrast enhancement procedure is slightly more complex for colorbased features. In order to preserve the relative color information between color channels, the image is first converted to the L\\*ab color space. Illumination is stored in the L channel, while the other channels hold the color information. Histogram equalization is performed on the L channel and the image is converted back into RGB space. An example of the preprocessing steps for the face and one of the eye regions can be seen in Figure 2.10. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "A local, patch-based approach is used for feature extraction with both the texture- and colorbased features. Each facial region image is divided into smaller images, called patches. Features are extracted from each patch and the feature vectors are concatenated to form the feature vector for the entire image. The MORPH and Pinellas experiments use a patch size of 10 $\\times$ 10 pixels and the FRGC experiments use a patch size of 20 $\\times$ 20 pixels. Since the FRGC images are of greater resolution, the patch sizes are larger so that the patch size relative to the image size is similar to MORPH and Pinellas. This also allows for a comparable number of features to be extracted from each of the datasets. ",
        "page_idx": 29
    },
    {
        "type": "image",
        "img_path": "images/c207569d5a7ef32bb1d1086dd68b48a70eb9a8557d3a97124d67f4edbfe2250c.jpg",
        "img_caption": [
            "Figure 2.9: Point subset from VeriLook annotation used to extract facial regions. "
        ],
        "img_footnote": [],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "2.3 Feature Extraction Methods ",
        "text_level": 1,
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Three main types of information can be found in 2D facial images: texture, color, and shape. Face images provide texture in both the skin and hair portions of the image. Color information can be gathered to provide hair, eye, and skin color. Moles and birthmarks can also be indicated by color. The outside contour of the face and position of the eyes, nose, mouth, and chin within the face give shape information that can be used for classification. The following chapters detail feature extraction methods and experiments performed in each of the three main categories of features. ",
        "page_idx": 30
    },
    {
        "type": "image",
        "img_path": "images/7413df605d0f942c58cc712b16a022d873e5ec6a6ed02671fdffcbe8536a7a6d.jpg",
        "img_caption": [
            "Figure 2.10: Preprocessing of a FRGC image following facial annotation. The original image is to the far left. The next image is corrected for in-plane rotation. The LEYE and FACE regions are extracted. The smaller images to the right represent the preprocessing steps for color (top) and texture (bottom) features for each region. The first images are contrast enhancement and resizing. The final images represent the patches used for feature extraction. "
        ],
        "img_footnote": [],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "2.4 Feature Reduction Methods ",
        "text_level": 1,
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Feature reduction techniques seek to eliminate noise and non-essential information from the feature vectors being used for classification. In this sense, feature extraction methods can be considered feature reduction techniques when the output of the extraction method is smaller than the original number of pixels in the image. Non-essential information could still be useful information, just not for the current problem. For example, information about a mole might be useful to help identify an individual, but not for gender classification. For this work, there is more interest in retaining features that encode gender or ethnicity information or even a combination of both, rather than identity. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Principal Component Analysis (PCA) is widely used for data compression and feature reduction. It is very useful for reconstructing data and is the basis for the well-known facial recognition algorithm “EigenFaces” [70]. PCA, known originally as the Karhunen-Loe´ve transform, finds the directions of maximum variance for a training set, regardless of class, using the eigenvectors of the covariance matrix. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "For feature reduction using PCA, some of these eigenvectors are discarded; those corresponding to the smallest eigenvalues. The larger the eigenvalue, the more the variance represented by that eigenvector is present in the training set. In practice, researchers normally keep the eigenvectors that account for 95% of the variance. This can be determined by how many eigenvalues, from largest to smallest, are needed to have 95% of the sum of all the eigenvalues. The retained eigenvectors are used to build a projection matrix. Features are projected into the PCA space by multiplying by the projection matrix. The Eigen library [25] was used to implement PCA for this work. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Another type of feature reduction is Linear Discriminant Analysis (LDA). This method is designed for classification problems with multiple examples of each class. This is another transform, similar to PCA, which projects the features into a lower-dimensional space; however, LDA takes into account the different classes that are present in the data. In this transform, the variance between classes is maximized, while the scatter within each class is minimized. By incorporating class information into the training, LDA does better at discriminating between classes than PCA in most classification cases. The number of features retained is equal to the number of classes used in the analysis. In preliminary experiments on FRGC, using LDA for feature reduction resulted in a large increase in training time, a large decrease in features used, but only small increase in performance for some features. Performance on some actually decreased. For this reason, feature reduction was limited to PCA. ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "2.5 Classification Methods ",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "2.5.1 $k$ -Nearest Neighbor ",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The $k$ -nearest neighbor ( $k$ -NN) is one of the simplest classification algorithms available to researchers. $k$ -NN is a simple classifier which relies on the training data. No training is necessary before classification starts. A test sample is classified according to the class of its $k$ closest neighbors. Each neighbor votes and whichever class has the most votes is the class assigned to the sample. In practice, $k$ should be odd to avoid ties, although this is not enforced within the algorithm. The FLANN library within OpenCV was used for experiments with $k$ -NN classification. ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The closeness of the neighbors can be determined by any distance metric. The choice of distance metric often depends on the type of data and the area of the problem. Common distance metrics used for any real-numbered data are the Manhattan, Euclidean, and Maximum distance metrics. These are also known as the $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ norms respectively. These three distance metrics are derived from the Minkowski or $p$ -norm measure, which can be defined as ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 33
    },
    {
        "type": "equation",
        "text": "$$\nL_{p}(x,y)=(\\sum_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\\frac{1}{p}},\n$$",
        "text_format": "latex",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "where $x$ and $y$ are vectors with length $n$ . Euclidean distance is the case where $p=2$ and corresponds to the intuitive idea that the distance between two points (in two dimensions) is a straight line. Manhattan distance, also known as city-block or taxicab, is the case where $p=1$ . The more colorful names stem from visualizing the distance measure on city streets. Manhattan distance is the distance a cab would have to travel on the street around square city blocks to get between two points, assuming all streets are two-way. Maximum distance is the case as $p$ approaches infinity. This simplifies to simply the maximum of the distances between each dimension. This is also known as Chebyshev distance. ",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Many of the features presented in this work will be in the form of histograms. Widely used distance measures for histogram data are the Chi-Square, Histogram Intersection, and Hellinger distances. The Chi-Square ( $\\chi^{2}$ ) distance is a weighted form of Euclidean distance with differences between larger elements being less important than differences between smaller elements. This distance can be defined as ",
        "page_idx": 33
    },
    {
        "type": "equation",
        "text": "$$\n\\chi^{2}(x,y)=\\frac{1}{2}\\sum_{i=1}^{n}\\frac{(x_{i}-y_{i})^{2}}{x_{i}+y_{i}}.\n$$",
        "text_format": "latex",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The Histogram Intersection distance measure starts out as a similarity metric, measuring the overlap between the two histograms. It is transformed into a distance metric by normalizing and subtracting from 1, as given by the follow equation: ",
        "page_idx": 33
    },
    {
        "type": "equation",
        "text": "$$\nH I(x,y)=1-{\\frac{\\sum_{i=1}^{n}\\operatorname*{min}(x_{i},y_{i})}{\\operatorname*{min}(|x|,|y|)}}.\n$$",
        "text_format": "latex",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The Hellinger distance, very similar to the Bhattacharyya distance is used in probability and statistics to provide a measure of similarity between two probability distributions. Hellinger distance between two discrete distributions is calculated as follows: ",
        "page_idx": 33
    },
    {
        "type": "equation",
        "text": "$$\nH(x,y)={\\frac{1}{\\sqrt{2}}}{\\sqrt{\\sum_{i=1}^{n}({\\sqrt{x_{i}}}-{\\sqrt{y_{i}}})^{2}}}.\n$$",
        "text_format": "latex",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "2.5.2 Support Vector Machine ",
        "text_level": 1,
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Support Vector Machines (SVM) are a popular classification method in gender and ethnicity classification [5, 9, 31, 38, 44, 47, 60]. As a supervised machine-learning algorithm, the SVM requires labeled training samples. The underlying idea of SVM training is to find the hyperplane that best separates the training samples. In kernel-based SVMs, the samples are transformed by the kernel into a higher dimensional feature space. This allows more room for the best separation to be found. The algorithm searches for the best hyperplane that separates the data with the largest margins on either side of the hyperplane to the training data. The training samples used to define the best hyperplane are known as support vectors. These vectors are stored with the equation of the hyperplane for testing. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "SVMs are inherently binary classifiers, but can be modified to work with multiple classes. The LIBSVM library [13], which was used for SVM classification, implements the one-against-one approach. If there are $k$ classes, the multi-class SVM consists of $\\binom{k}{2}$ , or $\\frac{k(k-1)}{2}$ , binary classifiers. The final class-decision of the multi-class SVM is the class that ‘won’ the most binary decisions. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Experiments performed with SVM classification used linear kernels. Radial Basis Function (RBF) kernels were investigated since literature shows that RBF SVM have equal or greater performance than linear SVMs. Unfortunately, these SVMs are very sensitive to chosen parameters. Using a tool provided in LIBSVM and features from one image per subject, for a given subset of subjects, a grid search was performed for cost (C) and gamma ( $\\gamma$ ) parameters. The cost parameter is the penalty given for misclassifying a sample and $\\gamma$ is a kernel specific parameter. Features from all images were not used to increase the speed of the parameter search and to avoid over-fitting. After choosing parameters with this technique, test results were mostly of only one class, which indicates the chosen parameters were not in the correct parameter space to achieve optimal performance. For this reason, RBF results are omitted. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "2.5.3 Artificial Neural Network ",
        "text_level": 1,
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Artificial Neural Networks (ANN) are a supervised machine-learning algorithm developed to imitate the way scientists believe the human brain works. Individual neurons are highly interconnected and are the basic blocks which compose the brain. In an ANN, a neuron is modeled by a weight vector of its incoming edges and an activation function. Networks can have any number of layers of neurons, but many have just three, an input layer, a hidden layer, and the output layer. ANNs use labeled training data. Training is an iterative process with labels being computed, an error function evaluated, and the weights for each neuron being updated. The final ANN models the relationship between inputs and outputs and can capture patterns present in the data. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Classification for multiple classes can be achieved by using an output layer of $c$ neurons where $c$ is the number of classes. Each class corresponds to a position in the output vector. The position that has a positive value indicates membership in the corresponding class, while all other positions are zero or negative. This is the method used in the classification experiments within this work. ANN classification was implemented using the machine-learning section of OpenCV 2.3.1 as well as the Fast Artificial Neural Network Library (FANN) [23, 52]. All ANNs used in this work are 3-layer with weights from training calculated by the resilient backpropagation (RPROP) algorithm. The RPROP algorithm is the default training method for OpenCV and is usually faster than classic backpropagation. Hidden layer sizes of 50, 100, and 200 were chosen for experiments to show performance with small and larger hidden layers. ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "2.6 General Experiment Setup ",
        "text_level": 1,
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "2.6.1 Performance Measures ",
        "text_level": 1,
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "In many classification experiments, the overall accuracy is used to report the performance. Overall accuracy is defined as the percentage of instances classified correctly regardless of the class or label for each instance. Another performance measure used in classification and machine learning provides a finer level of detail. A confusion matrix, or matching matrix, looks at both the predicted class and given class of an instance and places it into the correct position of the table. The rows of the table represent the given class while the columns represent the predicted class. This allows an observer to see the similarity between classes by looking at the misclassification rates of a given class to each of the other classes. Within the confusion matrix, the correct class-wise classification can be found along the diagonal. This is useful in instances when the test data is not balanced by class. An average class-wise accuracy would give a better idea of the performance of the classifier across the classes instead of the overall average which can be biased by an unbalanced set. ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "In many cases, a box plot of the class-wise accuracies will be shown. A box plot is a way to graphically represent groups of numbers. The box itself represents the first and third quartiles of the data while the line inside represents the median of the data. The whiskers of the box are the largest and smallest values within $1.5\\times1\\mathrm{QR}$ (interquartile range) which is the difference between the first and third quartiles. Points that do not fall within $\\pm$ IQR of the box are marked as outliers and plotted as points on the graph. ",
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/c1a7e070b1bc756bc3cd7453102cd49a2e9a6e8d959037fed67e92231c2847fd.jpg",
        "img_caption": [
            "Figure 2.11: Example ROC and DET curves. (L to R): ROC, DET. "
        ],
        "img_footnote": [],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "In biometric verification/authentication problems, a curve known as the Receiver Operating Characteristic (ROC) is used as a measure of performance. In this problem, a threshold is used on the match score by the system to determine whether two samples are from the same individual or different individuals. The ROC curve is created by varying the threshold from its lowest to highest possible values. At each threshold, a certain number of individuals are accepted or ruled as the same person, when they should not be. These are known as False Matches or False Accepts. At the same time, genuine matches are ruled to be different people and rejected. These are known as False Rejects or False Non-matches. The axes on the ROC curve are the False Accept Rate (FAR) and the False Reject Rate (FRR). An example two ROC curves can be seen in Figure 2.11. The curve which is closest to the origin is considered the best. In this instance ‘method 1’ outperforms ‘method 2’. Another measure to evaluate the performance is the Equal Error Rate (EER). This is the point on the graph where the FAR is equal to the FRR, in other words, the point where the curve intersects the diagonal in the graph. It is generally accepted that better systems will have a lower EER. ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Another curve, the Detection Error Trade-off curve (DET), can be used to display the FRR versus the FAR in a different format from the ROC. Instead of scaling the axes linearly, a logarithmic transformation is used on both the $x$ - and $y$ -axes. This results in a more linear trade-off curve than those seen in the ROC curves. An example can be seen in Figure 2.11. The EER can still be found by where the curve intersects a line going diagonally from the origin to the upper right corner. The curve which is closest to the origin is generally considered the best in these graphs. ",
        "page_idx": 36
    },
    {
        "type": "image",
        "img_path": "images/7b8fe2188a50e72c088b3592cec33630e9b5da9e648ada8fab2268ba11860aa3.jpg",
        "img_caption": [
            "Figure 2.12: Example CMC curve "
        ],
        "img_footnote": [],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "In biometric recognition/identification problems, a curve known as the Cumulative Match Characteristic (CMC) is used as a measure of performance. The CMC curve is created by sorting the match scores for each given probe. The axes on the curve are the rank in the sorted list $(1\\ldots N$ , where $N$ is typically the number of entries in the gallery) and the performance of the system at the given rank. In other words, the performance at Rank- $K$ is the percentage of probes where the true match is found in the first $K$ entries of the sorted match scores. The higher the Rank-1 performance and the faster the curve approaches 1 or $100\\%$ , the better the system when comparing CMC curves. An example of two CMC curves can been seen Figure 2.12. In this figure, ‘method 1’ has a higher Rank-1 performance than ‘method 2’. Both methods reach $100\\%$ very late, but ‘method 1’ would still be considered the better of the two, since its curve is higher than the curve for ‘method 2’ throughout the graph. ",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "2.6.2 Cross-Validation Evaluation ",
        "text_level": 1,
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Reliability and age experiments for color, shape, and texture are evaluated using a stratified five-fold cross-validation (CV) approach. In a five-fold CV experiment, the subjects are divided into five parts, keeping the proportions of classes in each part approximately equivalent to the proportions of the whole set. In each fold, a smaller experiment takes place. Images, or features from images, corresponding to the subjects from four parts are used for training while the other set is used for testing. For each fold, the part used for testing changes. In this way, each subject appears in the test set only once in the whole CV experiment. Cross-validation is done based on subjects instead of images so that images of the same subject will not appear in both the training set and testing set for any particular fold when multiple images are used per subject. The performance of a CV experiment is normally the average overall classification performance of the folds. Confusion matrices will also be shown to view the distribution of classifications for each class. ",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "2.6.3 Biometric Application Experiments ",
        "text_level": 1,
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Application experiments will have a different set-up than cross-validation. Specific images are partitioned to be used solely for training the classifiers. A list of gallery images was created with two images per subject. A list of probe images was created with one image per subject using the same subjects as the gallery list. Subjects in the probe and gallery sets are distinct from subjects in the training sets. Classification results on the probe will be used to filter which gallery entries will be used in recognition and verification experiments. ROC and CMC graphs will be used to measure the performance of these experiments as well as the EER and Rank-1 performance. Classification results will be reported with confusion matrices and average class-wise accuracies. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "The match scores for the base face experiment will be calculated using the VeriLook 5.4 Standard SDK previously mentioned during preprocessing. This is a commercial software development kit which supplies biometric functionality to its users. Functionality includes face detection and annotation, template generation, gender prediction, and template matching. All images that have templates generated for matching must pass a quality check, including lighting conditions and a minimum size requirement. More information can be found at the Neurotechnology website, http://www.neurotechnology.com/vl sdk.html. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "This concludes the description of data and methods to be used. The next three chapters will include results and discussion from reliability and age experiments for both gender and ethnicity classification using color, shape, and texture information. Color experiments will be discussed first. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Chapter 3 ",
        "text_level": 1,
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Color ",
        "text_level": 1,
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Images in digital color formats are normally saved in three channels. Therefore, each location in the image, known as a pixel, has three values associated with it. The values for the pixel depend on the color space the image is stored in. The most prevalent color space, or more accurately color model, is the RGB space. In this color space the values for each pixel represent the amount of red, green, and blue present in that particular location in the image. Some color spaces were developed to be independent of the device the image was captured with while others were developed as more device specific. With the exception of the RGB color space, the color spaces mentioned here divide luminance- or lighting-type from the chroma or color information. ",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "3.1 Feature Extraction Methods ",
        "text_level": 1,
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Color has been a useful feature in the area of content-based image retrieval. One simple color representation is Local Color Histograms (LCH), used in previous work for gender and ethnicity classifications on pericular images [46]. As mentioned before, each color pixel in an image has an intensity value for each channel, red, green, and blue for example. For this work, in the RGB color space, the red and green intensity values are quantized into four levels. Using both values to count occurrences in a two-dimensional histogram, a feature vector is produced of length 4 $\\times$ 4 or 16 elements per patch. These parameters were chosen based on findings in preliminary work [74]. Features from other color spaces will use the two channels that contain color information. ",
        "page_idx": 39
    },
    {
        "type": "image",
        "img_path": "images/c6413cf94002b9a08e768118d9de4473037d3d64472234811015537f884bfa44.jpg",
        "img_caption": [
            "Figure 3.1: RGB and HSI histogram visualization. Left: RGB, RG Histogram. B at 0%. Right HSI, HS Histogram. I at $50\\%$ . "
        ],
        "img_footnote": [],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "3.2 Color Spaces ",
        "text_level": 1,
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "A brief description for each of the color spaces that are used in this work follows, as well as a visualization of the colors found in each histogram bin. Since the histograms involve two channels, while images are composed of three, the value in the third channel is held constant. ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "3.2.1 RGB ",
        "text_level": 1,
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "The RGB color model is an additive color model. The vales for red, green, and blue are added together to form all the other colors available in this space. This model is based on the light spectrum where light of different wavelengths combine to produce color. As mentioned before, RGB is not a specific color space, but rather a color model. In a typical RGB image, values are quantized to 256 different levels. Figure 3.1 shows the various colors that can be found in each bin of a $4\\times4$ red-green histogram with no blue added. Other colors are possible with different levels of blue. ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "3.2.2 Device Dependent ",
        "text_level": 1,
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "3.2.2.1 HSI ",
        "text_level": 1,
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "The HSI color space is a cylindrical representation of points found in the RGB color model. The purpose of this arrangement is to more closely imitate how artists choose colors with a color wheel or a palette. The information for each pixel is divided into hue (H), saturation (S), and intensity (I) values. The intensity value gives the height on the cylinder, saturation the distance from the center, and hue gives the angle from the center of the cylinder. Other variations of this color space are the HSV and HSL spaces. The value (V) and lightness/brightness (L) channels are slightly different than the I channel of HSI, but mostly all represent the grayscale image with no color information. Hue information is roughly the same for all of the spaces, but the definition of saturation between the three color spaces is very different. Figure 3.1 shows the various colors that can be found in each bin of a $4\\times4$ histogram with 50% illumination. ",
        "page_idx": 40
    },
    {
        "type": "image",
        "img_path": "images/b488f9df6f191f2b8c8b11735dcb539f201d41220e1ae307d500b94e794c8066.jpg",
        "img_caption": [
            "Figure 3.2: YIQ and YCbCr histogram visualization. Left: YIQ, IQ Histogram. Y at 25%. Right: YCbCr, CbCr Histogram. Y at $25\\%$ . "
        ],
        "img_footnote": [],
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "3.2.2.2 YIQ ",
        "text_level": 1,
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "The YIQ color space was originally developed for use with televisions. The main usefulness was that images could be sent to both color and non-color television sets because the luminance information was roughly captured in the Y channel. The I stands for in-phase and Q stands for quadrature, referring to components used in quadrature amplitude modulation. The I channel encodes color in the orange-blue range while the Q channel encodes color in the purple-green range. Figure 3.2 shows the various colors that can be found in each bin of a $4\\times4$ histogram with 25% illumination. ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "3.2.2.3 YCbCr ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "The YCbCr color space is widely used for digital video storage and is related to the YIQ color space. The luminance information is found in the Y channel, just as it is in the YIQ space. The Cb and Cr channels encode the color information. The color information is stored as difference components. The Cb channel is the difference between the blue component of a pixel and a blue reference value, while the Cr channel is the same with the red component of a pixel. Figure 3.2 shows the various colors that can be found in each bin of a $4\\times4$ histogram with 25% illumination. ",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "3.2.3 Device Independent ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "The color spaces in this section were developed by the International Commission on Illumination (CIE). For this reason, these color spaces are often prefixed by CIE. To transform an image from an RGB space, the image must first be converted to the CIE XYZ color space. The XYZ space was one of the first mathematically defined color spaces, specified in 1931 by the CIE, and is the basis for the following spaces. Once there, conversions may be made to all the other CIE color spaces. ",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "3.2.3.1 LAB ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "The intention of the CIELAB, or $\\mathrm{L^{*}a^{*}b^{*}}$ , color space was to produce a color space where a change in the color value should produce the same amount of change in the perceived color. It was designed to approximate human vision and was adopted by the CIE in 1976. The CIELAB space was used during the contrast enhancement of the color images. The luminance information is stored in the $\\mathrm{L}^{*}$ channel while the color information was stored in the other two channels. The $\\mathrm{a}^{\\ast}$ channel holds information in the red to green range, while the b $^*$ channel holds information in the blue to yellow range. The reason this color space was not used for color histograms was that no hard boundaries could be found for the color channels a $^*$ and b $^*$ . ",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "3.2.3.2 LUV ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "The CIELUV color space also attempts to gain perceptual uniformity as mentioned for the CIELAB color space. It was adopted by the CIE in 1976. This space is widely used in computer graphics and other applications which use colored lights, as it is an additive space. It is related to ",
        "page_idx": 42
    },
    {
        "type": "image",
        "img_path": "images/9697701c9dbfb1a83e13c64c4e54e9c0160b011abb318f993bee8c7f0e67a599.jpg",
        "img_caption": [
            "Figure 3.3: LUV and LCH histogram visualization. Left: LUV, UV Histogram. L at $25\\%$ . Right: LCH, CH Histogram. L at $50\\%$ . "
        ],
        "img_footnote": [],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "CIELAB, in that each of the color spaces preserve the same L channel, but the chroma information is represented differently. Figure 3.3 shows the various colors that can be found in each bin of a $4\\times4$ histogram with 25% illumination. ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "3.2.3.3 LCH ",
        "text_level": 1,
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "The CIELCH color space is related to the CIELAB color space. It is a cylindrical version of CIELAB where C is the chroma channel and $\\mathrm{H}$ is the hue channel. As with the CIELAB and CIELUV color spaces, the LCH color space attempts to gain perceptual uniformity for changes in color values. Figure 3.3 shows the various colors that can be found in each bin of a $4\\times4$ histogram with $50\\%$ illumination. ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "3.3 Experiment Setup ",
        "text_level": 1,
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "The experiments for this section will be performed on subsets of the FRGC and MORPH databases, using the color images. Table 2.1 shows the breakdown of the subjects according to gender and ethnicity for the subsets used. Features are extracted using the RGB, HSI, LCH, LUV, YCbCr, and YIQ color spaces. Classification of gender and ethnicity for all six color spaces will be performed over all regions using the $k$ -NN classifiers ( $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ ). SVM and ANN classification will be performed on three of the color spaces, RGB, HSI, and LCH. The total number of color experiments is 180 for each dataset and demographic. Five runs of a stratified cross-validation experiment are performed for each feature, region, and classifier combination. ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "3.4 Analysis ",
        "text_level": 1,
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Global feature vectors were created to look at the color histograms within each region and color space. The two-dimensional histograms were flattened by taking rows from top to bottom. Figure 3.4 shows the average global features from the MORPH dataset. Many of the global feature vectors look similar over multiple regions. For all but the RGB features, the value in the majority of the bins is close to zero, indicating that colors within the region were limited. The range of skin tones is so small compared to the entire range of the color space, that a $4\\times4$ histogram over the entire space may not be sufficient to capture differences due to ethnic group or gender. Since the global vectors for each region are similar, the colors found in each region are similar to each other as well. This indicates that the different regions of an individual’s face are of a similar color. This is logical provided make-up, tattoos, and birthmarks do not dominate any specific region of the face. Differences between all combinations of regions will be calculated to further investigate the similarity of color over the entire face. ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "The features from the YCbCr color space are mostly concentrated in a single bin for the FRGC database, so this color space may not be very useful for classification purposes with the granularity used. It is possible that a 2D histogram with a finer granularity would capture more differences in the classes, but for now, it is not likely that the YCbCr features will work well for gender and ethnicity classification. In the MORPH dataset another very small peak is apparent in these features, which may increase the usefulness of the YCbCr space in differentiating between classes. ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Differences between the global feature vector for each region were computed for each facial image to gauge the similarity of color over the face. The distance measures used were $L_{1}$ , $L_{2}$ , $L_{\\infty}$ , Histogram Intersection, Hellinger, and $\\chi^{2}$ . Means were calculated according to each gender and ethnicity class. Similar trends were seen with each of the distance measures. Figure 3.5 shows the differences between regions in the RGB and HSI spaces using the Histogram Intersection distance measure for the FRGC dataset. The LCH, LUV, and YIQ color spaces had results similar to the RGB graphs shown. ",
        "page_idx": 44
    },
    {
        "type": "image",
        "img_path": "images/3a5afe56e3c76e97c1c950bcadabcf33093d893ebe46d2542dbc22a4da56651a.jpg",
        "img_caption": [
            "Figure 3.4: Average global feature vector for MORPH data. These are the normalized color histograms for each region. The $x$ -axis corresponds to the bin in the flattened histogram. The $y$ -axis corresponds to the relative frequency of values found in each bin. Top row (L to R): RGB, HSI. Middle row (L to R): YIQ, YCbCr. Bottom row (L to R): LUV, LCH. "
        ],
        "img_footnote": [],
        "page_idx": 45
    },
    {
        "type": "image",
        "img_path": "images/9c26d0eb41d315aedab08fda83d12af6d5524d6fc7ced1617b8ab4ca6cde82f6.jpg",
        "img_caption": [
            "Figure 3.5: Mean difference between FRGC global color region vectors by demographic. Differences shown are for the RGB (top) and HSI (bottom) color spaces using the Histogram Intersection distance. "
        ],
        "img_footnote": [],
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Overall, the differences between the regions were small, further indicating that color is fairly stable over the human face. The two pairs of regions most similar to each other in all the color features were LEYE/REYE, and NOSE/NTIP. The eye regions both hold an eye and for the majority of the population, both eyes are the same color. Congenital heterochromia iridis, two different colors in the eyes from birth, only occurs in approximately 6 out of 1,000 births [17], and in some cases is hardly noticeable. In this work, the nose and nose tip regions overlap. For these reasons, it makes sense that these two pairs of regions are the most similar out of all combinations. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "The two pairs of regions least similar to each other in the RGB, LCH, LUV, and YIQ color spaces were the NOSE/CHIN and NTIP/CHIN pairs. In the HSI color space, the chin region was furthest away from all the other regions. The chin region is likely to have different colors for several reasons. In males, this region likely has facial hair which provides a larger difference than when comparing regions in females. For all classes, the chin is the only facial region that is on the border of the face, so some neck or clothing color could also be included, depending on landmark detection accuracy. The nose regions are not likely to have facial hair, unless the subject has a mustache, which would partially be included in the nose regions. The nose is also prone to illumination which may wash out the color information in the original RGB image. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "In looking at the means of each gender class, the difference between the chin and any other region is noticeably larger for males than females. None of the other regions have a noticeable trend. Besides the chin region, color varies similarly between both males and females. This indicates that color is slightly more stable around the face for females than males. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "In many of the HSI region comparisons and several in each of the other color spaces, the FRGC Black and Indian classes have much higher region differences than the other classes. This could indicate that color is not as consistent across the face in these demographics, but it could also indicate that the colors found in these regions lie close to the histogram bin borders. Small changes in color could produce very different histograms in that case. Another reason for the large differences could be that with just 10-12 subjects in the FRGC set, the mean is representing more of an individual case than the class as a whole. Color around the face is much more stable in HSI for White, Asian, and Hispanic classes, whereas in the Black and Indian classes it resembles the variance found in the other color spaces. In the MORPH dataset, the Black class spikes in the CHIN region comparisons which is likely the influence of the large majority of males in the class. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "In the FRGC YCbCr space, the mean differences were much, much smaller than the other color spaces. The minimum distance between regions recorded was zero. This is the effect of what was seen in the global feature vectors, all colors concentrated in just one bin for a large portion of the subjects. In this color space, the mouth region was the most separated, likely due to the lips providing a color not found in the same histogram bin as the rest of the face. For MORPH, this color space was more similar to the other color spaces. ",
        "page_idx": 47
    },
    {
        "type": "table",
        "img_path": "images/92cce918caacc6767bb2a29d88bc704a4c15a0d1d841315f389dbf98bb3fe5da.jpg",
        "table_caption": [
            "Table 3.1: The number of images present per demographic label and age range for the MORPH dataset. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Age</td><td>Male</td><td>Female</td><td>Black</td><td>White</td><td>Hispanic</td><td>Asian</td><td>Ameative</td><td>Total</td></tr><tr><td>16-20</td><td>5,882</td><td>817</td><td>5,199</td><td>1,028</td><td>428</td><td>36</td><td>8</td><td>6,699</td></tr><tr><td>21-25</td><td>5,408</td><td>879</td><td>4,681</td><td>1,161</td><td>384</td><td>59</td><td>2</td><td>6,287</td></tr><tr><td>26-30</td><td>3,865</td><td>752</td><td>3,466</td><td>859</td><td>276</td><td>13</td><td>3</td><td>4,617</td></tr><tr><td>31-35</td><td>3,894</td><td>878</td><td>3,484</td><td>1,091</td><td>184</td><td>9</td><td>4</td><td>4,772</td></tr><tr><td>36-40</td><td>3,919</td><td>936</td><td>3,564</td><td>1,169</td><td>112</td><td>6</td><td>4</td><td>4,855</td></tr><tr><td>41-45</td><td>3,439</td><td>843</td><td>3,267</td><td>957</td><td>49</td><td>0</td><td>9</td><td>4,282</td></tr><tr><td>46-50</td><td>2,034</td><td>382</td><td>1,816</td><td>563</td><td>26</td><td>5</td><td>6</td><td>2,416</td></tr><tr><td>51-55</td><td>1,019</td><td>180</td><td>907</td><td>277</td><td>11</td><td>2</td><td>2</td><td>1,199</td></tr><tr><td>56-60</td><td>292</td><td>49</td><td>241</td><td>98</td><td>0</td><td>0</td><td>2</td><td>341</td></tr><tr><td>61-77</td><td>119</td><td>14</td><td>89</td><td>44</td><td>0</td><td>0</td><td>0</td><td>133</td></tr><tr><td>Total</td><td>29,871</td><td>5,730</td><td>26,714</td><td>7,247</td><td>1,470</td><td>130</td><td>40</td><td>35,601</td></tr></table></body></html>\n\n",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "MORPH features will also be analyzed by age. The subjects’ ages at the time of the image capture were recorded and provided with this dataset. The ages of the subjects ranged from 16 to 77 years old. The ages were grouped in age ranges of 5 years with the last group, 61-77, covering 17 years. This choice was made based on the low number of images present for that particular age group. Table 3.1 gives the number of images per class over the ages present in the experiment set. As the age of the subject goes above 60 years of age, the number of images present in the age range drops dramatically. The over 60 subjects are also not represented in three out of five ethnic classes. Since experiment sets were chosen without regard to age, it is most likely that performance on images of older subjects will be lower than that of younger subjects. A large majority of the training images will be on younger subjects. This may distort the training space if the features themselves are not age-invariant. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Discounting the $^{61+}$ group, the RGB features vary less than 5% across the age groups. The rest of the color spaces vary slightly more than 5% in pairs that include the chin or the mouth. This indicates that there is a difference of color in the chin and mouth regions with respect to age. As MORPH is a predominately male dataset, this could be facial hair. The eye regions do not necessarily include the eyebrow, which would possibly increase the similarity. The RGB color spaces features seem to be slightly more stable with respect to age than the other color spaces. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "The region differences in the younger age groups tend to be higher in the region comparisons with the most variance. The exceptions to this trend are the region comparisons involving the mouth. In looking at the mean differences per class, older males tend to have more variation in the comparisons with the mouth than younger males, while younger females have more variation in these comparisons than the older ones. The male variation is likely due to facial hair, while the female variation could be a result of lipstick or make-up. More work would be required to verify a relationship between age and facial hair or age and lipstick, so this remains a theory. One result of aging that could account for this is that lips thin over time [4], providing more skin tone and less lip color to help stabilize color in older females. As previously mentioned, most of the images are of younger subjects and a larger sample space allows for more variance within the classes. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "3.5 Gender ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "3.5.1 Reliability ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Nearest neighbor classification was performed using the $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ distance measures to explore which features had classes that grouped together. PCA was performed on the features for feature reduction, retaining 95% of the variance. Figure 3.6 shows the 1-NN results using gender on the FRGC dataset. The values plotted in the box plot are the class-specific accuracies from five runs of each cross-validation experiment. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "For most of the features in the different color spaces, the plots are fairly high and small, indicating that the performance for both Male and Female was good. The whiskers are all very short as well. The class-specific accuracies do not tend to vary much between cross-validation experiments. This results in two sets of bunched numbers. With just the two classes, these two bunches determine the height of the box plot, and the numbers that fall in the first and fourth quartiles will not fall far outside the box, resulting in short whiskers. This indicates that, most of the time, the features for the Male and Female classes have a close neighbor of the same class, and the results are not highly influenced by the random choice of training samples. The MOUTH and EYE regions perform the best out of the regions for several color spaces. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "The class accuracies for the CHIN region and the YCbCr color space were more spread out than the other experiments. For the YCbCr features, this is the result of all the features looking very similar and being concentrated mainly in one bin of the histogram. The nearest neighbor algorithm does not deal with ties. Depending on the implementation, the “closest” neighbor could be either the first neighbor the algorithm saw at that distance or the last. In this instance, samples in the Male class outnumber samples in the Female class, so the chances of being the first or last neighbor are better for the Male class. This results in most samples being classified as Male, giving the Male class a very high performance rate and the Female class a very low performance rate, which results in a large box plot. The CHIN region has a higher classification rate for Males than the other regions, but a lower rate for Females. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "In a large majority of the experiments, the Male class had a higher classification rate than the Female class. With more samples in the training space, the likelihood of the closest neighbor in an overlapping area is greater for the Male class than the Female class. The exceptions to this trend are found in the YCbCr experiments. Using the $L_{2}$ distance measure, the Female class has a higher accuracy in every region. The $L_{1}$ and $L_{\\infty}$ distance measures in this color space both follow the larger trend of higher Male classification rates. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Figure 3.7 shows the results of gender nearest neighbor classification on the MORPH dataset using the $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ distance measures. Performance on the MORPH dataset declines from the high-quality dataset results. The bias towards Male classification is more evident with larger discrepancies between the class-wise accuracies. In every instance, the class-wise accuracy is higher for the Male class than the Female class. This may not be due to the drop in image quality though. The ratio of female to male samples went from approximately 3:4 in FRGC to around 1:5 in the MORPH dataset, which could account for part of the difference, if not all. The RGB features have the smallest discrepancy between the class-wise accuracies, but it is still a large discrepancy. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Figure 3.8 shows the results of gender classification using the more sophisticated machinelearning classifiers ANN and SVM for both datasets. The RGB, LCH, and HSI color spaces were chosen for these experiments based on performance in 1-NN experiments and analysis. The performance gap between classes increased a little in the FRGC LCH and HSI results, but the performance increased overall. The RGB features increased performance and decreased the performance gap between genders. These results indicate that a more sophisticated classifier would be useful in further gender classification. For MORPH, performance in the RGB space improved for the most part by using a more sophisticated classifier. The most improvement was seen in the MOUTH region. The ",
        "page_idx": 50
    },
    {
        "type": "image",
        "img_path": "images/744b02fe312c7833b7ab864980628851fd561d188377af0f6aa8aefcf2f81229.jpg",
        "img_caption": [
            "Figure 3.6: 1-NN gender classification on FRGC color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV. "
        ],
        "img_footnote": [],
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/4919728d4ea4dcb124e834ac0b7fb123c75d862cd64f6ea31575a734533ea170.jpg",
        "img_caption": [
            "Figure 3.7: 1-NN gender classification on MORPH color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV. "
        ],
        "img_footnote": [],
        "page_idx": 52
    },
    {
        "type": "image",
        "img_path": "images/611322d3066d2d8b6dbd7c77981594573dbd4649dcf03681cb33bad8aac4e391.jpg",
        "img_caption": [
            "Figure 3.8: ANN and SVM gender classification on color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: RGB, HSI, LCH. L to R: FRGC, MORPH. "
        ],
        "img_footnote": [],
        "page_idx": 53
    },
    {
        "type": "image",
        "img_path": "images/dc4fecc3349f0b3f0dace07a9ec58d8f3383107ae6e3a05d9965a99acebda61a.jpg",
        "img_caption": [
            "Figure 3.9: Easy and hard subjects in color gender classification on MORPH images. The subject on the far left was never misclassified. The other subjects were misclassified in over $95\\%$ of the color experiments. The Male in this section was mislabeled as Female. "
        ],
        "img_footnote": [],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "NTIP is the only region that did not improve overall. The class performance of one class increased while the other decreased. That is actually the case for most of the LCH and HSI experiments as well. The ANN and SVM classifiers netted no overall improvement of performance in these spaces, with the exception of the MOUTH region in the HSI color space. The linear SVM results in the all the MORPH LCH and MORPH HSI upper face experiments where the box ranges from 0 to 100, show that these genders are not linearly separable. The poor performance also indicate that many of the features in these experiments are overlapping. MORPH results indicate that SVM and ANN classifiers would be most useful with RGB features or in HSI MOUTH experiments. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Over 80% of the MORPH images were misclassified in less than 25% of color experiments. One image was never misclassified in these experiments. No subjects were misclassified in all color gender experiments, but eight were misclassified in over 95% of the experiments. Images corresponding to these subjects can be seen in Figure 3.9. The classifiers were able to catch one error in the metadata. The Male image in the right grouping of the figure, misclassified over $95\\%$ of the time, is mistakenly labeled as Female. Taking that into account, that subject actually had a correct classification most of the time. These images suggest that subjects in the Black Female group are the hardest to predict gender on using color information. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Table 3.2 shows the results of baseline experiments on full face for the FRGC and MORPH datasets. Included are the results of gender classification using the VeriLook SDK. In the FRGC dataset, the eye regions, using the ANN classifiers, fall approximately $10\\%$ below the VeriLook ",
        "page_idx": 54
    },
    {
        "type": "table",
        "img_path": "images/0204f1a8a2f1b127f206f52621dbd93e5fc57d8f8d33e81d121f0288b9bae1c5.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Classifier</td><td colspan=\"3\">FRGC</td><td colspan=\"3\">MORPH</td></tr><tr><td>Male</td><td>Female</td><td>Average</td><td>Male</td><td>Female</td><td>Average</td></tr><tr><td>ANN100</td><td>99.95</td><td>0.13</td><td>50.04</td><td>96.07</td><td>61.68</td><td>78.88</td></tr><tr><td>SV MLinear</td><td>92.39</td><td>89.50</td><td>90.94</td><td>96.84</td><td>58.82</td><td>77.83</td></tr><tr><td>L2</td><td>79.48</td><td>69.22</td><td>74.35</td><td>87.45</td><td>25.62</td><td>56.54</td></tr><tr><td>VeriLook</td><td>92.72</td><td>98.45</td><td>95.59</td><td>99.14</td><td>67.75</td><td>83.45</td></tr></table></body></html>\n\n",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Table 3.2: Gender performance using color and full face. Percentage values represent the classspecific accuracies and the average class-specific accuracy for each experiment. VeriLook refrained from predicting gender on $2.75\\%$ and 0.89% of male and female images respectively in the FRGC dataset. In the MORPH dataset it predicted no gender on $0.5\\%$ and $11.26\\%$ of male and female images. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "gender classification accuracy. Following the same feature extraction but with face images, the gap is lessened to 5% with the best classifier, but the regions still perform less favorably than the whole face. In the MORPH dataset, the MOUTH region comes within 1-2% of the VeriLook performance. This indicates that, with images of a lower resolution, the color of the mouth region holds comparable gender information to the entire face. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "3.5.2 Age ",
        "text_level": 1,
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Figure 3.10 shows classification performance by age groups for the RGB space using 1- NN with Euclidean distance for classification. Other classification measures and color spaces give similar performance with exceptions noted below. The graphs show less variance on the top of the box suggesting that color in one of the classes is less impacted by age than the other. This would be the Male class in this instance. Again, the Male class does have the highest representation, which given any overlap, would provide that class an advantage. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "There does not seem to be an age that is easiest to classify by gender overall. Younger females are easier to classify in the eye regions than older females. This is true in face experiments as well. In the eye regions again, there is no definite trend in the male class. For the nose regions, an erratic downward trend is noticed in the class-wise accuracies as age increases, suggesting that the color of the nose is not age invariant and behaves similarly for both males and females. The mouth region has a definite peak for both male and female performance between 26 and 45 yrs old. This suggests that gender classification based on color is more difficult for younger and older subjects than middle-aged subjects in the mouth region. For the chin region, there is a small upward trend for male classification, but the trend is down for the female class. This indicates that 21-35 year old males are easier to classify by the chin region than others. ",
        "page_idx": 55
    },
    {
        "type": "image",
        "img_path": "images/1c2cb94e1cba6aa9305d4feac302caf1d3763e35e005c8bc28b1e1edd15b2270.jpg",
        "img_caption": [
            "Figure 3.10: Gender performance by age group for features in the RGB space using 1-NN classification ( $L_{2}$ distance). "
        ],
        "img_footnote": [],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "It seems as though all regions are impacted by age either in one class or the other. The regions that are least impacted by age are the nose, nose tip, and mouth regions. The most impacted regions, those with the most variance, are the eye regions, not for the Male class, but definitely for the Female class. This same trend is seen in full face experiments. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Over all $k$ -NN classification experiments, all the color spaces perform similarly. Some differences occur in the RGB, LCH, and HSI color spaces using the ANN and SVM classifiers. With the ANN classifier, the variance for females is more pronounced in the HSI space. In both ANN and SVM classification, more of an upward trend is noticed for females as age increases in the NTIP and MOUTH regions in the HSI and RGB color spaces. As females age, less color variance is noted in these regions resulting in better performance. The LCH and HSI color space features perform similar to 1-NN, except with the SVM classifier. In the eye and nose regions, the classifier does not get enough separation in the training set and ends up predicting all Male class. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "3.6 Ethnicity ",
        "text_level": 1,
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "3.6.1 Reliability ",
        "text_level": 1,
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "Figure 3.11 shows the results of ethnic nearest neighbor classification on the FRGC dataset using the $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ distance measures. The boxes in the ethnic results are much larger than the ones seen in the gender results. This indicates a wider range of class-wise accuracies. One or two classes may have good performance, as seen by the upper portion of the plot, but the other classes are not well recognized, extending the plot close to the zero line. The anomaly in these graphs is once again the performance of the YCbCr features, with outliers that fall well above the box plots. With 25 class-wise accuracies to plot, when one class performs very well and the others do not, the third quartile falls within the second-best group of accuracies. With a small IQR from the poor performance of the other classes, the best performances will be outliers on the graph. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "For most of the experiments, the class with the best performance is the White class, followed by the Asian class. Since the White and Asian classes have the largest number of samples present in the experiment set, it makes sense that they would have the best class-wise accuracies. With the $L_{2}$ distance measure on YCbCr features, the best performance is on the Indian class, followed by the White class for the nose regions and the chin. As previously discussed, not much variance is found with the YCbCr features, and since the trend is only present in one of the three distance measures it very likely deals with the peculiarity of the data and not differences within the ethnic groups. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "In most of the experiments, the class-wise accuracies of the Hispanic, Black, and Indian classes are less than 20%. In the LUV and YIQ and the eye and nose regions, the class-wise accuracy for the Indian class reaches up to $46\\%$ performance. The best accuracies for the Black class are found in the mouth region using the LCH features, but only reach 30%. The Hispanic class has a consistently low class-specific accuracy, rarely exceeding $15\\%$ , but it is less likely to have a $0\\%$ accuracy than the Black class. This indicates that there is a small subset of the Hispanic class which is more distinguishable than the rest, but also more generalized than just one person. The rest of the class, however, overlaps with another class. These results indicate that different regions might be better for ethnic classifications for different ethnic groups. With these three classes, the number of subjects was 15 or less, so any conclusions drawn from this information may not generalize to the larger problem. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "Figure 3.12 shows the results of ethnic nearest neighbor classification on the MORPH dataset using the $L_{1}$ , $L_{2}$ , and $L_{\\infty}$ distance measures. With this dataset, the best and worst performing classes change. The class with the best class-specific accuracies is the Black class, followed by the White and Hispanic classes. This is partially due to the number of samples for each of these classes. The remaining two classes have less than 100 subjects present in the experiment set. ",
        "page_idx": 57
    },
    {
        "type": "image",
        "img_path": "images/ffdd9550e0144bc3257f292820af3b8a0df579de3a855f359983f409823c5f49.jpg",
        "img_caption": [
            "Figure 3.11: 1-NN ethnic classification on FRGC color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV. "
        ],
        "img_footnote": [],
        "page_idx": 58
    },
    {
        "type": "image",
        "img_path": "images/cd1e7d037f04b710510988b07f0f869a5d5597ba3558a41181ad22ef00ce103a.jpg",
        "img_caption": [
            "Figure 3.12: 1-NN ethnic classification on MORPH color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of each experiment. Top row (L to R): RGB, HSI. Middle row (L to R): YCbCr, YIQ. Bottom row (L to R): LCH, LUV. "
        ],
        "img_footnote": [],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The Black class has the highest class-specific accuracies of the MORPH ethnic experiments with accuracies falling between 85% to 95% on average. The best results are found using the RGB or HSI features for all of the regions. The best class-wise accuracies for this class are in the MOUTH region using the RGB and HSI features. Performance in this region ranges up to 97%. The next best region performance is around $95\\%$ with either a nose or eye region depending on the color space and the distance measure. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The class with the next best performance is the White class. The RGB and HSI color spaces produce the best results for this class, reaching 79% in the MOUTH region. This class has a larger variance than the other ethnic classes with the majority of class-wise accuracies falling between $50\\%$ and 70%. After the MOUTH region, the next best performing regions are the nose regions. The best performance on these regions is between $70\\%$ and $75\\%$ with the HSI and LCH features. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The highest class-specific accuracies for the Hispanic class can be found using the RGB, HSI, and LCH color space features for most regions. The best performance for this class in 1- NN experiments was approximately $30\\%$ using RGB and HSI features in the MOUTH region. On average, the class accuracy for Hispanics was between $10\\%$ and $25\\%$ in these experiments. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "The best performance for the Asian class can be found using the RGB, LUV, and HSI spaces. This class has a fairly erratic performance indicating there is a lot of overlaps with another class and is highly dependent on the training data. Class-wise accuracies for the Asian class range between 0% and $10\\%$ for the most part. The features that most consistently achieved a non-zero class accuracy were found in the RGB space. The best region was different for each distance measure. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "With only 13 subjects, classification on the Native American class is very poor, most often with no correct classifications. In less than 30 out of 108 gender experiments, color features managed to achieve at least one correct classification for this class over each of the five CV runs. These experiments were concentrated in the eye and nose regions with a few using the MOUTH region. Several color spaces were represented within these experiments as well. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "Since the best ethnic performance is found for most of the classes in the RGB and HSI color spaces, these color spaces will be investigated using ANN and SVM classifiers. The hope is that with a more sophisticated classifier, class-wise accuracy will increase. The global feature vectors for these spaces show values in approximately half of the bins. This is different from the YIQ, YCbCr, and LUV color spaces with values in only a third of the bins. It is similar, however, to the global features in the LCH color space. On the assumption that the less sparse the feature vector, the better the classification, the LCH color space will also be examined with the ANN and SVM classifiers. ",
        "page_idx": 60
    },
    {
        "type": "table",
        "img_path": "images/1752b5a9e523a48e4b77918a6ddfdea2c7a86fd04a41c33d54d730dd77bcba8c.jpg",
        "table_caption": [
            "Table 3.3: Ethnic performance using color and full face, FRGC. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Native American</td><td>Average</td></tr><tr><td>ANN100</td><td>87.82</td><td>0.00</td><td>36.17</td><td>97.96</td><td>0.00</td><td>44.39</td></tr><tr><td>SV MLinear</td><td>84.98</td><td>11.59</td><td>39.99</td><td>97.89</td><td>0.00</td><td>46.89</td></tr><tr><td>L2</td><td>66.63</td><td>4.76</td><td>26.77</td><td>97.28</td><td>4.09</td><td>39.50</td></tr></table></body></html>\n\n",
        "page_idx": 61
    },
    {
        "type": "table",
        "img_path": "images/0491c5abc4f54ef68c4b8bc2fcdd250cbab9b1616cabe8e8f3139e10455e4783.jpg",
        "table_caption": [
            "Table 3.4: Ethnic performance using color and full face, MORPH. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Indian</td><td>Average</td></tr><tr><td>ANN100</td><td>99.95</td><td>0.13</td><td>0.00</td><td>0.00</td><td>0.00</td><td>20.01</td></tr><tr><td>SV MLinear</td><td>95.73</td><td>82.76</td><td>5.33</td><td>11.67</td><td>26.50</td><td>44.40</td></tr><tr><td>L2</td><td>84.30</td><td>56.01</td><td>3.67</td><td>9.17</td><td>10.75</td><td>32.78</td></tr></table></body></html>\n\n",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "Results of ethnicity classification using the RGB, HSI, and LCH color space features with the ANN and SVM classifiers are shown in Figure 3.13. Most of the experiments increased performance on the two most represented classes in the dataset, but did little to improve performance of the other classes. In most of the MORPH experiments, the class-wise accuracy for these classes decreased to $0\\%$ . This indicates that these classes do not have enough representation and that the samples that belong to them overlap with the larger classes. ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "Images from 30 subjects were never misclassified according to ethnicity. All of the subjects were males from the Black class. This is logical since this class has the largest representation and the largest classes have performed the best in the classifiers. Images from 20 subjects were misclassified by every run of every experiment. These subjects can be seen in Figure 3.14. With only 13 subjects present in the Native American class and 10 of them being misclassified in every experiment, there is a problem with this class. Either it is indistinguishable from another class based on color, or it is lacking the training samples needed to created an accurate representation of the class. The problem still exists for the Asian class, but not as badly with only 10 out of 50 subjects misclassified in all experiments. ",
        "page_idx": 61
    },
    {
        "type": "image",
        "img_path": "images/d907ee3536cfdadbf0665d012b9d595b7c50d2766018fc63f73eb8ae546db729.jpg",
        "img_caption": [
            "Figure 3.13: ANN and SVM ethnic classification on color features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: RGB, HSI, LCH. L to R: FRGC, MORPH. "
        ],
        "img_footnote": [],
        "page_idx": 62
    },
    {
        "type": "image",
        "img_path": "images/d988e80ee8c4ae2634c9876e6da890939e8d82594c09734c7b57c892b99f6a56.jpg",
        "img_caption": [
            "Figure 3.14: Hard subjects in color ethnicity classification on MORPH images. The subjects to the left are from the Asian class. The subject to the right are from the Native American class. "
        ],
        "img_footnote": [],
        "page_idx": 62
    },
    {
        "type": "text",
        "text": "Tables 3.3 and 3.4 show the results of ethnic classification on the full face using RGB color space features. In the FRGC dataset all regions but the CHIN region performed similarly on the best performance for each region with all the classifiers. The best CHIN performance was $10\\%$ below the face results. In the MORPH experiments, the MOUTH region compares favorably to the full face results with the eye regions slightly below. This indicates that the MOUTH and eye regions hold a comparable amount of color ethnicity information as the whole face. ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "3.6.2 Age ",
        "text_level": 1,
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "Figure 3.15 shows ethnic performance broken down over the age groups for MORPH experiments. The 41-45, 56-60 and 61+ age groups do not have subjects from each ethnic class in them, as seen in Table 3.1. The 41-45 age group has no Asian subjects in it. The 56-60 age group is missing Asian subjects as well as Hispanic subjects while the $^{61+}$ group only has subjects from the Black and White classes in it. If a class was not present in the age group, no value was plotted for the class-wise accuracy. This shows an improved performance for these age groups since the missing classes normally have poorer performance. ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "Performance on the best performing class does not vary much with age. There is a trend on most of these experiments for the next best performing classes. Excluding categories that do not have all age groups present, ethnic classification is more accurate on subjects under 30 years of age than older subjects in the second best performing class. The exceptions to this trend lie in the RGB features. Ethnic performance across the age groups varies less in this color space and does not show a definite trend. For all the others, the performance drops off as the subjects age until the spike in the 41-45 group. From there it depends on the region. In the CHIN region, ethnic performance on the 46-55 age groups increases again, but rarely more than the best performance on the younger subjects. ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "It seems as though all regions are impacted by age either in one class or another. The most heavily affected region for ethnic classification is the CHIN. Performance on all classes in this region show variance over age. In the other regions the best performing class is not as affected by age as it is in the CHIN region; however, the other classes do show some variance. The amount of variance per region is dependent on the color space. Features from the RGB space show less variance with age than the remaining color spaces. The most stable region with respect to age in that space is the MOUTH. ",
        "page_idx": 63
    },
    {
        "type": "image",
        "img_path": "images/becbf39d47b7f6d47b8a5921952049c5e57ea135a287e90e6666b17aad553832.jpg",
        "img_caption": [
            "Figure 3.15: Ethnic performance by age group for features in the RGB space using 1-NN classification ( $L_{2}$ distance). "
        ],
        "img_footnote": [],
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "3.7 Conclusions ",
        "text_level": 1,
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "Overall color is fairly stable around the face. The main color information comes from skin and is similar across the face. The region that is most likely to have different color information than the others is the CHIN region, very likely due to the presence of facial hair in some males. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "Upon further analysis, coarse 2D histograms covering the entire space may generalize color information too much. The range of possible skin tones is very small compared to the entire color space represented by the color histogram. Future work will most likely use a finer histogram over a smaller portion of the color space. Colors detected outside this area could indicate make-up, face paint, glasses, or really anything that could be on the face and be a different color. This could help give a measure of confidence. If all detected colors fall within the colors of interest, the classification could proceed. If a certain percentage fell outside the colors of interest, a flag could be raised or a low-confidence value returned. Features from different color spaces could also be combined to improve color representation. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "In high quality images, color information can be used to predict gender correctly over 80% of the time using the eyes, nose tip, and mouth regions. It is not as reliable for the lower-quality images, having between 75-85% average class-wise accuracy. In this dataset, the mouth provides the most reliable results, comparing favorably to face. The color of the eye regions performs fairly well which agrees with previous color experiments [46]. Based upon this, the conclusion is that color information is fairly reliable for gender classification. Color information is not as reliable in ethnic classification as it is in gender classification for the experiments included within, but the mouth and eye regions compare favorably to to the face. This suggests that better representation is needed, both in training samples and feature extraction, since color can be major indicator for human-based ethnic classification. It also suggests a need for better ethnic labels. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Ethnicity classification is, by nature, a hard problem. Ethnicity is based on how an individual perceives either himself or others. This perception may not coincide with how another person would perceive the same individual. Most databases rely on self-reported ethnicity which gives no measure of consistency between ethnicities on the various subjects. Two people with the same mixed heritage might identify themselves with different ethnic groups. Add that to the problems of nonbinary classification and less representation per class, and one can see how ethnicity results tend to be worse than those for gender classification. ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "All regions and colors are impacted in some way by age. The most affected regions for gender are the mouth and chin regions for both genders and the eye regions for females. The most impacted region in ethnic classification is CHIN. The color space that was least affected by age was the RGB color space. The least impacted region in gender is nose. The least impacted regions in ethnicity are the mouth and nose tip regions. This suggests that regions covering the nose are the most stable regions with respect to age in both gender and ethnic classifications. ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Based upon the results and analysis from this chapter, features from the RGB color space and the MOUTH and NTIP regions will be used for both gender and ethnicity classification in Chapter 6 fusion experiments. The next type of features to be investigated is shape. The following chapter will cover results and discussion on partial-face shape experiments. ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Chapter 4 ",
        "text_level": 1,
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Shape ",
        "text_level": 1,
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Shape can mean several different things. It can mean the actual shape of an object, the outline and contours, or the distribution of smaller pieces within the object, such as the eyes, nose, and mouth. The relationship between the pieces gives an idea of the shape. Each of these can be represented differently. Common representations of shape include Active Shape Models (ASM), edges and lines, and the relationship between specified points. ",
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "4.1 Feature Extraction Methods ",
        "text_level": 1,
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Face metrology has been used in previous works by Cao et al. to determine gender using points from the entire face [10]. Pairwise distances and angles between each possible pair of points were calculated. An example of some of the distances on the face can be seen in Figure 4.1. The features were ranked according to how well each one separated the classes in the training set, similar to d-prime. The most important features were chosen and used for gender classification. This method was adapted to work within the partial face schema. Only points found within a region were used. The only concession to full face was that all distances within each region were normalized by the interocular distance. The distance between two points was calculated using the Euclidean distance metric. The angle between two points was calculated by finding the inverse tangent between them. Table 4.1 shows the number of shape points used for each region and the length of the feature vectors. Since the number of points per region is small, all point pairs were considered important and no feature reduction was performed at this point. ",
        "page_idx": 66
    },
    {
        "type": "image",
        "img_path": "images/335302ff09f6b0f345a35eafd7c989cf14cc25ae8a2b354833d13d8e2c9c6096.jpg",
        "img_caption": [
            "Figure 4.1: Example of shape feature calculation on a full face image. "
        ],
        "img_footnote": [],
        "page_idx": 67
    },
    {
        "type": "table",
        "img_path": "images/cbac0bfb76f3cacb716cc31d143d079d1cf5b6c8f7c631e510ee22edba5dfc1f.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Region</td><td>Points</td><td>Pairs</td><td>Total</td></tr><tr><td>FACE</td><td>68</td><td>2,278</td><td>4,556</td></tr><tr><td>EYE</td><td>5</td><td>10</td><td>20</td></tr><tr><td>NTIP</td><td>8</td><td>28</td><td>56</td></tr><tr><td>NOSE</td><td>10</td><td>45</td><td>90</td></tr><tr><td>MOUTH</td><td>19</td><td>171</td><td>342</td></tr><tr><td>CHIN</td><td>3</td><td>3</td><td>6</td></tr></table></body></html>\n\n",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "Table 4.1: Number of points used per region for shape features. Total feature vector length is $2\\times{\\binom{n}{2}}$ where $n$ is the number of points per region. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "4.2 Experiment Setup ",
        "text_level": 1,
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "The experiments for this section will be performed on subsets of the FRGC and MORPH databases as previously described in Chapter 2. The points used in the feature extraction come from preprocessing performed on the color face images using the VeriLook SDK. Experiment sets remain the same from color experiments. Five runs of a stratified cross-validation experiment are run for each region and classifier combination. Experiments are performed with and without PCA feature reduction because of the small number of features present in each region. Eighty-four shape experiments are run for each demographic and dataset. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "4.3 Analysis ",
        "text_level": 1,
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Looking at the distance and angle features separately allows for several conclusions to be made. With respect to gender, the mean male subject has larger distances, and thus larger features in the chin and nose regions. The eyes and mouth regions are more similar between the genders. Even with this difference, the distance between points overlaps for the genders. In ethnicity, the Black class tends to have higher distances in the nose regions, indicating larger noses within this class. However, as with the genders, the difference between ethnic classes are still small and have a high overlap. The angle features show no trend with respect to either gender or ethnicity. ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "The shape of the nose changes as an individual ages, with distances between points increasing slightly as the age of a subject increases. This agrees with research findings summarized by Albert et al. [4] that the length and height of the nose increase with age. This trend is present in all demographic groups, both gender and ethnicity, although it is not as smooth in the Female class. The eyes, chin, and mouth regions show no consistent shape change with respect to age. Lip thickness likely changes in the mouth according to Albert et al. [4], but the age change is lost in the variability of the mouth due to changes in expression over the experiment set. ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "4.4 Gender ",
        "text_level": 1,
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "4.4.1 Reliability ",
        "text_level": 1,
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Figure 4.2 shows the class-specific accuracies on the FRGC and MORPH datasets using 1-NN classification. The gap between class-wise performances is smaller in FRGC, but the actual performance is not much greater than chance. The higher resolution of the FRGC data allows for better point localization during the face annotation phase. Feature reduction with these shape features loses valuable information and actually makes performance on the Female class worse in some instances. The most notable, and best performing, instances are the NOSE and NTIP regions, likely due to the size difference between genders. ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Figure 4.3 shows the results of gender classification using the ANN and SVM classifiers. Once again, feature reduction deteriorates performance. The nose regions seem to hold the most gender specific shape information for both MORPH and FRGC; however, the poor performance indicates that it is only a small amount. The shape features lack enough separation by gender to train a reliable classifier in most instances based upon the 0-100% performance in the MORPH experiments. The FRGC experiments show a little more separation than MORPH, with correct predictions in both classes, for all regions. ",
        "page_idx": 68
    },
    {
        "type": "image",
        "img_path": "images/1d972eda84692843af10b4a82d4682f94f75f6bd5d2b5d72436269ffafe69c7d.jpg",
        "img_caption": [
            "Figure 4.2: Gender nearest neighbor results on shape features. Values graphed are the class-wise accuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature reduction. Bottom: PCA feature reduction keeping $95\\%$ of the variance. "
        ],
        "img_footnote": [],
        "page_idx": 69
    },
    {
        "type": "image",
        "img_path": "images/b680c4c027d8d6c5665ea2046abe53425603735298dd62ce55cd53884f67f549.jpg",
        "img_caption": [
            "Figure 4.3: Gender ANN and SVM results on shape features. Values graphed are the class-wise accuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature reduction. Bottom: PCA feature reduction keeping $95\\%$ of the variance. "
        ],
        "img_footnote": [],
        "page_idx": 70
    },
    {
        "type": "image",
        "img_path": "images/30f0a66e81f89e076e07b5e1043b2ace7b5dfe6a600e71bdec95f03eb0beeb5c.jpg",
        "img_caption": [
            "Figure 4.4: Hard subjects in shape gender classification on MORPH images. Images are of subjects classified incorrectly in over 98% of shape gender experiment runs. "
        ],
        "img_footnote": [],
        "page_idx": 71
    },
    {
        "type": "table",
        "img_path": "images/1083b3bb76d5a24646cc0b8c41e80270475914f07f110b47743d18b3c8dfdc7a.jpg",
        "table_caption": [
            "Table 4.2: Gender performance using shape features and full face. Top section is without PCA, while the bottom section of the table is results using PCA for feature reduction. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. VeriLook refrained from predicting gender on $2.75\\%$ and $0.89\\%$ of male and female images respectively in the FRGC dataset. In the MORPH dataset it predicted no gender on $0.5\\%$ and $11.26\\%$ of male and female images. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Classifier</td><td colspan=\"3\">FRGC</td><td colspan=\"3\">MORPH</td></tr><tr><td>Male</td><td>Female</td><td>Average</td><td>Male</td><td>Female</td><td>Average</td></tr><tr><td>ANN100</td><td>100.00</td><td>0.00</td><td>50.00</td><td>100.00</td><td>0.00</td><td>50.00</td></tr><tr><td rowspan=\"2\">L2 SV MLinear</td><td>81.12</td><td>74.63</td><td>77.88</td><td></td><td></td><td></td></tr><tr><td>70.70</td><td>59.22</td><td>64.96</td><td>87.45</td><td>25.62</td><td>56.54</td></tr><tr><td>ANN100</td><td>100.00</td><td>00.00</td><td>50.00</td><td>95.54</td><td>34.42</td><td>64.98</td></tr><tr><td>SVMLinear</td><td>60.42</td><td>61.38</td><td>60.9</td><td>99.52</td><td>7.41</td><td>53.47</td></tr><tr><td>L2</td><td>67.09</td><td>55.64</td><td>61.36</td><td>87.45</td><td>26.23</td><td>56.84</td></tr><tr><td>VeriLook</td><td>92.72</td><td>98.45</td><td>95.59</td><td>99.14</td><td>67.75</td><td>83.45</td></tr></table></body></html>\n\n",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "No subjects were classified correctly in all shape experiments. The subjects and images that performed the worst can be seen in Figure 4.4 being misclassified in at least 98% of the shape experiments. The majority of the subjects are Black with one Native American and one White, but all the subjects are Female. ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Table 4.2 shows the results of gender classification using shape features in the full face. In FRGC, the highest performance of an individual region is 73% average class-wise accuracy with the NOSE region and SVM classifier. This is below shape classification on full face and definitely below ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "VeriLook classification. In MORPH, the results are even less favorable dropping to $55\\%$ with the MOUTH and one of the nearest neighbor classifiers. The face shape results are also low for MORPH. This differs from what was seen in the work by Cao et al. [10]. Full face shape was able to achieve gender classification within 5-10% of appearance based methods. Therefore, the choosing of the best features is an important part of the method. This is the step where the purposed method deviated, either choosing all features or using PCA for feature reduction. ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "4.4.2 Age ",
        "text_level": 1,
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Gender class-wise performance partitioned by age groups can be seen in Figure 4.5(a). In most of the regions, the points used are stable, like the eye corners. Age should not influence the performance of those regions, unless the features themselves change with age. Points in other regions, like the mouth, are less rigid. It is possible that variability in these regions may be due to behavioral factors, as well as age. For instance, it is possible that younger subjects smile more often in the experiment set than older subjects. ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "No definite trend is noticed in the results with respect to age, except in the nose regions. The Female class shows a decrease in performance as age increases. As discussed in the analysis of shape features, the samples in the Male class, on average, have larger noses than those in the Female class; however, as an individual ages, the size of the nose increases. This means older female samples are more likely to overlap with the Male class and be classified incorrectly. The face results follow the same trend for PCA reduced experiments. Other regions show little or inconsistent changes in gender classification with respect to age. ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "4.5 Ethnicity ",
        "text_level": 1,
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "4.5.1 Reliability ",
        "text_level": 1,
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Results of ethnicity classification using shape features can be seen in Figure 4.6. Once again, there is a slight performance gain when excluding the feature reduction step. Performance is still very low for both FRGC and MORPH. The NOSE and MOUTH regions seem to hold the most shape information related to ethnicity. For all experiments, one class performs well, which is the most well represented class, but the performances of the other classes suffer. This indicates that the partial face shape data for ethnicity classes overlaps and is not very useful for ethnicity classification. ",
        "page_idx": 72
    },
    {
        "type": "image",
        "img_path": "images/54680b34ea0b8141777c8c6a480689a863f0030904d910fb32d4299fe40f482f.jpg",
        "img_caption": [
            "Figure 4.5: Shape results by age for a) gender and b) ethnicity classification. Results shown are from the 1-NN experiments $\\left(L_{2}\\right)$ ) with no feature reduction. "
        ],
        "img_footnote": [],
        "page_idx": 73
    },
    {
        "type": "image",
        "img_path": "images/fa923d0e84b071a7155be23279ca5c845fbb651c9990807df36b7d85539f64fe.jpg",
        "img_caption": [
            "Figure 4.6: Ethnic nearest neighbor results on shape features. Values graphed are the class-wise accuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature reduction. Bottom: PCA feature reduction keeping $95\\%$ of the variance. "
        ],
        "img_footnote": [],
        "page_idx": 73
    },
    {
        "type": "image",
        "img_path": "images/fef81c2ed43d0ba712dad69ae469837cb48f7823974473d2c650d401d852bc26.jpg",
        "img_caption": [
            "Figure 4.7: Easy and hard subjects in shape ethnic classification on MORPH images. Subject on the far left was classified correctly in all experiments. The subjects to the right are a subset of those misclassified in all shape ethnicity experiments, 18 out of 69. From top to bottom the real ethnic labels are Hispanic, Native American, and Asian. Subjects in the last column are Female; the rest are Male. "
        ],
        "img_footnote": [],
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Figure 4.8 shows the results of ethnic classification using ANN and SVM classifiers. In most instances, better performance is achieved using a 1-NN classifier, indicating small clusters spread out over the classification space. One class does really well with the ANN and SVM classifiers, and the rest perform poorly, resulting in the outliers on the graphs. In FRGC this is the White class, in MORPH this is the Black class. Shape classification does improve on the next best class as the ANN gets larger. ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "One subject, Black Male, was correctly classified in all MORPH shape ethnicity experiments. Sixty-nine were misclassified in all of the experiments, some on multiple images. A subset of these subjects can be seen in Figure 4.7. The majority of these subjects are in the Male class, but Female subjects from each of the smaller classes are present as well. Half of the Asian class subjects are in this set, as well as 70% of the Native American class. The rest of the subjects are Hispanic, but are only responsible for approximately 6.5% of the subjects in that class. This supports the idea that the smaller classes overlap a lot with one another and the larger classes. ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Tables 4.3 and 4.4 show the results of ethnic classification on shape features on the whole face. In FRGC, the MOUTH region with the SVM classifier is the only one that approaches the performance on the face. Results in the MORPH dataset improve slightly. The nose and mouth regions obtain average accuracies within 5% of the face results. While not the best representation, shape features on the nose and mouth can reach comparable performance to similar shape features over the whole face. ",
        "page_idx": 74
    },
    {
        "type": "table",
        "img_path": "images/4a77d82fd8c41a9cb6001257a9012aa5fba3cdd28bfee2c42b9679f70a1a0cd7.jpg",
        "table_caption": [
            "Table 4.3: Ethnic performance using shape features and full face, FRGC. Top section is without PCA, while the bottom section of the table is results using PCA for feature reduction. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Indian</td><td>Average</td></tr><tr><td rowspan=\"3\">ANN100 L2 SVMLinear</td><td>100.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>20.00</td></tr><tr><td>84.36</td><td>57.32</td><td>1.00</td><td>4.33</td><td>3.00</td><td>30.00</td></tr><tr><td>76.86</td><td>38.49</td><td>0.67</td><td>1.83</td><td>4.25</td><td>24.42</td></tr><tr><td rowspan=\"3\">L2 ANN100 SV MLinear</td><td>100.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>20.00</td></tr><tr><td>73.31</td><td>52.23</td><td>1.67</td><td>5.83</td><td>5.75</td><td>27.76</td></tr><tr><td>75.45</td><td>33.29</td><td>3.33</td><td>1.17</td><td>7.00</td><td>24.05</td></tr></table></body></html>\n\n",
        "page_idx": 75
    },
    {
        "type": "table",
        "img_path": "images/63ea392066d6ca0d081c4a1b33a0b40e3afed336327bb96553bd125766fe208b.jpg",
        "table_caption": [
            "Table 4.4: Ethnic performance using shape features and full face, MORPH. Top section is without PCA, while the bottom section of the table is results using PCA for feature reduction. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Native American</td><td>Average</td></tr><tr><td rowspan=\"3\">ANN100 SV MLinear L2</td><td>0.00</td><td>0.00</td><td>0.00</td><td>100.00</td><td>0.00</td><td>20.00</td></tr><tr><td>80.40</td><td>2.00</td><td>8.44</td><td>96.21</td><td>0.00</td><td>37.41</td></tr><tr><td>42.10</td><td>1.41</td><td>8.44</td><td>86.43</td><td>0.00</td><td>27.68</td></tr><tr><td rowspan=\"3\">ANN100 L2 SV MLinear</td><td>80.35</td><td>0.00</td><td>1.90</td><td>96.22</td><td>0.00</td><td>35.69</td></tr><tr><td>81.05</td><td>0.00</td><td>0.00</td><td>96.63</td><td>0.00</td><td>35.54</td></tr><tr><td>42.63</td><td>0.77</td><td>8.91</td><td>86.46</td><td>0.00</td><td>27.75</td></tr></table></body></html>\n\n",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "4.5.2 Age ",
        "text_level": 1,
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "Ethnicity class-wise performance partitioned by age groups can be seen in Figure 4.5(b). Performance on the best class, Black, showed an upward trend in the eye regions as the age of subjects increased, suggesting that classification of ethnicity is easier on older subjects in the Black class. This is opposite to the trend found in the MOUTH and CHIN regions. The decrease in performance for the CHIN region could be due to the proclivity of the males in the class towards growing facial hair. This could possibly interfere with the accuracy of the facial annotation in that portion of the face and deteriorate performance. The nose regions perform similarly for all age groups in this class. With the Black class having larger measurements in these regions to begin with, an increase in the measurements would keep the separation between the classes. ",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "In the White and Hispanic classes, the eyes seem fairly stable with respect to age while the nose regions show a downward trend in these classes. An increase in the nose measurements with these classes could begin to overlap with the Black class, decreasing performance. Performance on the CHIN region for both these classes declines as age increases, likely due to facial hair and landmark accuracy. The MOUTH region improves noticeably with age in the White class but is inconsistent with the Hispanic class. The performance of the MOUTH on both of these classes is still markedly below the corresponding performance on the Black class. ",
        "page_idx": 75
    },
    {
        "type": "image",
        "img_path": "images/602ced00302476ab126f9f5990080a1224eb04e615507e69d80b469555bc4855.jpg",
        "img_caption": [
            "Figure 4.8: Ethnic ANN and SVM results on shape features. Values graphed are the class-wise accuracies over 5 runs of each experiment. (L to R): FRGC, MORPH. Top: no PCA feature reduction. Bottom: PCA feature reduction keeping $95\\%$ of the variance. "
        ],
        "img_footnote": [],
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "The Asian and Native American classes are the worst performing classes with the Native American class having very few correct classifications. In the Asian class, correct classifications more likely in the age groups under 40 years of age. From 16-25, correct classifications could be found in the eyes, nose, and mouth regions, but then the eyes and mouth became harder to classify, and just the achieved correct classification until subjects reach 40 years of age. This suggests that younger Asian individuals can be classified by eye shape, but the nose becomes a more important ethnic indicator as they age. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "4.6 Conclusions ",
        "text_level": 1,
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "These particular partial face shape features do not work well with gender and ethnic classification. Performance is better with these features in higher quality images which allow for better localization of the points used. This indicates that there are some differences in shape between gender and ethnic classes. It is possible that these differences could be represented better with a more sophisticated shape representation. In future work, a different shape representation, possibly Active Shape Models, will be investigated for use in partial face. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "The shape features perform better for the most part without feature reduction. The size of the original feature vectors in this category are much smaller than those used in color and texture experiments. Feature reduction on these small feature vectors resulted in a loss of too much information for both gender and ethnic classifications, resulting in lower classification performance than the original vectors. Baseline face experiments did not necessarily follow this trend. Some classifiers performed better after PCA reduction was performed. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "The most stable regions with respect to age for both gender and ethnicity are the eye and chin regions. The overall shape of the eye does not change much with age, neither does the shape of the chin. The chin region has its own variance due to facial hair, but the small section of chin used for shape here, does not vary much with age. These regions might be the most stable, but they are not the best performing regions for the shape features investigated. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "The nose regions show the best performance for both gender and ethnicity. These regions are also impacted the most by age. Males, on average, have larger noses than females which give the best performance in gender classification. Between the ethnic classes there are also some size differences which allow for better classification. However, the nose grows slightly with age, which can negatively impact both gender and ethnic performance as subjects get older. For future work, the age of subjects should be taken into account when planning the training and testing sets. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "Different types of classifiers performed better between gender and ethnicity. Nearest neighbor classification performed better for ethnicity classification than the ANN and SVM classifiers, indicating a large overlap between the classes that the more sophisticated classifiers were unable to classify correctly. The SVM classifiers performed well in the FRGC dataset with gender classification, but MORPH did not have enough separation to learn the Female class, always predicting Male. MORPH performed better with larger ANNs or nearest neighbor classification than SVM or ANN with the smallest hidden layer. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "For further experiments in the application section, the NOSE region will be used for only ethnicity classification. Since shape region experiments performed better without feature reduction, PCA will not be performed on the nose shape features. The linear SVM classifier will be used to classify this region in the hopes of obtaining the highest performance. The final category of features investigated for the reliability and age questions is texture. The following chapter will discuss the results on texture classification. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "Chapter 5 ",
        "text_level": 1,
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Texture ",
        "text_level": 1,
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Local texture can be found in the face in terms of skin texture, wrinkles, imperfections in the skin, and facial hair, including, but not limited to, eyebrows, mustaches, and beards. The stableness of texture depends on what is causing the specific texture. Skin texture will most likely be stable on a day to day basis, but change over the years as wrinkles increase. Imperfections in the skin and facial hair can change more quickly. Three different local texture representations will be investigated in this chapter: Histograms of Oriented Gradient, Local Binary Patterns, and Local Phase Quantization. A brief description of the extraction method follows for each texture representation. ",
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "5.1 Feature Extraction Methods ",
        "text_level": 1,
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "5.1.1 Histograms of Oriented Gradient ",
        "text_level": 1,
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Originally proposed by Trigg and Dalal [18] for the detection of human pedestrians, Histograms of Oriented Gradient (HOG) have been used for facial recognition purposes [20, 65]. The basic idea of HOG is that local shape and appearance, or texture, can be characterized by the distribution of the local image gradients. The image is divided into smaller regions, providing the localized area. The Prewitt convolution kernel is used to compute the image gradient. The gradient magnitude, $G_{M}$ , and the gradient angle, $G_{A}$ , are computed by ",
        "page_idx": 79
    },
    {
        "type": "equation",
        "text": "$$\nG_{M}=\\sqrt{G_{X}^{2}+G_{Y}^{2}}\\mathrm{~and~}G_{A}=\\mathrm{atan2}(G_{Y},G_{X}).\n$$",
        "text_format": "latex",
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "$G_{X}$ and $G_{Y}$ are the image gradients in the horizontal and vertical directions. The gradient orientations, or angles, for each pixel are used to select the histogram bin and increment by the gradient magnitude. For this work, the orientations are divided into $30^{\\circ}$ segments resulting in 12 bins and a feature vector of length 12 per patch. ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "5.1.2 Local Binary Patterns ",
        "text_level": 1,
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "Local Binary Patterns (LBP) were first introduced by Ojala et al. [53] to classify texture patterns. The LBP method looks at the neighborhood around each pixel in an image. The value of this feature representation is that it encodes different textures that can represent curved edges, spots, and even uniform areas. The texture for a specific pixel is represented by thresholding the intensity values of the neighboring pixels with the intensity value of the center pixel, given by the equation: ",
        "page_idx": 80
    },
    {
        "type": "equation",
        "text": "$$\nL B P_{P,R}(g_{c})=\\sum_{p=0}^{P-1}s(g_{p}-g_{c})2^{p},\\mathrm{where}s(x)=\\left\\{\\begin{array}{l l}{1,}&{\\mathrm{if~}x\\geq0}\\\\ {\\qquad\\mathrm{otherwise}}\\end{array}\\right..\n$$",
        "text_format": "latex",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "$P$ is the number of pixels in the neighborhood investigated along a circle of radius $R$ which is centered at pixel $c$ . $g_{c}$ and $g_{p}$ refer to the grayscale value for the given pixels. $L B P_{P,R}(g_{c})$ represents the texture pattern at pixel $g_{c}$ . The texture patterns are accumulated into a histogram. The granularity of the histogram is dependent on the choices for $P$ and $R$ . For the LBP features used in this work the values $P=8$ and $R=2$ were chosen. The “uniform” version of LBP was used which limits the patterns in the histogram to those with 2 or less changes between 0 and $^{1}$ , resulting in a feature vector length of 59 per patch. Patterns that are not uniform are counted in the last bin of the histogram. LBP was chosen having been used successfully for soft biometric classification using full facial images [41, 75]. ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "5.1.3 Local Phase Quantization ",
        "text_level": 1,
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "Local Phase Quantization (LPQ) was recently proposed by Ojansivu et al. [54] as a descriptor for texture which is robust to image blurring. Similar to HOG and LBP, the LPQ method looks at each pixel individually and accumulates the results into a histogram. The phase information is quantized and compiled into a histogram. By utilizing only the phase information, the method is also not affected by uniform illumination changes. It has been used successfully for facial recognition ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "[2, 11, 12]. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "The LPQ feature extraction method first performs a Discrete Fourier Transform (DFT) on the image, given by the equation, ",
        "page_idx": 81
    },
    {
        "type": "equation",
        "text": "$$\nF(u,x)=\\sum_{y\\in N_{x}}f(x-y)e^{-j2\\pi u^{\\top}y},\n$$",
        "text_format": "latex",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "where $u$ is the frequency, $x$ is the pixel location within the image, $N_{x}$ is a rectangular $M\\times M$ neighborhood, and $f(x)$ is the intensity value of a pixel in the image. Four frequencies are considered in this algorithm. The real and imaginary components of the frequencies are used to create a transform matrix such that $F_{x}=W f_{x}$ where $f_{x}$ is a vector containing the pixels in $N_{x}$ , $W$ is the transform matrix, and $F_{x}$ is the transform coefficient vector. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "After further processing of $F_{x}$ , including decorrelation and whitening, the coefficient vector is quantized by thresholding the vector at 0. The LPQ code, corresponding to the histogram bin, is calculated by ",
        "page_idx": 81
    },
    {
        "type": "equation",
        "text": "$$\nb=\\sum_{j=1}^{8}g_{j}2^{j-1},\n$$",
        "text_format": "latex",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "where $g_{j}$ is the thresholded value in $F_{x}$ . The feature vector for LPQ has 256 elements per patch. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "5.2 Experiment Setup ",
        "text_level": 1,
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "The experiments for this section will be performed on subsets of the FRGC and MORPH databases, using a grayscale version of the image. Experiment sets remain the same from color and shape experiments and preprocessing is performed as described in Chapter 2. Features are extracted using the HOG, LBP, and LPQ methods. Gender and ethnicity classifications are performed on all texture features using all classifiers. The total number of partial-face experiments in this chapter is 126 for each dataset and demographic. Five runs of a stratified cross-validation experiment are run for each feature, region, and classifier combination. Baseline face experiments are performed using LBP texture features. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "5.3 Analysis ",
        "text_level": 1,
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "Figures 5.1 show the average HOG features over each region for MORPH. Not much consistency exists across the regions. Even the eyes do not share similar texture indicating that texture around the eyes are independent. Issues may arise with the differences in texture between the regions. Texture may rely more on accurate localization of the facial features. If the region extraction is off, different texture may be present and not be classified correctly. These features also indicate that the mouth and chin region hold different types of textures than the other regions, see bins 4-7. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "Figures 5.2 show the average LBP features over all the regions for MORPH. The last bin in the histogram counts all the miscellaneous textures that are not counted in the rest of the bins. The FRGC features have twice the amount of miscellaneous textures as MORPH. This is most likely due to the higher resolution of the FRGC images allowing texture to be extracted in greater detail. Differences between regions in the rest of the bins are much smaller due to the content being spread over a greater number of bins. Other than magnitude, the shapes of the feature vectors are similar over both datasets. The texture features for the eyes are more similar than HOG, but several sections of the histogram are different, indicating that LBP texture is somewhat independent as well. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "The LPQ feature vectors are much larger than both HOG and LBP, and will not visualize well. With 256 bins in the histogram, the quantity in each bin is very small. Values are found in the majority of bins, unlike many of the color space features. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "Figure 5.3 shows the differences between the regions for each feature, using the histogram intersection distance measure. Differences between the left eye and the other regions were similar to differences between the right eye and the rest of the face. The same is true for the nose and nose tip regions; therefore the graphs have been condensed to show pairs with the left eye, nose tip, mouth and chin regions. The nose and the nose tip are the most similar region still because of the overlap present between the two. The largest differences in texture exist between the eye and chin regions. These differences are larger for females on average, so it is not entirely due to facial hair. The Black class has the smallest region differences for each texture representation, indicating texture is more stable around the face for that particular ethnic group. The region differences with texture features are larger than those reported in Section 3.4 indicating that texture is not as stable around the face as color. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "Region differences show less change with respect to age for the Female class than the Male class. This indicates that texture across the face is more stable over the years for females than males. This could be due to make-up and other facial care products that are more widely used by females which smooth skin texture. It could also be due to the variation in facial hair found in both classes. The largest difference that also shows a large variance with respect to age for the Male class is the eye to mouth comparison. Younger male faces show a larger difference in texture between these two regions than older males. This trend holds true for more than just the Male class and this region comparison. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "In the majority of the comparisons, if a variance is present with respect to age, the younger subjects have larger differences than older subjects, indicating that texture is not as stable around the face for younger subjects as it is for older subjects. The exception is present in some comparisons on the lower face. In the Black class, the distances between the nose and mouth or chin regions is larger for older subjects, and also in the White class in the mouth to chin comparison. In the Black class, the largest difference with high variance due to age is the difference between texture in the eye and mouth regions. Combined with the face that this class also has the most stable texture around the face, this suggests that wrinkles, especially in the eye region, show up later than in other classes. For the White class, the corresponding difference is found between the eye and nose regions. The Hispanic class has the most age variance between the same regions as both the Black and White classes. The other ethnic classes show no discernible pattern with respect to age. ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "5.4 Gender ",
        "text_level": 1,
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "5.4.1 Reliability ",
        "text_level": 1,
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "Figure 5.4 shows the results of 1-NN gender classification on the texture features for both FRGC and MORPH. FRGC shows there are differences between the genders based on texture with some performances above 80%. The LPQ features perform the best out of the three texture representations indicating that this feature representation encodes the most gender information. These features are able to distinguish gender correctly at least $70\\%$ of the time in all but the nose region. This region performed the worst in all three feature representations, indicating less gender texture information can be found in the nose. The CHIN region was able to distinguish between the genders the best. This is likely due to the presence of more facial hair, and thus different texture, in the CHIN region of the Male class. The HOG features performed the worst, which is a result of the texture not being consistent around the face. Small errors in localization result in different textures extracted, which leads to worse performance. ",
        "page_idx": 83
    },
    {
        "type": "image",
        "img_path": "images/b54d8271b21b4027892f7cf3328005b609ba0b1c1ffaf94c25fb6bc577dfe06d.jpg",
        "img_caption": [
            "Figure 5.1: Average global feature vector for MORPH HOG features. These are the normalized histograms for each region. The $x$ -axis corresponds to the bin in the histogram. The $y$ -axis corresponds to the relative frequency of values found in each bin. "
        ],
        "img_footnote": [],
        "page_idx": 84
    },
    {
        "type": "image",
        "img_path": "images/8d05ea3a15a23fdc70ff0251e1a9a326560753f3fbdfcf05459bdf38d3af81d2.jpg",
        "img_caption": [
            "Figure 5.2: Average global feature vector for MORPH LBP features. These are the normalized histograms for each region. The $x$ -axis corresponds to the bin in the histogram. The $y$ -axis corresponds to the relative frequency of values found in each bin. "
        ],
        "img_footnote": [],
        "page_idx": 84
    },
    {
        "type": "image",
        "img_path": "images/f670e3500d07aded64d35081c6603572704d7c936aa6525bb67c943c432238f9.jpg",
        "img_caption": [
            "Figure 5.3: Mean difference between MORPH region-wide global texture vectors by demographic. Differences shown are for HOG, LBP, and LPQ (top to bottom) using the Histogram Intersection distance. "
        ],
        "img_footnote": [],
        "page_idx": 85
    },
    {
        "type": "image",
        "img_path": "images/ebf31552d06142ffde910c98d4e70a6b0382209ff72c63bd249ceeeefa1ca68c.jpg",
        "img_caption": [
            "Figure 5.4: 1-NN gender classification on texture features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH "
        ],
        "img_footnote": [],
        "page_idx": 86
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "The performance on the MORPH dataset shows more overlap between the classes than is seen in the FRGC dataset. This is likely due to the lower quality of the MORPH dataset. The texture cannot be extracted as reliably because it is not present in the same detail as in FRGC. The CHIN region does not perform noticeably better like it does in the FRGC dataset. Performance on the Male class increases, but the performance of the Female class suffers for the $L_{1}$ distance measure. The performance of the HOG features in MORPH is much closer to the performance of the other two representations than it is in FRGC. This could be a positive side effect of the lower resolution. Since the texture is not extracted in as much detail, the HOG features are more similar and not as dependent on the precise localization of the region. ",
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "Figure 5.5 shows the results of gender classification on the texture features using the more complex classifiers, ANN and SVM. The results improve for both datasets, all regions, and all representations. This suggests that there is a lot of overlap in the feature space, but differences exist that set the classes apart, which the ANN and SVM classifiers are able to learn. The HOG features still perform the worst in FRGC, and the gap between HOG and the other two representations are more noticeable in MORPH with these classifiers. These results suggest that HOG is not the best texture feature for encoding gender information. ",
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "Figure 5.6 shows some of the easier and harder subjects to classify according to gender using texture features. The majority of the subjects classified correctly in all texture experiments were from the Black class. Some were from the White and Hispanic classes. All were from the Male class. The images that were misclassified in over 95% of texture gender experiments were all from subjects labeled Black Female. As previously seen in color classification results, one of the subjects labeled Female is not female. Texture classification also predicted the class correctly, but was marked incorrect since the subject was mislabeled. ",
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "In these results, the chin region still performs very well in the FRGC dataset, but it can also be seen that the mouth and nose regions perform better than the other regions while looking at the MORPH dataset. The lack of texture detail found in MORPH seems to shift the importance from the chin region to the mouth and nose regions. This indicates that the lower face regions hold more texture information specific to gender than the upper face regions. The eye regions also perform very well in FRGC, but the lack of texture detail in MORPH is more detrimental, putting their ",
        "page_idx": 87
    },
    {
        "type": "image",
        "img_path": "images/9c1fbe440e738a7c5fa6074fe22edcc6baeb801c0db19aa246e3d29fbc11ca83.jpg",
        "img_caption": [
            "Figure 5.5: ANN and SVM gender classification on texture features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH "
        ],
        "img_footnote": [],
        "page_idx": 88
    },
    {
        "type": "image",
        "img_path": "images/4ec0fa372b0d925352e410c0397153b7b111cc7173990e5ed2890f3f4cbdef1c.jpg",
        "img_caption": [
            "Figure 5.6: Easy and hard subjects in texture gender classification on MORPH images. Subjects on the left are a subset of those correctly classified in all texture experiments. Top to bottom: Black, White, and Hispanic. Subjects on the right those who were misclassified in over $95\\%$ of experiments. The subject on the bottom was labeled incorrectly as Female. "
        ],
        "img_footnote": [],
        "page_idx": 89
    },
    {
        "type": "table",
        "img_path": "images/044866534b4be71a1754301043b66d438b52f0a53bb557025ae8c84506c10c16.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Classifier</td><td colspan=\"3\">FRGC</td><td colspan=\"3\">MORPH</td></tr><tr><td>Male</td><td>Female</td><td>Average</td><td>Male</td><td>Female</td><td>Average</td></tr><tr><td>ANN100</td><td>94.85</td><td>15.80</td><td>55.33</td><td>一</td><td>一</td><td></td></tr><tr><td>SV MLinear</td><td>92.89</td><td>91.02</td><td>91.96</td><td>一</td><td>一</td><td></td></tr><tr><td>L2</td><td>79.21</td><td>64.35</td><td>71.78</td><td>97.43</td><td>22.16</td><td>59.80</td></tr><tr><td>VeriLook</td><td>92.72</td><td>98.45</td><td>95.59</td><td>99.14</td><td>67.75</td><td>83.45</td></tr></table></body></html>\n\n",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Table 5.1: Gender performance using LBP texture and full face. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. VeriLook predicted no gender on $2.75\\%$ and 0.89% of male and female images respectively in the FRGC dataset. In the MORPH dataset it predicted no gender on $0.5\\%$ and 11.26% of male and female images. ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "performance below the nose regions. This still follows what was seen in in previous work, the nose and eye regions perform among the best in previous texture experiments [47]. ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Table 5.1 shows the results of the baseline face experiments using LBP texture and gender classification. Texture on the CHIN region was able to achieve similar performance to the commercial classification of the entire face in FRGC. The MOUTH region, as well as the NOSE region, was able to do the same in the MORPH dataset. Since these regions achieve similar performance to the commercial results and not just the classification of the full face features, the gender information encoded in the texture of the MOUTH and CHIN seems to be sufficient for partial face classification. ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "5.4.2 Age ",
        "text_level": 1,
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "The results of gender classification on MORPH partitioned according to age can be seen in Figure 5.7. The classifier used in the experiments shown is the $A N N_{50}$ classifier. These graphs indicate that the Male class is impacted less by changes in age than the Female class; however the mouth and chin regions do show variance in both classes with respect to age. The nose regions show the least variance in classification accuracy in the LBP and LPQ features. The nose tip region shows less variance than the nose region, which makes sense, because the nose region overlaps the eye regions which show the most variance with respect to age. The impact of age is seen the most severely in the eye and chin regions for the Female class. Overall, the nose tip region is the least impacted by age, but the mouth region still performs well over all age groups, even with variance caused by age. ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "In full face experiments, little change is seen in the performance of the Male class according to age. The Female class shows a downward trend with respect to age, with the lowest performance on subjects 41 to 50 years of age. After that performance improves again. ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "5.5 Ethnicity ",
        "text_level": 1,
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "5.5.1 Reliability ",
        "text_level": 1,
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "The results of nearest neighbor ethnic classification on both datasets can be seen in Figure 5.8. Compared to gender classification, the results are much more spread out. In both datasets, one class performs rather well, with a high class-specific accuracy, but at least one or more of the classes also has a class-specific accuracy below $10\\%$ . MORPH most often has at least two classes below $10\\%$ , Asian and Native American, and one more under 20%, Hispanic. The best performing classes are still the most well-represented classes in the experiment set. ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "Figure 5.9 shows the results of ethnic classification of the texture features using the ANN and SVM classifiers. FRGC experiments improve overall, but still have a very low median class-wise accuracy. MORPH experiments improve on the best performing classes, but decline slightly on the less well-represented classes. The exceptions to these are in the LBP features using a SVM classifier. All but the CHIN region show a slight increase in the worst performing classes. In these experiments, MORPH results are likely to have the same two classes under $10\\%$ , but the other class, the Hispanic class, improves so that it more often has a class-specific accuracy above 20%. ",
        "page_idx": 90
    },
    {
        "type": "image",
        "img_path": "images/e49930526825e76bc0cb7636dd88ac281227529f9a5de8b03471a7b73bd5132a.jpg",
        "img_caption": [
            "Figure 5.7: Gender performance on texture features partitioned by age. From top to bottom the graphs represent the HOG, LPB, and LPQ experiment results. The classifier used is $A N N_{50}$ for each graph shown. "
        ],
        "img_footnote": [],
        "page_idx": 91
    },
    {
        "type": "image",
        "img_path": "images/2a5126dd5f9aaf29ab01faacc2f77f657b652e96e779aa0ef74c486c366a41c4.jpg",
        "img_caption": [
            "Figure 5.8: 1-NN ethnic classification on texture features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH "
        ],
        "img_footnote": [],
        "page_idx": 92
    },
    {
        "type": "image",
        "img_path": "images/f149814a65ba53c5ea6b5c46c213e34f808c5fbe167c702d1629b6e974b96b04.jpg",
        "img_caption": [
            "Figure 5.9: ANN and SVM ethnic classification on texture features over all regions. Values included in the box-and-whisker plot are class-specific accuracies from 5 runs of the experiment. Top to bottom: HOG, LPQ, LPQ. L to R: FRGC, MORPH "
        ],
        "img_footnote": [],
        "page_idx": 93
    },
    {
        "type": "image",
        "img_path": "images/78487d3f1b7885dbe96277c8eaf0b2a1f858c9208c0643115c6d4a05c8f1c7bd.jpg",
        "img_caption": [
            "Figure 5.10: Easy and hard subjects in texture ethnic classification on MORPH images. Images to the left are a subset of those correctly classified in all experiments. Images to the right are from each subject having an image that was never classified correctly. Most are from the Native American class. The three images in the lower right-hand corner correspond to Hispanic subjects. "
        ],
        "img_footnote": [],
        "page_idx": 94
    },
    {
        "type": "table",
        "img_path": "images/9a4f3f210a6cfcd83b76cf6000123fe9d40a6437491e908ea4e8ce0d98d9578f.jpg",
        "table_caption": [
            "Table 5.2: Ethnic performance using texture and full face, FRGC. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Indian</td><td>Average</td></tr><tr><td>ANN100</td><td>99.54</td><td>12.84</td><td>0.33</td><td>0.33</td><td>0.00</td><td>22.61</td></tr><tr><td>SV MLinear</td><td>97.28</td><td>92.01</td><td>2.33</td><td>13.00</td><td>14.25</td><td>43.78</td></tr><tr><td>L2</td><td>84.04</td><td>53.10</td><td>1.00</td><td>3.50</td><td>8.75</td><td>30.08</td></tr></table></body></html>\n\n",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "Images from 25 subjects were classified correctly in all texture ethnicity experiments. Subjects with images classified incorrectly for all experiments were from the Native American and Asian classes. Figure 5.10 shows images from some of these subjects. The hardest class to identify with texture was the Native American class. All 13 subjects from that class have at least one image that was never classified correctly. The easiest subjects to identify were Black subjects, specifically Male. ",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "Table 5.2 and 5.3 show the results of ethnic classification on the FRGC and MORPH datasets respectively using LBP features and the full face. The FRGC eye and nose regions outperform the full face experiments by at least $5\\%$ on the average class-wise accuracy. The same regions do not ",
        "page_idx": 94
    },
    {
        "type": "table",
        "img_path": "images/c1a3b26124d08cd835746dde3aebf061d256cc6e0d799b99eb538ff37d214d97.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Classifier</td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td><td>Native American</td><td>Average</td></tr><tr><td>ANN100</td><td>92.96</td><td>5.32</td><td>48.62</td><td>98.25</td><td>0.00</td><td>49.03</td></tr><tr><td>L2</td><td>50.87</td><td>2.38</td><td>18.15</td><td>95.56</td><td>0.00</td><td>33.39</td></tr></table></body></html>\n\n",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "Table 5.3: Ethnic performance using texture and full face, MORPH. Percentage values represent the class-specific accuracies and the average class-specific accuracy for each experiment. ",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "outperform MORPH face experiments, but come within 3-5% of full face results. This indicates that some of the texture ethnic information is lost with the lower resolution, but that enough can be found for comparable performance to full face experiments. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "5.5.2 Age ",
        "text_level": 1,
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "Ethnic results according to age can be seen in Figure 5.11. The results shown use the $A N N_{50}$ classifier. The graphs for each feature look similar, so only the graph for the LPQ features is shown. Overall the mean class-specific accuracy shows a downward trend with increasing age. The jump in performance for the 41-45 and 61-77 classes is due to the face that not all ethnic classes were present in these age groups and were omitted from the graph. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "The White and Black classes show fairly consistent performance across the age groups with very small declines present for some features and regions after 56 years of age. Class-specific accuracies for the Hispanic class peak around 46% in the 21-25 age group. Performance for that class drops below 25% in the 41-45 age group and does not recover. This suggest that younger Hispanic partial faces are easier to classify according to ethnicity than older ones. Classification of the Asian class is spotty, but most of the correct classifications fall within the first two age groups, 16-25. This also indicates that younger Asian partial faces are easier to classifier according to ethnicity than older faces in the same ethnic group. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "The regions that seem the least affected by age are the nose regions. The most affected region is the chin. However, in each of the regions, the Hispanic class is negatively affected by age, no matter the performance on the other classes. The results suggest that age affects different ethnic groups differently on the various parts of the face. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "Overall, the face shows the same trends as the partial face experiments. Younger subjects are easier to classify, especially for the Asian and Hispanic classes. The Black class is the least affected by age with performance only dropping 8% from its highest when subjects reach 56 years of age. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "5.6 Conclusions ",
        "text_level": 1,
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "Results from this chapter suggest that texture from partial face images can be used successfully for gender and ethnic classification. Analysis shows that texture is not as stable around the face as color. More texture gender information can be found in the lower regions of the face, while the upper face holds more ethnic texture information. The various regions of the face are affected differently by age. The texture information in the nose is least affected by age while the texture in the chin is the most affected. The LPQ and LBP features achieve higher performance than the HOG features. ",
        "page_idx": 95
    },
    {
        "type": "image",
        "img_path": "images/1bef3fc76926c9b76cf081ca9326bdcb6ee3cf1bd2e7c5a0a007842cf4417221.jpg",
        "img_caption": [
            "Figure 5.11: Ethnic performance on texture features partitioned by age. Results shown are from the LPQ experiments using the $A N N_{50}$ classifier. "
        ],
        "img_footnote": [],
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "The best results for gender classification were achieved using the chin region with the LBP and LPQ features and the ANN classifiers on high quality images and the mouth region with LPQ features and the linear SVM classifier with the MORPH images. The best average class-specific accuracy on FRGC images was 94% and the best on MORPH was 87%. The classifiers performed better on the Male class than the Female class indicating males have more distinct gender characteristics than females. The MOUTH and CHIN regions were able to achieve comparable results to full face classification using a commercial system. ",
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "The best results for ethnic classification were achieved using the left eye region with the LPQ features and the ANN classifiers on high quality images and the right eye region with LBP features and the linear SVM classifier with the MORPH images. The best average class-specific accuracy was 52% on FRGC and 49% on MORPH. The eye regions performed well in both the LPQ and LBP features. With the ANN classifiers, the eyes performed just 2-3% under the best performance on MORPH. The eye and nose regions were able to achieve comparable results to full face classification using similar features. ",
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "The effect of age seems to be present more by class than facial region. The Black and White classes were less impacted by age than the Hispanic class, but that might be due to the larger number of samples present for each age group in these classes. Younger Hispanics are easier to classify by ethnicity than older individuals. The same is true for the Asian class. The performance of the Male class overall holds steady over the different age groups while performance of the Female class holds steady until the 41-45 age group. The mouth region improves slightly with age while the nose regions decline the least. ",
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "Since one of the fusion methods will use the scores from ANN classification, the $A N N_{1}00$ classifier will be used for each region and feature combination. The REYE region with LPQ features will be used for ethnic classification and the MOUTH with LBP for gender classification as the best performing regions over all. The NOSE region with LPQ will also be used for both gender and ethnicity as a more age stable option. This concludes the analysis of reliability and the impact of age on partial-face images. The following chapter covers the fusion experiments using partial-face gender and ethnicity classifications to filter a face biometric experiment. ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "Chapter 6 ",
        "text_level": 1,
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "Application ",
        "text_level": 1,
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "Using the conclusions from the previous chapters, the experiments in this section will combine classification results with a typical face recognition or verification experiment in an effort to improve performance. The features and regions that will be used were chosen from color, texture, and shape. The following experiments will be performed on the Pinellas dataset. ",
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "6.1 Experiment Setup ",
        "text_level": 1,
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "Experiments in this section require three sets of images: training, gallery, and probe sets. Table 6.1 shows the demographic breakdown of each of these sets. In combining classification results with the performance of the VeriLook SDK face experiment, the Pinellas images had to pass the quality check within VeriLook. Images from previous FRGC and MORPH experiments did not have to meet this criteria. VeriLook also recommends 50-75 pixels between the eye centers. The average interocular distance for the Pinellas images used was 110.80. A maximum of four images were selected for each subject. Subjects with 3-4 good images were used in the gallery and probe sets, while subjects with 1-2 good images were used in the training set. In the probe and gallery sets, the image with the highest age was placed in the probe set. Any subject with images labeled with conflicting gender or ethnicity information was excluded from the lists. In order to create a more balanced training set, many subjects and images were discarded from the candidate training list. A limit of 10,000 subjects per gender and ethnicity class (i.e. White Male) was set. One image per subject was used. This limit affected both gender classes in the White and Black ethnicities. ",
        "page_idx": 98
    },
    {
        "type": "table",
        "img_path": "images/d2602bee03e9b4594fce7e688d607d50a56e95f263f1ae78bf9cf87dbec6c623.jpg",
        "table_caption": [
            "Table 6.1: Breakdown of subjects in the Pinellas experiment sets by gender and ethnicity. The number of images per subject in each class is given in parentheses. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Ethnicity</td><td colspan=\"3\"></td><td colspan=\"3\">Gender</td></tr><tr><td>Training</td><td>Probe</td><td>Gallery</td><td>Training</td><td>Probe</td><td>Gallery</td></tr><tr><td>White</td><td>20,000(1)</td><td>62,025(1)</td><td>62,025(2)</td><td>Male 30,555(1)</td><td>71,383(1)</td><td>71,383(2)</td></tr><tr><td>Asian</td><td>1,562(1)</td><td>738(1)</td><td>738(2)</td><td>Female 21,620(1)</td><td>37,208(1)</td><td>37,208(2)</td></tr><tr><td>Hispanic</td><td>10,613(1)</td><td>7,059(1)</td><td>7,059(2)</td><td></td><td></td><td></td></tr><tr><td>Black</td><td>20,000(1)</td><td>38,769(1)</td><td>38,769(2)</td><td></td><td></td><td></td></tr><tr><td>Total</td><td>52,175(1)</td><td>108,591(1)</td><td>108,591(2)</td><td>52,175(1)</td><td>108,591(1)</td><td>108,591(2)</td></tr></table></body></html>\n\n",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "Because of the relative small size of the Native American class, less than 200 subjects, and the poor performance of the class in the MORPH experiments, subjects of this class were excluded from these experiments. Out of 393,426 subjects with consistent gender and ethnicity labels, 52,175 were used for training and 108,591 for classification and the biometric experiment. ",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "Prior to training and classification, PCA is performed on the texture and color features, keeping 95% of the variance, similar to the previous experiments. The PCA projection for the classification experiments is learned on the training set. ",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "Two types of fusion will be investigated, using two levels, decision and score. In the first method, which can be described as a weighted sum decision fusion, the results of multiple classifications can be used in combination with the match scores from the recognition and verification experiments. These classifications will determine if the match score is used or discarded. Each of the probe classification results, $p_{i}$ , are provided to the experiment along with a weight, $m_{i}$ for each classification set and a threshold, where $i=1\\ldots C$ and $C$ is the number of classifications used. A particular match score, comparing probe $j$ to gallery $k$ is used when the following evaluates to true: $\\sum m_{i}(p_{i}(j)==g_{i}(k))>t h r e s h o l d$ . The value $p_{i}(j)$ is the predicted class of the $j^{\\mathrm{th}}$ value in the probe using the $i^{\\mathrm{th}}$ classification. Similarly, the value $g_{i}(k)$ is the given class of the $k^{\\mathrm{th}}$ value in the gallery using the $i^{\\mathrm{th}}$ classification. Basically, this is a weighted sum of which classifications match. If enough match, the value is above the threshold, and the score is used. This equation can be used to perform ‘AND’ and ‘OR’ decisions as well as equally weighted $\\cdot_{j}$ out of $k^{\\prime}$ . The second method is based on the scores generated by the classifier, one for each class. The maximum score is the class given as the decision, but it is possible that other scores are very close. If any of the scores are within a given threshold of the maximum score, the corresponding class is added to the predicted list. The match score will be used if the class of the gallery entry is found within the predicted list of the probe. Multiple classifiers may be used in this method as well. ",
        "page_idx": 99
    },
    {
        "type": "table",
        "img_path": "images/6b79d905c1ded00e008f1e9b10256a2debdece7fce72680fc731a13cc885e915.jpg",
        "table_caption": [
            "Table 6.2: Region/Feature/Classifier combinations chosen for application experiments. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Label</td><td colspan=\"3\">Ethnicity</td><td colspan=\"3\">Gender</td></tr><tr><td>Region</td><td>Feature</td><td>Classifier</td><td>Region</td><td>Feature</td><td>Classifier</td></tr><tr><td>1</td><td>MOUTH</td><td>LCH(RGB)</td><td>ANN100</td><td>MOUTH</td><td>LCH(RGB)</td><td>ANN100</td></tr><tr><td>2</td><td>NTIP</td><td>LCH(RGB)</td><td>ANN50</td><td>NTIP</td><td>LCH(RGB)</td><td>ANN100</td></tr><tr><td>3</td><td>REYE</td><td>LPQ</td><td>ANN100</td><td>MOUTH</td><td>LBP</td><td>ANN100</td></tr><tr><td>4</td><td>NOSE</td><td>LPQ</td><td>ANN100</td><td>NOSE</td><td>LPQ</td><td>ANN100</td></tr><tr><td>5</td><td>NOSE</td><td>Shape</td><td>SVMLinear</td><td></td><td></td><td>一</td></tr></table></body></html>\n\n",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "Table 6.2 lists the region/feature/classifier combinations that will be used in the experiments for this chapter. Two combinations were chosen for both gender and ethnicity classification for texture and color features. Shape features performed very poorly for gender classification, and so will only be used for ethnicity classification in this chapter. The MOUTH region was chosen for color as the best performing region for both gender and ethnicity. The NTIP region was chosen for being more stable with respect to age. The LCH features from the RGB color space perform the best over all color spaces investigated. The best texture region/feature combinations were the REYE/LPQ and MOUTH/LBP for ethnicity and gender classification respectively. The NOSE region with LPQ was chosen for its performance across the age groups. All except the shape classifier will use an ANN classifier to facilitate the score level fusion method. The ANN classification was close to linear SVM in the best cases, just a little lower. The shape classifier will use a linear SVM because all ANN classification was poor, even in ethnicity classification. ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "Results in this chapter will be reported following previous methods for classification, including confusion matrices and box-plots of class-specific accuracies. Verification and recognition results will be reported using DET and CMC plots as well as EER and Rank-1 values. ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "6.2 Analysis ",
        "text_level": 1,
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "6.2.1 Classification Results ",
        "text_level": 1,
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "Both the gallery and probe sets were classified according to the region/feature/classifier combinations given in Table 6.2. The results from classification of the gallery set will be reported here to give an idea of how well the combinations performed on Pinellas data. Table 6.3 shows the results for the combinations involving color, while Table 6.4 shows results using texture combinations. There are two main differences between MORPH and Pinellas ethnic classifications. Pinellas experiments ",
        "page_idx": 100
    },
    {
        "type": "table",
        "img_path": "images/1612beef8f627ecb7913b3e9d673ceb624f0d0ec61e239e189f95d5258021b4e.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td></tr><tr><td>White</td><td>45.58</td><td>0.95</td><td>49.24</td><td>4.22</td></tr><tr><td>Asian</td><td>12.13</td><td>4.20</td><td>72.76</td><td>10.91</td></tr><tr><td>Hispanic</td><td>5.06</td><td>0.34</td><td>88.95</td><td>5.65</td></tr><tr><td>Black</td><td>1.53</td><td>0.11</td><td>12.00</td><td>86.36</td></tr></table></body></html>\n\n",
        "page_idx": 101
    },
    {
        "type": "table",
        "img_path": "images/62d893942d19d92676c749e67627dbb3a864ae75e632c7d1ea7266795e130ddc.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td></tr><tr><td>White</td><td>33.23</td><td>1.89</td><td>55.05</td><td>9.83</td></tr><tr><td>Asian</td><td>3.66</td><td>14.16</td><td>59.55</td><td>22.63</td></tr><tr><td>Hispanic</td><td>2.94</td><td>0.96</td><td>86.99</td><td>9.11</td></tr><tr><td>Black</td><td>1.85</td><td>0.51</td><td>24.08</td><td>73.56</td></tr></table></body></html>\n\n",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "(a) 1e, average class-wise accuracy: $56.27\\%$ , overall accuracy: $62.68\\%$ . ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "(b) 2e, average class-wise accuracy: $51.99\\%$ , overall accuracy $51.00\\%$ . ",
        "page_idx": 101
    },
    {
        "type": "table",
        "img_path": "images/755f561c890223e84f174089ae81ffeb5f8d5203aeb8ad2c1d17d7a505d45c03.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>Male</td><td>Female</td></tr><tr><td>Male</td><td>81.45</td><td>18.55</td></tr><tr><td>Female</td><td>24.01</td><td>75.99</td></tr></table></body></html>\n\n",
        "page_idx": 101
    },
    {
        "type": "table",
        "img_path": "images/a6e238f1d0acfcfbf7d5d0afb25a398788c85e17c980fd1bf8be95f2759c8765.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>Male</td><td>Female</td></tr><tr><td>Male</td><td>84.94</td><td>15.06</td></tr><tr><td>Female</td><td>35.10</td><td>64.90</td></tr></table></body></html>\n\n",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "(c) 1g, average class-wise accuracy: $78.72\\%$ , overall accuracy: $79.58\\%$ . ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "(d) 2g, average class-wise accuracy: $74.92\\%$ , overall accuracy: $78.07\\%$ . ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "Table 6.3: Confusion matrices from classifying the gallery set on the chosen color region/ feature/ classifier combinations. The row is the true class and the column is the predicted class. All entries are percentages. Class-specific accuracies are highlighted. ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "only utilize four ethnic classes, leaving out the Native American class. Since this class averaged close to $0\\%$ in MORPH experiments, the Pinellas average class-wise accuracies will show an increase without that class. The other difference is in the class-specific accuracies for White and Hispanic. The performance on the White class is reduced by almost half while performance on the Hispanic class doubles. The biggest factor for the Hispanic class is likely the larger number of samples in the training set. While not equal to the number samples in the White and Black classes, the Hispanic training set is much closer to their proportions in Pinellas than MORPH. The decrease in performance in the White class is most likely related due to overlap between the classes since a large percentage of the Hispanic class were misclassified as White in MORPH experiments. ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "The results of ethnic classification on the nose shape features can be seen in Table 6.5. The average class-wise accuracy for this experiment is low, but it has a higher overall accuracy than other ethnicity experiments. This is due to the higher performance on the White class, which accounts for approximately 57% of the probe and gallery sets. ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "Gender classification on both texture and color combinations is similar to that found in the MORPH experiments. Performance on the Female class showed little change, but the performance on the Male class dropped approximately $10\\%$ . Males are still correctly classified more often than Females, just not as much. ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "The texture combinations still show the best performance when looking at average classwise accuracies. Color and shape are sometimes able to gain higher overall accuracies based on the distribution of classes in the set, but texture performs better for the majority of the classes. ",
        "page_idx": 101
    },
    {
        "type": "table",
        "img_path": "images/fceddcea7702a36674d36858046329f50485213a6dd57a601c0713db511baacf.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td></tr><tr><td>White</td><td>50.13</td><td>0.32</td><td>43.34</td><td>6.21</td></tr><tr><td>Asian</td><td>8.20</td><td>17.28</td><td>61.31</td><td>13.21</td></tr><tr><td>Hispanic</td><td>4.46</td><td>0.39</td><td>89.12</td><td>6.03</td></tr><tr><td>Black</td><td>1.93</td><td>0.10</td><td>14.00</td><td>83.97</td></tr></table></body></html>\n\n",
        "page_idx": 102
    },
    {
        "type": "table",
        "img_path": "images/24c78891180393e7be1a70ba2369e536e7176ac5546b76f9d894030778a53ec8.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td></tr><tr><td>White</td><td>42.50</td><td>0.50</td><td>50.19</td><td>6.81</td></tr><tr><td>Asian</td><td>5.49</td><td>12.87</td><td>57.45</td><td>24.19</td></tr><tr><td>Hispanic</td><td>3.58</td><td>0.40</td><td>86.53</td><td>9.49</td></tr><tr><td>Black</td><td>1.36</td><td>0.15</td><td>14.80</td><td>83.68</td></tr></table></body></html>\n\n",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "(a) 3e, average class-wise accuracy: $60.12\\%$ , overall accuracy: $64.52\\%$ . ",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "(b) 4e, average class-wise accuracy: $56.40\\%$ , overall accuracy $59.86\\%$ . ",
        "page_idx": 102
    },
    {
        "type": "table",
        "img_path": "images/49ff0d8f71565533f304f63cbf9894b8ccb91ca568fe289c9c5800f822d246a7.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>Male</td><td>Female</td></tr><tr><td>Male</td><td>87.64</td><td>12.36</td></tr><tr><td>Female</td><td>23.68</td><td>76.32</td></tr></table></body></html>\n\n",
        "page_idx": 102
    },
    {
        "type": "table",
        "img_path": "images/2091628e8c8cd2cc9ce53c310ddf8deebbc0ffabdbe341201e9a7765ab644fac.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>Male</td><td>Female</td></tr><tr><td>Male</td><td>85.73</td><td>14.27</td></tr><tr><td>Female</td><td>21.94</td><td>78.06</td></tr></table></body></html>\n\n",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "(c) 3g, average class-wise accuracy: $81.98\\%$ , overall accuracy: $83.76\\%$ . ",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "(d) 4g, average class-wise accuracy: $81.89\\%$ , overall accuracy: $83.10\\%$ . ",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "Table 6.4: Confusion matrices from classifying the gallery set on the chosen texture region/ feature/ classifier combinations. The row is the true class and the column is the predicted class. All entries are percentages. Class-specific accuracies are highlighted. ",
        "page_idx": 102
    },
    {
        "type": "table",
        "img_path": "images/ff98559d23d167eb6d0d08dcc883d5c9bda78a05d586dac9203b89845f1ab14e.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>White</td><td>Asian</td><td>Hispanic</td><td>Black</td></tr><tr><td>White</td><td>74.70</td><td>0.00</td><td>5.78</td><td>19.52</td></tr><tr><td>Asian</td><td>44.38</td><td>0.00</td><td>8.81</td><td>46.82</td></tr><tr><td>Hispanic</td><td>42.51</td><td>0.00</td><td>11.63</td><td>45.86</td></tr><tr><td>Black</td><td>10.06</td><td>0.00</td><td>3.40</td><td>86.53</td></tr></table></body></html>\n\n",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "Table 6.5: Confusion matrices from classifying the gallery set on the chosen shape combination, 5e. The row is the true class and the column is the predicted class. All entries are percentages. Class-specific accuracies are highlighted. Average class-wise accuracy: $43.22\\%$ , overall accuracy: $74.32\\%$ . ",
        "page_idx": 102
    },
    {
        "type": "image",
        "img_path": "images/37b886d3d59cd2d240f982e733875685932a2e3a0a380fbaf57f1cc756cd8414.jpg",
        "img_caption": [
            "Figure 6.1: Age distribution of Pinellas experiment sets. Values graphed represent the percentage of images in each set found in the age ranges listed. "
        ],
        "img_footnote": [],
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "6.2.2 Age ",
        "text_level": 1,
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "Figure 6.1 shows the distribution of ages across the three image sets used for Pinellas experiments. While compiling information on age, it was discovered that six images in the gallery set and one in the training set are labeled with negative ages. Each one is from a different subject and those in the gallery set have two corresponding images with a maximum age gap of 2 years between them. The images were stored with date of birth and date of image capture which were used to calculate the age. In some instances the correct date of birth was entered, but a default image capture date instead of the actual date was used, January 1, 1900 for example. These cases resulted in a negative age. Figure 6.2 shows images from the gallery which had negative ages. ",
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "Images with calculated ages less than sixteen were also found. While it is possible that the images labeled with an age of fifteen are legitimate, it is highly unlikely that subjects age two to nine years old are present in a database compiled from booking photographs. The image lists show 19 images of two to nine year-olds, 24 of ten to fourteen year-olds, and 50 of fifteen year-olds. Figure 6.3 shows some examples of images that are associated with ages fifteen and under. At the other end of the scale, the image lists show 15 images of subjects greater than one-hundred years old. This is possible; however, it is very unlikely that a subject would be in the database at both twenty-five and one-hundred and five years of age, which is the case for one probe/gallery combination. Figure 6.4 shows images from the probe and gallery sets with ages greater than one-hundred years old. These obvious errors are due to mistakes in the metadata which can be caused by mistyped or default information. It is possible that other errors in age occur in the middle ages, but these are not as obvious since those ages are well represented and have a high probability of naturally occurring in the database. ",
        "page_idx": 103
    },
    {
        "type": "image",
        "img_path": "images/5cab613992bfcda36cd5f95edbe55f0535093bf74060f219eb62e523f6bb6d87.jpg",
        "img_caption": [
            "Figure 6.2: Images from Pinellas with negative calculated ages. "
        ],
        "img_footnote": [],
        "page_idx": 104
    },
    {
        "type": "image",
        "img_path": "images/2c7feb3b1628b015e388788cd2dda31da362911afc1a6ee093f50b6625fde93a.jpg",
        "img_caption": [
            "Figure 6.3: Pinellas images with calculated ages below 16. Calculated ages of the images on the top row from L to R: 2 and 9; and on the bottom row: 13 and 15. "
        ],
        "img_footnote": [],
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "Each of the image sets, training, gallery, and probe, contain approximately the same distribution of ages. The training and gallery sets contain more younger subjects, but are within 5% of the amounts in the probe set. The creation of the probe and gallery sets put the younger images in the gallery set which accounts for the discrepancy between those lists. The selection of the training set did not account for age. ",
        "page_idx": 104
    },
    {
        "type": "image",
        "img_path": "images/892ef723d0907172571ad5ce69deaecc965b02035b16fdcb7687057618c0d5c5.jpg",
        "img_caption": [
            "Figure 6.4: Pinellas images with calculated ages over 100. Top row are images in the gallery set. Bottom row consists of corresponding probe images (also with an age over 100). "
        ],
        "img_footnote": [],
        "page_idx": 105
    },
    {
        "type": "image",
        "img_path": "images/0348ead0353c47810dfa615738ad1444fffc5d6b34ba8bb5c1e322985aac5762.jpg",
        "img_caption": [
            "Figure 6.5: Performance of chosen region / feature combinations on the probe set according to age. Results are from gender (left) and ethnicity (right) classification. "
        ],
        "img_footnote": [],
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "Figure 6.5 shows the performance of each of the region/feature/classifier combinations according to age for both gender and ethnicity classification. Gender classification shows more of an impact with respect to age than ethnicity, with the easiest ages to classify falling between 30 and 45. For most of the ethnic classes, younger subjects are easier to classify according to ethnicity than older ones. The decline is the most pronounced in the Hispanic class. These findings agree with previous observations on the MORPH experiments. ",
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "6.2.3 Rank-1 Score vs Genuine Score ",
        "text_level": 1,
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "The match scores of the straight face experiment directly impact how much improvement can be seen in the fusion experiments. The range of similarity scores for the straight whole-face experiment was 0 to 10080. Figure 6.6 shows an enlarged section of the distribution of scores between impostor and genuine comparisons. The area of overlap between impostor and genuine scores is of interest in this section. ",
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "Several aspects were investigated for individuals that had a genuine match not at Rank-1 for the straight whole-face experiment. The average score difference between the Rank-1 match and the highest genuine match was 51.51, with a standard deviation of 102.31, which is not much considering the range of scores present. Over 95% of the subjects had a difference of less than 200 between the genuine and Rank-1 match. 174 of them actually had a difference of zero. These instances were ties that were unaccounted for in the experiment. The system marked the lowest score as a match and did not update the match unless a score was strictly less than the match it held. ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "The average genuine score for probes with an incorrect Rank-1 match was 78.58 with a median of 54 and a standard deviation of 68.44. The average Rank-1 match score was 130.09 with a median of 88 and a standard deviation of 133.40. The average rank of the genuine match was 1,777, but with a median of 3, it seems highly skewed. The standard deviation was 12,368.48. The mean genuine score over all the probes was 199.36, with an average Rank-1 score of 207.18. This supports what can be seen in Figure 6.6, that the lower the genuine score, the more likely it is for the probe to have an impostor score as a Rank-1 match. ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "6.2.4 Ground Truth Fusion ",
        "text_level": 1,
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "A baseline experiment was run with no soft biometric classifications used to filter the results. This experiment, performed with the VeriLook system, obtained a Rank-1 performance of 84.83% and an EER of 3.27%. The straight facial experiment was also fused with the ground truth information for gender and ethnicity, both together and separately. This is the best performance possible if the classifiers performed perfectly. These graphs can be seen in Figure 6.7. For Rank-1 performance there is little change; the fused experiments increase the Rank-1 performance up to 85.29% for the experiment where gender and ethnicity both match. The biggest increases occur when ethnicity information is utilized. The CMC curves reflect the same trends, with the fused experiments having a steeper beginning than the straight experiment. The EERs increase with the fusion of both (AND) gender and ethnicity, increasing the most to $4.02\\%$ . This feels counter-intuitive. If obviously wrong scores are being excluded and all the genuine scores are being kept, it seems as if the performance should get better. Table 6.6 gives the Rank-1 performance and EER for each experiment as well as the total number of genuine and impostor scores used in the calculations. ",
        "page_idx": 106
    },
    {
        "type": "image",
        "img_path": "images/ae2c956d81afff360dc63aabbfd2534f807f976a1ecb5eb52e60ef12bca1c836.jpg",
        "img_caption": [
            "Figure 6.6: Distribution between impostor and genuine scores for straight whole face experiment. "
        ],
        "img_footnote": [],
        "page_idx": 107
    },
    {
        "type": "text",
        "text": "In verification experiments, the FAR and FRR are used to create the graphs and calculate the EER. For these ground truth experiments, the FRR will stay constant since all genuine scores were included. The FAR is what changes. In calculating the FAR, the number of impostor scores above the threshold is divided by the total number of impostor scores. In this way the EER is dependent on the number of impostor scores. In Table 6.6, as the number of impostor scores used increases, the EER decreases. Table 6.7 shows the average number of comparisons per probe for each of the baseline experiments. The greatest reduction, almost 75%, occurs when both gender and ethnicity need to match, but a 24% reduction still occurs when either gender or ethnic class matches are included. ",
        "page_idx": 107
    },
    {
        "type": "text",
        "text": "Figure 6.6 shows the distribution between genuine and impostor scores for the facial verification and identification experiments. The majority of the impostor scores are grouped with scores below 50, while the genuine scores are more spread out. 70% of the impostor scores are actually zero. Less than 1% of the genuine scores are also zero. It is likely that the genuine scores of zero contribute to the minimal increase in Rank-1 performance when using soft biometric information. Even if the candidate matches are narrowed to those matching in both gender and ethnicity, there is a good probability that one of the impostor scores will be greater than zero. ",
        "page_idx": 107
    },
    {
        "type": "table",
        "img_path": "images/829855886dddb10ab863356e15d3c25d466f05cbb887d8d97475c33def5fe6ce.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>Straight</td><td>23,583,793,380</td><td>217,182</td><td>84.82</td><td>3.27</td></tr><tr><td>Gender</td><td>12,959,718,724</td><td>217,182</td><td>84.99</td><td>3.65</td></tr><tr><td>Ethnicity</td><td>10,800,803,040</td><td>217,182</td><td>85.12</td><td>3.68</td></tr><tr><td>Both (AND)</td><td>5,906,125,240</td><td>217,182</td><td>85.29</td><td>4.02</td></tr><tr><td>Both (OR)</td><td>17,854,396,524</td><td>217,182</td><td>84.85</td><td>3.49</td></tr></table></body></html>\n\n",
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "Table 6.6: Baseline performance details on facial recognition fusion experiments. Highest Rank-1 performance and lowest EER are in bold. ",
        "page_idx": 108
    },
    {
        "type": "table",
        "img_path": "images/2c7bb4089f90a1e450e77cca90d908ef82a3a14851eebdf95d23e6ca10624d07.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor (Reduction)</td><td>Genuine</td></tr><tr><td>Straight</td><td>217,180.00 (0%)</td><td>2.00</td></tr><tr><td>Gender</td><td>119,344.37 (45%)</td><td>2.00</td></tr><tr><td>Ethnicity</td><td>99,463.15 (54%)</td><td>2.00</td></tr><tr><td>Both (AND)</td><td>54,388.72 (75%)</td><td>2.00</td></tr><tr><td>Both (OR)</td><td>164,418.75 (24%)</td><td>2.00</td></tr></table></body></html>\n\n",
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "Table 6.7: Average comparisons per probe on baseline experiments. Percentage by which the number of impostor scores are reduced as compared to the straight experiment is given in parenthesis. ",
        "page_idx": 108
    },
    {
        "type": "image",
        "img_path": "images/69d85fc342d5851e99f564ccc8277abd5770a7e0059f11837c32f5dfed56ccb9.jpg",
        "img_caption": [
            "Figure 6.7: Baseline performance on identification and verification experiments. Baseline experiments include the straight whole face experiment and fusion with ground truth gender and/or ethnicity. DET is on the left and CMC is on the right. "
        ],
        "img_footnote": [],
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "A large percentage of the probe/gallery pairs, 91.6%, have an age gap of 0-4 years, with 52.2% of the pairs actually having a gap of less than one year. Only 7.3% of the pairs have an age gap between 5 and 9 years. Of the rest, 1% of probe/gallery pairs have an age gap from 10-14 years, while only $0.1\\%$ have an age gap of 15 years or more. Some of these large age gaps stem from issues in the metadata mentioned earlier, such as a 25 year old with an image also appearing with an age of over 100. While the quality of the images is not as high of quality as found in FRGC, the images in these experiments had to meet the quality standards of the VeriLook SDK algorithm. The close age gap for most of the images and the multiple gallery entries, as well as the quality standards, work together to make the problem easier than it could have been. It is possible that, by making the original problem harder and changing any of those factors, more room for improvement using fusion can be seen. ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "6.3 Results ",
        "text_level": 1,
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "There is always a risk in filtering the gallery before matching that the true match will be excluded. That is why it is important to use multiple classifications to minimize the possibility of the genuine matches getting filtered out. However, the more classifications used and/or the more lenient the rule, the more gallery entries will be included. What good is it to do all the work for classification and yet still end up using the entire gallery due to a lenient filtering rule? It would be less work to just do all the gallery comparisons. Mixing the types of classification will help minimize this. For example, there are two gender classifications for the probe and each predicts a different gender, an ‘OR’ classification rule will not rule out any of the gallery entries. But if an ethnicity classification is added and the rule changed to ‘2 out of 3 (equal weights)’, some gallery entries would be excluded based on the ethnicity classification. Still, having multiple gender classifications will decrease the number of matches used, only on probes with differing classifications will all gallery entries be included. The following experiments will investigate using different numbers and types of classification, both feature and soft biometric trait. The legends in the following graphs will use the labels from Table 6.2 with an ‘e’ or a $\\mathrm{\\bar{g}}^{\\prime}$ following to indicate ethnicity or gender for the classification type. An indication of the rule used will also be included, such as ‘AND’ or ‘OR’. ",
        "page_idx": 109
    },
    {
        "type": "table",
        "img_path": "images/945267f9a35a38150f7b2bd61e14357f90d36ecc2a6e4a316eae10c3a5c8ab68.jpg",
        "table_caption": [
            "Table 6.8: Weighted sum fusion with color information. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>1g</td><td>12,707,142,872</td><td>171,434</td><td>67.46</td><td>3.40</td></tr><tr><td>2g</td><td>13,069,745,244</td><td>165,812</td><td>65.02</td><td>3.39</td></tr><tr><td>1g2g OR</td><td>16,174,486,048</td><td>198,884</td><td>77.85</td><td>3.40</td></tr><tr><td>1e</td><td>6,471,957,338</td><td>125,026</td><td>48.25</td><td>3.80</td></tr><tr><td>2e</td><td>5,518,189,458</td><td>99,636</td><td>38.57</td><td>3.63</td></tr><tr><td>1e2e OR</td><td>7,961,731,784</td><td>138,652</td><td>53.59</td><td>3.68</td></tr><tr><td>1g2g1e2e OR</td><td>18,694,632,076</td><td>209,012</td><td>81.58</td><td>3.40</td></tr></table></body></html>\n\n",
        "page_idx": 110
    },
    {
        "type": "table",
        "img_path": "images/2eb5c4feae464feb4a27aec202241b04e0c89dfcab095a8a9841bb94715b334f.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 6.9: Weighted sum fusion with shape and other information. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>5e</td><td>10,451,692,626</td><td>159,468</td><td>61.92</td><td>3.88</td></tr><tr><td>1e5e OR</td><td>12,374,469,974</td><td>190,666</td><td>74.49</td><td>3.50</td></tr><tr><td>2e5e OR</td><td>12,229,813,908</td><td>182,340</td><td>71.28</td><td>3.46</td></tr><tr><td>3e5e OR</td><td>12,420,118,146</td><td>190,800</td><td>74.53</td><td>3.48</td></tr><tr><td>4e5e OR</td><td>12,321,065,622</td><td>187,332</td><td>73.16</td><td>3.48</td></tr></table></body></html>\n\n",
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "6.3.1 Weighted Sum Decision Fusion ",
        "text_level": 1,
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "Table 6.8 shows the results of filtering the gallery results using classifications based on color, combinations 1 and 2, and the weighted sum decision rule. None of the experiments using just color classifications were able to get too close to the results of the straight experiment. This indicates that color alone is not what is needed to improve performance; however, since the color regions were not the best performing regions from the MORPH dataset, this is not unexpected. ",
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "It is interesting to note that the performance in the verification experiments (EER) does not change as dramatically as the performance in the recognition experiments (Rank-1). The CMC curve is affected more by the exclusion of the genuine matches than the verification measurements. The verification measurements include information about the number of impostor attempts, which can outweigh the genuine attempt information. ",
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "Results using ethnic shape classification combined with other ethnic classifications can be seen in Table 6.9. Shape ethnicity alone outperforms the color ethnicity experiments, but still does not achieve comparable performance to the straight facial recognition experiment. Performance can be traced back to good performance on the most prevalent class in the experiment set, White. ",
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "Table 6.10 shows the results of the weighted sum fusion using texture classifications. Results from these classifications are higher than those using color and shape, although shape outperformed the single texture ethnic experiments. Using both gender and ethnicity texture classifications, these experiments were able to come within 1% of the Rank-1 performance of the straight experiment. These results further support the conclusion that the most gender and ethnicity information can be found in texture features. ",
        "page_idx": 110
    },
    {
        "type": "table",
        "img_path": "images/bc54174dc99039d005b4fb0aacb8359547585793a6f0c7361d9935bde6be1590.jpg",
        "table_caption": [
            "Table 6.10: Weighted sum fusion with texture information. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>3g</td><td>12,843,138,128</td><td>192,678</td><td>75.54</td><td>3.54</td></tr><tr><td>4g</td><td>12,716,977,840</td><td>178,866</td><td>70.30</td><td>3.46</td></tr><tr><td>3g4g OR</td><td>15,415,754,464</td><td>210,044</td><td>82.19</td><td>3.49</td></tr><tr><td>3e</td><td>6,893,394,536</td><td>129,712</td><td>50.30</td><td>3.67</td></tr><tr><td>4e</td><td>6,451,954,140</td><td>120,486</td><td>46.66</td><td>3.66</td></tr><tr><td>3e4e OR</td><td>8,819,949,264</td><td>154,140</td><td>59.71</td><td>3.63</td></tr><tr><td>3g4g3e4e OR</td><td>18,436,420,630</td><td>215,202</td><td>84.09</td><td>3.44</td></tr></table></body></html>\n\n",
        "page_idx": 111
    },
    {
        "type": "table",
        "img_path": "images/7ddaf05ad3a638e0c310d7097c6346fa322752a2b1c66506f5e14012b3b758f0.jpg",
        "table_caption": [
            "Table 6.11: Weighted sum fusion with mixed color and texture features. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>1g3g OR</td><td>15,522,684,680</td><td>207,358</td><td>81.17</td><td>3.47</td></tr><tr><td>1g4g OR</td><td>15,742,372,896</td><td>203,056</td><td>79.57</td><td>3.44</td></tr><tr><td>2g3g OR</td><td>16,197,310,236</td><td>209,090</td><td>81.87</td><td>3.44</td></tr><tr><td>2g4g OR</td><td>15,871,225,112</td><td>199,762</td><td>78.25</td><td>3.42</td></tr><tr><td>1g2g3g OR</td><td>17,498,432,304</td><td>212,966</td><td>83.33</td><td>3.42</td></tr><tr><td>1g2g4g OR</td><td>17,439,113,114</td><td>209,562</td><td>81.98</td><td>3.40</td></tr><tr><td>1g3g4g OR</td><td>16,962,513,126</td><td>213,370</td><td>83.46</td><td>3.44</td></tr><tr><td>2g3g4g OR</td><td>17,278,916,008</td><td>213,618</td><td>83.57</td><td>3.42</td></tr><tr><td>1g2g3g4g OR</td><td>18,227,744,400</td><td>215,076</td><td>84.12</td><td>3.40</td></tr></table></body></html>\n\n",
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "The results in Table 6.11 show results using a mixture of color and texture classifications, either all gender or all ethnicity. Since the ethnic classifiers do not perform as well as the gender classifiers, fusion experiments using only ethnic classifications are unable to match the performance of the experiments utilizing gender classifications and are so omitted from the table. All four texture and color gender classifiers together are able to achieve a higher Rank-1 performance than any of the experiments using just color or texture, but it is still below the Rank-1 performance of the straight experiment. ",
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "Taking the top performing classifiers from gender and ethnicity, 3g, 3g4g, 5e, and 3e5e, gender and ethnicity classifications from different feature types were used together. The results can ",
        "page_idx": 111
    },
    {
        "type": "table",
        "img_path": "images/efef2de753d54c3e6c1cad25062c17d8df5f4296c4318d49eb1b4b4cf0fc1aab.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>3g5e OR</td><td>17,526,217,284</td><td>209,666</td><td>81.93</td><td>3.45</td></tr><tr><td>3g3e5e OR</td><td>18,434,508,896</td><td>213,634</td><td>83.48</td><td>3.44</td></tr><tr><td>3g4g5e OR</td><td>18,997,060,648</td><td>214,982</td><td>84.00</td><td>3.42</td></tr><tr><td>3g4g3e5e OR</td><td>19,678,094,918</td><td>216,108</td><td>84.43</td><td>3.40</td></tr></table></body></html>\n\n",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "Table 6.12: Weighted sum fusion with mixed texture and shape classifications, best single and double taken for gender and ethnicity. Highest Rank-1 performance and lowest EER are in bold. ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "be found in Table 6.12. The best result seen so far can be found in these experiments utilizing 3g4g and 3e5e. Replacing 4e with 5e from the all texture combination results in an improvement of less than $0.5\\%$ . ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "One possible way to improve the results would be to compare the predicted class of the probe with the predicted class of the gallery. This would allow someone who is consistently misclassified by the classifier to have the genuine match score included. Using the predicted gallery class provides a 5-7% improvement in Rank-1 when using color gender classifications and an improvement of 20-30% when using color ethnicity. When using both gender and ethnicity, the improvement shrinks to only $3\\%$ . Texture shows an improvement of 2-3% on gender and approximately 20% on ethnicity, but less than $1\\%$ when utilizing both. Shape experiments improve 8-10% when using the predicted gallery classes. The results using the best classifiers improve less than $0.5\\%$ . The best Rank-1 results when using predicted gallery is still 3g4g3e5e, 84.70%, followed by all gender classifiers, 84.69%, and all texture, $84.66\\%$ . All these results are still 0.1% below the straight facial recognition system. The related CMC and DET plots can be seen in Figure 6.8. ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "6.3.2 Score Fusion ",
        "text_level": 1,
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "Experiments in this section have a slight advantage over those in the previous section. By using the scores returned by the classifier instead of just the decision, the experiment has a little more information with which to make the decision if the probe and gallery entry might be a match. Table 6.13 shows the results of a score-level fusion on color classifications to decide if match scores are used in the recognition experiment. Rank-1 scores are $2\\%$ higher using color gender and 5% higher with color ethnicity than the corresponding weighted sum fusion experiments. However, using both color gender and ethnicity classifications, the Rank-1 performance still falls 2% below the straight face experiment. Table 6.14 shows the results using texture scores. Gender results improve less than 2% from weighted sum, while ethnicity shows an improvement of approximately $10\\%$ . Using both gender and ethnicity improves less than 1% from the other fusion method, but the Rank-1 performance is within $1\\%$ of the straight face experiment. There are no shape experiments in this section, because this fusion requires scores from an ANN classifier, and the shape classifier used a SVM classifier. ",
        "page_idx": 112
    },
    {
        "type": "image",
        "img_path": "images/3bda216c4ff35eb0ed2b2b4d65dec154786858dc7d097a1c9f7e369cd3acfda3.jpg",
        "img_caption": [
            "Figure 6.8: Best weighted sum fusion experiments compared to the straight experiment. DET is on the left and CMC is on the right. "
        ],
        "img_footnote": [],
        "page_idx": 113
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 113
    },
    {
        "type": "text",
        "text": "Experiments mixing both gender and ethnicity classification using both color and texture information allow for a small increase as seen in Table 6.15. At this point the number of impostor scores and genuine scores are nearing the numbers for the straight face experiment. As with weighted sum fusion, it is possible that matching to the predicted gallery would increase the Rank-1 performance by allowing more genuine scores to be included. Using all color combinations, the Rank-1 performance is just $0.01\\%$ under the performance of the straight experiment, but less than 5% of the impostor scores are actually ruled out from the experiment. Using all texture combinations, the Rank-1 performance improves to just $0.01\\%$ over the performance of the straight experiment. Again, approximately $5\\%$ of the impostor scores are excluded. The CMC and DET graphs for these experiments can be seen in Figure 6.9. The EERs are not as good for the fusion experiments, and the performance on the CMC is very similar. So the improvement is very small and possibly not worth the cost of four partial face gender and ethnicity classifications. ",
        "page_idx": 113
    },
    {
        "type": "table",
        "img_path": "images/c595bd8400566d049c929521963eec5359e1b86bfb0245e2423a0ef519144a72.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 6.13: Score fusion with color information. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>1g 2g</td><td>13,193,346,780 14,258,951,034</td><td>175,740 176,196</td><td>69.13 69.06</td><td>3.40 3.39</td></tr><tr><td>1g2g OR 1e 2e</td><td>17,003,923,860 7,716,306,302 6,746,179,502</td><td>203,298 137,362 113,470</td><td>79.57 53.01 43.89</td><td>3.38 3.72 3.62</td></tr><tr><td>1e2e OR 1g2gle2e (G AND E)</td><td>9,634,780,128 6,934,431,200</td><td>152,988 144,566</td><td>59.13 56.03</td><td>3.59 3.76</td></tr><tr><td>1g2gle2e (G OR E) All (G OR E) predicted gallery</td><td>19,704,272,788 22,587,493,941</td><td>211,720 217,153</td><td>82.67 84.81</td><td>3.37 3.30</td></tr></table></body></html>\n\n",
        "page_idx": 113
    },
    {
        "type": "table",
        "img_path": "images/f7d053e9dfdb6a876278c4a68637b0b90632971db8000885dcc4ad5940dbec5b.jpg",
        "table_caption": [
            "Table 6.14: Score fusion with texture information. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>3g</td><td>14,303,998,630 14,422,806,168</td><td>192,596 192,442</td><td>75.58 75.51</td><td>3.43</td></tr><tr><td>3g4g OR 4g</td><td>17,163,555,578</td><td>211,572</td><td>82.81</td><td>3.45 3.42</td></tr><tr><td>3e 4e</td><td>9,047,946,474 8,455,913,224</td><td>155,040 143,936</td><td>60.26 55.78</td><td>3.56 3.61</td></tr><tr><td>3e4e OR</td><td>11,400,418,864</td><td>177,956</td><td>69.16</td><td>3.70</td></tr><tr><td>3g4g3e4e (G AND E)</td><td>8,314,459,576</td><td>173,258</td><td>67.49</td><td>3.72</td></tr><tr><td>3g4g3e4e (G OR E) All (G OR E) predicted gallery</td><td>20,249,514,866 22,383,990,928</td><td>216,270 216,920</td><td>84.49</td><td>3.37</td></tr></table></body></html>\n\n",
        "page_idx": 114
    },
    {
        "type": "table",
        "img_path": "images/0137ddc44e33f7a07663a2ab55e2b04555988da0220da565a37f161d14d1b254.jpg",
        "table_caption": [
            "Table 6.15: Score fusion with mixed gender, ethnicity, color and texture classifications. Highest Rank-1 performance and lowest EER are in bold. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Impostor Attempts</td><td>Genuine Attempts</td><td>Rank-1 (%)</td><td>EER (%)</td></tr><tr><td>3g4g1e3e4e</td><td>20,586,393,362</td><td>216,570</td><td>84.60</td><td>3.36</td></tr><tr><td>1g3g4g1e3e4e</td><td>21,032,866,890</td><td>216,826</td><td>84.70</td><td>3.35</td></tr></table></body></html>\n\n",
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "6.4 Conclusions ",
        "text_level": 1,
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "While the gender and ethnicity information is useful in decreasing the number of impostor scores, in the proposed fusion experiments, multiple classifications on both the probe and gallery sets are needed to achieve a very slight improvement, 0.01% in Rank-1 and only a $5\\%$ reduction in comparisons. If the classifiers were perfect, the increase in Rank-1 in this particular scenario would be slightly larger, 0.47%, but the reduction of comparisons is much greater. Any of the region/feature/classifier combinations by themselves were unable to improve Rank-1 recognition. The small improvement in performance is not worth the cost multiple partial-face classifications in this instance, but this may not be true in all cases. The score-level fusion allowed for better results with the same or fewer classifications than the weighted sum decision fusion. ",
        "page_idx": 114
    },
    {
        "type": "image",
        "img_path": "images/9ec1ccc32cd0857f675ec6d4c540b8ed4cb8cebd7ed82530653428438bee1b7d.jpg",
        "img_caption": [
            "Figure 6.9: Best score fusion experiments. DET is on the left and CMC is on the right. "
        ],
        "img_footnote": [],
        "page_idx": 115
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 115
    },
    {
        "type": "text",
        "text": "It is possible that some performance could be gained by making several changes to the proposed fusion methods. The first change was implemented by comparing the probe classification to the gallery classification instead of the ground truth information for the gallery subject. This resulted in a small gain in Rank-1 performance. Another possibility is to compare the probe classification with both the gallery classification and the ground truth and combine those in a weighted sum. The weights could be individually learned in an authentication experiment, so that, for an individual who is generally misclassified more weight is given to the classification comparison and for an individual whose classification is haphazard, the weights could give equal consideration to both. For a verification system, the weights would most likely be learned off-line from a training set or set as a system parameter. The final change would be to start with a partial face experiment as opposed to a whole face experiment. This would be more for the identification experiments than for verification experiments. With less information to make the decision of who an individual is, the soft biometric information might be able to leverage a larger gain in performance. ",
        "page_idx": 115
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "Improving the classification accuracy would also help increase fusion experiment performance. In looking at the confusion matrices and misclassification rates for this section and the others, a better representation of the Asian and White subjects is needed. Class-wise accuracy on these classes is lower compared to the others. Key-point features as opposed to local-appearance based features might be a better fit for these classes. The color and texture features used in this work were extracted using local-appearance-based features. This would also play into using partialface experiments. If feature extraction could be done without facial alignment, similar to the work by Liao and Jain [42], it would open up a lot more options for biometric applications. The system would not need the whole face and it would not need specific points to reliably extract a certain part of the face. This would allow a more covert acquisition of facial images and help deal with occlusion and image quality. Another method to improve classification accuracy is to incorporate boosting into the training algorithm so that the classifier spends more time learning how to classify difficult subjects and less time on individuals that are easy to classify [21]. ",
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "This concludes the discussion of fusion experiments performed using partial-face classifications to filter a face experiment. The final chapter will summarize the conclusions made from all experiments and discuss directions for future work. ",
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "Chapter 7 ",
        "text_level": 1,
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "Conclusions and Future Work ",
        "text_level": 1,
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "7.1 Reliability ",
        "text_level": 1,
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "The first question considered in this work was which parts of the face hold reliable gender and ethnicity information for machine-based classification. To the human eye, different parts of the face can have a feminine or masculine quality which can agree or disagree with a person’s identity. The same can occur with ethnicity. Experiments were performed on two datasets, for both gender and ethnicity classification. Regions of the face considered were the left eye, right eye, nose tip, nose, mouth and chin. ",
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "From 360 partial-face color experiments on the FRGC and MORPH datasets, the conclusion was made that the mouth and eye regions provide the most gender information. The mouth region achieved similar performance to full face indicating the potential for as much gender information as the whole face. From the same number of ethnicity classifications, the mouth and eye regions also provide the most ethnicity information, again, comparable to face. The best region depends on the particular ethnic group. For most it was the mouth region, but Asian class was erratic. The best color space for feature extraction was the RGB color space. The difference between this space and the others was that RGB included color information in all three channels and did not compress it into two. This agrees with what was seen in previous work, that the eye regions are useful for gender and ethnicity classification and that the RG channels from the RGB color space work well. ",
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "Partial-face shape experiments for FRGC and MORPH numbered 168 for each trait, gender and ethnicity. Results suggest that partial-face shape is not reliable for either gender or ethnicity. ",
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "This is possibly due to feature representation and selection as opposed shape encoding no information. Full face results in previous work were able to achieve similar gender classification performance to texture features. Differences were seen between the gender and ethnic classes, but the overlap between them was too large for classifiers to adequately separate them. The best classifications used all features with no reduction. This is another indication that shape shows promise. It is possible that a better representation or different feature selection could perform better. ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "In 252 partial-face texture gender experiments over the FRGC and MORPH datasets, it was determined that texture features encode the most gender information in the lower face regions, namely the mouth and the chin. These regions achieved performance comparable to the gender classification of a commercial face system. In the corresponding ethnic experiments, it was determined that the texture of the upper face, namely the eye regions, holds the most ethnicity information. The eye and nose regions were able to achieve similar performance to the texture classification of the entire face. Previous work has shown most regions of the face to be useful for gender classification using either texture or pixels, so these results agree with some and contradict others. Previous ethnic research has shown that the eye region holds discriminating ethnic information, which is confirmed in texture experiments. ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "Over all 780 gender and 780 ethnicity experiments, texture features encode the most reliable gender and ethnicity information. Not all texture features are equal; however. The best texture representations for this work were LBP and LPQ. The color features encode more soft biometric information than HOG features, but less than LBP and LPQ, indicating that it may be a useful feature when texture cannot be accurately extracted. The best regions for gender classification are the mouth. For ethnic classification, it is the eyes followed by the nose. The eyes perform well in gender classification as well. Region size does not seem to matter although the resolution of the original image does. ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "Ethnicity is a harder problem than gender classification. Not only does it have more possible classes, the classes are open to interpretation by individual. An individual’s self-perception of ethnicity does not always agree with anyone else’s or even society’s perception of their ethnic identity. This makes classifications of this nature more difficult. ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "Within these experiments, no clear conclusion could be drawn in regards to the best classification method. Both SVM and ANN classifiers had good performance and bad performance. It seemed to depend more on the feature than on the actual classifier. In most cases, both of these ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "classifiers outperformed the $k$ -NN classifier. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "7.2 Age ",
        "text_level": 1,
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "The second question considered in this work was the effect of age on the machine classification of gender and ethnicity using partial-face images. The human face changes as it ages, with skin becoming more coarse and gaining wrinkles. MORPH experiments were analyzed here to see if any region/feature combinations were less susceptible to changes in age than others. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "From the experiments in Chapters 3, 4, and 5, the conclusion was made that no region or feature remained unaffected by changes in age. The regions including the nose were determined to be the the most stable for gender classification and fairly stable for ethnic classification. The chin region was stable for both as well, but had the lowest performance in ethnicity classification. Ethnicity classification was less impacted by age than gender classification, which follows the trends seen in previous work classification involving the whole face. The impact of age on specific features was dependent on the region. No clear conclusion could be drawn on the invariance of specific features to age. Color features did seem less susceptible to changes in age than texture features, but were still negative impacted by age. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "Different classes were affected differently by age. The Female class was more impacted by age than the Male class, especially in the eye regions. Younger subjects were easier to classify in most cases, both gender and ethnicity. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "7.3 Application ",
        "text_level": 1,
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "The third question considered in this work was the extent to which partial-face gender and ethnicity classification could be used to improve the performance in a biometric application. Experiments were performed on one dataset not previously used in this work. Choices on the proposed methods were made based on the experiments in Chapters 3, 4, and 5. Two types of fusion were investigated, one each at the score and decision levels. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "While the partial-face gender and ethnicity classifications were useful in decreasing the number of impostor scores, the proposed fusion methods gained little to no improvement in Rank-1 performance as compared to the straight face experiment. An improvement of $0.01\\%$ in Rank-1 performance using score fusion only excluded 5% of impostor scores. This contradicts the idea that the soft biometric information used must be independent of the biometric modality, but follows the trend that the more dependent the traits and the modality are, the smaller the improvement in biometric performance. Multiple partial-face classifications of both gender and ethnicity were needed to gain the small improvement in Rank-1. The complexity and number of classifiers most likely outweighs the reduction in comparisons achieved. Other experiments excluded more impostor scores, but this resulted in the exclusion of more genuine matches and decreased Rank-1 performance. Using ground truth information only netted a small gain in Rank-1 performance, 0.47% and resulted in higher EERs. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 120
    },
    {
        "type": "text",
        "text": "As it stands, partial-face gender and ethnicity did not improve the performance of the chosen whole-face biometric application much. The conclusion was made that more improvement is possible, just not with the given whole-face application. Several changes were suggested to improve upon the methods given in this work, including using a partial-face recognition system, a single gallery experiment, basing the fusion on the comparison of the predicted probe class to the predicted gallery class, and changing how the soft biometric information is incorporated into the experiment. Other suggestions were based on improving the classification performance such as using key-point based features instead of local-appearance and incorporating boosting algorithms into the training of the classifiers. ",
        "page_idx": 120
    },
    {
        "type": "text",
        "text": "7.4 Future Work ",
        "text_level": 1,
        "page_idx": 120
    },
    {
        "type": "text",
        "text": "Future work directly tied to improving the proposed methods in this work can be divided into two sections: improving the soft biometric classification and improving the performance of the fusion of soft biometric classifications with a biometric application. To improve the classification performance, the use of boosting algorithms during training may prove useful. Also, both classification methods used in this work are machine-learning based, so trying a statistical classification method may provide more insight into the problem. Features used for classification were confined to local-appearance based texture and color features. Key-point features as well as shape features might encode more reliable information for classification. To improve fusion performance, the base experiment can be changed to a partial-face experiment and the fusion method changed from filtering to a feature to be compared. ",
        "page_idx": 120
    },
    {
        "type": "text",
        "text": "There are several other topics that would be helpful to pursue but are not directly tied to improving the proposed methods in this work. In the field of biometrics, there is literature on the ‘biometric menagerie’ or ’Doddington’s Zoo’ [73]. This classification refers to a subject’s likelihood of contributing to the FAR and FRR in an authentication problem. The categories are Sheep, Lambs, Goats, and Wolves. Subjects who are well separated from the others and rarely get rejected are labeled as sheep, whereas subjects who are very hard to recognize and contribute to the FRR are classified as goats. Lambs are subjects who overlap significantly with other users and contribute to the FAR. Subjects classified as wolves are able to pass for other subjects and contribute to the FAR as well. It would be interesting to apply this theory to soft biometric classification. The categories would not be quite the same, since it would be a classification problem instead of an authentication problem, but it would be interesting to look at what characteristics of a particular demographic make it stand apart from the others or what characteristics that individuals within that demographic have that are similar to another demographic. Categorizations of individuals would could help determine weights in an updated fusion scheme. ",
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "Intrinsic to a soft biometric zoo problem, as well as to the soft biometric classification problem, is the issue of the demographic labels themselves. Ethnic and gender identities are no longer as black and white as they once were as cultures blend together and technologies advance. For instance, someone’s self-perceived ethnic identity could be different from how others perceive their ethnic identity, which is different from their ethnic heritage, which might not fit into any labels that a soft biometric system may have learned to classify. It would be interesting to do a study on the perception of ethnic and gender identity to see if the results impacted the zoo classifications or provided insight as to which labels soft biometric systems should learn. ",
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "An evaluation of partial-face soft biometric classification under varying image quality would also be useful. Knowing the minimum quality requirements for a specified performance would provide a guide for researchers to incorporate the soft biometric classifier into their research. Quality could include resolution, image blur, lighting, occlusion, and distortion. ",
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "Glossary ",
        "text_level": 1,
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "$k$ -NN $k$ -Nearest Neighbor, simplistic machine learning based classifier.. 25, 36, 49, 112 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "ANN Artificial Neural Network , machine learning based classifier. 11, 27, 28, 36, 111 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "AR face database collected by Aleix Martinez and Robert Benavente. 7, 8 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "BioID face database. 7, 8 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "BioSecure iris database. 7 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "CAS-PEAL Chinese Academy of Sciences-Pose, Expression, Accessories, and Lighting, face database. 7–9 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "CASIA Chinese Academy of Sciences Institute of Automation, iris database. 7, 8 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "CMC Cumulative Match Curve, performance measure used in recognition/identification problems 30, 31, 93, 99, 103 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "CMU-PIER iris database collected at Carnegie Mellon University. 7, 8 confusion matrix performance analysis used in classification problems. 28 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "CV cross-validation, experiment technique that does not use specific training and testing sets. 30 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "DET Detection Error Trade-off, performance measure used in verification/authentication problems. 29, 93 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "EER Equal Error Rate, place in a ROC or DET graph where FRR and FAR are equal. 29, 31, 93, 99, 100, 113 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "FAR False Accept Rate, percentage of false matches accepted at a given threshold in a verification/authentication problem. 29, 30, 100, 114 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "FERET Face Recognition Technology, face database. 6–9 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "FRGC Face Recognition Grand Challenge, face database. 7, 8, 15, 16, 18, 19, 23, 36, 60, 74, 102 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "FRR False Reject Rate, percentage of true matches rejected at a given threshold in a verification/authentication problem. 29, 100, 114 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "HOG Histograms of Oriented Gradient, texture feature representation. 10, 11, 72, 89, 111 ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "LBP Local Binary Patterns, texture feature representation. 10, 11, 73, 89, 111   \nLCH Local Color Histograms, color feature representation. 11, 32   \nLDA Linear Discriminant Analysis, statistical analysis method for feature reduction which seeks to maximize between-class scatter while minimizing within-class scatter. 25   \nLPQ Local Phase Quantization, texture feature representation. 11, 73, 74, 89, 111   \nMBGC Multiple Biometric Grand Challenge, face and iris database. 7, 8   \nMORPH Craniofacial Longitudinal MORPHological Face, face database. 8, 16–19, 23, 36, 60, 74   \nPCA Principal Component Analysis, statistical analysis method typically used in biometrics for feature reduction. 24, 25, 92   \nPinellas face database. 8, 18, 19, 23, 91   \nROC Receiver Operating Characteristic, performance measure used in verification/authentication problems. 29–31   \nSoftopia Japan face database available at: http://www.hoip.softopia.pref.gifu.jp/. 7, 8   \nSUMS Standford University Medical Student, face database. 7, 8   \nSVM Support Vector Machine, machine learning based classifier. 11, 27, 36, 111   \nUBIRIS iris database (noisy, visible wavelength). 7, 8   \nUND face database collected by the University of Notre Dame. 7   \nUPOL iris database collected by researchers at Palack´y University Olomouc. 7, 8   \nXM2VTS Extended Multi-Modal Verification for Teleservices and Security, face database available at: http://www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/. 6–9 ",
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "Bibliography ",
        "text_level": 1,
        "page_idx": 124
    },
    {
        "type": "text",
        "text": "[1] H. Abdi, D. Valentin, B. Edelman, and A. J. O’Toole. More about the difference between men and women: evidence from linear neural networks and the principal-component approach. Perception, 24(5):539 – 562, 1995.   \n[2] T. Ahonen, E. Rahtu, V. Ojansivu, and J. Heikkila. Recognition of blurred faces using local phase quantization. In Proc. 19th Int. Conf. Pattern Recognition ICPR 2008, pages 1–4, 2008.   \n[3] R. Akbari and S. Mozaffari. Performance enhancement of PCA-based face recognition system via gender classification method. In Proc. 6th Iranian Machine Vision and Image Processing (MVIP), pages 1–6, 2010.   \n[4] A. M. Albert, K. Ricanek, Jr., and E. Patterson. A review of the literature on the aging adult skull and face: Implications for forensic science research and applications. Forensic Science International, 172(1):1 – 9, 2007.   \n[5] Y. Andreu and R. Mollineda. The role of face parts in gender recognition. In A. Campilho and M. Kamel, editors, Image Analysis and Recognition, volume 5112 of Lecture Notes in Computer Science, pages 945–954. Springer Berlin / Heidelberg, 2008. 10.1007/978-3-540-69812-8 94.   \n[6] R. Bolle, J. Connell, S. Pankanti, N. Ratha, and A. Senior. Guide to Biometrics. SpringerVerlag, 2003.   \n[7] R. Bolle, J. Connell, S. Pankanti, N. Ratha, and A. Senior. The common biometrics. In Guide to Biometrics, Springer Professional Computing, pages 31–49. Springer New York, 2004.   \n[8] S. Buchala, N. Davey, R. Frank, T. Gale, M. Loomes, and W. Kanargard. Gender classification of face images: The role of global and feature-based information. In N. Pal, N. Kasabov, R. Mudi, S. Pal, and S. Parui, editors, Neural Information Processing, volume 3316 of Lecture Notes in Computer Science, pages 763–768. Springer Berlin / Heidelberg, 2004. 10.1007/978- 3-540-30499-9 117.   \n[9] S. Buchala, N. Davey, R. J. Frank, and T. M. Gale. Dimensionality reduction of face images for gender classification. In Proc. 2nd Int Intelligent Systems IEEE Conf, volume 1, pages 88–93, 2004.   \n[10] D. Cao, C. Chen, M. Piccirilli, D. Adjeroh, T. Bourlai, and A. Ross. Can facial metrology predict gender? In Biometrics (IJCB), 2011 International Joint Conference on, pages 1–8, 2011.   \n[11] C. H. Chan, J. Kittler, N. Poh, T. Ahonen, and M. Pietik¨ainen. (Multiscale) Local Phase Quantisation histogram discriminant analysis with score normalisation for robust face recognition. In Proc. IEEE 12th Int Computer Vision Workshops (ICCV Workshops) Conf, pages 633–640, 2009.   \n[12] C. H. Chan, M. Tahir, J. Kittler, and M. Pietik¨ainen. Multiscale local phase quantization for robust component-based face recognition using kernel fusion of multiple descriptors. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(5):1164–1177, 2013.   \n[13] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www. csie.ntu.edu.tw/∼cjlin/libsvm.   \n[14] S. R. Coleman and R. Grover. The anatomy of the aging face: Volume loss and changes in 3-dimensional topography. Aesthetic Surgery Journal, 26(1, Supplement):S4 – S9, 2006.   \n[15] T. Cootes, G. Edwards, and C. Taylor. Active appearance models. In H. Burkhardt and B. Neumann, editors, Computer Vision - ECCV’98, volume 1407 of Lecture Notes in Computer Science, pages 484–498. Springer Berlin / Heidelberg, 1998. 10.1007/BFb0054760.   \n[16] G. W. Cottrell and J. Metcalfe. Empath: Face, emotion, and gender recognition using holons. In Advances in Neural Information Processing Systems, volume 3, pages 564–571, 1991.   \n[17] A. A. Dahl. Heterochromia iridis symptoms, causes, treatments. Online: http://www. medicinenet.com/heterochromia iridis/page2.htm, 2013. Accessed November 2 2014.   \n[18] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893 vol. 1, 2005.   \n[19] M. Demirkus, K. Garg, and S. Guler. Automated person categorization for video surveillance using soft biometrics. In Proceedings of SPIE, volume 7667, pages 76670P–76670P–12, 2010.   \n[20] O. De´niz, G. Bueno, J. Salido, and F. D. la Torre. Face recognition using histograms of oriented gradients. Pattern Recognition Letters, 32(12):1598 – 1603, 2011.   \n[21] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, Inc., 2nd edition, 2001. Chapter 9.   \n[22] B. Edelman, D. Valentin, and H. Abdi. Sex classification of face areas: how well can a linear neural network predict human performance. Journal of Biological System, 6(3):241–264, 1998.   \n[23] Fast artification neural network library (FANN). Online: http://leenissen.dk/fann/ wp/. Accessed May 8, 2014.   \n[24] B. A. Golomb, D. T. Lawrence, and T. J. Sejnowski. Sexnet: A neural network identifies sex from human faces. In Proceedings of the 1990 conference on Advances in neural information processing systems 3, NIPS-3, pages 572–577, San Francisco, CA, USA, 1990. Morgan Kaufmann Publishers Inc.   \n[25] G. Guennebaud, B. Jacob, et al. Eigen v3. Online: http://eigen.tuxfamily.org, 2010. Accessed May 12, 2014.   \n[26] G. Guo, C. R. Dyer, Y. Fu, and T. S. Huang. Is gender recognition affected by age? In Proc. IEEE 12th Int Computer Vision Workshops (ICCV Workshops) Conf, pages 2032–2039, 2009.   \n[27] G. Guo and G. Mu. A study of large-scale ethnicity estimation with gender and age variations. In Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), pages 79–86, 2010.   \n[28] S. Gutta, J. R. J. Huang, P. Jonathon, and H. Wechsler. Mixture of experts for classification of gender, ethnic origin, and pose of human faces. Neural Networks, IEEE Transactions on, 11(4):948–960, 2000.   \n[29] S. Gutta, H. Wechsler, and P. J. Phillips. Gender and ethnic classification of face images. In Proc. Third IEEE Int Automatic Face and Gesture Recognition Conf, pages 194–199, 1998.   \n[30] S. Hosoi, E. Takikawa, and M. Kawade. Ethnicity estimation with facial images. In Proc. Sixth IEEE Int Automatic Face and Gesture Recognition Conf, pages 195–200, 2004.   \n[31] Y. Hu, J. Yan, and P. Shi. A fusion-based method for 3D facial gender classification. In Proc. 2nd Int Computer and Automation Engineering (ICCAE) Conf, volume 5, pages 369–372, 2010.   \n[32] A. Jain, J. Huang, and S. Fang. Gender identification using frontal facial images. In Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on, page 4 pp., July 2005.   \n[33] A. Jain, K. Nandakumar, X. Lu, and U. Park. Integrating faces, fingerprints, and soft biometric traits for user recognition. In D. Maltoni and A. Jain, editors, Biometric Authentication, volume 3087 of Lecture Notes in Computer Science, pages 259–269. Springer Berlin / Heidelberg, 2004. 10.1007/978-3-540-25976-3 24.   \n[34] A. K. Jain, S. C. Dass, and K. Nandakumar. Soft biometric traits for personal recognition systems. In International conference on Biometric Authentication, pages 731–738, 2004.   \n[35] T. Kawano, K. Kato, and K. Yamamoto. A comparison of the gender differentiation capability between facial parts. In Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on, volume 1, pages 350 – 353 Vol.1, Aug 2004.   \n[36] T. Kawano, K. Kato, and K. Yamamoto. An analysis of the gender and age differentiation using facial parts. In Proc. IEEE Int Systems, Man and Cybernetics Conf, volume 4, pages 3432–3436, 2005.   \n[37] A. Lapedriza, M. Marin-Jimenez, and J. Vitria. Gender recognition in non controlled environments. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 3, pages 834–837, 2006.   \n[38] A. Lapedriza, D. Masip, and J. Vitria. Are external face features useful for automatic face classification? In Computer Vision and Pattern Recognition - Workshops, 2005. CVPR Workshops. IEEE Computer Society Conference on, page 151, June 2005.   \n[39] G. Li, G. Sun, and X. Zhang. Robust face recognition in low resolution and blurred image using joint information in space and frequency. In J. Park, A. Zomaya, S.-S. Yeo, and S. Sahni, editors, Network and Parallel Computing, volume 7513 of Lecture Notes in Computer Science, pages 616–624. Springer Berlin Heidelberg, 2012.   \n[40] Y. Li, M. Savvides, and T. Chen. Investigating useful and distinguishing features around the eyelash region. In Proc. 37th IEEE Applied Imagery Pattern Recognition Workshop AIPR ’08, pages 1–6, 2008.   \n[41] H.-C. Lian and B.-L. Lu. Multi-view gender classification using multi-resolution local binary patterns and support vector machines. International Journal of Neural Systems, 17(6):479–487, 2007.   \n[42] S. Liao, A. Jain, and S. Li. Partial face recognition: Alignment-free approach. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(5):1193–1205, May 2013.   \n[43] L. Lu and P. Shi. A novel fusion-based method for expression-invariant gender classification. In Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing ICASSP 2009, pages 1065–1068, 2009.   \n[44] L. Lu, Z. Xu, and P. Shi. Gender classification of facial images based on multiple facial regions. In Proc. WRI World Congress Computer Science and Information Engineering, volume 6, pages 48–52, 2009.   \n[45] X. Lu and A. K. Jain. Ethnicity identification from face images. Proceedings of SPIE, 5404:114– 123, 2004.   \n[46] J. R. Lyle, P. E. Miller, S. J. Pundlik, and D. L. Woodard. Soft biometric classification using local appearance periocular region features. Pattern Recognition, 45(11):3877 – 3885, 2012.   \n[47] F. S. Manesh, M. Ghahramani, and Y. P. Tan. Facial part displacement effect on templatebased gender and ethnicity classification. In Proc. 11th Int Control Automation Robotics & Vision (ICARCV) Conf, pages 1644–1649, 2010.   \n[48] J. Merkow, B. Jou, and M. Savvides. An exploration of gender identification using only the periocular region. In Biometrics: Theory Applications and Systems (BTAS), 2010 Fourth IEEE International Conference on, pages 1–5, Sept 2010.   \n[49] A. Mian, M. Bennamoun, and R. Owens. An efficient multimodal 2D-3D hybrid approach to automatic face recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(11):1927–1943, 2007.   \n[50] Minority Report. Dir. Steven Spielberg. Twentieth Century Fox Film Corporation, 2002. Film.   \n[51] MORPH — IISIS. Online: http://www.faceaginggroup.com/morph/. Accessed October 17, 2013.   \n[52] S. Nissen. Neural networks made simple. Software Developer’s Journal, 2005. Online: http: //fann.sourceforge.net/fann en.pdf. Accessed May 12, 2014.   \n[53] T. Ojala, M. Pietik¨ainen, and T. M¨aenp¨a¨a. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(7):971–987, 2002.   \n[54] V. Ojansivu and J. Heikkil¨a. Blur insensitive texture classification using local phase quantization. In Proceedings of the 3rd international conference on Image and Signal Processing, ICISP ’08, pages 236–243, Berlin, Heidelberg, 2008. Springer-Verlag.   \n[55] O. O¨ zbudak, M. Kirci, Y. C¸ akir, and E. O. G¨une¸s. Effects of the facial and racial features on gender classification. In Proc. MELECON 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conf, pages 26–29, 2010.   \n[56] U. Park and A. K. Jain. Face matching and retrieval using soft biometrics. IEEE Transactions on Information Forensics and Security, 5(3):406–415, 2010.   \n[57] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman, J. Marques, J. Min, and W. Worek. Overview of the face recognition grand challenge. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 947– 954 vol. 1, June 2005.   \n[58] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, and W. Worek. Preliminary face recognition grand challenge results. In Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on, pages 15–24, 2006.   \n[59] X. Qiu, Z. Sun, and T. Tan. Global texture analysis of iris images for ethnic classification. In D. Zhang and A. Jain, editors, Advances in Biometrics, volume 3832 of Lecture Notes in Computer Science, pages 411–418. Springer Berlin / Heidelberg, 2005. 10.1007/11608288 55.   \n[60] X. Qiu, Z. Sun, and T. Tan. Learning appearance primitives of iris images for ethnic classification. In Proc. IEEE Int. Conf. Image Processing ICIP 2007, volume 2, 2007.   \n[61] A. W. Rawls and K. Ricanek, Jr. MORPH: development and optimization of a longitudinal age progression database. In Proceedings of the 2009 joint COST 2101 and 2102 international conference on Biometric ID management and multimodal communication, BioID MultiComm’09, pages 17–24, Berlin, Heidelberg, 2009. Springer-Verlag.   \n[62] K. Ricanek and B. Barbour. What are soft biometrics and how can they be used? Computer, 44(9):106–108, 2011.   \n[63] K. Ricanek and T. Tesafaye. MORPH: a longitudinal image database of normal adult ageprogression. In Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on, pages 341–345, 2006.   \n[64] Y. Saatci and C. Town. Cascaded classification of gender and facial expression using active appearance models. In Proc. 7th Int. Conf. Automatic Face and Gesture Recognition FGR 2006, pages 393–398, 2006.   \n[65] W. Schwartz, H. Guo, and L. Davis. A robust and scalable approach to face identification. In K. Daniilidis, P. Maragos, and N. Paragios, editors, Computer Vision - ECCV 2010, volume 6316 of Lecture Notes in Computer Science, pages 476–489. Springer Berlin / Heidelberg, 2010. 10.1007/978-3-642-15567-3 35.   \n[66] C. Shan. Learning local binary patterns for gender classification on real-world face images. Pattern Recognition Letters, 33(4):431 – 437, 2012. Intelligent Multimedia Interactivity.   \n[67] S. Tamura, H. Kawai, and H. Mitsumoto. Male/female identification from 8x6 very low resolution face images by neural network. Pattern Recognition, 29(2):331 – 335, 1996.   \n[68] V. Thomas, N. V. Chawla, K. W. Bowyer, and P. J. Flynn. Learning to predict gender from iris images. In Proc. First IEEE Int. Conf. Biometrics: Theory, Applications, and Systems BTAS 2007, pages 1–5, 2007.   \n[69] M. Toews and T. Arbel. Detection, localization, and sex classification of faces from arbitrary viewpoints and under occlusion. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(9):1567 –1581, sept. 2009.   \n[70] M. Turk and A. Pentland. Face recognition using eigenfaces. In Computer Vision and Pattern Recognition, 1991. Proceedings CVPR ’91., IEEE Computer Society Conference on, pages 586– 591, 1991.   \n[71] Y. Wang, K. Ricanek, C. Chen, and Y. Chang. Gender classification from infants to seniors. In Proc. Fourth IEEE Int Biometrics: Theory Applications and Systems (BTAS) Conf, pages 1–6, 2010.   \n[72] L. Wiskott, J.-M. Fellous, N. Kr¨uger, and C. von der Malsburg. Face recognition and gender determination. In Proceedings of the International Workshop on Automatic Face and Gesture Recognition, pages 92–97, June 1995.   \n[73] M. Wittman, P. Davis, and P. Flynn. Empirical studies of the existence of the biometric menagerie in the FRGC 2.0 color image corpus. In Computer Vision and Pattern Recognition Workshop, 2006. CVPRW ’06. Conference on, pages 33–33, June 2006.   \n[74] D. Woodard, S. Pundlik, J. Lyle, and P. Miller. Periocular region appearance cues for biometric identification. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, pages 162–169, 2010.   \n[75] Z. Yang and H. Ai. Demographic classification with local binary patterns. In S.-W. Lee and S. Li, editors, Advances in Biometrics, volume 4642 of Lecture Notes in Computer Science, pages 464–473. Springer Berlin / Heidelberg, 2007. 10.1007/978-3-540-74549-5 49.   \n[76] R. Zewail, A. Elsafi, M. Saeb, and N. Hamdy. Soft and hard biometrics fusion for improved identity verification. In Proc. 47th Midwest Symp. Circuits and Systems MWSCAS ’04, volume 1, 2004. ",
        "page_idx": 124
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 125
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 126
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 127
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 128
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 129
    }
]