![](images/884c729f9d0eb7d70a182401a489f0e3f0547b3729bc6509bfbbde46ac98b6bd.jpg)  

#  

![](images/f5254e01418a0fabc70dd82e52b468e4629ce2c1b49c374bb20db5559f2ea870.jpg)  

# REGULATING BIOMETRICS Global Approaches and Urgent Questions  

# Edited by Amba Kak  

# September 2020  

Cite as: Amba Kak, ed., “Regulating Biometrics: Global Approaches and Urgent Questions” AI Now Institute, September 1 2020, https://ainowinstitute.org/regulatingbiometrics.html.  

# ACKNOWLEDGMENTS  

I would like to acknowledge and thank Luke Strathmann for his steadfast editorial support, without which this compendium would not have come together. Thanks also to Caren Litherland for her meticulous copyediting. I’m immensely grateful to the authors of the chapters in this compendium for their seamless collaboration, despite an unexpectedly challenging year in the midst of a pandemic. I’m equally grateful, as always, to my colleagues Meredith Whittaker, Alejandro Calcaño, Theodora Dryer, Sarah Myers West, Varoon Mathur, and Inioluwa Deborah Raji for their detailed feedback and edits; and to Jason Schultz and Kate Crawford for their guidance on an earlier draft. A special thank you to Carly Kind (Ada Lovelace Institute), Ella Jakubowska (EDRi), and Vidushi Marda (Article 19) for their generous feedback on the introductory chapter.  

# ARTWORK  

The images used on the cover and throughout this compendium are by Heather Dewey-Hagborg, Visiting Assistant Professor of Interactive Media at NYU Abu Dhabi and Artist Fellow at AI Now.  

In How Do You See Me? Dewey-Hagborg developed custom software to produce a series of images that are detected as “faces” or are recognized as her. Starting from primitive curves and gradients, images evolve to more strongly elicit the algorithmic detection and recognition response.  

We see the face reduced to a white circle, laying bare the racial assumptions that underpin facial detection technologies.  

And we see abstract shapes and patterns, images that seemingly bear no resemblance to faces, emerge as neighboring facial vectors to the artist’s own.  

The outcome of these experiments is a series of images that give us a window into how we are seen by the opaque technologies of artificial intelligence and facial recognition.  

Learn more about the project at https://deweyhagborg.com/projects/how-do-you-see-me.  

# NOTE FROM THE EDITOR  

Amid heightened public scrutiny, interest in regulating biometric technologies has grown across the globe. Public advocacy has driven this scrutiny across globally dispersed and distinctive local political contexts. Common across these diverse movements is a growing sense that these technologies are no longer inevitable, accompanied by questions as to whether they are necessary at all. Advocates continue to remind developers, profiteers, and those using and regulating these biometric systems that the future course of these technologies must—and will—be subject to greater democratic control. The next few years are poised to produce wideranging legal regulation in many parts of the world that could alter the future course of these technologies. Addressing this moment of possibility, this compendium presents eight case studies from academics, advocates, and policy experts offering a variety of perspectives and national contexts. These expert contributors illuminate existing attempts to regulate biometric systems, and reflect on the promise, and the limits, of the law.  

The compendium begins with an introduction and a summary chapter that identifies key themes from existing legal approaches, and poses open questions for the future. These questions highlight the critical research needed to inform ongoing national policy and advocacy efforts to regulate biometric recognition technologies.  

# MINOW  

# CONTENTS  

Chapter 0.  

Introduction Amba Kak  

Chapter 1.   
The State of Play and Open Questions for the Future 16 Timeline of Legal Developments 42 Amba Kak Chapter 2.   
Australian Identity-Matching Services Bill 44 Jake Goldenfein and Monique Mann Chapter 3.   
The Economy (and Regulatory Practice) That Biometrics Inspires:   
A Study of the Aadhaar Project 52 Nayantara Ranganathan Chapter 4.   
A First Attempt at Regulating Biometric Data in the European Union 62 Els Kindt Chapter 5.   
Reflecting on the International Committee of the Red Cross’s Biometric Policy:   
Minimizing Centralized Databases 70 Ben Hayes and Massimo Marelli Chapter 6.   
Policing Uses of Live Facial Recognition in the United Kingdom 78 Peter Fussey and Daragh Murray   
Chapter 7. A Taxonomy of Legislative Approaches to Face   
Recognition in the United States 86   
Jameson Spivack and Clare Garvie Chapter 8.   
BIPA: The Most Important Biometric Privacy Law in the US? 96 Woodrow Hartzog Chapter 9.   
Bottom-Up Biometric Regulation: A Community’s Response to Using Face Surveillance in Schools   
Stefanie Coyle and Rashida Richardson  

![](images/c728f2b362ab9abefef2b00b72c9a22043eee70ceaba8ff30eb30cc850438628.jpg)  

![](images/756eb075cbbcb996e88427237ab524a566911cecdc01c15742803a69e3dd507d.jpg)  

![](images/8624fa2d9960fccf7674abb27e00ced4c173295cfb11d2587453d81f1a79a761.jpg)  

# Introduction  

Amba Kak  

A lthough the terminology varies,1 we use the phrase biometric recognition technologies to describe systems that $"\mathsf{f}_{1}\mathsf{x}^{\prime\prime2}$ official identities to bodily, physiological, or behavioral traits,3 providing new ways for individuals to identify themselves, and also to be identified or tracked. While fingerprints have the longest history as a marker of identity and continue to be used in a number of applications across the world, other bodily markers like face, voice, and iris or retina are proliferating, with significant research exploring their potential large-scale application. Emerging areas of interest in this field include using behavioral biometrics like gait (i.e., how a person walks), keyboard keystroke patterns, and multimodal combinations of biometrics to identify and potentially make inferences about individuals.4  

Beyond identifying people, these systems increasingly claim to be able to infer demographic characteristics, emotional states, and personality traits from bodily data. (This practice is sometimes referred to as “soft biometrics”5 in technical literature.) In other words, there has been a change in questioning that historian Jane Caplan has summarized as a shift from “What person is that?” to “What type of person is that?”6 Scholars have pointed to the fact that many of these systems that claim to detect interior characteristics from physical information are built on debunked and racist scientific and cultural assumptions about who looks like what “type” of person,7 and lead to demonstrated harms when applied in sensitive social domains like hiring or education.8  

The rapid expansion of the biometrics industry coincides with advancing technical methods and features and decreasing costs of hardware and software. Camera- and video-analytics technologies being produced today are designed to have higher resolution, the ability to work from greater distances, and night-vision sensors that create the conditions for live facial recognition and real-time surveillance in public spaces.9 Body-worn cameras that can attach to clothing or helmets have found a huge market among law enforcement agencies.10 And advanced voice recorders that are able to pick up recordings from a greater distance are transforming voice recognition into a tool that could enable persistent remote surveillance.11  

Meanwhile, the ubiquity of face photographs and voice recordings tagged with people’s names on the internet has greatly decreased the financial and technical resources required to create the databases that underpin face and voice recognition systems. Clearview AI provides an example. The company was an inconspicuous start-up until it attracted global controversy in early 2020 for the three billion labeled face images (matched to names) it scraped from the web without consent. The company then used these photos to market surveillance tools to a range of private and public actors, claiming that its system could pull up identity and other intimate information about anyone whose image was in its database.12 Recent reporting demonstrates that Clearview AI is not unique. In July 2020, the German digital rights blog Netzpolitik uncovered a Polish company called PimEyes that creates a similar “face search engine” with a database of nine hundred million images scraped from the web.13 The magnitude of these companies’ systems, along with their relative obscurity, demonstrates the way the market for biometric recognition systems consists of a number of nontransparent vendors that sell their systems globally without any oversight or scrutiny.14  

A range of mostly proprietary algorithmic processes enable vendors to transform these databases into biometric recognition systems capable of identifying individuals at a large scale. Creating such a system requires a combination of human and computational labor, as well as a formidable technical, financial, and political infrastructure. Labeling and tagging biometric data in order to make it searchable and to prepare it to feed into machine learning systems requires significant, on-demand human labor power. There is no reliable way to create these systems without such labeled data. At present, much of this data labeling work, often contingent and underpaid, is outsourced to firms across the world, with a high concentration in countries in the Global South.15 Using machine-learning techniques such as deep learning and readily available neural network architectures, these large datasets of images are used by firms to train and calibrate computer models that are designed and optimized to predict “matches” within a database, which in turn confirm or reveal identity.16  

The frenzied growth of biometrics into a global multibillion-dollar industry has not happened organically.17 Powerful state and private actors promote the belief that these technologies are effective, necessary, and beneficial. Their core claim is that a strong connection exists between bodily traits and identity, and that biometric identifiers can be uniquely attributed to a particular individual with a high degree of accuracy and continuity over time.18 This claim is naturalized in biometric systems, as is the corollary belief that these digital technologies have lower chances of fraud compared to non-biometric and analog means of identification.  

These claims of accuracy and efficiency are often taken as a given, and transposed onto broader societal and economic values like security, safety, and more efficient service delivery.19 While fingerprints have the longest history as a marker of identity and continue to be used in a number of applications across the world,20 other bodily markers like face, voice, and iris or retina are proliferating, with significant research exploring their potential large-scale application. Police agencies use data produced by facial recognition systems to identify suspects, make arrests, and confirm guilt or innocence through system matches.21 It is also being used as a tool to do ID checks for those who lack identification documents,22 to monitor large events or public spaces for known criminals, and to surveil protests.23 Beyond face-based systems, a recent investigation revealed that dozens of prisons across the US were creating voice-print databases of inmates and applying voice recognition to their phone communication to detect when particular voice prints appear, track call recipients of interest, and even to identify external people who were contacting people in prison most often.24 Meanwhile, amid an environment of heightened xenophobia and anti-immigrant political rhetoric, the use of biometrics is proliferating as a form of border control technology.25 The rationale of security is by no means restricted to law and immigration enforcement. It has driven the use of these tools as access control technologies for workplaces, schools, and apartment complexes, where they automate identity verification and even evaluate behavior to determine entry permissions.26  

In some ways, this growth and normalization of biometric recognition technology follows a similar trajectory to the rapid growth of closed-circuit television (CCTV) use through the 2000s, despite no clear evidence that it was effective in controlling crime. Security systems are often installed as a reaction to severe crimes, but without evidence that they would have prevented that crime in the first place. Indeed, research shows that the rapid proliferation of video surveillance followed from “crises, triggered by particular events such as, a child-kidnapping, a class-room murder, a terrorist outrage or rising concerns over crime.”27  

Today, governments across the world are the largest customer of the global biometrics industry, sustaining and shaping its growth. The development of tools for this wide range of government functions is typically outsourced to private firms that develop, market, and maintain these systems. A 2019 market-research report says that the “government segment is the highest revenue generating segment among all the applications of biometric authentication and identification.”28 Outside of security functions, governments are increasingly adopting biometric identifiers as a routine part of service delivery, with the active support of international development institutions and donor agencies. Biometric IDs are promoted as a means to prevent service delivery fraud. Many of the ID systems are being rolled out in Global South countries—like in India, the Philippines, Kenya, and Brazil—and are not sector-specific, but are instead “generalpurpose” IDs that construct a digital, biometric identity for each resident.29  

Outside of government, biometric recognition systems have been normalized as part of everyday experiences, largely driven by the goal of preventing fraud. Biometric locks are now a staple feature of many smartphones and laptops, and biometric profiles of customers offer a way to uniquely identify individuals across their transactions online or at ATMs. Biometrics are also being promoted as a novel and promising consumer advertising technology,30 where individuals can walk through cameras in a shopping space and be offered personalized advertising or be verified for loyalty programs seamlessly.31  

The last few years mark a critical juncture, perhaps even a turning point, in the trajectory of continued biometric expansion. Civil-society advocates have challenged the foundational arguments made by companies and governments that produce and promote these technologies, highlighting the tangible harms caused by their use. Mounting research demonstrates that these systems perform poorly when used in real-life contexts,32 even when the system meets narrow assessment standards that the industry relies on to back claims of accuracy.33 Even systems that boast high accuracy rates have unevenly distributed errors. They perform less well on certain demographics than on others,34 with particularly high failure rates for Black women, gender minorities, young and old people, members of the disabled community, and manual laborers.35 Beyond accuracy, research and civil society are also challenging the dominant discourses  

of security, safety, and efficiency that have driven marketing and demand for these systems. Advocates are increasingly asking for whom such systems provide safety and security. The claim that biometric surveillance “makes communities safer” is heavily marketed but loosely backed. Companies and governments make access to details on these systems and their use difficult to obtain, but even so, there is strong evidence that these systems are being deployed in ways that harm historically marginalized people and communities. For example, in the US, there have been multiple cases where facial recognition has resulted in misidentification of suspects, including cases where facial recognition is used as primary evidence to determine guilt.36 This harm is compounded by the systematic denial of basic due process rights during trial, in which defendants are denied access to information about whether and how these systems were used.37 Even outside of law enforcement, there is no transparency at all when it comes to privately created “watch list” databases, which are likely being shared and institutionalized through their use at large-scale events, retail stores, and housing complexes. At a recent Taylor Swift concert, all attendees were subject to facial recognition without their knowledge or consent, creating public debate around the lack of safeguards people would have recourse to if they were blacklisted unfairly by these systems.38  

As new applications of these technologies are created, so are new forms of pushback. Realtime monitoring of protests with facial recognition (e.g., in Hong Kong,39 Delhi,40 Detroit,41 and Baltimore42) has been met by fierce community opposition. This type of pervasive real-time surveillance can potentially produce chilling effects on the democratic exercise of rights to free speech and movement in public life. Organized tenants groups have contested the use of facial recognition and other property technologies (“PropTech”) to control access to residential buildings, arguing that they provide landlords with greater unaccountable control, and the ability to harass and surveil tenants.43 Meanwhile, coalitions between digital-rights organizations and social welfare and accountability activists have challenged biometric ID schemes for social service  

delivery on the basis of their impacts on privacy as well as the denial of basic entitlements due to technical or operational failures in these systems.44 Advocacy campaigns continue to question the use of facial recognition at airports, as well as the reuse of driver’s licenses and other civilian biometric databases for immigration enforcement and private investigation purposes.45  

While public advocacy is increasing in many parts of the world, and each campaign has its unique characteristics related to local political and economic contexts, what unites the current wave of pushback is the insistence that these technologies are not inevitable. Questioning technological inevitability has become a popular refrain, and reminds those acquiring, promoting, and regulating these systems that the future course of these technologies must and will be subject to greater democratic control.  

Calls for regulation include demands to introduce new laws (e.g., like data-protection frameworks); to reform and update existing laws (e.g., laws that currently only regulate fingerprints and DNA use in the criminal process); to pause these systems; or to outright ban their use. In Kenya and India, there have been demands to pass data-protection laws amid the rollout of large-scale biometric ID projects without such laws in place.46 Parliamentarians and government officials in the $\mathsf{U K}^{47}$ and a government-appointed advisory group in Scotland have acknowledged the need for a broad regulatory framework for biometric use, alongside the need to update existing laws that only apply to fingerprint and DNA biometrics.48 The clearest pushback on the idea that these technologies are inevitable has come in the form of advocacy championing complete bans or moratoria on the use of these systems, irrespective of context.49 Similarly, while a recent Indian Supreme Court decision eventually upheld the constitutionality of the country’s biometric ID project, a dissenting opinion from one of the judges also made clear that it’s not too late to turn back, ordering that “all such data be destroyed.”50  

Advocacy and the threat of regulation have spurred companies to act proactively to mitigate, and potentially undercut or postpone, demands for prohibition or strict regulation. Microsoft and Amazon have released calculated public statements in support of facial recognition regulation.51 More recently, IBM, Microsoft, Amazon, and others committed to pause their use of these technologies, citing disproportionate harms to people of color amid widespread antiracist Black Lives Matter mobilization in the US and around the globe.52 Activists responded by reminding legislators that these voluntary gestures were not nearly enough: “Facial recognition, like American policing as we know it, must go.”53  

Amid heightened public scrutiny, interest in regulating biometric technologies has grown significantly. The degree of openness to legislating technology varies, and for some countries regulation is not a realistic or appropriate intervention at all. Yet in many parts of the world, the next few years do seem poised to produce wide ranging regulation and with that, offer the possibility to alter the future course of biometric technologies. This compendium responds to this environment of possibility, compiling detailed case studies of existing attempts to regulate biometric systems that post emergent and open questions for the future.  

This collection of eight essays from diverse contributors covers widely divergent contexts:  

Australian Identity-Matching Services Bill: Jake Goldenfein (Melbourne Law School) and Monique Mann (Deakin University) track the institutional and political maneuvers that resulted in Australia’s ambitious centralized facial recognition program (“The Capability”). They draw lessons from what they term the “futility of regulatory oversight.”  

The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project: Nayantara Ranganathan (lawyer and independent researcher, India) explains how law and policy around India’s Biometric ID (“Aadhaar”) project eventually served to construct biometric data as a resource for value extraction by private companies. She explores how regulation was influenced by the logics and cultures of the project it sought to regulate.  

A First Attempt at Regulating Biometric Data in the European Union: Els Kindt (KU Leuven) provides a detailed account of the European Union’s General Data Protection Regulation (GDPR) approach to regulating biometric data. As many countries are set to implement similarly worded national laws, she warns of potential loopholes and highlights key areas for reform.  

Reflecting on the International Committee of the Red Cross’s Biometric Policy: Minimizing Centralized Databases: Ben Hayes (AWO Agency, Consultant legal advisor to the International Committee of the Red Cross [ICRC]) and Massimo Marelli (Head of the ICRC Data Protection Office) explain ICRC’s decision-making process as they formulated the institution’s first biometrics policy in the context of humanitarian assistance, with a focus on minimizing the creation of databases and risks to vulnerable populations.  

Policing Uses of Live Facial Recognition in the United Kingdom: Peter Fussey (University of Essex) and Daragh Murray (University of Essex), lead authors of the independent empirical review of the London Metropolitan Police’s trial of Live Facial Recognition (LFR), explain how existing legal norms and regulatory tools fell short, with broader lessons for the regulation of LFR in the UK and elsewhere.  

A Taxonomy of Legislative Approaches to Face Recognition in the United States: Jameson Spivack and Clare Garvie (Georgetown Center on Privacy and Technology) write about the dozens of bans and moratoria legislation on police use of facial recognition in the US, providing a detailed taxonomy that goes beyond these broad categories, and lessons learned from their implementation.  

BIPA: The Most Important Biometric Privacy Law in the US? Woodrow Hartzog (Northeastern University) explores the promise and pitfalls of the State of Illinois’ Biometric Information Privacy Act (BIPA) and, more broadly, of the private right of action model. He questions the inevitable limits of a law that is centered on notice and consent.  

# Bottom-Up Biometric Regulation: A Community’s Response to Using Face Surveillance in  

Schools: Stefanie Coyle (NYCLU) and Rashida Richardson (Rutgers University, AI Now Institute, NYU) examine the controversial move by a school district in Lockport, New York, to implement a facial and object recognition system. They highlight the community-driven response that incited a national debate and led to statewide legislation regulating the use of biometric technologies in schools.  

![](images/c7736e740f801cb4a4c7412630e4e81485be9ba630b56cebac43adfdba3b948c.jpg)  

# The State of Play and Open Questions for the Future  

Amba Kak  

T lsheiysstcoehnmaspsf.treoWrmesydenrxtaihswetsininzgseilsgehgbtarsloaprpdiptmroearanilcdyhsferasonamdcotrhopesesne sqmsuaelytsistpiiloenntcshoifusonrcttroihemespraegnud acutoimontneoxstfsub.rifBoaemcyeotnridc analysis of the current state of play, we pose open questions about where regulation needs revision, or reimagination. We explore the rapidly evolving policy conversation around new kinds of regulatory interventions but also, crucially, the limits of the law in capturing or resolving concerns about these technologies.  

Regulation of biometric systems has largely been through data-protection laws. Biometric data is typically designated as an especially sensitive category of personal data and is regulated through a series of restrictions on the collection, retention, and disclosure of such data.1 The 2016 European Union’s General Data Protection law (GDPR) is emblematic of this approach, and there are currently over 140 countries with national data-protection laws that cover private- and publicsector use of data.2 The United States lacks a comprehensive federal data privacy regulation similar to the GDPR, but state laws like the 2018 Illinois Biometric Information Privacy Act (BIPA)  

follow a similar data-protection approach to regulating biometric data.3 Key elements of this approach are also included in laws that establish and govern biometric ID systems like India’s 2016 Aadhaar Act,4 Australia’s 2019 Identity Services Matching Bill,5 and Kenya’s 2019 Huduma Namba bill.6 Section 1 (“The Data-Protection Lens”) examines these approaches to regulating biometrics, highlighting key concerns that have become apparent through their implementation.  

While data-protection laws have made fundamental shifts in the way companies and government approach the collection, retention, and use of personal data, there are clear limitations on their ability to address the full spectrum of potential harms produced by new forms of data-driven technology, like biometric identification and analysis. Their focus on individual (rather than group) conceptions of harm fails to meaningfully address questions of discrimination and algorithmic profiling.7 The focus on data as the object of regulation has also sometimes obscured the broader challenges to social and institutional practices that these systems and platforms exert on society, in which imperfect but established methods of accountability, contestation, and democratic decision-making are undercut by the introduction of opaque automated technology.8 In contrast, there has been a flurry of legislation, mostly in the United States, that bans the use of these systems in particular sectors, across certain uses, or for lengths of time until there is a more participatory and deliberative process of decision-making in place. Sector-specific rules have also emerged, like those that address the harms of biometric systems in criminal justice or employment or education domains. Sections 2 and 3 of this chapter track these emergent concerns and legal approaches.  

The following is a summary of the questions that this compendium raises, pointing to research, regulation, and community engagement that will be needed to inform ongoing national policy and advocacy efforts:  

# 1.	The Data-Protection Lens  

•	 How should regulation define “biometric data”?   
•	 Why have data protection laws had limited effectiveness in curbing the expansion of biometric surveillance infrastructure by government? Is meaningful notice and consent possible in the context of biometric systems? What are the limitations of a consent-based approach and what supplements or alternatives might be required?  

# 2.	Beyond Privacy: Accuracy, Discrimination, Human Review, and Due Process  

•	 How should regulatory frameworks address concerns about accuracy and nondiscrimination in biometric systems?   
•	 To what extent should regulation rely on standards of performance and accuracy set by technical standards-setting bodies?   
Does requiring “meaningful human review” of biometric recognition systems ensure oversight and accountability?   
•	 Should regulatory frameworks create a risk-based classification between “identification” and “verification” uses of biometric recognition? •	 What are the potential risks of a permissive regulatory approach to verification?   
•	 What kinds of due process safeguards are required for law enforcement use of biometric recognition? •	 Should law enforcement have access to these systems to begin with?   
•	 Are systems that process bodily data for purposes beyond establishing individual identity, like making inferences around emotional state, personality traits, or demographic characteristics covered under existing biometric regulation? •	 Should such systems be permitted at all, given their contested scientific foundations and mounting evidence of harm?  

# 3.	Emerging Regulatory Tools and Enforcement Mechanisms  

•	 What different types of “bans” and moratoria have been passed in the US over the past few years? •	 How can moratoria conditions be strengthened to ensure that eventual legislative or deliberative processes are robust?   
How will bans and moratoria on government use impact the private development and production of biometric systems?   
•	 What regulatory tools can be used to create public transparency around the development, purchase, and use of biometric recognition tools?   
•	 What role can community-led advocacy play in shaping the priorities and impact of regulation?  

# SECTION 1. THE DATA-PROTECTION LENS  

# How should regulation define “biometric data”?  

Under the dominant data-protection approach to regulating biometric systems, meeting the definition of “biometric data” has been the threshold condition for legal protections to apply. Recent regulatory attempts move away from this with “systems” rather than data as the object of regulation.  

In laws that establish and regulate biometric ID systems, the definition of biometric data has typically been left open-ended to allow governments to add or change the types of biometrics collected under these projects.  

In defining biometric data and systems, the law not only reflects but also entrenches certain perceptions about the stability and accuracy of biometrics as an identification technology. For example, the GDPR states that biometric data is bodily, physiological, and behavioral data that “allow or confirm the unique identification of that natural person,”9 while the Illinois BIPA provides an exhaustive list of identifiers that count as biometric data and requires that they are “used to identify an individual.”10 These foundational beliefs about the ability of biometric data to uniquely identify an individual are not stable and are today highly contested. Research has demonstrated vulnerabilities as people age, and the inaccuracies that creep in when these systems are used to identify people of color, young and old people, manual laborers, those who speak English with a non-native accent, and many other demographic and phenotypic subgroups.11 Biometric regulation does not interrogate these questions, but simply takes these claims of accuracy and equivalence to real identity as given.  

In data-protection laws, fulfilling the definition of “biometric data” or “biometric information” is the threshold condition for legal protections to apply. It also determines the stage (for, e.g., collection, processing, storage, and use) at which these protections are activated. When part of a broader personal data-protection law like the GDPR, such definitions usually work to distinguish biometric data from other kinds of personal data in order to offer special or stricter levels of protection. In laws like BIPA, which is solely focused on biometric data, the definition determines the scope of the legislation as a whole. Laws that establish government biometric ID projects, on the other hand, have tended toward an expansive definition that allows agencies to expand on the kinds of biometric data they can collect. The Kenyan draft law12 and the Indian Aadhaar legislation13 list a series of identifiers that are currently collected under the project but allow the government to add to these categories of data collected at will.  

As legislation moves beyond traditional data privacy and security concerns to questions of accountability around whether or how to use these systems, and who is liable if these systems fail, some recent bills shift the focus from “data” to “systems.” For example, recent US legislation that restricts the use of these technologies does not define biometric data at all, and instead focuses on “face recognition systems” or “services $\mathbf{\chi}^{\prime\prime\prime},$ or face/biometric “surveillance systems” as the object of regulation.15 The definitions of these terms emphasize the eventual uses or intentions that drive the application of such systems in social contexts (such as surveillance, identification, verification, or tracking).  

The legal definition of biometric data is usually restricted to data that has been technically processed for use in an algorithmic system by specifying a particular digital representation (e.g., “template” or “print”). The definition often explicitly excludes photographs and voice recordings and creates a loophole around foundational stages when data is collected, processed, and stored.  

The definition of biometric data has generally been restricted to mean a technically defined digital representation of bodily traits that have already been processed for machine or algorithmic analysis. This is suggested by semi-technical terms like “templates,” “geometry,” “prints,”16 or, in the GDPR, data that has already been subject to “specific technical processing.” Terms like “template” refer to the initial stage of algorithmic processing where data is extracted from, say, an image or voice recording. Modern machine learning systems do not need “all” of the data, but instead rely on extracting meaningful subparts from voice or image data, which can then be easily compared to existing “templates” in a database.17 This is the logic that leads to photographs of faces being expressly excluded from the definition of biometric data in the BIPA18 and the GDPR.19  

This narrow technical definition of biometric data creates a set of troubling loopholes. In her chapter, Els Kindt explains that the exclusion of photographs, voice recordings, or other forms of so called “raw” biometric data adversely limits the impact of the GDPR. She argues that heightened protections, like explicit consent, are foregone in the initial stage of data collection and storage (such as when a photo is uploaded to a social media site) and that use of such data without consent is often permitted by particular exceptions for law enforcement agencies after such data has been collected.  

The exclusion of photographs and voice recording is also troubling given the realities of how commercial and government surveillance systems are developed and deployed today. The harvesting of face images matched to individual names from the web is a common method used to create face-name databases. These databases are the foundation of sophisticated and covert surveillance tools created by private firms, who often do so in secret and proceed with almost no oversight.20 The same covert surveillance practices are emerging with voice recordings.21  

The definition of biometric data offered in the California Consumer Protection Act (CCPA) of 2018 stands apart from existing definitions and could be instructive as a way to close this loophole. Rather than the current representation of the data, CCPA’s definition focuses on the ability to extract an identifier template that can be algorithmically processed in order to determine whether it falls within the scope of the law.22  

# Why have data-protection laws had limited effectiveness in curbing the expansion of biometric surveillance infrastructure by government?  

Principles of data minimization and purpose limitation have rarely been applied to challenge the creation or expansion of biometric systems. Rather than an evidence-based scrutiny of the link between the means and the ends, the broad rationale of security and efficiency in service delivery has usually served to enable rather than restrict the use of biometric systems.  

Necessity and proportionality are common legal principles in international human rights law and reflected in a number of data-protection laws across the world.23 They require that any infringement of privacy or data-protection rights be necessary and strike the appropriate balance between the means used and the intended objective. The proportionality principle is also central to constitutional privacy case law across the world, and while there are regional differences, these tests generally involve a balancing exercise where the right to privacy is balanced against a competing right or public interest.24  

In data-protection regulation, these principles are reflected in the types of data categories that are collected,25 how the data can be used,26 and how long it can be stored.27 Under the GDPR and similar data-protection laws, the “data minimization” provision in Article 5 requires that entities limit personal data collection to that which is ‘’adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed.” For law enforcement agencies, the Data Protection Law Enforcement Directive (DP LED) requires a higher standard of whether that biometric data collection is “strictly necessary.”28  

Taken seriously, these provisions question whether the collection of biometric data is necessary in the first place.29 For example, the Swedish Data Protection Authority outlawed the use of facial recognition in schools on the grounds that its use for attendance was a disproportionate means to achieve this goal when far less intrusive means exist.30 The French Data Protection Authority (Commission Nationale de l’Informatique et des Libertés, or CNIL) and the regional court of Marseille also ruled similarly to declare the trial of facial recognition attendance systems illegal in France.31  

In Ben Hayes and Massimo Marelli’s chapter, they explain how the International Committee of the Red Cross (ICRC) applied data-protection proportionality principles to the use of biometrics for aid distribution to people in need of humanitarian assistance. While the ICRC eventually determined that there was a “legitimate interest” in using biometric systems for this purpose, they limited the use to a “token-based system” (i.e., a card on which people’s biometric data is securely stored). The ICRC decided not to collect, retain, or further process people’s biometric data, and therefore not to establish a biometric database. If people want to withdraw or delete their biometric data, they can either return the card or destroy it themselves.  

Unfortunately, the application of these principles to challenge the creation of biometric systems and databases is rare, especially during the key initial or “pilot” stages before these systems are built and used.32 More often than not, inquiries into “necessity” are structured to enable rather than restrict the use of biometric systems. Even where data minimization principles exist, the notoriously broad but powerful rationale of “efficiency” or “law and order” and “national security” serve to grant most government uses of biometrics a free pass without any evidence-based scrutiny of the relationship between means and ends.33 As noted in the European Digital Rights (EDRi) 2020 report on biometric mass surveillance, this uneven application of the law can also be attributed to the European Union’s inadequately resourced and politically disempowered National Data Protection Authorities. On the other hand, in countries that still lack data-protection laws and data-protection authorities (DPAs), when biometric ID projects have faced constitutional challenges in the Court, the proportionality test is often overlooked in favor of broad claims around the efficiency of biometric service delivery systems, with scant analysis of alternative, less rights-infringing means to achieve that goal.34  

Legal principles of “purpose limitation” are often ineffective given the broader political and institutional trends working to dissolve boundaries between civilian, criminal, and immigration biometric databases. Driver’s license face databases are a key site for this kind of “function creep” and require urgent policy intervention.  

The “purpose limitation” principle restricts the use of data for purposes beyond what it was originally collected for; a specified purpose must not be used for another “incompatible” purpose. Yet pervasive “security” imperatives often blur the boundaries between criminal, welfare, and immigration processes and, consequently, obfuscate what is perceived and understood as a “compatible” purpose. Under the US federal Secure Communities program (S-COMM), states submit fingerprints of arrestees to criminal as well as immigration databases, allowing Immigration and Customs Enforcement (ICE) to access this information.35 ICE has also requested face recognition searches of driver’s license databases in multiple states in the US.36 In Australia, the Home Affairs department has been centralizing state driver’s license face databases to use for broader policing and law enforcement purposes.37 India’s biometric ID project Aadhaar is  

widely known as a welfare delivery system, yet government officials may use the data for national security purposes in limited circumstances,38 and the National Crime Bureau has publicly stated their desire to use the system for criminal investigations.39 These systems are structured to evade and remove the purpose limitations on data use.  

The failure of proportionality safeguards is also borne out in the context of centralized biometric ID systems where legislation has frequently been introduced only after these systems are developed, and in some cases after they’re already deployed and in use.  

Large-scale biometric ID projects that span welfare, criminal, and immigration contexts have typically been implemented as technocratic exercises driven by executive agencies, often with the glaring absence of law. Even as advocacy efforts focus on demanding legal frameworks to ensure legislative and public scrutiny, legislation often comes too little, too late. For one, many projects do not receive proper scrutiny or are passed through extraordinary measures that forgo scrutiny altogether.40 In other cases, weak procedural safeguards are proposed, but the broader centralization of power in a few agencies remains unchallenged.  

In Jake Goldenfein and Monique Mann’s chapter, they argue that the Australian Identity Services Bill provided the Home Affairs department with authorization to become the central node (“the hub”) through which all identity and suspect identification requests would be routed. They conclude that “a true proportionality analysis” might have questioned whether a centralized facial recognition database was in fact necessary to address the stated purpose of curbing identity fraud, but in reality “this framing is operationalized in ways that enable continuing expansion of surveillance systems.”  

The mere existence of procedural safeguards like data security or consent can obscure the root of the problem, only serving to legitimize the continued existence of these systems. When faced with existential threats, like the potential of being invalidated by the highest courts, data-privacy rules have repeatedly been held up as an adequate safeguard against the concerns raised, leading to widespread skepticism about the role these laws play.41 In the case of India’s nationwide biometric ID project (Aadhaar), legislation authorizing and regulating the project came nearly a decade after biometric data collection began. This massive delay is even more concerning given the absence of a data-privacy law that applied to government agencies. In her contribution to this compendium, Nayantara Ranganathan challenges foundational assumptions about the role of the law in relation to these projects, characterizing regulation as a legitimizing force that reflects the interests of the powerful actors that drive these systems. She argues that Aadhaar’s regulation functioned to “consolidate the developments of the first seven years of the project, and also presented a revisionist history of the actual goals of the project, obscuring the stakes for private interests...[M]any of the problems with Aadhaar should not be understood as failures of law or regulation, but products of law and regulation.”  

# Is meaningful notice and consent possible in the context of biometric systems? What are the limitations of a consent-based approach and what supplements or alternatives might be required?  

Given the predominance of the data-protection approach, notice and consent has been a cornerstone of biometric regulation, yet the well-documented limitations of this model underscore the need for additional necessity and proportionality limits even after consent has been obtained. Recent AI legislation also requires broader “explainability” requirements as a core component of meaningful notice.  

While notice and consent has traditionally been the cornerstone of data-protection and privacy approaches globally, its limitations have been laid bare in recent years, leading to skepticism about (if not outright rejection of) the idealized conception of “individual control.”42 In their chapter, Ben Hayes and Massimo Marelli explain why the Red Cross removed consent as a legal “ground of processing”43 in emergency humanitarian contexts where, the authors argue, consent can never be assumed to be “freely given.”  

At the same time, the individual’s right to refuse or revoke permission for the collection or use of their data has been an important tool in challenging biometric systems like live facial recognition in public spaces that are designed to evade such active permission. As described in Woodrow Hartzog’s chapter, under BIPA, the failure to obtain consent from individuals before using their biometric data has led to several successful lawsuits against some of the largest tech companies in the world and is the basis for the lawsuit recently launched against Clearview AI.44  

In the GDPR, consent is supplemented by several general limits of proportionality and necessity45 that hold irrespective of whether consent is obtained. By contrast, US state laws like BIPA focus on notice and consent with few additional restrictions on collection or use beyond the prohibition against selling biometric data for profit and limits on retention.  

As Hartzog concludes, BIPA has done “very little to bring about the kind of structural change and substantive limits necessary.” For one, he explains how most of us are simply “not capable of meaningfully exercising our agency over modern data practices” and argues that BIPA provides little protection from the “post-permission risks” of biometric technologies.46 This underscores the need for additional transparency and accountability, including bright-line restrictions alongside a robust notice and consent regime.  

Emerging regulatory approaches for algorithmic or AI systems include a broader understanding of notice that goes beyond simply informing the individual that algorithmic tools are being used. These newer approaches also take into account how these systems work, the context in which these systems are used, and what criteria are informing algorithmic decisions. This broad scope will be especially valuable in regulating biometric systems that serve purposes beyond identification and verification. The Illinois Artificial Intelligence Video Interview Act is an example of a notice provision tailored to the specific context of job interviews; it requires that all job applicants be informed when AI systems used to assess their performance as a candidate are deployed during interviews. In addition to this, it requires that each applicant be provided clear information about “how the artificial intelligence works and what general types of characteristics it uses to evaluate applicants.”47 Whether such explanations are possible, and whether they can work to inform meaningful choices on the part of job seekers given the power dynamics at work in the context of a job interview, have yet to be seen.  

# SECTION 2. BEYOND PRIVACY: ACCURACY, DISCRIMINATION, HUMAN REVIEW, AND DUE PROCESS  

How should regulatory frameworks address concerns about accuracy and non-discrimination in biometric systems?  

To what extent should regulation rely on standards of performance and accuracy set by technical standards-setting bodies?  

While accuracy and discrimination concerns are at the forefront of public debate, corresponding legal protections have been rare in existing regulatory frameworks. However, recent legislation and advocacy efforts in the US have mandated accuracy and nondiscrimination audits for facial recognition systems, going as far as to require such audits as a condition for lifting a moratorium on use.  

While technical standards (e.g., NIST’s Face Recognition Vendor Test) are evolving to account for bias and inaccuracy, they generally underperform in “real-life” contexts and are limited in their ability to address the broader discriminatory impact of these systems as they are applied in practice. If such standards are positioned as the sole check on facial recognition systems, they could function to obfuscate, rather than mitigate, harm.  

Accuracy and “error rates” metrics are a staple of the mainstream conversations around biometrics and are used as a tool in the machine learning field to compare systems and assess progress. Accuracy claims have been a simple way for those developing, marketing, and applying these systems to “prove” effectiveness, and to demonstrate that automation offers an improvement over manual processes. In the past two years, however, the same facial recognition systems that boast high accuracy rates according to such narrow metrics have been shown to perform less well when accuracy rates are stratified across demographics like age, race, gender, and disability.48 “Errors” in these systems are not evenly distributed, and reflect historical patterns of racism, gender bias, and ableist discrimination. To remedy this problem, researchers have called for auditing on accuracy across specific demographic and phenotypic subgroups, accompanied by measures that can close performance gaps where they arise.49  

To accomplish such audits, many are turning to technical standard-setting bodies that set benchmarks for accuracy, performance and safety. Auditing protocols like the National Institute of Standards and Technology (NIST) 2019 Face Recognition Vendor Test (part three) evaluate whether the algorithm performs differently across different demographics in the dataset.  

Regulators and lawmakers have also begun to take notice, calling for audits by technical standards-setting bodies that set benchmarks for accuracy, performance, and safety. In March 2020, the UK Equality and Human Rights Commission called to suspend the use of facial recognition in England and Wales until discrimination against protected groups has been independently scrutinized. Recent legislation in the US includes accuracy and nondiscrimination audits as a condition for the use of facial recognition. The Washington State Bill SB 6280, passed in March 2020, requires that face recognition companies cooperate to allow for independent testing for “accuracy and unfair performance” across subgroups including race, skin tone, ethnicity, gender, age, or disability status. If independent testing reveals “material unfair performance differences,” companies are required to rectify the issues within ninety days. Another proposed federal bill (S.2878: the Facial Recognition Technology Warrant Act of 2019) requires federal law enforcement agencies to work with $N|\mathrm{S}\top50$ to establish testing systems to ensure consistent accuracy across gender, age, and ethnicity.  

While these standards are a step in the right direction, it would be premature to rely on them to assess performance, and they do not adequately capture the broader discriminatory impacts these systems might have when they are used. First, researchers and advocacy organizations have found that many of the systems that “pass” current benchmark evaluations continue to underperform in real-life contexts.51 Additionally, there is currently no standard practice to document and communicate the histories and limits of benchmarking datasets, and thus no way to determine their applicability to a particular system or suitability for a given context.  

Moreover, creating a solely technical threshold to judge discriminatory impact can distort the biased practical implementation of these technologies and their weaponization against specific groups. For example, facial recognition systems are deployed disproportionately in minority communities, so even the most accurate systems will be discriminatory. They also “run the risk of providing ‘checkbox certification,’ allowing vendors and companies to assert that their technology is safe and fair without accounting for how it will be used, or its fitness for a given context.”52  

# Does requiring “meaningful human review” of biometric recognition systems ensure oversight and accountability?  

Recent legislation includes provisions that mandate “meaningful human intervention” in the results of biometric systems. However, a large body of research suggests that the people who review the results of biometric systems overwhelmingly overestimate credibility, and often respond inaccurately and with bias.  

“Human intervention” in automated decisions has gained considerable acceptance as a legal approach to provide a meaningful check on the potential harms these systems represent. Article 22 of the GDPR, for example, includes a restriction on “solely automated decisions,” and requires human intervention when automated systems impact “legal or similarly significant” decisions about people’s lives. The recently passed and heavily criticized53 Washington State facial recognition law similarly includes provisions for “meaningful human review” and periodic officer training as conditions for the use of biometric technology. Human review is defined in terms of “review or oversight by one or more individuals...who have the authority to alter the decision under review.”54  

However, a large body of research demonstrates that human intervention in these systems does not address major concerns about transparency or control. Individuals who review results are often unable to accurately evaluate the quality or fairness of the outputs, and often respond to predictions in biased and inaccurate ways.55 The ACLU has pointed to the imprecisely defined notion of meaningful human review as “deeply flawed” given its vague definition. They maintain that it should not become a rubber stamp that allows for the use of facial recognition or similar systems in sensitive social domains like welfare and criminal justice.56  

In their chapter, Peter Fussey and Daragh Murray show that human operators who asses live facial recognition “matches” often defer to the algorithm’s output, despite the known inaccuracy of such output—a phenomenon referred to as “automation bias.” In their research, they found that “humans overwhelmingly overestimated the credibility of the system.”57 The Indian government established a system of “manual overrides” to address the issue of biometric errors that lead to exclusion from government benefits and systems.58 However, studies suggest that even these legal norms did not always govern the behavior of those operating the biometric systems on the ground. Those managing these systems often failed to exercise this option and refused people access to services because of “‘incorrect’ (or rather complete lack of) human intention in overcoming technological failure.”59  

An open question for future legal approaches is how to incentivize and ensure real capacity for human oversight. This would include an assessment of the gaps in knowledge, biases, or inefficiencies that limit accountability and prevent human operators from assessing or anticipating problems with these systems.  

Should regulatory frameworks create a risk-based classification between “identification” and “verification” uses of biometric recognition?  

What are the potential risks of a permissive regulatory approach to verification?  

Recent official policy documents in the EU suggest that “verification” (1:1) is an inherently less risky use compared to identification (1:n) in terms of accuracy, data security vulnerabilities, and the capacity for meaningful consent.  

However, any broad-brush permissive approach to verification in the law should be avoided. Even if participation in a verification system is with knowledge, these systems might not afford individual’s real choice when they act as gatekeepers to access essential spaces or services.  

The distinction between verification and identification is often described in terms of the technical shorthand 1:1 versus 1:n. 1:1 verification (or authentication) aims to determine whether people are who they claim to be through a one-to-one match that queries biometric information (e.g., a facial scan by a smartphone) against the data that the person has previously provided (e.g., the person stores their photograph on the phone when they first purchase it).60 Identification, or 1:n, is a more technically involved process that compares the biometric information of an  

unknown person against a database of many people’s biometric data. An algorithm determines if the person is represented in the database and who they might be. Some identification systems provide a number of “similar faces” that meet a specified confidence or accuracy threshold.61  

Recent official policy documents62 as well as data-protection authorities in the EU63 suggest that verification is an inherently less risky use of biometrics in terms of accuracy, data security vulnerabilities, and the capacity for meaningful consent. Biometric locks on phones are a common example used to demonstrate these claims, and San Francisco recently amended its facial recognition moratorium to allow employees to use biometric lock features on governmentissued cell phones.64 By contrast, some of the most controversial reported cases of facial recognition largely pertain to identification (1:n) systems like live facial recognition (LFR), which has a record of high error rates.  

These accounts of verifications often link or even conflate the claim of higher accuracy with meaningful consent. The claim is that with verification systems, people are willing to present their biometrics in a “cooperative” way (like a frontal face with eyes open), whereas with identification, people could be unaware of being identified, which increases the error rates.65 Any general assumption that verification systems involve the active and targeted participation of the individual, however, rests on shaky foundations. While these systems might have higher accuracy rates than identification systems, they are still predictive and not immune to the same kinds of errors and biases across lines of race, gender, and other demographic traits. More importantly, even if participation is done with volition and knowledge, these systems might not afford individuals real choice when they act as gatekeepers to access to essential spaces and services. This became a focal point in the opposition against biometric ID systems in India and Kenya,66 as well as in the use of biometric systems in humanitarian contexts.67  

Proponents of permissive approaches to verification typically argue that these systems involve local data storage, which minimizes the data security risks that come with centralized databases. However, access control at borders, airports, and buildings often centralize biometric authentication systems for access to services, and many of these systems maintain centralized storage and authentication records.68 Risks associated with biometric use are certainly contextual, but any broad-brush permissive approach to verification in the law should be avoided, especially where it can create loopholes that allow more harmful implementations of verification.  

# What kinds of due process safeguards are required for law enforcement use of biometric recognition?  

Should law enforcement have access to these systems to begin with?  

Outside of a complete ban on law enforcement use, recent regulatory approaches have focused on strengthening due process safeguards. This includes requiring warrants for ongoing surveillance, restricting the use of facial recognition to serious crimes, and ensuring defendants get meaningful access to biometric evidence that is used against them.  

While facial recognition has received special regulatory attention, these tools should be understood as part of a broader set of algorithmic police surveillance tools, including drone surveillance, license plate recognition, and predictive policing.  

The use of biometric technologies in policing raises a range of legal issues, many of which have been debated and litigated over the years in the context of fingerprinting and DNA.69 These include the conditions under which biometric data can be taken (whether it should be at arrest or upon conviction), and the circumstances under which it should be deleted from such databases (for example, if a person is never convicted or if a conviction is overturned). The increasing shift to use of face and voice identifiers has exacerbated some of these existing concerns and created new ones. Indeed, law enforcement use of facial recognition has been the subject of intense public and regulatory scrutiny recently. These systems have misidentified people and been disproportionately used to target communities of color. Moreover, the vast majority of cases involving face recognition searches are not disclosed, depriving defendants of the ability to challenge evidence that could determine their fate in criminal trials.70  

Outside of complete bans, there are multiple proposals that seek to regulate different aspects of law enforcement use. In their chapter, Jameson Spivack and Clare Garvie outline emergent regulatory approaches in the US that focus on limiting the use of facial recognition. Some limitations are based on the seriousness of the crime (e.g., only for violent felonies), while others ban the use in conjunction with body cameras or drones. There are also bills that would require a court order to run facial recognition searches,71 as well as one that would require that defendants have access to source code and other information necessary to exercise their due process rights when algorithms are used to analyze evidence in their case.72  

While facial recognition has received special regulatory attention, these tools should be understood as part of a broader set of algorithmic surveillance tools, including drone surveillance, license plate recognition, and predictive policing.73 These systems raise similar challenges for established principles around procedural fairness, such as notice, hearing, the disclosure of evidence, establishing reasons for decisions, and the ability to challenge these decisions.  

Law enforcement use of live facial recognition (LFR) has been the subject of intense public and regulatory scrutiny. Advocacy demands range from requiring a specific authorizing law to calls to ban law enforcement use of LFR altogether.  

Live facial recognition systems in public spaces are particularly controversial. Typically, cameras are deployed at a fixed location and the list of people who are identified is communicated to law enforcement officers on the ground.74 Despite LFR’s implications for privacy, criminal due process, and freedom of speech or expression, these tools have largely been rolled out without undergoing public and parliamentary scrutiny.  

In their chapter, Peter Fussey and Daragh Murray describe London’s expansive LFR program,75 and discuss how the London Metropolitan Police successfully argued before the High Court that LFR was part of their inherent powers, and thus did not need new legislation to explicitly authorize its use.76 The case is on appeal, but one of the factors that contributed to the Court’s decision was the notion that LFR was not “invasive” technology and therefore did not require special sanction. Buenos Aires has also conducted an expansive LFR program.77 In this case, the municipal government pushed through a resolution with truncated processes that authorized the use of these systems with minimal safeguards. Advocacy organizations have challenged the constitutionality of this ordinance.78  

Increasingly, privacy advocates are calling for a complete ban on LFR, viewing it as incompatible with fundamental rights and entailing risks that cannot be mitigated through procedural safeguards.  

Are systems that process bodily data for purposes beyond establishing individual identity, like making inferences around emotional state, personality traits, or demographic characteristics, covered under existing biometric regulation?  

Should such systems be permitted at all, given their contested scientific foundations and mounting evidence of harm?  

Since many emotion recognition and personality prediction systems rely on face and voice data that could be used to identify an individual (even if that is not its current purpose), these systems could fulfill the definitional threshold of data-protection laws like the GDPR. Many recent moratorium bills in the US include systems that infer “emotion, associations, activities, or the location of an individual.”  

However, many organizations are calling to ban these systems altogether given discredited scientific foundations and mounting evidence of harm.  

It is unclear whether existing biometric regulation will apply to systems where the primary purpose is to infer emotional states, interior characteristics, or identities like gender, race, ethnicity, and age.79 The fact that these systems rely on face or voice data that could be used to confirm or establish an individual’s identity (even if that is not its current purpose) could mean that these systems fulfill the definitional threshold of biometric data under data-protection laws like the GDPR. The European Digital Rights Initiative (EDRi) has also argued that biometric processing under the GDPR should be interpreted to include “detection of appearance, inferred behavior, predicted emotions or other personal characteristics.”80  

Many recent moratorium bills in the US include systems that use facial data for broader inferences, such as inferring “emotion, associations, activities, or the location of an individual.”81 The 2019 moratorium bills introduced in New York82 and Washington83 include any automated process by which characteristics of a person’s face are analyzed to determine “the person’s sentiment, state of mind, or other propensities including, but not limited to, the person’s level of dangerousness.” In specific contexts, these systems will require additional norms around explainability or transparency about how inferences are made, such as in the Illinois AI Videoconferencing Act 2019, which regulates the use of these tools in hiring.  

# SECTION 3. EMERGING REGULATORY TOOLS AND ENFORCEMENT MECHANISMS  

What are the different types of “bans” and moratoria that have been passed in the US over the last few years?  

How can moratoria conditions be strengthened to ensure that eventual legislative or deliberative processes are robust?  

How will bans and moratoria on government use impact the private development and production of biometric systems?  

Over the past few years, a wave of municipal legislation has sought to ban government use of facial recognition in the US, and some states have also proposed similar bills. Many of these bans focus on law enforcement use. While some large tech companies have come out in favor of regulation, they have consistently pushed back against bans, often favoring much less stringent approaches.  

The term “moratorium” is shorthand for a range of regulatory interventions with varying conditions for when the restrictions would be lifted—from straightforward time-bound goals for drafting and authorizing legislation to the establishment of deliberative, consultative processes. Some moratorium bills prescribe specific conditions to ensure the quality of the legislation and meaningful community participation in any deliberative process.  

Many cities and states in the US have recently introduced legislation that bans government use of facial recognition, with a primary focus on law enforcement use.84 These legislative interventions have played an outsize role in shaping the regulatory landscape by introducing a complete prohibition as a regulatory option against which other, less strict interventions will be compared. As Jameson Spivack and Clare Garvie point out in their chapter, advocates have been critical of weaker regulatory bills for “using up available political capital” and potentially undercutting demands for bans in the future.85  

Some of the largest technology companies that develop and sell these systems to law enforcement have been deeply engaged in these legislative processes, often publicly championing the need for some “regulation” but simultaneously lobbying against moratoria and bans. For example, Microsoft celebrated Washington State’s SB 6280 (“Finally, progress on regulating facial recognition,” Brad Smith, the company’s general counsel, announced), only to face questions and criticisms about their involvement in pushing through a law that was considered weak by many organizations, and that effectively undercut a potential ban on government use.86  

Moratoria and bans are often used interchangeably, yet Spivack and Garvie argue that this shorthand conceals a wide spectrum of regulatory interventions. Moratoria, in particular, contain a range of approaches that vary widely in terms of strictness and the conditions for lifting restrictions. While some moratoria stop all use of face recognition for a predetermined time, there is a risk that the legislature fails to act before the period is over and facial recognition use recommences without any further legislative intervention. On the other hand, directive moratoria ban the use of facial recognition until a law is passed and/or a statutory body (e.g, a task force or committee) is formed to submit recommendations for what to include in the law.  

Moratoria can work to fast-track a deliberative or legislative process where one might not otherwise have been possible. While this is welcome, it is also eventually susceptible to the vested public and private interests that will push for weak or no legislation. There is a risk that a task force created by these laws “may not be representative of affected communities; may lack authority; or may be inadequately funded.”87 Some moratoria do more to prevent weak regulation than others. A 2019 Massachusetts law sets minimum requirements for what future legislation should achieve, including data privacy safeguards, auditing requirements, and protection for civil liberties. Similarly, the recently passed Washington State law specifies that the legislative task force be comprised of “advocacy organizations that represent consumers or protected classes of communities historically impacted by surveillance technologies including, but not limited to, African American, Hispanic American, Native American, and Asian American communities, religious minorities, protest and activist groups, and other vulnerable communities.”88  

# What regulatory tools can be used to create public transparency around the development, purchase, and use of biometric recognition tools?  

Transparency around the development, purchase, and use of biometric recognition tools remains a key barrier to creating public awareness and enforcing existing regulation. Recent advocacy demands include mandatory impact assessments, public notice comment periods, and publicly accessible registries of vendors and uses.  

Enforcing existing regulations has been a challenge in part because the development, purchase, and use of biometric tools is often shrouded in secrecy, driven by private firms that have no duty to reveal such “proprietary information.” Once these tools are built, the purchase and subsequent implementation by government, particularly law enforcement agencies, proceeds in ways that are often deliberately hidden from the public. Yet as scrutiny of Clearview AI and subsequent investigations have made clear, there are hundreds of globally distributed vendors selling biometric recognition technology without people’s knowledge or explicit consent. It was only when the Clearview AI story broke that lawsuits were filed under Illinois BIPA, prompting quick action from the company that had violated informed consent requirements when scraping millions of face images off the web.89  

In recent years, privacy advocates have demanded regulatory tools that ensure transparency as early in the process as possible. Many of these policies target government use to ensure that there is public notice and consultation before these tools are acquired and implemented. For example, in June 2020, after years of civil society advocacy, and in the context of sustained protest against anti-Black police brutality, New York City passed The POST Act, a law that would require the New York Police Department (NYPD) to issue a surveillance impact and use policy about any surveillance technology in use (including biometric recognition tech).90 This assessment would include information about capabilities, processes and guidelines, and any safeguards and security measures in place. In the EU, advocacy organizations like Access Now and Algorithm Watch have called for a mandatory disclosure scheme for all AI systems used in the public sector, in conjunction with a mandatory human rights or algorithmic impact assessment.91  

Advocates have also demanded that regulation should ensure that external researchers and auditors have access to algorithmic systems in order to understand their workings, as well as the design choices and incentives that informed their development and commercialization, and to engage the public and impacted communities in the process. Meaningful access includes making software toolchains and APIs open to auditing by third parties.  

While advocates continue to push for more transparency, some laws have already enacted certain checks and balances. The GDPR currently has provisions for data-protection impact assessments (DPIA) and “privacy by design” assessments that kick in when there is any “large-scale” processing of biometric data and also in cases of surveillance in publicly accessible spaces. In theory, these offer a robust assessment of the rights implications of the use of these systems, including fundamental questions about necessity and proportionality. However, as Els Kindt notes in her chapter, DPIAs have been challenging to implement in practice, with wide variations across different member countries of the EU. Moreover, the predominant focus on data-protection concerns can leave out inquiries about accuracy or discriminatory impact. Recent proposals around algorithmic impact assessments (AIAs) are structured to include this broader range of concerns and ensure the participation of directly impacted communities in the risk-identification process.92  

While transparency and accountability measures have gained momentum, procurement contracts with third-party vendors can inhibit the government’s ability to comply.93 Government procurement of biometric and other forms of AI systems is often confidential due to trade secrecy or other intellectual property claims. When challenged, governments have denied any knowledge or ability to explain and remedy the problems created by these systems. Recent advocacy by civil society organizations and certain city governments in Europe focuses on including standard contractual clauses in these contracts that include waivers to trade secrecy, non-disclosure agreements, or other confidentiality clauses, as well as terms that ensure the process of procurement involves open bidding and public notice.94  

# What role can community-led advocacy play in shaping the priorities and impact of regulation?  

Community advocacy to regulate biometrics is growing, playing a crucial role in surfacing evidence of harm, and shaping the rights and protections that policy interventions eventually offer.  

Advocacy and mobilization against the use of biometric systems have taken many forms. While traditional digital rights or privacy groups remain active, over the past few years, directly impacted communities have also organized to push back against these systems based on their lived experiences of harm.  

In India, a coalition of privacy groups and grassroots welfare activists formed to publicly protest and legally challenge the Aadhaar biometric ID project.95 Against the broad claims of efficiency by the government, the coalition surfaced specific examples of exclusion due to the technical and bureaucratic failures of the system. In their chapter, Stefanie Coyle and Rashida Richardson recount the community-driven advocacy in Lockport, New York, where a group of parents organized against the school district’s decision to purchase and deploy facial recognition in schools. Eventually, Coyle and Richardson note, “[p]arents shifted the discourse from debating whether the biometric surveillance system was necessary to focus on the real harms posed to students if the school district decided to move forward.” As a result of that advocacy, the New York State Senate introduced a moratorium bill that “mirrors the concerns raised by residents in the community and advocates across the state and country.”  

Large-scale biometric projects are often promoted in terms of lofty claims about security, accuracy, and efficiency. Community advocacy, particularly on the part of those directly impacted by these systems, has been critical in surfacing key questions like: Efficiency for whom? (In) security for whom? Those required to live under biometric surveillance possess an expertise that cannot be gained by examining these systems at a technical or policy level. There is no way to guarantee the just use of these technologies without centering the experiences of those affected by their use. Recent attempts demonstrate how community interventions can be structured, for example, the “Citizen Biometric Councils” run by the Ada Lovelace Institute in the $\mathsf{U K},^{96}$ and the New York City ADS Task Force “Shadow Report” prepared by a civil society coalition with detailed recommendations to ensure community engagement is meaningful and equitable.97 Ultimately, this underscores the importance of community deliberation to the processes that decide whether these systems are used, but also to the kinds of rights and protections that policy interventions eventually offer.  

![](images/371d290d4864f52d391c0b6e75f6f4cd744ccea55ebdb9c7899e0154aa146c2f.jpg)  

![](images/4bc49a4353253bb42d47c65941ab503854df6c19fa08f10e12cb602d204695c6.jpg)  

![](images/f7342667c51a75fde939c0545b4de675851553865c9a7913643afbcabf312a56.jpg)  

# TIMELINE OF LEGAL DEVELOPMENTS  

This timeline tracks the key legal and regulatory developments analyzed in this compendium. The specific chapters where they are discussed are noted below.  

# October 2008  

![](images/8d0498936c651e3035837e1ac1275e6d1f86757a7b7515fd53ee7fc4e059d84d.jpg)  

United States   
Illinois Biometric Information Privacy Act (BIPA)   
enacted (See Chapter 8)  

# March 2016  

# India  

![](images/2854a9606014d33a809b9c0260c342df8148ac060fe194a3a27668a0218cb3df.jpg)  

Aadhaar Act enacted (See Chapter 3)  

<html><body><table><tr><td></td><td colspan="3"></td></tr><tr><td></td><td colspan="3"></td></tr><tr><td>2008</td><td>2016 2017</td><td>2018</td><td>2019</td></tr></table></body></html>  

# April 2019  

![](images/f2433ff630d8c973c7e7329d5b874556acfa1f8f89b2fc2c4390311d9166bf7f.jpg)  

Jamaica   
Jamaican Supreme Court rules biometric ID system   
unconstitutional (See Chapter 1)  

# May 2019  

![](images/e0b9e27dacfec55535c989a0e54fe3b14ffc5e5cf76f5d11e42372da6f10768d.jpg)  

United States San Francisco ban on government use of facial recognition technology passed (See Chapter 7)  

# June 2019  

![](images/d0c7f8ed76349e40cf259210babe809500f21c41a79ca50e7e2515fd6d062442.jpg)  

United States Somerville, MA ban on government use of facial recognition technology (See Chapter 7)  

# July 2019  

United States Oakland, CA ban on government use of facial recognition technology (See Chapter 7)  

![](images/7eeaa5550999cb911e0aa6bddade3fc29baf21cfffce85238f35337aee4f1b04.jpg)  

Australia   
Identity Service Matching Bill introduced (See   
Chapter 2)  

# Kenya  

Huduma Bill (legal authorization for NMIMS project) introduced (See Chapter 1)  

# April 2018  

![](images/77c723e8127b38aef602dc3dfe59901877b89cd3b96a7d3862e909bca24047ef.jpg)  

European Union   
General Data Protection Regulation (provisions on   
biometric data) enacted (See Chapter 4)   
Data Protection Law Enforcement Directive enacted   
(See Chapter 4)  

# September 2018  

![](images/4136bd3aa96e2fefff1341ffce86dfe40bd583ae5b2421e048a731ae9ade7b1c.jpg)  

India Indian Supreme Court restricts private use of Aadhaar Biometric ID system (See Chapter 3)  

# August 2019  

![](images/3f6ba9369be7cd4cd5babf62401a447c8044d2c8362f5a8553f9b58a685c4b44.jpg)  

International Committee of Red Cross ICRC assembly adopts Biometrics Policy (See Chapter 5)  

# September 2019  

![](images/c221fd3f26d19770d381c4913d7b46fb3d0cc8d21abcaae154ff75c20c88407a.jpg)  

United States   
California Body Camera Accountability Act   
(A.B 1215) (moratorium on existing use of face   
recognition on body-worn cameras till 2023) passed   
(See Chapter 7)   
United Kingdom   
UK High Court finds Live Facial Recognition   
permissible, rules out need for new authorizing   
legislation (See Chapter 6)  

![](images/f7104458eda6762ee9f3a9f0d5b43bf3648f9f1f07c96e5ed5521d68d8367f87.jpg)  

![](images/f42093570fd2eac67910b10b0e1eded69c79ec5a87dbcb70ef75c9ec4b996357.jpg)  

United States Justice in Forensic Algorithms Act of 2019 (HR 4368) introduced (See Chapter 1)  

# October 2019  

![](images/98dbd6a2abd3f8ae27e85ec3b0bdc086dee1a0a97957fe7ef46f0836caabb77e.jpg)  

United States   
No Biometric Barriers to Housing Act (S 2689)   
introduced (See Chapter 1)   
Australia   
Identity Service Matching Bill rejected by Australian   
Parliament (See Chapter 2)  

# United States  

Berkeley, CA ban on government use of facial recognition technology passed (See Chapter 7)  

![](images/04a3585af8a00846b1727488824211063ae05a1f1cc67febb153202b20486218.jpg)  

# Argentina  

Constitutional challenge to Buenos Aires Live Facial Recognition project (See Chapter 1)  

# November 2019  

![](images/c04202595ebd3045fdd70af603933820c002c23b7c993f8b51fbe68931c1d1e3.jpg)  

United States The Facial Recognition Technology Warrant Act of 2019 (S 2878) introduced (See Chapter 1)  

# December 2019  

![](images/c28fc95537f3c8b78d042a4ad741e32143090a998cec4eade99eb4136ba5e239.jpg)  

United States Northampton, MA ban on government use of facial recognition technology passed (See Chapter 7)  

# United States  

Alameda, CA ban on government use of facial recognition technology passed (See Chapter 7)  

# United States  

Brookline, MA ban on government use of facial recognition technology passed (See Chapter 7)  

<html><body><table><tr><td></td><td></td><td></td></tr><tr><td>2019</td><td></td><td>2020</td></tr></table></body></html>  

# January 2020  

![](images/2462f14e576ff6f35c44b9020365ae4115765302ad79321d686d81322fa0b194.jpg)  

United States Cambridge, MA ban on police use of facial recognition technology passed (See Chapter 7)  

![](images/08e987be6e6f45739e78f36e2d0fd185bb40c2709d70d747663c6e8e6b616c69.jpg)  

Kenya   
Kenyan High Court suspends NMIMS biometric ID   
project (See Chapter 1)  

# United States  

California Consumer Privacy Act (provisions on biometric data) enacted (See Chapter 1)  

# February 2020  

![](images/61c637680db5a38f1cbf978dddbb158c40d97374ea246b4c891585358585d284.jpg)  

United States Springfield, MA moratorium on government use of facial recognition technology passed (See Chapter 7)  

# March 2020  

![](images/a17e651dd13cbf851cef0f878c657126f1b4e0473081f7a3b61a7fb574e7d21e.jpg)  

United States   
Washington SB 6280 (regulates government use   
of facial recognition technology) passed (See   
Chapter 7)  

# June 2020  

![](images/28e353c1896944822877d0f30e798d6c40c4446e2412721c99a22bc5dae43746.jpg)  

United States Facial Recognition & Biometric Technologies Moratorium Bill  S 4084 introduced (See Chapter 7)  

United States   
New York Public Oversight of Surveillance   
Technology (POST) Act (Int 0487-2018) passed (See   
Chapter 1)  

# July 2020  

![](images/fe67ac8fba2ecc467d7e3fb98eb796e7acd916063b47cd4c8022c1c41a51d26f.jpg)  

# United States  

New York Senate Bill S5140B (regulating biometric technologies in school) passed (See Chapter 9)  

# August 2020  

![](images/3385e4445b7adf0b44e8aba8b726ae2771f8e2a1f722b26fd7c7b9b2c2e75abf.jpg)  

United States   
National Biometric Privacy Act (S ___) introduced   
(See Chapter 1)  

# Australian Identity-Matching Services Bill  

Jake Goldenfein (Melbourne Law School) Monique Mann (Deakin University)  

S itTnohcemsa2ek0e1ff7,aortctihsae,l  rpAeaucrstotrgoafn iata lnonfnetgde-tecerhranlmoglaonvgedyrcnmomonreteintwuhindages  yepxuapsvahinelsadiboflonertopofoscliutviriclvaealinladanpdcolele cpgiaonlwgceahrgasenbngyceitseh Australian federal government, have culminated in a new biometric identity-information system. Federal authorities have argued that facial recognition technology is useful for law enforcement and preventing identity fraud, but to achieve those benefits, they have combined civil and criminal, as well as state and federal, identity systems into a powerful intelligence apparatus controlled by a single government department: the Australian Department of Home Affairs.  

Home Affairs was created in 2017 through a merger of the Department of Immigration and Border Protection and the Australian Border Protection Service. As a result of the merger, Home Affairs assumed multiple policing and intelligence competencies from the Attorney General’s Department (AGD), including those related to national security, immigration, organized crime, cybersecurity, and public safety policing. Home Affairs also took over control and operation of the national identity-matching services, which included the one-to-one facial recognition verification system known as the “Face Verification Service” (FVS).1  

The Australian government has been developing the institutional, technical, and legal architecture for facial recognition capabilities for several years,2 culminating in the 2019 federal IdentityMatching Services Bill.3 The original bill was rejected, however, for a lack of privacy protection and oversight, and is presently being redrafted. The new bill will likely increase parliamentary oversight of the system and the amount of necessary reporting, but will not challenge the fundamental institutional changes that are already underway, such as the aggregation of civil and criminal systems, or increased control of state-level civic data within a federal intelligence system.  

Although governments have always had the function of identifying their citizens,4 they have not always linked those identities to intelligence dossiers or made them available to law enforcement agencies. Indeed, the intermingling of civil and criminal identity systems has been the concern of human rights jurisprudence for some time.5 Biometrics are of particular concern to the linkage of criminal and civil systems, and surveillance more generally, because they act as a conduit between an individual’s physical presence and digital databases, thus amplifying surveillance capacities. By advancing a centralized identity matching system, Australia is pushing beyond the limits of legitimate state function.  

# BIOMETRICS DEVELOPMENT IN AUSTRALIA  

Australia has collected biometric information, including images for facial recognition, since at least 2007. This began with border-protection agencies collecting information from noncitizens, such as people caught fishing illegally in Australian waters, and eventually from visa applicants. It has progressively expanded to include information collected from Australian citizens, both at the border and through civic licensing agencies.6 States have also used biometric systems for matching against their police information holdings (i.e., mug shot databases) since at least 2009.7  

The 2007 Intergovernmental Agreement to a National Identity Security Strategy8 proposed the development of a national biometric interoperability framework,9 which was launched in 2012.10 Plans for a further national facial biometric matching “Capability” to enable cross-jurisdictional sharing of identity information, the precursor to the identity matching system operated by Home Affairs, were announced in 2014.11  

The one-to-one face verification system (FVS) that Home Affairs took over from the AttorneyGeneral’s Department (AGD) began operating in 2016, but only included passport images held by the federal Department of Foreign Affairs and Trade (DFAT).12 Given the uptake of driver’s licenses in the general population and the ambition for a national system, the policy goal has long been to integrate state-controlled driver’s license images into a general database for policing and intelligence.13 Efforts by federal entities to access driver’s license images have been, however, frustrated by state privacy laws, which prohibit providing federal agencies direct access to their databases.14 The result has been limited and complex arrangements for cross-jurisdictional information sharing. This began to change, however, with the 2017 Intergovernmental Agreement on Identity Matching Services (IGA)15—the precursor to the Identity-Matching Services Bill—and the corresponding formation of the Department of Home Affairs, with its very broad federal policing and intelligence remit.  

# CENTRALIZATION OF IDENTITY DATABASES  

In 2017, the Australian states agreed multilaterally to enable federal access to their identity data under the auspices of the IGA. Some states made explicit the value they saw in the system, with the Queensland Minister for Police noting the value that one-to-many facial recognition would contribute to enhanced security at the Commonwealth Games.16 Other states were more reluctant, raising the alarm about possible contravention of state-level human rights protections, and suggesting that there were inadequate protections for civil liberties.17  

Nonetheless, the IGA established the framework for a data-sharing regime, gave immunity from state-level privacy laws, and introduced new identity-matching services, including a one-to-many facial identification service (FIS) to complement the FVS. Such systems are the primary facial recognition tool used in policing in Australia. The system allows for law enforcement, national security, and related entities at state and federal level to run queries through the technical infrastructure of a host agency: originally the AGD, and then the Department of Home Affairs. Importantly, while the IGA introduced a technical architecture for information sharing, it left control over identity databases with the states.18  

A few months later, the government introduced the Identity-Matching Services Bill, which ostensibly legislated for the IGA. In reality, however, the bill went significantly further, shifting the system from one that facilitated information sharing into one that enabled the aggregation and centralization of identity information in the Department of Home Affairs.  

This increased centralization is in no way integral to satisfying the objectives of the system, at least as publicly stated. The bill’s explanatory memorandum, for instance, outlined the primary goal as preventing fraud and identity theft (described as an enabler of organized crime and terrorism), but not to build an intelligence apparatus.19 Despite the limited technical capacity necessary to achieve that stated objective, the system specified in the bill would fold state-level transport authorities’ data and images into the data-intensive apparatuses of federal security and intelligence agencies.  

The centralizing dimensions of the system architecture become apparent when looking closely at the differences between the IGA and the bill. Beyond addressing identity fraud, we suggest these changes reveal the true underlying political rationalities and motivations for establishing this national facial recognition system as a radical shift in identity data governance arrangements.  

# LEGAL CONCENTRATION OF POWER  

The Identity-Matching Services Bill sought to establish Home Affairs as the “hub” through which government identity-verification and law enforcement suspect-identification requests are processed, establishing Home Affairs as the central point of information processing across the public sector and for law enforcement agencies. But there were meaningful departures from the system described in the bill and the 2017 IGA.  

The IGA outlined two technical architectures: 1) The National Driver License Facial Recognition Solution (FRS), a biometric identity image database; and 2) the “interoperability hub,” a communications system for processing and routing data access requests from agencies around Australia.  

In the IGA, the FRS was described as a federated database system, in which state-level data would be partitioned, and state agencies could control the conditions of access. Databases would be linked through Home Affairs, which would operate the facial recognition technology that performs identity matching. The FRS was described as retaining only biometric identity templates and no other identity or personal data. The IGA stipulated that the host agency (initially the AGD, but subsequently Home Affairs) could not view, modify, or update information in partitioned federated databases containing state-level information. However, the bill only prescribed that Home Affairs could not modify or update that data; in other words, it could still view it.20 In fact,  

the legislation clarified that Home Affairs could collect, effectively without limit, information flowing through the systems for satisfaction of its “community safety” purposes, which include law enforcement, national security, community safety, protective security, and road safety, along with identity verification. The bill effectively vested control over the databases of driver’s license images squarely within Home Affairs, and enabled unrestrained collection of information.  

With respect to the “interoperability hub,” the IGA described it as a “router” through which agencies around the country could request and transmit information to one another. That is, it could be used for “relaying electronic communications between bodies and persons for the purposes of requesting and providing identity-matching services.” Rather than simply routing information from place to place, however, the bill enabled Home Affairs to collect data flowing through the hub whenever an agency used an identification, verification, or information sharing service, both for the sake of operating that database,21 as well as for its identity and community protection activities.22 The bill thus enhanced the legal capacity of Home Affairs from an infrastructure provider into a data aggregator.  

Other important elements of the bill gave greater power than envisaged to the Department of Home Affairs. For instance, the bill enabled the Minister for Home Affairs to expand the powers under the regime without parliamentary oversight. Furthermore, the identity information that could be collected through those systems was far broader than anticipated by the ${|\mathsf{G A},^{23}}$ including information held by agencies that is about or associated with the identity document.  

It is difficult to identify a single rationale that may have motivated the changes between the IGA and the Identity-Matching Services Bill. New technological affordances associated with facial recognition may have animated interest in developing a comprehensive national system, especially considering international trends. The institutional culture and political power of the Department of Home Affairs may also have made centralization and the use of civil documents in intelligence investigation more feasible. Indeed, its participation in forms of intelligence work and political policing connects it to a policing tradition that has always involved information aggregation, not necessarily in line with traditional liberal political limits.24 That expansion of political and technological power is also consistent with Home Affairs’ broad portfolio.  

Australia lacks enforceable human rights protections at the federal level (though some states have their own independent human rights protections), which raises a number of issues and concerns with the centralization of data and surveillance capabilities within federal agencies. Under the Australian Constitution,25 crime control and criminal justice are a competency of the states, not the federal government. Policing agencies are historically restricted to identity matching against data in local policing information systems (such as mug shots), which  

have comprehensive rules and limits on retention.26 As Home Affairs moves to aggregate and centralize biometric data, it is violating privacy norms by way of “scope creep,” i.e., generating data for one government purpose (e.g., licensing drivers), and using it for another (e.g., policing or other punitive applications).  

# PJCIS REJECTS THE BILL  

Ultimately, the Identity-Matching Services Bill did not pass parliamentary scrutiny and was rejected by the Parliamentary Joint Committee on Intelligence and Security (PJCIS). But the specific issues that led to its rejection are unlikely to halt the system’s development. In fact, the rejection can be interpreted as an endorsement of the general system and the resultant centralization, subject to privacy and accountability “tweaking.”  

When the bill reached the PJCIS, it was rejected largely due to concerns that it would grant too much executive authority to the Department of Home Affairs, meaning that the Minister for Home Affairs could change rules without legislative oversight.27 The PJCIS also echoed the fears of privacy advocates around the possibility of a real-time, facial recognition-powered CCTV mass surveillance system which could end anonymity in public and stifle political action like protesting. The report also noted accountability issues like the absence of judicial warrant requirements, and the lack of a dedicated biometric oversight body (both of which exist in the United Kingdom).  

On a broader level, the PJCIS expressed anxieties around the system not being proportionate to the issues it purported to solve, or sufficiently privacy-protective. But those concerns were connected to possible problematic “uses” of the system, not the broader structural issues of data centralization or the aggregation of civil and criminal identity databases. Instead, there was general approval that this type of data sharing would occur subject to a binding legislative framework rather than through creative interpretations of law enforcement and security exemptions to privacy laws.28  

The PJCIS accordingly recommended redrafting the bill to make its function and purpose clearer to the ordinary reader, reduce Ministerial rule-making power, fund a biometric oversight commission, and require more comprehensive reporting.29 The PJCIS did not, however, completely reject the bill, the use of facial recognition technology, or the new data governance arrangements that would power the system.  

# FUTILITY OF AUSTRALIAN REGULATORY OVERSIGHT  

The Identity-Matching Services Bill is presently being redrafted, with the new text yet to be released. Nonetheless, the states continue to upload identity images to the system in anticipation of the law passing and the system developing along similar lines. One reason political review has failed to meaningfully challenge the general structure of the identity matching and facial recognition system is that the debate, especially as expressed in the PJCIS report, has taken up a “privacy versus security” framing. International human rights law requires that state surveillance be “reasonable” and “proportionate,” and this language clearly influenced the PCJIS.  

Under a human rights framework, to legitimately limit fundamental freedoms like privacy, a surveillance intervention must be directly related to, and the least restrictive measure for, the “necessary” purpose pursued. A true proportionality analysis might question whether such dramatic data governance rearrangements are necessary to address the stated purpose of identity fraud. In reality, however, this framing is operationalized in ways that enable continuing expansion of surveillance systems, especially in nations like Australia, where it is not backed up by actionable protections.  

When privacy is pitched against security, the benefits of centralization and surveillance technology to purposes like identity fraud are taken as given, and the question becomes: Which civil liberties are we willing to curtail or limit in exchange? Blanket data sharing for policing and intelligence agencies is thus readily accepted and normalized as a necessary response to crime and insecurity, subject to privacy balancing intended to curtail its most abusive and authoritarian dimensions.30 That framing fails to address the reality that the system fundamentally eliminates the need for the largest policing and intelligence apparatus in the country to justify its access to personal data that was previously distributed to the states. This goes beyond agencies using biometrics for their democratically constituted civic purposes (e.g., driver’s licenses), and beyond the stated intention of the bill (e.g., detecting identity fraud). By pushing this bill forward, Home Affairs is promoting facial recognition technology as a necessary solution to identity crime, while sidelining concerns around the institutional and data governance rearrangements that it claims are necessary for its introduction.  

From this position, it becomes impossible to challenge the construction of the surveillance system, or to fight the technical or institutional architecture, in any meaningful way. The institutional momentum also makes resisting significant data governance rearrangements difficult. One recent positive development, however, has been the Australian Human Rights Commissioner calling for a moratorium on the use of facial recognition technology as part of the Technology and Human Rights Project, which mirrors some international trends.31 However, it is uncertain what impact this will have on the design, development, and eventual deployment of facial recognition technology in Australia, especially considering the extent to which the infrastructure is already in place.  

Finally, technologies like Clearview AI, which has aggregated billions of identified images from the public web, complicate how to parse these developments.32 Private providers, not constrained in the same way, can undermine relevant privacy protections or political processes by secretly selling surveillance services to government, while using their own privately operated infrastructure. When governments procure those services, they bypass whatever regulatory or financial obstacles might have prevented or limited those developments by the state itself. To that end, it is at least admirable that the Australian identity matching regime will be implemented in law, subject to democratic process and parliamentary oversight. Nonetheless, even when that is the case, the purposes expressed to justify new facial recognition implementations for the sake of those democratic processes appear not to tell the full story. It remains imperative to identify and address the institutional realignments and data governance reconfigurations connected to technologies like facial recognition and not be distracted by any single new surveillance capacity.  

# The Economy (and Regulatory Practice) That Biometrics Inspires: A Study of the Aadhaar Project  

Nayantara Ranganathan (lawyer and independent researcher, India)  

T thdheaetGaboiavmesroenftmhpraeotnvtwi doifnuIlgnddsditaeonlrtaeiufibncicaothimeoednt rtfihocer naAfllaordrehmsaiadatrieonbntiso(.f1minTegthreircpiridonejtenstc, titryicsaplrsloecjdaenfcots ,r nan2cd0e0pn9throawtliotzgher for every individual resident in India, indexed alongside their demographic information and a unique twelve-digit “Aadhaar” number. India’s now-dissolved Planning Commission formed the Unique Identification Authority of India (UIDAI) to plan the project, as well as implement and perform regulatory functions.2 The scale and ambitions of the project are matched only by the long and rich history of resistance to it. Economists, technologists, people’s movements, and concerned citizens have questioned the amplified surveillance dangers, indignities from exclusion due to failures in biometric identification systems, and lack of institutional accountability.3 The project proceeded without any legal framework to govern it for seven years after its inception (government use of data in India is still not governed by any dedicated law).  

Responding to the glaring lack of accountability raised by public advocacy and litigation, the Indian government passed the Aadhaar (Targeted Delivery of Financial and Other Subsidies, Benefits and Services) Act4 in 2016, with negligible public or parliamentary debate.5 While the law provided some procedural safeguards around biometric data security and consent, the law should be understood as a part of a broader institutional and economic project to instrumentalize biometric information in the service of the data economy. This essay explores the continuing legal and regulatory complicity in constructing data as a resource for value extraction, and how regulatory practice mimics the logics and cultures of the technologies it seeks to regulate.  

# MAKING DATA MARKET-READY  

The law goes to great lengths to sustain the idea of biometric data as signifying truth, supporting and maintaining an infrastructure that is foundational for the data economy.  

# The Truth about Biometrics  

Early planning documents of the Aadhaar project refer to biometrics as a fundamental identity, while older forms of identification based on demographic information are considered “surrogates of identity.”6 Yet biometric information is also a class of media, offering representations of bodily attributes captured at a particular moment in time under specific material conditions, and of no greater epistemic caliber. However, when coupled with the moral timbre of truth, biometric information can perform the important function of instituting people as data points within databases. This allows datafication of flows like cash exchanges or road traffic to be easily mapped onto signifiers of “real” people within databases, making these newly captured and latent dataflows more meaningful and profitable.  

In the Aadhaar project, high-resolution photographs of people’s irises and fingerprints were collected at the time of enrollment into the database, along with standard photographs of faces.7 An equivalence between media artifacts captured about a person and their true identity might  

be common in popular parlance, but with Aadhaar, such an equivalence was crystallized in law.8 Yet this equivalence is neither natural nor logical. The jump from the material facts of these representational media to their revelatory quality is a tactical one that several actors in the data economy are invested in maintaining. Aadhaar intends to act as both a unique and ubiquitous9 signifier, offering itself as part of an “identity layer” that may then be used as a foundation for the datafication of realms like finance, taxation, healthcare, and education.10  

# Grooming Uniqueness as Truth  

For practical as well as ethical reasons, the use of biometrics as a stand-in for unassailable truth about people is suspect.11 But law and regulation have worked to prop up this fiction and attach market value to it, through the legally defined processes of “deduplication,” the mandatory updating of biometrics information, and the reputational coupling of demographic and biometrics data.  

Deduplication: At the time of enrollment in Aadhaar, all biometrics information is checked against every other entry in the database.12 Deduplication is often seen as a best practice in biometrics enrollment, but its role in solidifying assumptions about the nature or suitability of biometrics for purposes of identification or authentication is not equally recognized. Deduplication confirms the uniqueness of each entry’s biometric information, within the database and for the limited purpose of the database.  

Updating biometrics information and technology: While uniqueness is architected through deduplication, fidelity of the media at the time of enrollment to the biological attributes of individuals cannot be sustained for reasons like fingerprints and irises changing over time, as well as several types of fraud.13 Rather than questioning the wisdom of using biometric information as a fundamental identity and an authentication key, the law uses minor fixes while still equating biometric information with biological attributes. The law gives UIDAI powers to  

require Aadhaar holders to update their biometric information from time to time, at their own cost, “to ensure continued accuracy” and not, say, to correct the inevitable deterioration of fidelity of biometric information.14 The law even anticipates, supports, and relies on ever-better biometrics technologies, bridging any imagined distance between a thing and its representation.15  

Lending truth to demographic data: Biometrics’ reputation of truth, and its resultant market value, are also transposed onto the corresponding demographic information (like name, gender, address) and the unique twelve-digit Aadhaar number generated.16 However, such demographic data does not benefit from the same heightened data-security protections,17 is unverified, and is unaudited.  

# KEEPING DATA MARKET-READY  

Aadhaar has been described as “a government programme run with the energy of a private sector start-up,”18 and is emblematic of the close cooperation between private actors and UIDAI. As a result of these close ties, regulation of the Aadhaar project enacts itself as cybernetic feedback loops that are constantly adapting to unfavorable changes, optimized toward keeping an infrastructural building block of the data economy alive.  

# Aadhaar as a Building Block  

From the outset, the UIDAI envisioned Aadhaar as an identity “platform”: an infrastructure that would provide authentication and verification services, and satisfy a necessary precondition for the data economy to thrive.19 Indeed, the law emphasizes the importance of Aadhaar as a source of identification for the marginalized, and to enable efficient and targeted welfare delivery.20  

However, Aadhaar’s market function has been understated; early documents indicate that the project was preoccupied with its role of instituting people as data points within existing and new databases.21  

At the outset, this authentication infrastructure took the form of application programming interfaces (APIs)22 for use by government agencies and third parties, for verification and authentication of identity information.23 Such an instrumentalization of the Aadhaar database not only drastically reduced the costs of performing door-to-door verification required of banking and telecom service providers, but also held the promise of entirely new use cases for businesses.24  

These APIs are part of “India Stack,” a growing set of APIs built by a group of self-styled volunteers called India Software Products Roundtable (iSPIRT), or Product Nation.25 iSPIRT designs and builds these APIs for use by government entities and businesses alike, in the process creating novel opportunities for value extraction from data flows and populating the Aadhaar ecosystem.  

# Aadhaar Integration with Cooperation of Sectoral Regulatory Institutions  

With a strong need for identity verification, the finance sector was the first to fully embrace Aadhaar. With the close cooperation of UIDAI and iSPIRT, institutions within the finance sector26 led efforts to build technology products forming a cashless layer atop the Aadhaar identity layer.  

These products allowed banks to use the Aadhaar number to make remittances27 or to authorize Aadhaar-linked bank accounts to transact through biometric authentication,28 and allowed firms to query the Aadhaar database to verify and onboard customers.29 With these Aadhaar integrations into legacy banking services in place, NPCI launched a payments system that introduced interoperability between different payments and settlements systems through the  

introduction of Aadhaar biometric authentication, among others.30 In practical terms, private firms could now build payment-related products and users could easily make payments through their smartphones. As a cohesive suite of technology “platforms,” these products and switches31 enabled the creation, capture, and monetization of data flows in finance.32  

UIDAI and its private-sector financial partners planned for the interoperability between Aadhaar and financial tools from the start,33 and conflicts of interest were notable. People associated with building the cashless layer went on to launch startups that created novel ways of payment-data monetization. Venture capitalists associated with the cashless layer went on to back these very startups.34  

Nevertheless, the government and financial sector have argued that this ecosystem is a boon for financial inclusion.35 As a testament to its value, Nandan Nilekani notes that securing a loan has now become as simple as having “a richer digital footprint.”36  

However, these narratives recast complex sociopolitical issues like lack of access to banking as individual journeys of competition for artificially scarce resources, to be won by participating and winning in the data economy. These interventions are far from actually addressing issues of financial inclusion.37 While the financial sector led the efforts to monetize Aadhaar, many other industries continue to follow suit (e.g, with “technology stacks” for healthcare, lending, telemedicine, and agriculture).38  

# Agile Regulation Keeps the Ecosystem Alive  

As private-sector use of Aadhaar took off, many harms materialized39 and several entities submitted petitions to challenge the law.40 Even as the Supreme Court struck down privatesector uses of Aadhaar in $2018,^{41}$ and dealt an existential blow to entire sectors42 built with its affordances, in practice this did not ultimately limit companies from using Aadhaar for private gain.  

As if seeing the Supreme Court’s verdict as a procedural complication and not a principled opposition to private use, the Ministry of Law and Justice introduced an ordinance amending the Aadhaar Act and other finance laws to keep authentication possibilities alive by introducing “offline verification” and “alternative virtual identity.”43 This allowed Aadhaar number holders to produce digitally signed copies of their Aadhaar acknowledgement letter by producing a QR code or .xml file downloaded from the UIDAI website.44 Despite these shoddy and dangerous accommodations, businesses were still disgruntled, as the ease and low costs of verification were nevertheless affected.45  

In response, the Central Government issued a note to allow private entities to use Aadhaar-based verification facilities upon the fulfillment of certain conditions, and at the discretion of UIDAI and the appropriate regulator.46 With this cue, the finance-sector regulator allowed the use of Aadhaar for opening bank accounts,47 and UIDAI allowed private firms to regain access to Electronic Know Your Customer (eKYC) authentication.48  

# REMAKING REGULATION IN TECHNOLOGY’S IMAGE  

Regulatory practice surrounding Aadhaar indicates that regulation is becoming beholden to the same values, managerial styles, procedural cadence, interests, and language of communication as the applications of technologies it seeks to regulate.  

# Regulation as Public Relations and Marketing  

For the first seven years of its existence, Aadhaar had little oversight and was shaped by UIDAI, a body preoccupied with the market importance of Aadhaar. Even after the passage of the law, regulation and technology development have worked hand-in-hand to create and maintain the conditions for use of the biometric data by private companies, to the artificial exclusion of socioeconomic concerns.49 Regulations not only consolidated the developments of the first seven years of the project, but also presented a revisionist history of the actual goals of the project, obscuring the stakes for private interests.50 For this and other reasons, many of the problems with Aadhaar should not be understood as failures of law or regulation, but as products of law and regulation.  

While law and regulation were meant to address the risks of Aadhaar, the instruments uncritically adopted disingenuous jargon like “financial inclusion,” “innovation,” and “efficiency.” What was righteously proclaimed by UIDAI as public buy-in for the project owed some credit to incentives provided to enrollment agencies,51 as well as expertise drawn from “multiple areas of marketing, creative communication, research, understanding of past social marketing efforts, media channels, branding and positioning.”52  

# Regulation as Technology Product  

The private sector’s direction and influence in the development and adoption of technology projects has a key feature of anticipating concerns around data use, and making data protection itself a product, feature, and layer.  

In the case of Aadhaar and data governance in India, the private-sector group building India Stack took it upon itself to “innovate” around encoding data-protection safeguards (e.g., through “consent” and “transparency”) within the technology ecosystem and to solve for data protection. This maneuver simultaneously tries to foreclose demands for a data-protection law (which India does not have) and, more importantly, distracts from broader questions about whether such datafication is at all necessary and who benefits from it, making the present trajectory seem inevitable.  

Consent: Arguably one of the biggest issues with Aadhaar has been its coercive nature and absolute disregard for consent, which has continued to be an issue even after courts have attempted to intervene.53 Perhaps learning from the problems caused by the pesky need for consent, India Stack evolved a “consent layer”54 consisting of two products: Account Aggregator and Data Empowerment and Protection Architecture (DEPA).55 The former is an entity legally instituted by the Reserve Bank of India, which is tasked with consolidating, organizing, and retrieving data about a customer’s different types of financial arrangements, including mutual funds and insurance schemes. The latter aims to provide “a modern privacy data sharing framework” and introduces convenience into the process of sharing personal data in exchange for finance, healthcare, and other services by building an interface for the purpose. The contents of this layer effectively make consent a bureaucratic formality and logistical complication to be simplified by technology, obscuring the instrumentalization of people’s lives toward value creation for private firms.  

The consent-related products blindside the need to consider whether such datafication is at all necessary, or what the subsequent terms of use of this data might be, ultimately cornering regulation into becoming a mimicry of the direction the market for data takes.  

Transparency: The Aadhaar project documents are littered with references to the importance of transparency. One of the main sources of proactive disclosure about Aadhaar is the UIDAI dashboard,56 where monthly data about enrollments, updates, and authentication are maintained. However, this data is a far cry from the granularity or consistency of useful information that people have been demanding for a long time,57 like the number of failed biometric authentications.  

The transparency-related artifacts use aesthetic devices like dashboards, data visualizations, and social media campaigns that have little substance and remain inert to demands for meaningful information.  

# Regulation as Optimization  

The regulatory framework around Aadhaar has been perennially agile and adaptive to the needs of the data economy. Within the broader vision for technology-enabled governance, agencies are encouraged to roll out projects “as soon as possible, and iterated rapidly, rather than waiting to roll out a perfect system.”58  

Besides aligning regulatory priorities with the workflows and cultures of technology firms, there is a push for regulatory practice to adopt the same logics (prediction, optimization) as technology directions within the industry. For example, Nandan Nilekani argues that the market is a perfectly responsive system: “Digital systems enable early-warning systems and more precise regulatory interventions, e.g., for managing loan defaults.”59  

# CONCLUSION  

The data economy relies on instituting individuals as data points within databases. Law and regulation around Aadhaar cooperate to create the perfect conditions under which this might be possible: architecting biometric information as truth, and facilitating its use, integration, and maintenance within other systems. Even then, law and regulation maintain a depoliticized reading of economic enrichment from data, and a false dichotomy between questions of rights and questions of enrichment.  

Instead of treating biometric information simply as data to be guarded, law and regulation should reckon with the entire range of powerful market interests that the networked subject kicks into motion, as well as regulation’s own malleability in the face of these forces.  

# A First Attempt at Regulating Biometric Data in the European Union  

Els Kindt (KU Leuven)  

# INTRODUCTION  

n 2004, the European Union (“Union”) enacted legislation that obligated Member States (“MS”) to store facial images and fingerprints in citizens’ passports and travel documents.1 Around the same time, the Union set up large-scale databases containing the biometric data of asylum and visa seekers and an information system for protecting the Schengen Area.2 It wasn’t long before this spilled over into public and private entities, which began using biometric data for crowd control, access control in the workplace, and monitoring in schools. While acknowledging that the use of biometric technology has many potential benefits, the Council of Europe warned that biometric data should be considered as “sensitive” data that presents risks, because it contains information about health and race, has the ability to identify people, can make it easier to link records, and is irrevocable.3  

Despite the risks, the general data-protection framework and most national legislation did not contain specific provisions on biometric data use and processing,4 and guidance remained limited while these technologies were being developed.5 To address these gaps, some national supervisory data protection authorities (SAs) developed frameworks for biometric use.6 As part of these frameworks, SAs have focused on the sensitive nature of the data, the risks of maintaining databases, and the possibility of “function creep.”7 The SAs also focused on whether the use of biometrics was proportionate to the legitimate aim sought to be achieved (i.e., the “proportionality principle”), leaving much room for discretionary policy considerations and unpredictable outcomes when applying the proportionality principle.8  

It was against this backdrop that the Union introduced the General Data Protection Regulation 2016/679 (GDPR) in 2016. The regulation is directly applicable in Member States and includes provisions for both public and private biometric data processing. The Union also introduced Directive 2016/680 (Data Protection Law Enforcement Directive, or DP LED), which applies specifically to personal data processing for the prevention, detection, investigation, or prosecution of crime by law enforcement authorities (LEAs).  

# THE EU’S REGULATORY APPROACH TO BIOMETRIC DATA PROCESSING  

Both the GDPR and DP LED provide, for the first time, a definition of biometric data: “personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic [fingerprint] data.”9 A particularly noteworthy aspect is the “specific technical processing”10 component, which effectively excludes “raw” data stored and retained in databases (e.g., of facial images captured on CCTV, voice recordings, or fingerprints),11 or when published on a website or social network. The GDPR accounts also mention that the “processing of photographs should not systematically be considered to be processing of special categories of personal data...”12 Video footage of an individual is also not considered biometric data as long as it has not been specifically technically processed in order to contribute to the identification of the individual.13  

While the GDPR states that “processing of biometric data for the purposes of uniquely identifying” is prohibited,14 there are many exceptions to this prohibition, including when the data “are manifestly made public” or if processing is “necessary for reasons of substantial public interest, on the basis of Union or Member State law which shall be proportionate to the aim pursued, respect the essence of the right to data protection and provide for suitable and specific measures to safeguard the fundamental rights and the interests of the data subject.”15 Because the exceptions remain vague (e.g., “substantial public interest’”) and are numerous,16 the GDPR still allows the processing of biometric data in many circumstances, including those where people give explicit consent.17 Finally, the GDPR specifies that Member States may maintain or introduce further conditions or limitations.18  

Under the DP LED, LEAs do not face a prohibition and may process biometric data to uniquely identify people where strictly necessary, subject to appropriate safeguards, and only in three situations: if authorized by law, to protect vital interests, or where the processing relates to data manifestly made public by the data subject.19 While the DP LED has data processing restrictions only if there is “specific technical processing,” LEAs may collect data (e.g., facial images or voice recordings) without biometric specific limitations imposed by the DP LED.  

In cases where new technologies lead to processing that is “likely to result in a high risk” or in case of large-scale processing of special categories of personal data, the GDPR and DP LED require entities to conduct Data Protection Impact Assessments (DPIA). A DPIA is also required for systematic monitoring of a publicly accessible area on a large scale.20 DPIAs mandate entities to conduct a comprehensive assessment of the risks of processing, as well as of the necessity and proportionality of the technology.21 In some cases, private or public entities will have to ask the SA for prior consultation and authorization.22 Furthermore, if the biometric data processing interferes with fundamental human rights and freedoms, including the right to privacy and the right to personal data protection, the fundamental rights framework shall be applied as well.23  

The following sections outline the key learnings from these regulatory attempts, discuss their effectiveness, and highlight learnings for future regulation.  

# ASSESSMENT AND EFFECTS OF THE REGULATORY CHOICES  

Impact of Definitional Choices  

Since the GDPR and DP LED definitions of biometric data require “specific technical processing,” the collection and storage of data like facial images or voice recordings do not receive more or stricter protection than any other personal data, such as the requirement of explicit consent or necessity and the need for law as required under Article 9.2 GDPR. It is the use of the data, rather than its sensitive nature or its ability to enable identification, that determines when data becomes biometric.24  

Because of the way the definition was written, the risks of biometric data collection are not covered. Data that should get special protection does not, because it is not currently being processed. This is particularly concerning because later use, particularly by law enforcement agencies, may be less transparent or restricted, and the data could be used without any notice to the individuals concerned or to the public.25 Under the permissions granted under the GDPR and the DP LED, this implies that companies and government can collect large databases of images (e.g., similar to the information collected by Clearview), which might later be used for law enforcement purposes.26  

Finally, the definition is not in line with the European Court of Human Rights case law, which has repeatedly stated that the practice of capturing, collecting, and storing unique human characteristics in databases interferes with the right to respect for private life.27 Such interference was confirmed for facial images in Gaughran v. The United Kingdom, where the Court took facial recognition and facial mapping techniques into account, and “found that the retention of the applicant’s DNA profile, fingerprints and photograph amounted to an interference with his private life.”28  

An appropriate definition should offer legal protections to unique human characteristics that are fit for identification purposes or could be used by automated processes, and regulation should also restrict the storage of this data in databases.29 An alternative definition of biometric data could be: “all personal data (a) relating directly or indirectly to unique or distinctive biological or behavioural characteristics of human beings and (b) used or fit for use by automated means (c) for purposes of identification, identity verification, or verification of a claim of living natural persons.”30  

# Lack of Clarity around Biometric “Prohibition” and Sweeping Exceptions  

The law should take into account how different biometric systems function, and these functionalities should be regulated depending on how the data is processed. For example, while prohibitions on use and processing are outlined in the law, Article 9.1 GDPR does not distinguish between one-to-one (1:1) biometrics comparisons (i.e., verification), and one-to-many (1:n) comparisons (i.e., identification).31  

Meanwhile, the Council of Europe and SAs have stated that biometric verification contains less risk than biometric identification because no database is needed.32  

On the other hand, one-to-many comparisons (i.e., identification) introduce additional risks, including the large-scale collection and storage of biometric information in databases, probabilitybased matching (which raises concerns about accuracy and false positives), and privacysurveillance concerns. Because the GDPR and DP LED do not differentiate between the two functionalities, there is legal uncertainty for companies that want to invest in biometric verification technologies and privacy-enhancing methods.33 Appropriate regulation should meaningfully address the relative risks of each functionality, discouraging or banning those that pose real risks, and potentially encouraging those that have the potential to offer real privacy and security protections.  

Finally, the broad exceptions and overall vagueness of the law leaves the door open for specifically risky uses of biometric data like live facial recognition (LFR). The GDPR exceptions are general, and include language allowing biometric data processing for “reasons of substantial public interest” based on law. Because of the way this and other exceptions are worded, it remains unclear whether these serve as a legal basis that authorizes public or private entities to deploy LFR (e.g., at large stadium events).34 The GDPR and DP LED alone will not resolve these questions, and additional specific EU and national laws are needed.35  

# CONCLUSION  

The GDPR and DP LED approaches to defining biometric data exclude the collection of so-called “raw” data like facial images, yet protection is most important at the initial stage of the creation of biometric systems and infrastructures. The GDPR and DP LED also deviate from Europe’s human rights case law and its own approach to data “processing,” which is that data protection should start at the collection stage. A comprehensive legal framework should also aim to restrict any biometric data storage in databases, and should offer clear guidance as to any undesirable or forbidden biometric identification, unless allowed under strict legal conditions, and biometric verification solutions, under precise conditions. More precise laws around police collection and use of such data and policing techniques are needed, in addition to a strict interpretation of the necessity and proportionality tests as they apply to law enforcement use.  

Apart from stronger legal and procedural safeguards under the GDPR and DP LED, and enhanced consideration of the fundamental rights’ three-steps test, policymakers should adopt special regulation to strengthen and reinforce fundamental rights. These could include bans or moratoria against particular uses of biometric technology like LFR unless strictly necessary and proportionate for substantial public interests described in law. This is crucial, especially if LFR directly contradicts and affects the essence of fundamental rights, such as the right to peaceful assembly, which should not be left to case-by-case assessment.  

As other states or countries look to the Union for guidance around regulating biometric data collection and use, this chapter has aimed to highlight the challenges posed by uncritically adopting the text of the GDPR and DP LED. For any future legislation, it will be important to recognize the risks and functionalities of biometric data systems, starting from the collection and storage of the data, not just during its processing or use, and to reconsider broadly worded exceptions that provide loopholes for companies, governments, and authorities to exploit.  

![](images/a6d70ad1f79870bcd0ac1e3f83783c29fb7ef9ffbb56e8d4b00f72c21c83219d.jpg)  

![](images/6ef3e17e1c83733c95ea177101e60ffb073aaf46510a355be88f7b3ac3d37618.jpg)  

![](images/deb6c539497672655e62d3ad60ef311eb2188f9208c9916617e23ce5b8a2b3da.jpg)  

# Reflecting on the International Committee of the Red Cross’s Biometric Policy: Minimizing Centralized Databases  

Ben Hayes (AWO agency, Consultant legal advisor to the ICRC) Massimo Marelli (Head of the ICRC Data Protection Office)  

T vhbuye InanretrmearebndlaetcipoenofalpilclCet oianmtdhmoeitthweoerrlosdif,t tuphareto voRindeisdnogCfrhvouisosmle(aInCcitRea.C1r i)Laiwnkeoarskmsiasnwtyiatnhoctsheoetrmohepuompf utalhnaiet iaomrinoasnta organizations, the ICRC is exploring new technologies to support its operations and beneficiaries. As part of its digital transformation agenda, the ICRC developed a Biometrics Policy (“the Policy”) that both facilitates the responsible use of biometrics and addresses data-protection challenges. ICRC adopted the Policy in August $2019,^{2}$ which recognizes the legitimacy and value of using biometrics to support its programmatic and operational objectives while also ruling out the creation of any central, biometric databases in the short term. This article discusses some of the factors brought to bear on the decision-making process we went through as an institution.3  

# BIOMETRICS IN THE HUMANITARIAN SECTOR  

The ICRC works in more than ninety countries and is part of a global humanitarian network of over eighty million people.4 It provides healthcare, food, basic shelter, clothing, access to education, employment, and assistance to detained persons, and also helps restore family links by reuniting separated persons and finding missing persons. To address the logistical challenges of protection and assistance programs, some humanitarian organizations use biometric identification systems to enroll people in humanitarian programs and verify their identity when providing services or assistance. The primary justification for this use is that recipients of humanitarian assistance frequently lack identity documents, which poses a challenge if they need to be identifiable.  

Humanitarian organizations have intensely debated when and how people “need” to be identifiable, and the legitimacy of using biometrics to perform that function.5 On one side, continuity of healthcare and some forms of humanitarian assistance clearly need people to be identifiable (e.g., for provision of travel documents or financial services). For example, the United Nations Refugee Agency (UNHCR) has a clear mandate to identify refugees and asylum seekers, and to provide them with identity documents6 (though it has been heavily criticized for deploying biometrics7). However, most humanitarian organizations do not have a formal mandate to provide people with an identity or supporting documentation. They have primarily developed and implemented biometric ID systems because of the perceived efficacy and accountability gains such systems provide.8  

While existing ID cards, social security numbers, and other documents may be used by humanitarian organizations to check or verify an individual’s identity, these cannot be unequivocally associated with a single individual in the way that a biometric ID can. Biometric databases can also be used to prevent the same individual from registering in an aid program more than once, which is attractive for humanitarian organizations that are concerned about individuals or families obtaining more assistance than has been earmarked for them.9 Indeed, biometrics have played an increasingly large role in the scaling up of cash-transfer programs (CTPs).10 For financial service providers that are obligated to verify the identity of account holders and cash recipients, biometric data could offer a simple and straightforward way to meet multiple operational needs and legal obligations.11  

These are crucial issues for humanitarian staff, who want operations to be as efficient as possible, and to ensure that scarce humanitarian services and assistance are provided to intended recipients. There is also implicit pressure to use biometrics from donors, which increasingly demand “end-to-end auditability” (allowing the tracking of humanitarian funds from donor to recipient) and make funding contingent on anti-fraud and accountability processes. All of this has contributed to a tangible impetus for humanitarian organizations to use biometrics for beneficiary registration and aid distribution. And why not, if everyone else is doing it?  

# RISKS AND CONCERNS  

Concerns about the use of biometrics in the humanitarian sector are well known, but are often overlooked.12 Biometric data are unique, immutable, and create a permanently identifiable record for individuals in vulnerable humanitarian contexts who may not want to be identifiable forever. The creation of a permanent biometric record underpins concern that this record could increase the risk of harm to the persons concerned in the event it was subsequently accessed by or provided to the regime or non-State actor they had fled.  

Biometrics constitute particularly sensitive data13 due to the potential for reuse or misuse, as well as “function creep,” i.e., the possibility that biometrics may be used in a new way, separate from the original purpose and without the understanding or consent of the affected individuals. For example, biometrics could be shared with non-humanitarian organizations or governments for non-humanitarian purposes, such as security and migration control.14 This is particularly concerning when biometric identity management systems are developed during a crisis or  

emergency, where data could be used in ways that recipients of humanitarian assistance do not want, understand, or consent to. Humanitarian databases may, for example, be integrated or made interoperable with other social registries or national ID systems run by development or government partners. Technology may also advance to allow biometric profiles to be used to ascertain additional information about the data subject—for example regarding their health, ethnicity, or genetic makeup.  

States have shown increasing interest in biometrics to monitor the movement of populations and identify security “threats.” In December 2017, the UN Security Council called for the enhanced use of biometric ID systems to identify terrorist suspects, mandating all UN Member States to “develop and implement systems to collect biometric data, which could include fingerprints, photographs, facial recognition, and other relevant identifying biometric data, in order to responsibly and properly identify terrorists, including foreign terrorist fighters, in compliance with domestic law and international human rights law.”15 Some humanitarian organizations have already come under pressure from States to disclose biometric data for non-humanitarian purposes, though these requests are generally not in the public domain. Organizations are also vulnerable to cyber-operations by State and non-State actors seeking unauthorized access to their data.16  

Biometric data use was a central theme at the 33rd International Conference of the Red Cross and Red Crescent, held in December 2019.17 To safeguard the independence, neutrality, and trust in humanitarian organizations, the Conference adopted a landmark resolution on “restoring family links while respecting privacy.”18 Founded on the principle of purpose limitation, the resolution “urges States and the Movement to cooperate to ensure that personal data is not requested or used for purposes incompatible with the humanitarian nature of the work of the Movement.”19  

# RATIONALIZING BIOMETRICS AT THE ICRC  

Prior to the adoption of its biometrics policy, the ICRC was already employing biometrics in limited use cases, for example in forensics and the restoration of family links, and by putting fingerprints on the travel documents it issues (but not into any database). In addition to using DNA profiling  

to help identify human remains to determine the fate of the missing, the ICRC is exploring facial recognition technology to locate persons sought by family members following separation due to humanitarian emergencies.20  

This is part of a broader ICRC strategy to transform and adapt its humanitarian response by seizing the opportunities that new technologies offer its operations and beneficiaries. Managing the attendant risks is central to this digital transformation agenda.21 Early in 2018, following significant interest in expanding biometric data use, the ICRC Directorate requested an assessment of the operational, ethical, and reputational risks involved, as well as an institutionwide policy that would facilitate both innovation and data protection.  

ICRC developed the policy over an eighteen-month period that included extensive research, analysis, consultation, and reflection. ICRC reviewed all scenarios in which the ICRC processed or considered the use of biometrics, evaluated the “legitimate basis” and specific purposes for the processing, and identified organizational, technical, and legal safeguards. Although the ICRC is not bound by national or regional data-protection law, it has adopted similar rules that require it to identify a legitimate basis (equivalent to a legal basis) for all of its data-processing activities.22  

In some cases, ICRC’s rationale for biometric data use was straightforward: for instance, when used with specific objectives associated with its international mandate and where particular objectives cannot be realized without using biometrics. Examples include using DNA to determine the fate or whereabouts of the missing, or using facial recognition to match missing and sought persons in its work on restoring family links.23 In these cases, the ICRC processes the biometric data as a matter of “public interest.”24 Subject to appropriate safeguards, biometric data processing provides the ICRC with tools that greatly enhance its capacity to implement its mandate with respect to persons separated or missing in humanitarian emergencies.  

Other cases are much more challenging: for example, when the potential use case involves biometrics for beneficiary management and aid distribution, where requiring the identification of individuals may not be viewed as an integral part of an ICRC mandate-based activity. Because the purpose is primarily efficiency, and aid can be (and long has been) distributed without the need for biometrics, the ICRC determined that the “legitimate interest” of using a biometric identitymanagement system did not outweigh the potential concerns over rights and freedoms. This balancing test is typical of data-protection laws (e.g., as in GDPR), whenever a data controller relies on their own interests as a basis for processing.25  

After careful consideration, ICRC concluded that it was possible to leverage the efficiency and effectiveness gains of biometric authentication, as well as end-to-end accountability in its aid distributions, while also minimizing the risks to its beneficiaries. This balance rests on using biometric data in beneficiary registration and verification, and limiting the processing to a tokenbased system. In practice, this means that beneficiaries could be issued a card on which their biometric data is securely stored, but that the ICRC will not collect, retain, or further process their biometric data (and therefore not establish a biometric database).  

The token/card could be used to verify beneficiaries during aid distributions to ensure that the aid reaches those individuals for whom it has been earmarked, but no other use will be possible. If the beneficiary wants to withdraw or delete their biometric data, they may return or destroy the card. If authorities seek to compel humanitarian organizations in a particular country to hand over the biometric data of beneficiaries, the ICRC will not face such pressure because it will not have the data.  

# KEY FEATURES OF THE POLICY  

Adopted by the ICRC Assembly in August 2019, the ICRC Biometrics Policy sets forth staff and program roles and responsibilities,26 the legitimate basis for processing biometric data by the ICRC,27 the specific purposes and use cases for which the use of biometrics is authorized, 28 and the types of biometric data that may be processed by the ICRC.29 Specifically, it allows the ICRC to:  

include the fingerprints of the holder on travel documents issued by the ICRC to persons who have no valid identity papers, enabling them to return to their country of origin or habitual residence or to go to a country which is willing to receive them;   
use biometric identification systems to restrict access to strictly confidential information and/or mission-critical resources such as servers and control rooms in ICRC premises; use fingerprints, facial scans, and DNA to identify human remains recovered from disaster or conflict zones or in connection with other situations of violence;   
use digitized photographs for the purposes of tracing and clarifying the fate of separated or missing persons;   
use biometric data to ascertain the identity or fate of specific individuals in the course of investigations related to the abduction of, or attacks upon, ICRC staff members;   
on a case-by-case basis, where it has been determined that it is in the best interest of the persons concerned, collect biological reference samples for the purposes of DNA profiling to facilitate family reunification or to determine the fate of a missing person; and   
use biometrics to provide beneficiaries with a token-based verification credential such as a card that can be used to verify their receipt of those services, where the token is held solely by the Data Subject.  

There are additional caveats:  

The use of fingerprints for travel documents remains limited to ink prints on hard-copy documents (with no further biometric processing by the ICRC permitted). Delegations may not use biometrics for routine premises control (only specific assets that require a high level of security and where profiling is limited to staff authorized to access them). DNA profiling for family reunion purposes is strictly limited to cases where proof that two persons are actually related is required under national law or policy.  

The Policy also expressly rules out the creation of biometric databases with respect to the authorized use cases. Finally, where ICRC programs or delegations wish to process biometric data pursuant to an authorized use case, they must first conduct a data-protection impact assessment and ensure that detailed data protection by design and by default requirements are implemented as the process or system is developed.30  

The Biometrics Policy also addresses some other common data-protection challenges, including “consent,” which humanitarian organizations have traditionally sought from the people who use their services or receive assistance. In some contexts, like medical treatment, these processes have been quite robust. In others, however, people have routinely signed “consent forms” or provided a thumbprint in lieu of a signature (e.g., for those unable to write; as part of its biometrics review, the ICRC is also putting an end to this practice). “Informed consent” in data processing is subject to high standards: the ICRC Rules on Personal Data Protection require “freely given, specific, informed indication of his or her wishes by which a Data Subject signals agreement to the Processing of Personal Data relating to him or her.”31  

While the ICRC is firmly committed to transparency, it does not believe that consent provides a legally valid basis for data processing in many emergency situations. Consent to data processing cannot be regarded as valid if the individual has no real choice: for example, where the provision of aid is effectively dependent on the provision of personal information, and consent is therefore unlikely to be “freely given.” In addition, power imbalances may imply no real “choice,” and individuals may be induced to accept what is proposed by a humanitarian organization. Where biometrics are concerned, it is extremely difficult to ensure that consent is genuinely “informed,” since affected populations may not be able to fully comprehend the technology, information flows, risks, or benefits that underpin biometric data processing.  

The Biometrics Policy requires that the ICRC explain the basis and purpose of data processing to its beneficiaries, including any data-sharing arrangements, regardless of the basis for the processing.32 The ICRC also seeks to ensure that beneficiaries have the opportunity to ask questions and object if they wish, particularly where data may be shared with third parties.33 If people do not want to provide their biometric or other personal data, or share their data with partners, the ICRC will respect their wishes.34 The ICRC will only use biometric data where it enhances the capacity of the organization to implement its humanitarian mandate.35  

Finally, under no circumstances will the ICRC share biometric data with third parties, including authorities, that may use them for non-humanitarian purposes.36 Even where exclusively humanitarian grounds for sharing biometric data can be identified, strict conditions must be satisfied before ICRC will transfer any data.37  

The ICRC will review the Biometrics Policy at least every three years,38 including the decision not to establish biometric databases for the purposes of identity management. ICRC will review developments around the availability, security, cost, effectiveness, and impact of biometric technology, and may amend the Policy to widen the scope for using biometrics, or to introduce new safeguards.  

# LESSONS LEARNED  

During its deliberations, the ICRC considered the option of not adopting a biometrics policy and leaving decisions about how and when to use these data to programs, operations, and delegations in the field. This option was rejected as “high risk on the basis that it could undermine, inter alia, the rights of the ICRC’s beneficiaries, the ‘do no harm’ principle, and ICRC’s reputation.” While the internal organizational debates have been challenging, the Policy has provided much needed clarity and operating procedures for staff who were struggling to balance the perceived benefits and risks of specific uses.  

ICRC consulted internal staff and external stakeholders in order to answer questions around operational needs, data-protection requirements, technology options, ethics, and risk appetite. Case-by-case assessment of the existing and possible use cases was fundamental in shaping the ICRC Biometrics Policy. However, ICRC faced many challenges because it was already using biometrics, and the new Policy could have led to changes in practice or prohibitions against certain processing options or operations. Finally, the ICRC Biometrics Policy benefited from considerable dialogue and investment in innovative compromises such as the token-based solution, which might not have been achieved through a less coherent or constructive exercise. As biometric data use-case law and data-protection enforcement actions continue to expand, the need for humanitarian organizations to develop proactive policies only becomes more important.  

# Policing Uses of Live Facial Recognition in the United Kingdom  

Peter Fussey (University of Essex) Daragh Murray (University of Essex)  

# BACKGROUND TO THE USE OF FACIAL RECOGNITIONIN THE UK  

ondon has a long history of trialing advanced surveillance technology. Police agencies first L installed closed-circuit television (CCTV) cameras in the city in 1953, and until recently London likely had more CCTV cameras per person than any country in the world.1 The city deployed one of the world’s first automatic license plate recognition (ALPR) systems in the mid-1990s, and has since introduced crowd-modeling video analytics to survey its mass transit systems.2 London was also one of the first cities in the world to trial facial recognition (FR) in the east of the city during the late 1990s, although technological limitations at the time led to its abandonment.3  

With rapid advancements in FR technology, the Metropolitan Police Service (MPS) conducted a series of ten live facial recognition (LFR) test deployments between 2016 and 2019, moving to operational deployments in early 2020.4 South Wales Police have also been using LFR since 2017, mostly at large concerts, festivals, and sporting events.5 Both constabularies deploy LFR by installing temporary cameras at a fixed geographic location6 for a fixed time period.7 Police generally mount the cameras on an LFR van with a control center used to monitor the live LFR feeds and to communicate with officers on the ground. LFR cameras scan the faces of all individuals passing through their field of vision, and then officers check the resultant biometric profiles against a watch list containing persons of interest. To date, police have only deployed LFR technology in this standalone manner, and have not, for example, integrated it into existing infrastructure, such as CCTV networks.8  

Police use of LFR has resulted in significant controversy, with a number of human rights and civil society organizations leading opposition against LFR deployments. Many of these organizations have initiated advocacy campaigns calling for either a moratorium on the use of $\mathsf{L F R},9$ or an outright prohibition.10 South Wales Police’s use of LFR is currently subject to legal challenge, and an initial hearing before the Court of Appeal took place in June 2020.11  

In order to examine issues relating to operational effectiveness and human rights compliance, the MPS invited the authors to provide an independent academic report on the last six LFR test deployments.12 We conducted ethnographic observations from beginning to end of each deployment, of pre-deployment police briefings and post-deployment debriefings, and of a range of other planning meetings. We also held interviews with key stakeholders and analyzed large quantities of MPS internal documents. In this piece, we draw on this research to explore three key themes relating to the regulatory regime: 1) the legal requirement for an authorizing law for LFR; 2) the inability and failure of existing institutions and laws to meaningfully restrict this technology; and 3) the operational considerations unique to LFR. Our focus is on working toward human rights compliance. A key element not addressed in this piece is the “necessity” of police LFR deployments. However, this consideration only comes into play if an appropriate legal basis exists.  

Police LFR deployments directly engage / interfere with several distinct human rights protections The right to privacy of all individuals passing through a camera’s field of vision (and thus subject to biometric processing) is directly engaged. Additional, discrete right-to-privacy issues are raised by any retention or analysis of the resultant footage.13 The use of LFR may also engage discrimination laws as a result of the technology’s biases.14 Importantly, the deployment of LFR technology may generate a chilling effect, whereby individuals refrain from lawfully exercising their democratic rights due to a fear of the consequences that may follow.15 This may harm a number of rights, including the right to freedom of expression, the right to freedom of assembly and association, and the right to freedom of religion.16  

Significantly, the UK’s Human Rights Act 1998 (implementing the European Convention on Human Rights17), requires that any interference with a right be “in accordance with the law.” As such, any measure interfering with human rights protections must have a legal basis, and that legal basis must be of sufficient quality to protect against arbitrary rights interferences. Key in this regard is the foreseeability of the law.18 If a measure fails to satisfy the “in accordance with the law” requirement, it is unlawful in and of itself.  

# THE COMMON LAW AS A LEGAL BASIS FOR LIVEFACIAL RECOGNITION  

United Kingdom common law establishes the core common law principles for police: protecting life and property, preserving order, preventing the commission of offenses, and bringing offenders to justice.19 Although no legislation exists that explicitly authorizes police use of LFR, the government has claimed that these common-law powers provide sufficient implicit legal authorization to satisfy the “in accordance with the law” test.  

In Bridges v. South Wales Police, the UK High Court agreed with the Government,20 indicating that the common law establishes sufficient legal basis for LFR.21 This judgment is currently subject to appeal, and this finding is a key point of contention.  

At the heart of the matter is the fact that police powers under the common law are expressed in broad terms. The common law is inappropriately vague and, for example, does not delimit the circumstances in which a particular measure may be deployed, such that those circumstances are foreseeable, thereby protecting against arbitrary rights interference. Relying on the common law to provide a legal basis for LFR therefore arguably fails to satisfy the “in accordance with the law” requirement established under human rights law, and presents a clear risk of arbitrariness.22  

A key reason for the High Court’s conclusion that the common law was a sufficient legal basis for LFR, and that new statutory powers were not required, was the classification of LFR as a nonintrusive means of obtaining information,23 and as “no more intrusive than the use of CCTV in the streets.”24 This is clearly contentious: it appears inconsistent with common understandings of the surveillance capacity inherent in LFR, and has been challenged by a number of key figures in the UK. It also appears inconsistent with the High Court’s own finding that—as a form of biometric processing—LFR engaged the right to privacy of all individuals passing through an LFR camera’s field of vision.25  

Concerns regarding the arbitrary exercise of powers mean that reliance on the common law to provide the legal basis for the use of LFR is likely to be incompatible with the UK’s obligations under the Human Rights Act or European Convention on Human Rights. The Bridges line of cases will provide further guidance in this regard. However, irrespective of the outcomes of these cases, establishing an explicit legal and regulatory basis for the use of LFR would provide much needed clarity, both for the public and for the police.  

# OTHER LAWS, LEGISLATIONS, AND AGENCIES THAT APPLY TO LFR  

Police documentation and political debate have consistently referred to the oversight roles of the multiple data-protection and surveillance-related authorities in the UK.26 These include the UK’s data-protection authority, the Information Commissioner’s Office (ICO); the Surveillance Camera Commissioner; the Biometrics Commissioner; and the Investigatory Powers Commissioner’s Office. While these agencies have contributed to the debate, each body is narrowly relevant to a specific aspect of LFR and, critically, they do not have explicit authorization to limit LFR deployments. Indeed, while many of these regulatory bodies are heralded as a safeguard to promote appropriate use, their mandates do not provide meaningful oversight. This is explained in the following table:  

<html><body><table><tr><td>Authority</td><td>Role</td><td>Application to LFR</td></tr><tr><td>The Information Commissioner's Office (ICO)</td><td>Oversees issues relating to data protection in the UK, particularly the Data Protection Act 2018 and the General Data Protection Regulation.27 In 2017, they published "In the Picture: A Data Protection Code of Practice for Surveillance Cameras and Personal Information," which provided best practices for automated recognition technologies.28</td><td>Although important, data protection law cannot adequately address the broad range of potential human rights harms brought about by police LFR deployments. It does not, for example, fully address issues relating to whether the use of LFR is necessary or proportionate. As such, the impact of the ICO on the overall LFR debate is relatively limited.</td></tr><tr><td>The Surveillance Camera Commissioner</td><td>Established by the Protection of Freedoms Act 2012 to oversee the use of closed-circuit television systems (CCTV). 29</td><td>While they are primarily focused on CCTV systems, and LFR is implemented through standalone video systems, they have published guidance on police use of LFR.30</td></tr><tr><td>The Biometrics Commissioner</td><td>Established by the Protection of Freedoms Act 2012 to oversee retention and use of biometric information.31</td><td>The Biometrics Commissioner's role is restricted in statute to fingerprints and DNA data, and so does not extend to LFR. The Commissioner has published several statements questioning the use of LFR and has said that "we need proper governance of new biometric technologies such as LFR through</td></tr><tr><td>The Investigatory Powers Commissioner's Office</td><td>Established under the Investigatory Powers Act 2016, has authority to oversee covert police deployments.33 </td><td>legislation."32 As currently deployed, the principal uses of LFR by police in the UK are not classified as covert. This may change going forward.</td></tr></table></body></html>  

As it stands, police use of LFR in the UK is not subject to adequate oversight or meaningful regulation. This must urgently be addressed.  

# ISSUES ARISING IN THE CONTEXT OF POLICE LIVE FACIAL RECOGNITION DEPLOYMENTS  

This section examines a number of issues arising in the context of LFR deployments, including watch lists, the “presumption to intervene” and associated deficits in effective human oversight, how accuracy is determined, and potential discrimination. These operational elements illustrate the uncertainty associated with LFR deployments, contesting police claims of utility, and highlight problems arising from the absence of appropriate regulation.  

# Operational Considerations  

Measuring LFR performance is complex and includes both partial and instrumental use of statistics. Some technical evaluations compare the number of false matches to an estimate of the total number of individuals passing through an LFR camera’s field of vision during a given deployment. These numbers are widely cited by supporters of LFR, yet they offer only a tiny ratio of numbers of faces scanned to those correctly or incorrectly matched.34 A variation of this approach was adopted by the MPS in a recently published evaluation of their LFR trial deployments,35 leading to widely publicized claims that the technology was $"70\%$ effective.”36 However, such claims often conflate two different forms of data, merging “blue list” data (where volunteers are sent past the cameras to measure their effectiveness—the measure used to support the claim of 70 percent effectiveness) and live data (camera performance when there is no certainty about whether suspects will walk past the cameras).  

Another shortcoming of this methodology is the way it de-emphasizes the impact of LFR on those flagged by the technology by contextualizing their experience against larger quantities of data that are arguably less relevant. This makes this measure less suitable for understanding the individual rights-based interferences brought by LFR. Other measures of LFR performance compare how often a human operator discards a computer-suggested alert.37 One challenge of this approach is the potential for readers to conflate human and computer decision-making: a human might decide the LFR system is wrong, regardless of the veracity of the computational decision.  

We designed our independent academic review of the MPS system to address the above challenges, focusing on human rights considerations and protection against arbitrary rights interferences. The research used the same statistics as the MPS study above,38 and asked two straightforward questions to determine accuracy and examine the role of human oversight:  

a.	 When an LFR system matches someone to the watch list, how often is it verifiably correct?39   
b.	 To what extent do human adjudicators consider LFR matches to be credible? To understand if a computer match is correct, it needs to be tested against something—in this case, an identity check of the suspect.  

For (a), our research found that out of forty-two computer-generated LFR matches, eight were verifiably correct (19.05 percent). For (b), human adjudicators judged twenty-six out of forty-two matches were sufficiently credible to apprehend the matched individual (61.91 percent), meaning that humans overwhelmingly overestimated the credibility of the system. Four of these matched individuals were lost in the crowd. The remaining fourteen were incorrectly matched by the LFR system.  

Two conclusions can be drawn from this. First, there is a “presumption to intervene” on behalf of human operators assessing the credibility of LFR matches. Second, this tendency of deference to the algorithm exists despite the computer being either incorrect or not verifiably correct in a large majority of cases.  

These conclusions hold relevance for considerations over the form of human adjudication taking place around LFR systems. Policy emphasizes the importance of “the human in the loop” as a safeguard against algorithmic-induced harms. That human adjudication takes place is not in question, however. The issue at stake is the form it takes, and the degree of critical human scrutiny applied. Moreover, a presumption to intervene suggests LFR frames and structures suspicion ahead of human engagement with the technology.  

A final question is whether LFR is discriminatory. UK police forces have made repeated claims that LFR technology is nondiscriminatory in terms of racial characteristics.40 However, this is a complex issue and covers both the capability of the technology in identifying faces from a range of ethnic groups and the composition of databases of suspects (watch lists). It is difficult for law enforcement agencies to undertake an analysis of sufficient scope to support definitive conclusions. For example, the US National Institute of Standards and Technology (NIST) reviewed 189 facial recognition algorithms and revealed marked “demographic differentials” in the performance of facial recognition algorithms across different ethnicities.41 Accordingly, claims made in the technical evaluation of the MPS LFR scheme that “differences in FR algorithm performance due to ethnicity are not statistically significant”42 may arise simply because the total number of matches themselves are not statistically significant.  

According to the MPS statistics, twenty-eight people were engaged by a police officer after being matched by LFR systems across their ten test deployments. We contend that it is impossible to make robust and definitive conclusions over demographic disparities from such small numbers. Concerns over this issue have been most recently articulated in March 2020 in calls by Great Britain’s Equality and Human Rights Commission to suspend the use of facial recognition in England and Wales until its impact has been independently scrutinized.43  

# CONCLUSION  

This piece highlights three key concerns. First, the legal basis underpinning LFR is inappropriately vague, negatively affecting foreseeability, and arguably failing to meet the “in accordance with the law” test established by human rights law. Second, although a number of UK regulatory bodies engage in this area, there is no dedicated body with authority to limit or effectively oversee LFR deployments. Third, operational realities contest police claims of LFR’s utility, the effectiveness of human oversight, and discriminatory outcomes. These highlight the practical consequences and harms arising in the absence of appropriate legal or regulatory frameworks.  

# A Taxonomy of Legislative Approaches to Face Recognition in the United States  

Jameson Spivack (Georgetown Center on Privacy and Technology) Clare Garvie (Georgetown Center on Privacy and Technology)  

# INTRODUCTION: POLICE FACE RECOGNITION IN THE UNITED STATES  

O n December 25, 2015, Florida resident Willie Allen Lynch was arrested for selling fifty dollars’ worth of crack cocaine to two undercover Jacksonville sheriffs three months earlier. The only thing tying Mr. Lynch to the crime was a face recognition search comparing photographs the officers had taken of the drug sale to the county’s mugshot database. The search returned five possible matches—Mr. Lynch and four other suspects. Mr. Lynch and his defense attorney were given no information about the use of face recognition: its accuracy, potential biases, or even a list of the other possible suspects. Despite this, Mr. Lynch, who maintains his innocence, was sentenced to eight years in prison.1  

Police use of face recognition is pervasive, affects most Americans, and, until very recently, has persisted under a widespread lack of transparency, oversight, and rules governing its use.2 Police departments across the United States have deployed face recognition technology in thousands of criminal investigations since as early as 2001.3 At least one agency has also used face recognition to identify protesters,4 and by 2016, one quarter of the nearly eighteen thousand agencies across the country had access to a face recognition system.5 Because thirty-one states allow police searches of DMV databases, more than half of all American adults can be identified through police face recognition simply by having a driver’s license.6 Many police departments have also used Clearview AI’s face recognition service, which has amassed a database of an additional three billion images scraped from Facebook, Instagram, Twitter, Venmo, YouTube, and elsewhere.7  

In 2016, the Government Accountability Office (GAO) published an extensive report on the use of face recognition by the FBI.8 It made recommendations to increase transparency, enhance privacy protections, and better test the accuracy of their systems to guard against misidentification. This and many other reports have highlighted unique risks posed by police face recognition use:  

Face recognition poses a threat to privacy. Under the Fourth Amendment of the US Constitution, the right to privacy extends beyond the home, protecting “reasonable expectations of privacy” in some public settings and activities.9 Face recognition gives police the power to conduct identity-based surveillance and the ability to scan and identify groups of people in secret, as well as to track someone’s whereabouts through a network of security cameras. Without a warrant, this power may violate the Fourth Amendment, interpreted in the Supreme Court’s 2018 decision in Carpenter v. United States as including a right to privacy in our movements across time and space.10 The enrollment of most American adults into biometric databases used in criminal investigations represents an unprecedented expansion of law enforcement access to personal data, to which the American public did not consent.11  

Face recognition risks having a chilling effect on free speech. The First Amendment of the US Constitution protects the right to free speech, assembly, and association.12 As law enforcement agencies themselves have cautioned, face recognition surveillance has the potential to “make people feel extremely uncomfortable, cause people to alter their behavior, and lead to self-censorship and inhibition”—chilling our ability to participate in constitutionally protected activities.13  

Searches may lead to misidentifications. While the algorithms behind face recognition have improved significantly since 2001, misidentification is still a major issue. Low-quality images, edited photos, and unreliable inputs such as forensic sketches and “celebrity lookalikes” increase the odds that the wrong person will be investigated, arrested, and charged with a crime they did not commit.14  

•	 Face recognition may have a disparate impact on communities of color. Communities of color are disproportionately enrolled in face recognition databases and targeted by surveillance.15 In San Diego, for example, police have used face recognition technology and license-plate readers up to two and a half times more on people of color than expected by population statistics.16 The technology performs less accurately on people of color, meaning the risks of the face recognition police use, and the mistakes it may make, will not be distributed equally.  

•	 The failure to disclose a face recognition search may deprive a defendant of due process. The risks of misidentification and bias are not mitigated by a fair, transparent court process. Face recognition searches produce evidence that speaks directly to a defendant’s guilt or innocence. Per the constitutional right to due process and the Supreme Court’s decision in Brady v. Maryland, evidence must be turned over to the defense.17 Yet as in Mr. Lynch’s case, and indeed the vast majority of cases involving a face recognition search, this information is not disclosed.18  

In response to growing concern over the risks that the use of unregulated police face recognition poses to our civil rights and liberties, legislators have begun introducing—and passing—face recognition bans, moratoria, and regulatory bills.19  

# PROPOSED AND ENACTED LEGISLATION  

Generally, there have been three legislative approaches to regulating face recognition in the United States: complete bans, moratoria, and regulatory bills. Moratoria can be further broken down into two types: time-bound moratoria, which “pause” face recognition use for a set amount of time; and directive moratoria, which “pause” face recognition use and require legislative action—such as a task force or express statutory authorization—to supersede the moratoria. Most of these bills have covered all government use of face recognition, with particular attention given to limits placed on police use. This section focuses on police use as well.  

<html><body><table><tr><td>Type of legislation</td><td>What it does</td><td>Examples</td></tr><tr><td>Ban</td><td>Complete shutdown of all face recognition use</td><td>Enacted: San Francisco, CA;20 Cambridge, MA21 Proposed: Nebraska22</td></tr><tr><td>Moratorium: time-bound   Face recognition use paused</td><td>for a set amount of time</td><td>Enacted: Springfield, MA23 Proposed: Maryland24</td></tr><tr><td>Moratorium: directive</td><td>Face recognition use paused, requires legislative action to supersede</td><td>Proposed: Massachusetts25</td></tr><tr><td>Regulatory bill</td><td>Regulates specific elements of face recognition, along</td><td>Enacted:</td></tr><tr><td></td><td>a spectrum from narrowly</td><td>California: prohibited in conjunction with police body-worn cameras26</td></tr><tr><td></td><td>focused to broader</td><td>(narrower)</td></tr></table></body></html>  

# A. Bans  

The strongest legislative response is to ban the use and acquisition of the technology completely. Bans can focus on state use of face recognition, commercial or private sector use, or both. To date, only local municipal governments have implemented bans, concentrated in towns and cities in California and Massachusetts. As of July 2020, the following municipalities had banned face recognition: Alameda, California; Berkeley, California; Boston, Massachusetts; Brookline, Massachusetts; Cambridge, Massachusetts; Easthampton, Massachusetts; Northampton, Massachusetts; Oakland, California; San Francisco, California; and Somerville, Massachusetts.28 A number of states proposed bans on face recognition during the 2019–2020 legislative session: Nebraska, New Hampshire, New York, and Vermont.29  

City governments have passed bans following robust public dialogue about the risks and benefits of face recognition technology. They represent what is possible with a transparent, democratic process, and the power of proactive localities. In the words of the San Francisco city supervisor who sponsored the ban: “We have an outsize responsibility to regulate the excesses of technology precisely because they are headquartered here.”30 It is unclear at this point, however, whether face recognition bans will take hold at the local, state, or federal level. Some jurisdictions may also find the bans to be unintentionally overbroad, restricting uses of the technology deemed to be necessary or uncontroversial.31  

# B. Moratoria  

Another strong measure that a legislature can take is to place a moratorium on the technology,32 which has two forms: time-bound and directive.  

# 1. Time-bound moratoria  

Time-bound moratoria stop virtually all use of face recognition for a predetermined amount of time.33 The purpose of this pause is to give elected officials and the public time to learn about face recognition, reconvening later once the moratorium expires. At this point, legislators can decide if, and how, to regulate face recognition.  

At the municipal level, in early 2020, Springfield, Massachusetts, placed a moratorium on face recognition until 2025.34 At the state level, a 2020 bill introduced in the Maryland legislature would prohibit all public and private use of face recognition for one year.35 The bill does not include any other provisions or directions, but rather states the moratorium “shall remain effective for a period of one year from the date it is enacted and, at the end of the one–year period, this Act, with no further action required by the General Assembly, shall be abrogated and of no further force and effect.”36  

Time-bound moratoria raise the possibility for public engagement and the future implementation of either a permanent ban or strong regulation. These bills prompt discussion within legislative committees—the members of which are often unfamiliar with face recognition—about the technology, including its potential harms. There is a risk, however, that if the legislature fails to act once the moratorium period is over, use of face recognition will recommence with no safeguards in place.  

# 2. Directive moratoria  

Directive moratoria temporarily stop face recognition use while explicitly instructing the legislature or other government officials to take additional steps. Often this entails the creation of a task force, working group, or commission organized by either the legislature or attorney general to study face recognition and recommend policy responses.37  

A bill introduced in Washington state in 2019 proposed a moratorium on government use of face recognition technology while setting up a task force to study the technology. The task force would be composed of members of historically oversurveilled communities, and would deliver a report to the legislature about potential effects. The bill would also require the attorney general to provide a report certifying the tools in use did not contain accuracy or bias issues, as tested by an independent third party.38  

Directive moratoria can also pause face recognition use until the legislature passes certain laws. In contrast to the above example, in which decisions about future policy are left to the working group, this kind of moratorium sets minimum thresholds that future legislation must achieve.  

For example, a bill introduced in Massachusetts in 2019 would place a moratorium on government use of biometric surveillance, including face recognition, “[a]bsent express statutory authorization.” That authorization must provide guidance on who is able to use biometric surveillance systems, their purposes, and prohibited uses; standards for data use and management; auditing requirements; and rigorous protections for civil rights and liberties, including compliance mechanisms.39  

At the federal level, the Facial Recognition and Biometric Technology Moratorium Act of 2020 prohibits federal use of certain biometric technologies such as face recognition until Congress explicitly allows their use, with certain limitations. It also conditions federal grant funding to state and local agencies on their adoption of moratoria similar to that proposed in the federal bill.40  

These bills encourage jurisdictions to research the full implications of face recognition use and engage with members of the public before enacting a more permanent law. Moratoria also limit the risk of reverting to status quo use once the time period is over. However, there is a risk that a task force or commission may not be representative of affected communities; may lack authority; or may be inadequately funded, restricting its effectiveness.41  

# C. Regulatory Bills  

Regulatory bills seek to place restrictions on face recognition’s use, rather than stop it altogether. Regulatory bills range along a spectrum from more narrowly focused (regulating only specific uses or other elements of face recognition) to broader (regulating more of these elements).  

# 1. Common elements of regulatory bills  

Face recognition bills propose a wide range of measures, including:  

•	 Task force or working group: groups must study face recognition and make policy recommendations.   
•	 Requirements on companies: face recognition vendors must open up their software to accuracy and bias testing; commercial users must get consent or provide notice of use, as well as allow data access, correction, and removal.  

•	 Accountability and transparency reports: implementing agencies must provide details on the face recognition tools they use, including how and how often, to elected officials. Some require reports before implementation, and many require ongoing reports.42  

•	 Implementing officer process regulations: officers must receive periodic trainings, conduct meaningful reviews of face recognition search results, and disclose to criminal defendants that face recognition was used in identifying them.  

•	 Explicit civil rights and liberties protections: such as prohibiting the use of face recognition to surveil people based on characteristics including but not limited to race, immigration status, sexual orientation, religion, or political affiliation.  

Data and access restrictions: such as prohibiting the sharing of face recognition data with immigration enforcement authorities, limiting federal access to face recognition systems, and prohibiting use on state driver’s license databases.  

•	 Targeted bans: prohibiting specific uses, such as live facial recognition, or in conjunction with body-worn cameras or drones. Face recognition use can also be limited by type of crime—for example, only to investigate violent felonies.  

Court order requirements: law enforcement must obtain a court order backed by probable cause (or, in some instances, only reasonable suspicion43) to run face recognition searches. Some bills more narrowly apply this requirement to ongoing surveillance or real-time tracking only.44 This can also apply narrowly to law enforcement seeking face recognition data from private entities that have collected it, rather than law enforcement searches themselves.  

# 2. Examples of regulatory bills  

A narrower bill proposed in Indiana calls for a “surveillance technology impact and use policy,” but includes no other restrictions.45 In New Jersey, a proposed bill requires the attorney general to arrange for third-party accuracy and bias testing.46 In 2019, the California legislature passed a law prohibiting “a law enforcement agency or law enforcement officer from installing, activating, or using any biometric surveillance system in connection with an officer camera or data collected by an officer camera.”47  

At the other end of the spectrum, broader regulatory bills address multiple elements of face recognition development and use. Though they address a wider range of concerns, this does not mean they necessarily address all legitimate areas of concern related to face recognition, or that the proposed rules are substantive or enforceable.  

For example, in March 2020, Washington state passed a law that regulates numerous elements of face recognition.48 The bill includes provisions like these: a pre-implementation accountability report documenting use practices and data management policies for any new face recognition systems; “meaningful human review” when face recognition is used in legal decisions; testing in operational conditions; face recognition service APIs made available for independent accuracy and bias testing; periodic training for officers; mandatory disclosure to criminal defendants; warrants for ongoing, “real-time” or “near-real-time” use; civil rights and liberties protections; and prohibitions against image tampering in face recognition searches.49  

Regulatory bills seek to strike a balance between the benefits and harms of face recognition use. For example, while a separate privacy bill introduced in Washington in 2019 garnered industry support for its light-touch approach to regulating face recognition, it elicited criticism from privacy advocates for containing loopholes and providing inadequate enforcement mechanisms.50 Narrowly targeted bills have a greater likelihood of passing through support from well-resourced law enforcement and company stakeholders, yet often fail to meaningfully protect against the true scope of possible harms.51 Some advocates are also critical of regulatory bills, particularly more limited ones, for using up available political capital and possibly eliminating the chance of stronger regulation in the future.  

# CONCLUSION  

In the past year, the United States has turned a significant corner in its approach to face recognition. There is now widespread agreement that regulation is necessary, even as lawmakers, advocates, law enforcement, and other stakeholders may disagree on exactly what that looks like.52 The status quo—expansive, unregulated, secret face recognition use—is no longer acceptable.  

![](images/aeb73fbb0b613d06648f1555fa5796dc0ffac3f081af82c0193d78abbf33e345.jpg)  

![](images/256eec3a7844310ded64a22e02531381260be2eec4065ebef6b09915d9147cb2.jpg)  

![](images/3865f78e6354e945eaa5688a633912cd792053b81b4e4d68604a2abd66390989.jpg)  

# BIPA: The Most Important Biometric Privacy Law in the US?  

Woodrow Hartzog (Northeastern University)  

n May 2020, Clearview AI abruptly ended all service contracts with all non-law enforcement entities based in Illinois.1 The reason? It hoped to avoid an injunction and potentially large damages under one of the most important privacy laws in America: the Illinois Biometric Information Privacy Act (BIPA).2  

Enacted in 2008 in the wake of the bankruptcy of a high-profile fingerprint-scan system, lawmakers designed BIPA to provide “safeguards and procedures relating to the retention, collection, disclosure, and destruction of biometric data.”3 It was the first state law in the US to specifically regulate biometrics. Remarkably, as the bill was being deliberated by the Illinois legislature, “there were no questions or discussion, and the bill proceeded immediately to a vote and unanimously passed in the House.”4  

BIPA’s substantive rules follow a traditional approach to data protection. Compared to omnibus and complex data-protection laws like GDPR, BIPA’s rules are simple. Private entities must get informed consent before collecting or disseminating a person’s biometric information.5 They are prohibited from selling, leasing, trading, or otherwise profiting from a person’s biometric information.6 Companies must also follow specific retention and destruction guidelines.7 Finally, the statute binds private entities to a standard of care in transmitting, storing, and protecting biometric information that is equal to or more protective than for other confidential and sensitive information.8  

While other states such as Texas and Washington have passed standalone biometrics laws,9 BIPA is the only biometric privacy law in the United States with a private cause of action. Multiple states require notice and consent before parties can collect biometric identifiers, require reasonable security measures for covered information, restrict the disclosure of biometric identifiers to specific circumstances, and limit companies’ retention of biometric identifiers. But only in Illinois can people who have been aggrieved by companies that violated the rules bring their own action against the alleged violation instead of waiting for the government to file a complaint or levy a penalty.  

Given the limited scope of biometric laws, BIPA’s private cause of action might not seem monumental—yet it is revelatory in how is has distinguished itself from other biometrics laws. For example, Texas and Washington both authorize their state attorneys general to enforce their biometric privacy laws in ways similar to how states enforce their general data-privacy rules.10 In contrast, BIPA’s private cause of action has meaningfully shaped the practices of companies who deploy biometrics. It has also forced judges to resolve longstanding issues of injury and standing for privacy violations, among the most vexing issues for all privacy-related claims by plaintiffs in civil courts.  

Plaintiffs alleging privacy-related harms from things like data breaches, abusive surveillance, and unauthorized disclosure have had a notoriously difficult time in court. Some of this is attributable to the general erosion of access to the American court system through tort reform. Plaintiffs struggle to certify classes for mass litigation, and arbitration clauses are embedded in the ubiquitous terms-of-use agreements online. But a huge roadblock for plaintiffs is the slippery nature of privacy harms.11 Courts have long been skeptical of emotional and reputationa damages absent a more obvious physical or financial harm.12 The Federal Trade Commission, the premier privacy regulator in the US, creates waves when it even hints at the idea that something more than physical or financial harm or extreme emotional suffering should be considered in determining whether particular acts are unfair.13 This is to say nothing of the high-stakes debate over whether less specific harms such as anxiety and exposure to risk of data abuses, standing alone, can constitute an actionable injury in the context of claims of negligence which led to a data breach.14  

But most discrete and individual privacy encroachments are not catastrophic. The modern privacy predicament is more akin to death by a thousand cuts. Small intrusions and indiscreet disclosures could lead to compromised autonomy, obscurity, and trust in relationships. What’s more, it can be difficult to specifically articulate and identify the ways in which data breaches make us more vulnerable. Torts require a clear line of causation from fault to harm. That’s usually relatively easy to prove with things like physical injuries from car wrecks, though it is less so with data breaches. Even if it’s clear that a malicious actor has gained access to peoples’ information, criminals don’t always straightforwardly use data obtained from a breach to inflict direct financial or emotional injury upon the data subject. They often aggregate the information in a pool for further exploitation or sit on it for years so as not to arouse suspicion. Often people have no idea who wronged them online. American data-privacy law simply isn’t built to respond to this kind of diffuse and incremental harm.15  

BIPA has spurred a key intervention into this morass. Specifically, with BIPA, several judicial opinions have affirmed the argument that regardless of whether wrongful acts with biometric information resulted in chilling effects or financial or emotional injury, the collection and processing of biometric data without notice and consent is alone a cognizable injury because it is an affront to a person’s dignity and autonomy. Two cases in particular demonstrate the importance of BIPA.  

In Rosenbach v. Six Flags Entm’t Corp., a mother brought a claim on behalf of her son against Six Flags amusement park for the company’s failure to give notice or obtain consent when collecting the child’s fingerprints for their biometric identification system.16 At issue was whether the plaintiffs alleged sufficient actual or threatened injury to have standing to bring suit. Plaintiffs did not allege financial or extreme emotional harm, but rather a harm resulting solely from the prohibited collection and processing of personal biometric data without making the required  

disclosures or obtaining written consent. The Appellate Court of Illinois held that “a plaintiff is not ‘aggrieved’ within the meaning of the Act and may not pursue either damages or injunctive relief under the Act based solely on a defendant’s violation of the statute. Additional injury or adverse effect must be alleged.”17 However, the Supreme Court of Illinois disagreed.  

Chief Justice Lloyd A. Karmeier, writing the opinion of the court, noted that if the Illinois legislature had wanted to impose an injury requirement beyond disclosure and consent failures, they likely would have done so, as they have in other legislation.18 Using accepted principles of statutory construction, the court interpreted BIPA’s language that “[a]ny person aggrieved by a violation of this Act shall have a right of action” according to its commonly understood legal meaning. Specifically, they found that “to be aggrieved simply ‘means having a substantial grievance; a denial of some personal or property right.’”19 Justice Karmeier wrote, $\mathrm{^{\prime\prime}A}$ person who suffers actual damages as the result of the violation of his or her rights would meet this definition of course, but sustaining such damages is not necessary to qualify as ‘aggrieved.’”20  

The court in Rosenbach found that Six Flags violated BIPA’s “right to privacy in and control over their biometric identifiers and biometric information.”21 BIPA’s disclosure and consent requirements give shape to that right. Thus, if a company violates BIPA, then the data subject is legally “aggrieved” because their right to privacy in and control over their biometric data has been compromised.22  

Perhaps the most significant passage in Rosenbach concerned the court’s response to the defendant’s argument that its BIPA violations were merely “technical” in nature. The court argued that such a characterization misunderstands not only what the legislature was trying to accomplish but also the unique nature of how biometrics threaten peoples’ privacy and how procedural rules mitigate that threat. “The Act vests in individuals and customers the right to control their biometric information by requiring notice before collection and giving them the power to say no by withholding consent.”23 Peoples’ unique biometric identifiers, now easily wholesale collected and stored, are not like other kinds of authenticators like passwords and social security numbers because if they are compromised, they cannot be changed. Even beyond identity theft, the court noted that biometrics are particularly concerning because their full risks are not known. The court was direct in its finding:  

When a private entity fails to adhere to the statutory procedures, as defendants are alleged to have done here, “the right of the individual to maintain [his or] her biometric privacy vanishes into thin air. The precise harm the Illinois legislature sought to prevent is then realized.”...This is no mere “technicality.” The injury is real and significant.24  

The court also highlighted how integral a private cause of action was in implementing the legislature’s privacy goals for BIPA. When companies face liability for legal violations without burdening plaintiffs to show some additional injury, “those entities have the strongest possible incentive to conform to the law and prevent problems before they occur and cannot be undone.”25 The court noted that the cost of complying with BIPA is “likely to be insignificant compared to the substantial and irreversible harm that could result if biometric identifiers and information are not properly safeguarded; and the public welfare, security, and safety will be advanced.”26 According to the court, to force plaintiffs to wait until they could prove some sort of financial or emotional harm would counteract BIPA’s prevention and deterrence goals.  

The other case illustrative of BIPA’s potency, Patel v. Facebook,27 involves federal standing doctrine as required by Article III of the US Constitution, a concept linked to injury and harm thresholds. Standing doctrine requires that plaintiffs “must have suffered an ‘injury in fact’—an invasion of a legally protected interest which is (a) concrete and particularized; and (b) actual or imminent, not conjectural or hypothetical.”28 In a landmark 2016 US Supreme Court case, Spokeo, Inc. v. Robins affirmed that an injury-in-fact for information-related complaints like those against data brokers for mishandling, inaccuracies, and indiscretion must be “concrete,” though the court was frustratingly vague about what kinds of harms met that threshold.29  

Patel v. Facebook involved a complaint that Facebook violated BIPA with its use of facial recognition tools. The Ninth Circuit applied a two-part test to determine “(1) whether the statutory provisions at issue were established to protect [the plaintiff’s] concrete interests (as opposed to purely procedural rights), and if so, (2) whether the specific procedural violations alleged in this case actually harm, or present a material risk of harm to, such interests.”30 The Ninth Circuit answered yes to both questions.  

In determining that BIPA protected a concrete interest rather than a purely procedural protection, the Ninth Circuit noted that privacy rights have long served as the basis for legal action in the common law, constitutional law, and in statues at both the state and federal level. The court noted the significant vulnerabilities created by facial recognition technology:  

[T]he facial-recognition technology at issue here can obtain information that is “detailed, encyclopedic, and effortlessly compiled,” which would be almost impossible without such technology...Once a face template of an individual is created, Facebook can use it to identify that individual in any of the other hundreds of millions of photos uploaded to Facebook each day, as well as determine when the individual was present at a specific location. Facebook can also identify the individual’s Facebook friends or acquaintances who are present in the photo...[It] seems likely that a face-mapped individual could be identified from a surveillance photo taken on the streets or in an office building. Or a biometric face template could be used to unlock the face recognition lock on that individual’s cell phone.31  

The court concluded that “the development of a face template using facial-recognition technology without consent (as alleged here) invades an individual’s private affairs and concrete interests. Similar conduct is actionable at common law.”32 The court cited the language in Rosenbach in holding that “‘the statutory provisions at issue’ in BIPA were established to protect an individual’s ‘concrete interests’ in privacy, not merely procedural rights,” and that by alleging a BIPA violation the “the plaintiffs have alleged a concrete injury-in-fact sufficient to confer Article III standing.33  

BIPA has a number of virtues. Thanks to BIPA’s private cause of action, it has become the key for holding companies that use biometric systems accountable.34 In the absence of a private cause of action, enforcement of biometrics and consumer protection laws is generally left to state attorneys general (AG). While state AGs are certainly key to privacy policymaking in the US, they have limited resources and a host of issues on their plate.35 Even with unlimited bandwidth, state AGs have limited legal ability and political capital to extract the kind of fines necessary to sufficiently deter companies. The same holds true for the Federal Trade Commission, which is America’s primary privacy regulator.36  

So far, only private causes of action seem capable of meaningfully deterring companies from engaging in practices with biometrics based on business models that inevitably lead to unacceptable abuses. Regulators are more predictable than plaintiffs and are vulnerable to political pressure. Facebook’s share price actually rose 2 percent after the FTC announced its historic $\$5$ billion fine for the social media company’s privacy lapses in the Cambridge Analytica debacle.37 Meanwhile, Clearview AI specifically cited BIPA as the reason it is no longer pursuing non-government contracts.38 On top of that, Clearview AI is being sued by the ACLU for violating  

BIPA by creating faceprints of people without their consent.39 It is no wonder that the private cause of action is one of two reasons the United States does not have an omnibus federal data privacy law (the other being federal preemption of state privacy frameworks).40 In general, businesses have opposed private causes of action more than other proposed privacy rules, short of an outright ban.41  

Even given BIPA’s virtues and remarkable effectiveness, it is probably not the best model for America’s biometric privacy identity. A private cause of action is necessary, but not sufficient, to respond to the risk of biometrics. BIPA is rooted in a myopic and atomistic “notice and choice” approach to privacy.  

There are two major problems with building a biometric privacy framework almost exclusively around concepts of transparency and informational self-determination. First, by focusing on giving people control over their data and mandating procedural disclosure obligations, these frameworks fail to impose substantive limits on how far companies can encroach into our lives and how deeply these systems can be entrenched. Procedural transparency and consent regimes end up serving as a justification mechanism for all kinds of encroachments without any clear backstop to how vulnerable we can be made to these systems, so long as we consent. Furthermore, BIPA fails to address the issues around privacy in public spaces or in data that already has been exposed to the public. For example, judges considering privacy claims have said repeatedly that “there can be no privacy in that which is already public.”42  

Privacy is about more than just informational self-determination. It is about trust, dignity, freedom from oppression, and laying the preconditions for human flourishing. But those values are not necessarily reflected in the net outcome of billions of individual decisions. Moreover, companies create structured environments that can heavily influence these discrete choices, with powerful incentives to get us to say “yes” any way they can.43  

BIPA is simply not capable of providing individuals with meaningful agency over modern data practices.44 “Informed consent” is a broken privacy regulatory mechanism.45 It doesn’t scale, it offloads risk onto the person giving the consent, and it is easily manufactured by companies who control what we see and what we can click. Companies deploy malicious user interfaces and a blizzard of dense fine print to overwhelm our decision-making process. Consent regimes give the illusion of control while justifying dubious practices that people don’t have enough time or cognitive resources to understand. Even if people were able to adequately gauge the risks and benefits of consenting to biometric practices, they often don’t have a meaningful choice in front of them since they cannot afford to say no and decline a transaction or relationship. While people should be protected regardless of what they consent to, BIPA is largely agnostic to the postpermission risks of biometric technologies.  

BIPA is far more effective than any other law on the books in protecting our biometric privacy with respect to private companies. However, it does not confront the structural change and substantive limits necessary for a sustainable future with biometric technologies. BIPA allows companies to exploit people as their consent is harvested through systems designed to have them hurriedly click “I Agree” and get on with their busy lives. BIPA’s success entrenches an overly individualistic and procedural approach to privacy, but has shown lawmakers what is indispensable in a biometric privacy framework. It is a guide not just because of what it provides but also because of what it lacks.  

# Bottom-Up Biometric Regulation: A Community’s Response to Using Face Surveillance in Schools  

Stefanie Coyle (NYCLU) Rashida Richardson (Rutgers University, AI Now Institute, NYU)  

P ublic schools are increasingly turning to invasive technological solutions to address a wide range of school safety issues. Because events like school shootings are both nuanced and politically or socially charged, school administrators often rush to embrace technological tools without proper consideration or community consultation. The risks, concerns, and bureaucratic pitfalls of this approach are most salient in the context of biometric technologies used in schools. This case study examines the controversial move by a school district in Lockport, New York, to implement a facial and object-recognition system, and the community-driven response that sparked a national debate and led to state-wide legislation regulating the use of biometric technologies in schools.  

# FACIAL RECOGNITION SURVEILLANCE IN SCHOOLS  

Surveillance technologies are becoming a norm in many public schools.1 School administrators are turning to a rapidly growing market of “free”2 or subsidized tools that monitor student emails for concerning phrases, measure student bathroom breaks, proctor exams, or provide real-time alerts of potential crises,3 often without proper consideration or community consultation. School administrators have shown significant interest in biometric and other access-control technologies for targeting nuanced school safety issues, with few existing regulations to hold them back.4 In 2019, Wired “identified eight public school systems, from rural areas to giant urban districts, that have moved to install facial recognition systems,” though national use statistics remain unknown.5  

Because these technologies can be enabled as “add-on” features or easily integrated with existing systems used by a school or school district (e.g., closed-circuit television), administrators often adopt or test them without fully considering the risks they entail.6 For example, school administrators may face legal obligations regarding the storage and use of biometric data, and may not have policies in place to deal with a data breach or sufficient funding available for maintenance of these systems.  

Biometric technologies present a veneer of social control or risk mitigation,7 but in reality they pose unique social and legal concerns for students, particularly in the K–12 setting. Though students have some enhanced data-privacy protections and greater expectations regarding government oversight and enforcement,8 they are particularly vulnerable because the consequences of privacy and other legal violations may not be immediately felt or obvious. Moreover, for decades, critical scholars and educators have criticized these types of reactionary educational policies and practices because they are not long-term solutions. Indeed, they tend to reproduce, maintain, and naturalize structural inequities that pervade the American education system and allow policymakers to avoid necessary structural reforms.9  

Internationally, some national authorities have opposed facial recognition and other biometric technologies in schools, finding some uses in schools to be unlawful although not banning the use of the technology in other settings.10 At the same time, several states and localities have passed or are considering laws that will ban government use of facial recognition technologies, which applies to public schools.11 US civil society organizations Fight for the Future and Students for Sensible Drug Policy created a campaign to ban use of facial recognition technology on college campuses.12 This campaign successfully forced the University of California Los Angeles (UCLA) to reverse its plans to implement facial recognition for campus security,13 and has garnered support from teachers’ unions that are expanding the campaign’s call to extend to K–12 schools.14  

In 2019, New York became the first state to introduce legislation that explicitly sought to bar school districts from purchasing biometric surveillance technologies, and directed the State Education Commissioner to conduct a study on the use of such technologies in schools and issue statewide recommendations.15 This legislation was in response to and in collaboration with a community-led advocacy effort in Lockport, New York.  

# LOCKPORT, NEW YORK: A CASE STUDY IN COMMUNITY-DRIVEN PUSHBACK TO FACIAL RECOGNITION IN SCHOOLS  

In 2014, New York voters approved the Smart Schools Bond Act (SSBA), which set aside $\$2$ billion for school districts to “improve learning and opportunity for students throughout” New York State.16 An inconspicuous provision within the SSBA allowed school districts to utilize the funds on “high-tech security” projects, with little guidance. The SSBA is a reimbursement scheme that requires school districts to submit proposals and records of community engagement to the Smart Schools Review Board for review and approval.17  

Since 2014, many school districts have applied for and obtained reimbursement for funding to acquire student instructional technology, such as laptops, smart boards, and 3D printers, and to upgrade aging internet and Wi-Fi systems.18 As part of the application process, districts must certify that they have engaged stakeholders on the projects— specifically requiring that parents, students, teachers, and the community be notified of the project. Districts are also required to hold a public hearing about the proposals and post the proposal documentation on the district’s website for at least thirty days.19 Ostensibly, these requirements are designed to ensure that school community members are able to give input about the wisdom of the district’s proposed use of state funding.  

In 2016, the Lockport City School District proposed the use of \$3,810,833 in SSBA funds for “new cameras and wiring...to provide viewing and automated facial and object recognition of live and recorded surveillance video,” as well as “additional surveillance servers...to provide enhanced storage of recorded video and processing.”20 Lockport allegedly purchased the system to prevent school shootings.21 It held its required public hearing on the proposal in the middle of summer break; unsurprisingly, it did not receive any comments or questions from the community about the purchase.22 Lockport certified that it had engaged with all required stakeholders and its proposal was approved by the Smart Schools Review Board in November 2017.23  

The first public criticism of the project started in February 2018 when the local newspaper, the Lockport Union-Sun & Journal, published a piece on one of two resolutions approved at the February 2018 Lockport school board meeting.24 The resolution was to allow the use of “a new facial and shape recognition software” in the school system.25 Lockport resident and parent Jim Shultz was alarmed by the revelation and wrote an article in his opinion column for the newspaper questioning the need for such a system, underscoring other, better uses for the funds, and  

warning of the risks to privacy for students and teachers.26 Shultz created a petition asking the school district to put the project on hold and to schedule a public hearing to receive input from the community.27 The petition, signed by over a hundred Lockport residents, raised additional questions about the district’s engagement with stakeholders, potential conflicts of interest between the district and the security consultant that pitched the product, and the effectiveness of the system.28  

After the petition was turned in, the Lockport Journal editorial board called on the district to postpone its scheduled vote to award an installation contract for the system.29 Despite this call to action, the Lockport school board approved the contract.30 Shultz then called for residents to vote down Lockport’s proposed school budget until the district agreed to stop its facial recognition proposal, but was unsuccessful.31 The Lockport Journal, however, continued to run pieces on the dangers of facial recognition technology, questioning its accuracy and, in particular, discrepancies in the systems’ ability to identify people of color.32 Shultz wrote monthly columns about the project, and enlisted local support through Lockport’s Facebook group. He also solicited the help of the New York Civil Liberties Union, which targeted the district and the New York State Education Department (NYSED) with letters and requests under New York’s freedom of information law.33  

This advocacy garnered the attention of Monica Wallace, Democrat Assembly member representing New York’s 143rd Assembly District, which borders Lockport and includes the town of Depew.34 Wallace was aware of the school district’s proposal because the superintendent of the Depew Union Free School District had expressed interest in obtaining the same system.35 Wallace reached out to advocates in an effort to understand the concerns. As a lawyer and parent, she understood the tension between safety and privacy, but worried that the system had the potential to do more harm than good.  

In March 2019, Wallace introduced a bill (A6787) in the New York State Assembly that would place a moratorium on the use or purchase of any “biometric identifying technology” in a school system.36 This broad definition covers not only facial recognition technology, but any technology that uses a fingerprint, handprint, iris, retina, DNA sequence, voice, gait, or facial geometry to identify a person.37 Wallace consulted with advocates on the bill draft to make sure it addressed concerns about the system. The bill requires NYSED to commission a study on the following issues: the privacy implications of collecting sensitive biometric information; the risks of false identification for certain subgroups of individuals; whether information from the system might be shared with outside individuals, including law enforcement; the length of time information from the system can be retained; the risk of an unauthorized breach; maintenance costs; audits of the vendors; and how the technology should be disclosed to the public.38 These questions are critical for analyzing the utility, efficacy, and harms of such systems, as is involving the public in decisions relating to the use of surveillance technology in schools.  

The bill requires NYSED to consult with many New York State agencies in preparing the report and requires the Commissioner of Education to hold public hearings seeking feedback from teachers, school administrators, parents, and experts in school safety, data privacy, and civil rights and civil liberties.39 In many ways, Wallace’s bill mirrors the concerns raised by residents in the community and advocates across the state and country. A senate version of the bill was introduced in April 2019.40 The bill passed the New York Assembly with a bipartisan vote of 128 to 19 on the final day of the 2019 legislative session.41 The bill was not considered in the Senate, effectively killing the bill for the 2019 legislative session and teeing up a new fight in 2020.  

Meanwhile, the community continued its efforts to prevent the use of the technology. Connor Hoffman, a reporter from the Lockport Journal, attended every school board meeting and filed multiple requests for information from the school district and NYSED. Hoffman received information that had not yet been publicly disclosed about the accuracy rates of Lockport’s system, revealing that Black women are sixteen times as likely as white men to be misidentified by the system.42 The persistent reporting led to national press coverage, including a feature in the New York Times,43 a New York Times op-ed by Shultz,44 and an MTV News documentary.45 Without the diligence of concerned citizens and the local and national press, Lockport’s acquisition of the facial recognition system and NYSED’s failure to regulate this type of technology might have gone unnoticed.  

Despite the pushback, Lockport activated its facial recognition system on January 2, 2020.46 Parents and students were not notified ahead of the deployment, nor were they given a chance to publicly comment on the system.47 It remains unclear why the district pushed ahead with the system given the concerns of the community.  

Despite this setback, there have been promising developments in the community’s fight against this technology. Though the 2020 legislative session was interrupted by the COVID-19 global pandemic, the bill was amended in both houses to increase the amount of time for the moratorium until July 2022 or until the Commissioner of Education explicitly authorizes the use of the technology after issuance of the report, whichever occurs later.48 The bill has widespread support from across the state and across the country, even garnering support from the United Federation of Teachers (UFT), the New York City affiliate of the American Federation of Teachers.49 During the week of July 20, 2020, the bill passed both the Assembly and the Senate, and now awaits signature by the Governor to become law.50  

In February 2020, the New York Civil Liberties Union led a town hall in Lockport attended by nearly fifty parents and concerned community members about the system. The town hall was headlined by Shultz and a recent alumna of the school district.51 For many, it was the first time they had all been in a room together to discuss the system. Several people asked the school board members in attendance why there had not been a community forum sponsored by the district to answer questions and hear concerns. Community members expressed consternation over Lockport’s lack of responsiveness, but planned to continue vocalizing their opposition and making their voices heard at school board meetings.52  

# THE IMPORTANCE OF COMMUNITY-DRIVEN POLICY ADVOCACY  

The community-driven advocacy response in Lockport demonstrates that persistent and organized public scrutiny can illuminate bureaucratic failures, shape necessary reforms, and shift narratives. The district’s decision to purchase and use a facial recognition system follows a common yet flawed pattern that government officials rely on to justify the adoption of surveillance technologies. The school district conflated an abstract or speculative risk to student safety with an objective fact of real harm. They installed an unproven and potentially ineffective system that will likely undermine students’ civil rights and liberties. Though school safety concerns are legitimate and warrant critical review, the school district’s actions demonstrated the inherently political nature of privileging certain risks and interests over community needs.53 Rather than consult the community to assess actual needs and concerns, the district adopted a technological solution in search of a problem.  

The community-driven advocacy made the flawed logic of this approach apparent. Parents shifted the discourse from debating whether the biometric surveillance system was necessary to focusing on the real harms posed to students if the school district decided to move forward against community opposition. In particular, Shultz’s early writings on the facial recognition system emphasized the dangers to student and teacher privacy at a time when the district trivialized the idea that the system could negatively impact student privacy.54 In 2019, however, the superintendent of the school district reluctantly acknowledged that “[p]rivacy matters are a big deal nowadays.”55 This emphasis on privacy is echoed in the current legislation.56  

Lockport also failed to acknowledge that deployment of a flawed facial recognition system could compound preexisting racial-equity concerns regarding its school safety practices and policies. For instance, the district has struggled to address existing issues of disproportionate discipline when it comes to students of color, a problem that can be exacerbated by the use of an inaccurate and racially biased facial recognition system.57  

During Lockport’s school board elections this year, a new slate of parents, energized by the fight against facial recognition technology, organized to run for multiple open seats on the board.58 This year’s voter turnout was four times higher than the district’s five-year average turnout.59 Though the fight in Lockport and New York State continues, this community-driven advocacy effort demonstrates the importance of empowering those directly affected by problematic government decision-making to lead the change they want to see.  

# MINOW  